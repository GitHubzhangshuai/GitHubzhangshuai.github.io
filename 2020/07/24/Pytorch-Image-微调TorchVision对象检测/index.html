<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="pytorch,深度学习,pytorch1.5.1官网教程,Pytorch1.5.1官网教程-Image," />





  <link rel="alternate" href="/atom.xml" title="张帅的Blog" type="application/atom+xml" />






<meta name="description" content="Pytorch-Image-微调TorchVision对象检测:">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch-Image-微调TorchVision对象检测">
<meta property="og:url" content="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/index.html">
<meta property="og:site_name" content="张帅的Blog">
<meta property="og:description" content="Pytorch-Image-微调TorchVision对象检测:">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_6_0.png">
<meta property="og:image" content="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_7_0.png">
<meta property="og:image" content="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image03.png">
<meta property="og:image" content="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image04.png">
<meta property="og:image" content="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_31_0.png">
<meta property="og:image" content="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_33_0.png">
<meta property="article:published_time" content="2020-07-24T06:46:24.000Z">
<meta property="article:modified_time" content="2020-07-24T08:44:59.684Z">
<meta property="article:author" content="Zhangshuai">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch1.5.1官网教程">
<meta property="article:tag" content="Pytorch1.5.1官网教程-Image">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_6_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/07/24/Pytorch-Image-微调TorchVision对象检测/"/>





  <title>Pytorch-Image-微调TorchVision对象检测 | 张帅的Blog</title>
  








<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">张帅的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">用hexo搭建的简易博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            Commonweal 404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhangshuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张帅的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Pytorch-Image-微调TorchVision对象检测</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-07-24T14:46:24+08:00">
                2020-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pytorch/" itemprop="url" rel="index">
                    <span itemprop="name">pytorch</span>
                  </a>
                </span>

                
                
              
            </span>
          


          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Pytorch-Image-微调TorchVision对象检测:<br><a id="more"></a></p>
<h1 id="TorchVision-0-3-Object-Detection-finetuning-tutorial"><a href="#TorchVision-0-3-Object-Detection-finetuning-tutorial" class="headerlink" title="TorchVision 0.3 Object Detection finetuning tutorial"></a>TorchVision 0.3 Object Detection finetuning tutorial</h1><p>For this tutorial, we will be finetuning a pre-trained <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a> model in the <a href="https://www.cis.upenn.edu/jshi/ped_html/" target="_blank" rel="noopener"><em>Penn-Fudan Database for Pedestrian Detection and Segmentation</em></a>. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.</p>
<p>First, we need to install <code>pycocotools</code>. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line">pip install cython</span><br><span class="line"><span class="comment"># Install pycocotools, the version by default in Colab</span></span><br><span class="line"><span class="comment"># has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354</span></span><br><span class="line">pip install -U <span class="string">'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</span></span><br></pre></td></tr></table></figure>
<pre><code>Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)
Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI
  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-h3isg2r5
  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-h3isg2r5
Requirement already satisfied, skipping upgrade: setuptools&gt;=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.1.0)
Requirement already satisfied, skipping upgrade: cython&gt;=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)
Requirement already satisfied, skipping upgrade: matplotlib&gt;=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)
Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.4.7)
Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.8.1)
Requirement already satisfied, skipping upgrade: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.18.5)
Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (0.10.0)
Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.2.0)
Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.15.0)
Building wheels for collected packages: pycocotools
  Building wheel for pycocotools (setup.py) ... [?25l[?25hdone
  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266460 sha256=3292fbae19c3df30ceb54183f71e1e7288d447743b1dcb8f88257833cf2f23e1
  Stored in directory: /tmp/pip-ephem-wheel-cache-y2jf5d3p/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a
Successfully built pycocotools
Installing collected packages: pycocotools
  Found existing installation: pycocotools 2.0.1
    Uninstalling pycocotools-2.0.1:
      Successfully uninstalled pycocotools-2.0.1
Successfully installed pycocotools-2.0
</code></pre><h2 id="Defining-the-Dataset"><a href="#Defining-the-Dataset" class="headerlink" title="Defining the Dataset"></a>Defining the Dataset</h2><p>The <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision reference scripts for training object detection, instance segmentation and person keypoint detection</a> allows for easily supporting adding new custom datasets.<br>The dataset should inherit from the standard <code>torch.utils.data.Dataset</code> class, and implement <code>__len__</code> and <code>__getitem__</code>.</p>
<p>The only specificity that we require is that the dataset <code>__getitem__</code> should return:</p>
<ul>
<li>image: a PIL Image of size (H, W)</li>
<li>target: a dict containing the following fields<ul>
<li><code>boxes</code> (<code>FloatTensor[N, 4]</code>): the coordinates of the <code>N</code> bounding boxes in <code>[x0, y0, x1, y1]</code> format, ranging from <code>0</code> to <code>W</code> and <code>0</code> to <code>H</code></li>
<li><code>labels</code> (<code>Int64Tensor[N]</code>): the label for each bounding box</li>
<li><code>image_id</code> (<code>Int64Tensor[1]</code>): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation</li>
<li><code>area</code> (<code>Tensor[N]</code>): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.</li>
<li><code>iscrowd</code> (<code>UInt8Tensor[N]</code>): instances with <code>iscrowd=True</code> will be ignored during evaluation.</li>
<li>(optionally) <code>masks</code> (<code>UInt8Tensor[N, H, W]</code>): The segmentation masks for each one of the objects</li>
<li>(optionally) <code>keypoints</code> (<code>FloatTensor[N, K, 3]</code>): For each one of the <code>N</code> objects, it contains the <code>K</code> keypoints in <code>[x, y, visibility]</code> format, defining the object. <code>visibility=0</code> means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt <code>references/detection/transforms.py</code> for your new keypoint representation</li>
</ul>
</li>
</ul>
<p>If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.</p>
<p>Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a <code>get_height_and_width</code> method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via <code>__getitem__</code> , which loads the image in memory and is slower than if a custom method is provided.</p>
<h3 id="Writing-a-custom-dataset-for-Penn-Fudan"><a href="#Writing-a-custom-dataset-for-Penn-Fudan" class="headerlink" title="Writing a custom dataset for Penn-Fudan"></a>Writing a custom dataset for Penn-Fudan</h3><p>Let’s write a dataset for the Penn-Fudan dataset.</p>
<p>First, let’s download and extract the data, present in a zip file at <a href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip" target="_blank" rel="noopener">https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the Penn-Fudan dataset</span></span><br><span class="line">wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .</span><br><span class="line"><span class="comment"># extract it in the current folder</span></span><br><span class="line">unzip PennFudanPed.zip</span><br></pre></td></tr></table></figure>
<pre><code>--2020-07-23 13:46:08--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip
Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d
Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 53723336 (51M) [application/zip]
Saving to: ‘PennFudanPed.zip’

PennFudanPed.zip    100%[===================&gt;]  51.23M  1009KB/s    in 48s     

2020-07-23 13:46:58 (1.06 MB/s) - ‘PennFudanPed.zip’ saved [53723336/53723336]

--2020-07-23 13:46:58--  http://./
Resolving . (.)... failed: No address associated with hostname.
wget: unable to resolve host address ‘.’
FINISHED --2020-07-23 13:46:58--
Total wall clock time: 50s
Downloaded: 1 files, 51M in 48s (1.06 MB/s)
Archive:  PennFudanPed.zip
   creating: PennFudanPed/
  inflating: PennFudanPed/added-object-list.txt  
   creating: PennFudanPed/Annotation/
  inflating: PennFudanPed/Annotation/FudanPed00001.txt  
  inflating: PennFudanPed/Annotation/FudanPed00002.txt  
  inflating: PennFudanPed/Annotation/FudanPed00003.txt  
  inflating: PennFudanPed/Annotation/FudanPed00004.txt  
  inflating: PennFudanPed/Annotation/FudanPed00005.txt  
  inflating: PennFudanPed/Annotation/FudanPed00006.txt  
  inflating: PennFudanPed/Annotation/FudanPed00007.txt  
  inflating: PennFudanPed/Annotation/FudanPed00008.txt  
  inflating: PennFudanPed/Annotation/FudanPed00009.txt  
  inflating: PennFudanPed/Annotation/FudanPed00010.txt  
  inflating: PennFudanPed/Annotation/FudanPed00011.txt  
  inflating: PennFudanPed/Annotation/FudanPed00012.txt  
  inflating: PennFudanPed/Annotation/FudanPed00013.txt  
  inflating: PennFudanPed/Annotation/FudanPed00014.txt  
  inflating: PennFudanPed/Annotation/FudanPed00015.txt  
  inflating: PennFudanPed/Annotation/FudanPed00016.txt  
  inflating: PennFudanPed/Annotation/FudanPed00017.txt  
  inflating: PennFudanPed/Annotation/FudanPed00018.txt  
  inflating: PennFudanPed/Annotation/FudanPed00019.txt  
  inflating: PennFudanPed/Annotation/FudanPed00020.txt  
  inflating: PennFudanPed/Annotation/FudanPed00021.txt  
  inflating: PennFudanPed/Annotation/FudanPed00022.txt  
  inflating: PennFudanPed/Annotation/FudanPed00023.txt  
  inflating: PennFudanPed/Annotation/FudanPed00024.txt  
  inflating: PennFudanPed/Annotation/FudanPed00025.txt  
  inflating: PennFudanPed/Annotation/FudanPed00026.txt  
  inflating: PennFudanPed/Annotation/FudanPed00027.txt  
  inflating: PennFudanPed/Annotation/FudanPed00028.txt  
  inflating: PennFudanPed/Annotation/FudanPed00029.txt  
  inflating: PennFudanPed/Annotation/FudanPed00030.txt  
  inflating: PennFudanPed/Annotation/FudanPed00031.txt  
  inflating: PennFudanPed/Annotation/FudanPed00032.txt  
  inflating: PennFudanPed/Annotation/FudanPed00033.txt  
  inflating: PennFudanPed/Annotation/FudanPed00034.txt  
  inflating: PennFudanPed/Annotation/FudanPed00035.txt  
  inflating: PennFudanPed/Annotation/FudanPed00036.txt  
  inflating: PennFudanPed/Annotation/FudanPed00037.txt  
  inflating: PennFudanPed/Annotation/FudanPed00038.txt  
  inflating: PennFudanPed/Annotation/FudanPed00039.txt  
  inflating: PennFudanPed/Annotation/FudanPed00040.txt  
  inflating: PennFudanPed/Annotation/FudanPed00041.txt  
  inflating: PennFudanPed/Annotation/FudanPed00042.txt  
  inflating: PennFudanPed/Annotation/FudanPed00043.txt  
  inflating: PennFudanPed/Annotation/FudanPed00044.txt  
  inflating: PennFudanPed/Annotation/FudanPed00045.txt  
  inflating: PennFudanPed/Annotation/FudanPed00046.txt  
  inflating: PennFudanPed/Annotation/FudanPed00047.txt  
  inflating: PennFudanPed/Annotation/FudanPed00048.txt  
  inflating: PennFudanPed/Annotation/FudanPed00049.txt  
  inflating: PennFudanPed/Annotation/FudanPed00050.txt  
  inflating: PennFudanPed/Annotation/FudanPed00051.txt  
  inflating: PennFudanPed/Annotation/FudanPed00052.txt  
  inflating: PennFudanPed/Annotation/FudanPed00053.txt  
  inflating: PennFudanPed/Annotation/FudanPed00054.txt  
  inflating: PennFudanPed/Annotation/FudanPed00055.txt  
  inflating: PennFudanPed/Annotation/FudanPed00056.txt  
  inflating: PennFudanPed/Annotation/FudanPed00057.txt  
  inflating: PennFudanPed/Annotation/FudanPed00058.txt  
  inflating: PennFudanPed/Annotation/FudanPed00059.txt  
  inflating: PennFudanPed/Annotation/FudanPed00060.txt  
  inflating: PennFudanPed/Annotation/FudanPed00061.txt  
  inflating: PennFudanPed/Annotation/FudanPed00062.txt  
  inflating: PennFudanPed/Annotation/FudanPed00063.txt  
  inflating: PennFudanPed/Annotation/FudanPed00064.txt  
  inflating: PennFudanPed/Annotation/FudanPed00065.txt  
  inflating: PennFudanPed/Annotation/FudanPed00066.txt  
  inflating: PennFudanPed/Annotation/FudanPed00067.txt  
  inflating: PennFudanPed/Annotation/FudanPed00068.txt  
  inflating: PennFudanPed/Annotation/FudanPed00069.txt  
  inflating: PennFudanPed/Annotation/FudanPed00070.txt  
  inflating: PennFudanPed/Annotation/FudanPed00071.txt  
  inflating: PennFudanPed/Annotation/FudanPed00072.txt  
  inflating: PennFudanPed/Annotation/FudanPed00073.txt  
  inflating: PennFudanPed/Annotation/FudanPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00001.txt  
  inflating: PennFudanPed/Annotation/PennPed00002.txt  
  inflating: PennFudanPed/Annotation/PennPed00003.txt  
  inflating: PennFudanPed/Annotation/PennPed00004.txt  
  inflating: PennFudanPed/Annotation/PennPed00005.txt  
  inflating: PennFudanPed/Annotation/PennPed00006.txt  
  inflating: PennFudanPed/Annotation/PennPed00007.txt  
  inflating: PennFudanPed/Annotation/PennPed00008.txt  
  inflating: PennFudanPed/Annotation/PennPed00009.txt  
  inflating: PennFudanPed/Annotation/PennPed00010.txt  
  inflating: PennFudanPed/Annotation/PennPed00011.txt  
  inflating: PennFudanPed/Annotation/PennPed00012.txt  
  inflating: PennFudanPed/Annotation/PennPed00013.txt  
  inflating: PennFudanPed/Annotation/PennPed00014.txt  
  inflating: PennFudanPed/Annotation/PennPed00015.txt  
  inflating: PennFudanPed/Annotation/PennPed00016.txt  
  inflating: PennFudanPed/Annotation/PennPed00017.txt  
  inflating: PennFudanPed/Annotation/PennPed00018.txt  
  inflating: PennFudanPed/Annotation/PennPed00019.txt  
  inflating: PennFudanPed/Annotation/PennPed00020.txt  
  inflating: PennFudanPed/Annotation/PennPed00021.txt  
  inflating: PennFudanPed/Annotation/PennPed00022.txt  
  inflating: PennFudanPed/Annotation/PennPed00023.txt  
  inflating: PennFudanPed/Annotation/PennPed00024.txt  
  inflating: PennFudanPed/Annotation/PennPed00025.txt  
  inflating: PennFudanPed/Annotation/PennPed00026.txt  
  inflating: PennFudanPed/Annotation/PennPed00027.txt  
  inflating: PennFudanPed/Annotation/PennPed00028.txt  
  inflating: PennFudanPed/Annotation/PennPed00029.txt  
  inflating: PennFudanPed/Annotation/PennPed00030.txt  
  inflating: PennFudanPed/Annotation/PennPed00031.txt  
  inflating: PennFudanPed/Annotation/PennPed00032.txt  
  inflating: PennFudanPed/Annotation/PennPed00033.txt  
  inflating: PennFudanPed/Annotation/PennPed00034.txt  
  inflating: PennFudanPed/Annotation/PennPed00035.txt  
  inflating: PennFudanPed/Annotation/PennPed00036.txt  
  inflating: PennFudanPed/Annotation/PennPed00037.txt  
  inflating: PennFudanPed/Annotation/PennPed00038.txt  
  inflating: PennFudanPed/Annotation/PennPed00039.txt  
  inflating: PennFudanPed/Annotation/PennPed00040.txt  
  inflating: PennFudanPed/Annotation/PennPed00041.txt  
  inflating: PennFudanPed/Annotation/PennPed00042.txt  
  inflating: PennFudanPed/Annotation/PennPed00043.txt  
  inflating: PennFudanPed/Annotation/PennPed00044.txt  
  inflating: PennFudanPed/Annotation/PennPed00045.txt  
  inflating: PennFudanPed/Annotation/PennPed00046.txt  
  inflating: PennFudanPed/Annotation/PennPed00047.txt  
  inflating: PennFudanPed/Annotation/PennPed00048.txt  
  inflating: PennFudanPed/Annotation/PennPed00049.txt  
  inflating: PennFudanPed/Annotation/PennPed00050.txt  
  inflating: PennFudanPed/Annotation/PennPed00051.txt  
  inflating: PennFudanPed/Annotation/PennPed00052.txt  
  inflating: PennFudanPed/Annotation/PennPed00053.txt  
  inflating: PennFudanPed/Annotation/PennPed00054.txt  
  inflating: PennFudanPed/Annotation/PennPed00055.txt  
  inflating: PennFudanPed/Annotation/PennPed00056.txt  
  inflating: PennFudanPed/Annotation/PennPed00057.txt  
  inflating: PennFudanPed/Annotation/PennPed00058.txt  
  inflating: PennFudanPed/Annotation/PennPed00059.txt  
  inflating: PennFudanPed/Annotation/PennPed00060.txt  
  inflating: PennFudanPed/Annotation/PennPed00061.txt  
  inflating: PennFudanPed/Annotation/PennPed00062.txt  
  inflating: PennFudanPed/Annotation/PennPed00063.txt  
  inflating: PennFudanPed/Annotation/PennPed00064.txt  
  inflating: PennFudanPed/Annotation/PennPed00065.txt  
  inflating: PennFudanPed/Annotation/PennPed00066.txt  
  inflating: PennFudanPed/Annotation/PennPed00067.txt  
  inflating: PennFudanPed/Annotation/PennPed00068.txt  
  inflating: PennFudanPed/Annotation/PennPed00069.txt  
  inflating: PennFudanPed/Annotation/PennPed00070.txt  
  inflating: PennFudanPed/Annotation/PennPed00071.txt  
  inflating: PennFudanPed/Annotation/PennPed00072.txt  
  inflating: PennFudanPed/Annotation/PennPed00073.txt  
  inflating: PennFudanPed/Annotation/PennPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00075.txt  
  inflating: PennFudanPed/Annotation/PennPed00076.txt  
  inflating: PennFudanPed/Annotation/PennPed00077.txt  
  inflating: PennFudanPed/Annotation/PennPed00078.txt  
  inflating: PennFudanPed/Annotation/PennPed00079.txt  
  inflating: PennFudanPed/Annotation/PennPed00080.txt  
  inflating: PennFudanPed/Annotation/PennPed00081.txt  
  inflating: PennFudanPed/Annotation/PennPed00082.txt  
  inflating: PennFudanPed/Annotation/PennPed00083.txt  
  inflating: PennFudanPed/Annotation/PennPed00084.txt  
  inflating: PennFudanPed/Annotation/PennPed00085.txt  
  inflating: PennFudanPed/Annotation/PennPed00086.txt  
  inflating: PennFudanPed/Annotation/PennPed00087.txt  
  inflating: PennFudanPed/Annotation/PennPed00088.txt  
  inflating: PennFudanPed/Annotation/PennPed00089.txt  
  inflating: PennFudanPed/Annotation/PennPed00090.txt  
  inflating: PennFudanPed/Annotation/PennPed00091.txt  
  inflating: PennFudanPed/Annotation/PennPed00092.txt  
  inflating: PennFudanPed/Annotation/PennPed00093.txt  
  inflating: PennFudanPed/Annotation/PennPed00094.txt  
  inflating: PennFudanPed/Annotation/PennPed00095.txt  
  inflating: PennFudanPed/Annotation/PennPed00096.txt  
   creating: PennFudanPed/PedMasks/
  inflating: PennFudanPed/PedMasks/FudanPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00014_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00017_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00027_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00052_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00061_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00065_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00068_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00069_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00071_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00073_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00014_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00017_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00027_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00052_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00061_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00065_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00068_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00069_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00071_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00073_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00075_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00076_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00077_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00078_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00079_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00080_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00081_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00082_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00083_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00084_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00085_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00086_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00087_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00088_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00089_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00090_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00091_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00092_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00093_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00094_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00095_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00096_mask.png  
   creating: PennFudanPed/PNGImages/
  inflating: PennFudanPed/PNGImages/FudanPed00001.png  
  inflating: PennFudanPed/PNGImages/FudanPed00002.png  
  inflating: PennFudanPed/PNGImages/FudanPed00003.png  
  inflating: PennFudanPed/PNGImages/FudanPed00004.png  
  inflating: PennFudanPed/PNGImages/FudanPed00005.png  
  inflating: PennFudanPed/PNGImages/FudanPed00006.png  
  inflating: PennFudanPed/PNGImages/FudanPed00007.png  
  inflating: PennFudanPed/PNGImages/FudanPed00008.png  
  inflating: PennFudanPed/PNGImages/FudanPed00009.png  
  inflating: PennFudanPed/PNGImages/FudanPed00010.png  
  inflating: PennFudanPed/PNGImages/FudanPed00011.png  
  inflating: PennFudanPed/PNGImages/FudanPed00012.png  
  inflating: PennFudanPed/PNGImages/FudanPed00013.png  
  inflating: PennFudanPed/PNGImages/FudanPed00014.png  
  inflating: PennFudanPed/PNGImages/FudanPed00015.png  
  inflating: PennFudanPed/PNGImages/FudanPed00016.png  
  inflating: PennFudanPed/PNGImages/FudanPed00017.png  
  inflating: PennFudanPed/PNGImages/FudanPed00018.png  
  inflating: PennFudanPed/PNGImages/FudanPed00019.png  
  inflating: PennFudanPed/PNGImages/FudanPed00020.png  
  inflating: PennFudanPed/PNGImages/FudanPed00021.png  
  inflating: PennFudanPed/PNGImages/FudanPed00022.png  
  inflating: PennFudanPed/PNGImages/FudanPed00023.png  
  inflating: PennFudanPed/PNGImages/FudanPed00024.png  
  inflating: PennFudanPed/PNGImages/FudanPed00025.png  
  inflating: PennFudanPed/PNGImages/FudanPed00026.png  
  inflating: PennFudanPed/PNGImages/FudanPed00027.png  
  inflating: PennFudanPed/PNGImages/FudanPed00028.png  
  inflating: PennFudanPed/PNGImages/FudanPed00029.png  
  inflating: PennFudanPed/PNGImages/FudanPed00030.png  
  inflating: PennFudanPed/PNGImages/FudanPed00031.png  
  inflating: PennFudanPed/PNGImages/FudanPed00032.png  
  inflating: PennFudanPed/PNGImages/FudanPed00033.png  
  inflating: PennFudanPed/PNGImages/FudanPed00034.png  
  inflating: PennFudanPed/PNGImages/FudanPed00035.png  
  inflating: PennFudanPed/PNGImages/FudanPed00036.png  
  inflating: PennFudanPed/PNGImages/FudanPed00037.png  
  inflating: PennFudanPed/PNGImages/FudanPed00038.png  
  inflating: PennFudanPed/PNGImages/FudanPed00039.png  
  inflating: PennFudanPed/PNGImages/FudanPed00040.png  
  inflating: PennFudanPed/PNGImages/FudanPed00041.png  
  inflating: PennFudanPed/PNGImages/FudanPed00042.png  
  inflating: PennFudanPed/PNGImages/FudanPed00043.png  
  inflating: PennFudanPed/PNGImages/FudanPed00044.png  
  inflating: PennFudanPed/PNGImages/FudanPed00045.png  
  inflating: PennFudanPed/PNGImages/FudanPed00046.png  
  inflating: PennFudanPed/PNGImages/FudanPed00047.png  
  inflating: PennFudanPed/PNGImages/FudanPed00048.png  
  inflating: PennFudanPed/PNGImages/FudanPed00049.png  
  inflating: PennFudanPed/PNGImages/FudanPed00050.png  
  inflating: PennFudanPed/PNGImages/FudanPed00051.png  
  inflating: PennFudanPed/PNGImages/FudanPed00052.png  
  inflating: PennFudanPed/PNGImages/FudanPed00053.png  
  inflating: PennFudanPed/PNGImages/FudanPed00054.png  
  inflating: PennFudanPed/PNGImages/FudanPed00055.png  
  inflating: PennFudanPed/PNGImages/FudanPed00056.png  
  inflating: PennFudanPed/PNGImages/FudanPed00057.png  
  inflating: PennFudanPed/PNGImages/FudanPed00058.png  
  inflating: PennFudanPed/PNGImages/FudanPed00059.png  
  inflating: PennFudanPed/PNGImages/FudanPed00060.png  
  inflating: PennFudanPed/PNGImages/FudanPed00061.png  
  inflating: PennFudanPed/PNGImages/FudanPed00062.png  
  inflating: PennFudanPed/PNGImages/FudanPed00063.png  
  inflating: PennFudanPed/PNGImages/FudanPed00064.png  
  inflating: PennFudanPed/PNGImages/FudanPed00065.png  
  inflating: PennFudanPed/PNGImages/FudanPed00066.png  
  inflating: PennFudanPed/PNGImages/FudanPed00067.png  
  inflating: PennFudanPed/PNGImages/FudanPed00068.png  
  inflating: PennFudanPed/PNGImages/FudanPed00069.png  
  inflating: PennFudanPed/PNGImages/FudanPed00070.png  
  inflating: PennFudanPed/PNGImages/FudanPed00071.png  
  inflating: PennFudanPed/PNGImages/FudanPed00072.png  
  inflating: PennFudanPed/PNGImages/FudanPed00073.png  
  inflating: PennFudanPed/PNGImages/FudanPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00001.png  
  inflating: PennFudanPed/PNGImages/PennPed00002.png  
  inflating: PennFudanPed/PNGImages/PennPed00003.png  
  inflating: PennFudanPed/PNGImages/PennPed00004.png  
  inflating: PennFudanPed/PNGImages/PennPed00005.png  
  inflating: PennFudanPed/PNGImages/PennPed00006.png  
  inflating: PennFudanPed/PNGImages/PennPed00007.png  
  inflating: PennFudanPed/PNGImages/PennPed00008.png  
  inflating: PennFudanPed/PNGImages/PennPed00009.png  
  inflating: PennFudanPed/PNGImages/PennPed00010.png  
  inflating: PennFudanPed/PNGImages/PennPed00011.png  
  inflating: PennFudanPed/PNGImages/PennPed00012.png  
  inflating: PennFudanPed/PNGImages/PennPed00013.png  
  inflating: PennFudanPed/PNGImages/PennPed00014.png  
  inflating: PennFudanPed/PNGImages/PennPed00015.png  
  inflating: PennFudanPed/PNGImages/PennPed00016.png  
  inflating: PennFudanPed/PNGImages/PennPed00017.png  
  inflating: PennFudanPed/PNGImages/PennPed00018.png  
  inflating: PennFudanPed/PNGImages/PennPed00019.png  
  inflating: PennFudanPed/PNGImages/PennPed00020.png  
  inflating: PennFudanPed/PNGImages/PennPed00021.png  
  inflating: PennFudanPed/PNGImages/PennPed00022.png  
  inflating: PennFudanPed/PNGImages/PennPed00023.png  
  inflating: PennFudanPed/PNGImages/PennPed00024.png  
  inflating: PennFudanPed/PNGImages/PennPed00025.png  
  inflating: PennFudanPed/PNGImages/PennPed00026.png  
  inflating: PennFudanPed/PNGImages/PennPed00027.png  
  inflating: PennFudanPed/PNGImages/PennPed00028.png  
  inflating: PennFudanPed/PNGImages/PennPed00029.png  
  inflating: PennFudanPed/PNGImages/PennPed00030.png  
  inflating: PennFudanPed/PNGImages/PennPed00031.png  
  inflating: PennFudanPed/PNGImages/PennPed00032.png  
  inflating: PennFudanPed/PNGImages/PennPed00033.png  
  inflating: PennFudanPed/PNGImages/PennPed00034.png  
  inflating: PennFudanPed/PNGImages/PennPed00035.png  
  inflating: PennFudanPed/PNGImages/PennPed00036.png  
  inflating: PennFudanPed/PNGImages/PennPed00037.png  
  inflating: PennFudanPed/PNGImages/PennPed00038.png  
  inflating: PennFudanPed/PNGImages/PennPed00039.png  
  inflating: PennFudanPed/PNGImages/PennPed00040.png  
  inflating: PennFudanPed/PNGImages/PennPed00041.png  
  inflating: PennFudanPed/PNGImages/PennPed00042.png  
  inflating: PennFudanPed/PNGImages/PennPed00043.png  
  inflating: PennFudanPed/PNGImages/PennPed00044.png  
  inflating: PennFudanPed/PNGImages/PennPed00045.png  
  inflating: PennFudanPed/PNGImages/PennPed00046.png  
  inflating: PennFudanPed/PNGImages/PennPed00047.png  
  inflating: PennFudanPed/PNGImages/PennPed00048.png  
  inflating: PennFudanPed/PNGImages/PennPed00049.png  
  inflating: PennFudanPed/PNGImages/PennPed00050.png  
  inflating: PennFudanPed/PNGImages/PennPed00051.png  
  inflating: PennFudanPed/PNGImages/PennPed00052.png  
  inflating: PennFudanPed/PNGImages/PennPed00053.png  
  inflating: PennFudanPed/PNGImages/PennPed00054.png  
  inflating: PennFudanPed/PNGImages/PennPed00055.png  
  inflating: PennFudanPed/PNGImages/PennPed00056.png  
  inflating: PennFudanPed/PNGImages/PennPed00057.png  
  inflating: PennFudanPed/PNGImages/PennPed00058.png  
  inflating: PennFudanPed/PNGImages/PennPed00059.png  
  inflating: PennFudanPed/PNGImages/PennPed00060.png  
  inflating: PennFudanPed/PNGImages/PennPed00061.png  
  inflating: PennFudanPed/PNGImages/PennPed00062.png  
  inflating: PennFudanPed/PNGImages/PennPed00063.png  
  inflating: PennFudanPed/PNGImages/PennPed00064.png  
  inflating: PennFudanPed/PNGImages/PennPed00065.png  
  inflating: PennFudanPed/PNGImages/PennPed00066.png  
  inflating: PennFudanPed/PNGImages/PennPed00067.png  
  inflating: PennFudanPed/PNGImages/PennPed00068.png  
  inflating: PennFudanPed/PNGImages/PennPed00069.png  
  inflating: PennFudanPed/PNGImages/PennPed00070.png  
  inflating: PennFudanPed/PNGImages/PennPed00071.png  
  inflating: PennFudanPed/PNGImages/PennPed00072.png  
  inflating: PennFudanPed/PNGImages/PennPed00073.png  
  inflating: PennFudanPed/PNGImages/PennPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00075.png  
  inflating: PennFudanPed/PNGImages/PennPed00076.png  
  inflating: PennFudanPed/PNGImages/PennPed00077.png  
  inflating: PennFudanPed/PNGImages/PennPed00078.png  
  inflating: PennFudanPed/PNGImages/PennPed00079.png  
  inflating: PennFudanPed/PNGImages/PennPed00080.png  
  inflating: PennFudanPed/PNGImages/PennPed00081.png  
  inflating: PennFudanPed/PNGImages/PennPed00082.png  
  inflating: PennFudanPed/PNGImages/PennPed00083.png  
  inflating: PennFudanPed/PNGImages/PennPed00084.png  
  inflating: PennFudanPed/PNGImages/PennPed00085.png  
  inflating: PennFudanPed/PNGImages/PennPed00086.png  
  inflating: PennFudanPed/PNGImages/PennPed00087.png  
  inflating: PennFudanPed/PNGImages/PennPed00088.png  
  inflating: PennFudanPed/PNGImages/PennPed00089.png  
  inflating: PennFudanPed/PNGImages/PennPed00090.png  
  inflating: PennFudanPed/PNGImages/PennPed00091.png  
  inflating: PennFudanPed/PNGImages/PennPed00092.png  
  inflating: PennFudanPed/PNGImages/PennPed00093.png  
  inflating: PennFudanPed/PNGImages/PennPed00094.png  
  inflating: PennFudanPed/PNGImages/PennPed00095.png  
  inflating: PennFudanPed/PNGImages/PennPed00096.png  
  inflating: PennFudanPed/readme.txt  
</code></pre><p>Let’s have a look at the dataset and how it is layed down.</p>
<p>The data is structured as follows<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PennFudanPed&#x2F;</span><br><span class="line">  PedMasks&#x2F;</span><br><span class="line">    FudanPed00001_mask.png</span><br><span class="line">    FudanPed00002_mask.png</span><br><span class="line">    FudanPed00003_mask.png</span><br><span class="line">    FudanPed00004_mask.png</span><br><span class="line">    ...</span><br><span class="line">  PNGImages&#x2F;</span><br><span class="line">    FudanPed00001.png</span><br><span class="line">    FudanPed00002.png</span><br><span class="line">    FudanPed00003.png</span><br><span class="line">    FudanPed00004.png</span><br></pre></td></tr></table></figure></p>
<p>Here is one example of an image in the dataset, with its corresponding instance segmentation mask</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">Image.open(<span class="string">'PennFudanPed/PNGImages/FudanPed00001.png'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mask = Image.open(<span class="string">'PennFudanPed/PedMasks/FudanPed00001_mask.png'</span>)</span><br><span class="line"><span class="comment"># each mask instance has a different color, from zero to N, where</span></span><br><span class="line"><span class="comment"># N is the number of instances. In order to make visualization easier,</span></span><br><span class="line"><span class="comment"># let's adda color palette to the mask.</span></span><br><span class="line">mask.putpalette([</span><br><span class="line">    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># black background</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># index 1 is red</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>, <span class="comment"># index 2 is yellow</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">153</span>, <span class="number">0</span>, <span class="comment"># index 3 is orange</span></span><br><span class="line">])</span><br><span class="line">mask</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_7_0.png" alt="png"></p>
<p>So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a <code>torch.utils.data.Dataset</code> class for this dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PennFudanDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms=None)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># load all image files, sorting them to</span></span><br><span class="line">        <span class="comment"># ensure that they are aligned</span></span><br><span class="line">        self.imgs = list(sorted(os.listdir(os.path.join(root, <span class="string">"PNGImages"</span>))))</span><br><span class="line">        self.masks = list(sorted(os.listdir(os.path.join(root, <span class="string">"PedMasks"</span>))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="comment"># load images ad masks</span></span><br><span class="line">        img_path = os.path.join(self.root, <span class="string">"PNGImages"</span>, self.imgs[idx])</span><br><span class="line">        mask_path = os.path.join(self.root, <span class="string">"PedMasks"</span>, self.masks[idx])</span><br><span class="line">        img = Image.open(img_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        <span class="comment"># note that we haven't converted the mask to RGB,</span></span><br><span class="line">        <span class="comment"># because each color corresponds to a different instance</span></span><br><span class="line">        <span class="comment"># with 0 being background</span></span><br><span class="line">        mask = Image.open(mask_path)</span><br><span class="line"></span><br><span class="line">        mask = np.array(mask)</span><br><span class="line">        <span class="comment"># instances are encoded as different colors</span></span><br><span class="line">        obj_ids = np.unique(mask)</span><br><span class="line">        <span class="comment"># first id is the background, so remove it</span></span><br><span class="line">        obj_ids = obj_ids[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split the color-encoded mask into a set</span></span><br><span class="line">        <span class="comment"># of binary masks</span></span><br><span class="line">        masks = mask == obj_ids[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get bounding box coordinates for each mask</span></span><br><span class="line">        num_objs = len(obj_ids)</span><br><span class="line">        boxes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_objs):</span><br><span class="line">            pos = np.where(masks[i])</span><br><span class="line">            xmin = np.min(pos[<span class="number">1</span>])</span><br><span class="line">            xmax = np.max(pos[<span class="number">1</span>])</span><br><span class="line">            ymin = np.min(pos[<span class="number">0</span>])</span><br><span class="line">            ymax = np.max(pos[<span class="number">0</span>])</span><br><span class="line">            boxes.append([xmin, ymin, xmax, ymax])</span><br><span class="line"></span><br><span class="line">        boxes = torch.as_tensor(boxes, dtype=torch.float32)</span><br><span class="line">        <span class="comment"># there is only one class</span></span><br><span class="line">        labels = torch.ones((num_objs,), dtype=torch.int64)</span><br><span class="line">        masks = torch.as_tensor(masks, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        image_id = torch.tensor([idx])</span><br><span class="line">        area = (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]) * (boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>])</span><br><span class="line">        <span class="comment"># suppose all instances are not crowd</span></span><br><span class="line">        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">        target = &#123;&#125;</span><br><span class="line">        target[<span class="string">"boxes"</span>] = boxes</span><br><span class="line">        target[<span class="string">"labels"</span>] = labels</span><br><span class="line">        target[<span class="string">"masks"</span>] = masks</span><br><span class="line">        target[<span class="string">"image_id"</span>] = image_id</span><br><span class="line">        target[<span class="string">"area"</span>] = area</span><br><span class="line">        target[<span class="string">"iscrowd"</span>] = iscrowd</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img, target = self.transforms(img, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure>
<p>That’s all for the dataset. Let’s see how the outputs are structured for this dataset</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed/'</span>)</span><br><span class="line">dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(&lt;PIL.Image.Image image mode=RGB size=559x536 at 0x7FEBFE8767F0&gt;,
 {&#39;area&#39;: tensor([35358., 36225.]), &#39;boxes&#39;: tensor([[159., 181., 301., 430.],
          [419., 170., 534., 485.]]), &#39;image_id&#39;: tensor([0]), &#39;iscrowd&#39;: tensor([0, 0]), &#39;labels&#39;: tensor([1, 1]), &#39;masks&#39;: tensor([[[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]],

          [[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})
</code></pre><p>So we can see that by default, the dataset returns a <code>PIL.Image</code> and a dictionary<br>containing several fields, including <code>boxes</code>, <code>labels</code> and <code>masks</code>.</p>
<h2 id="Defining-your-model"><a href="#Defining-your-model" class="headerlink" title="Defining your model"></a>Defining your model</h2><p>In this tutorial, we will be using <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>, which is based on top of <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image03.png" alt="Faster R-CNN"></p>
<p>Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image04.png" alt="Mask R-CNN"></p>
<p>There are two common situations where one might want to modify one of the available models in torchvision modelzoo.<br>The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).</p>
<p>Let’s go see how we would do one or another in the following sections.</p>
<h3 id="1-Finetuning-from-a-pretrained-model"><a href="#1-Finetuning-from-a-pretrained-model" class="headerlink" title="1 - Finetuning from a pretrained model"></a>1 - Finetuning from a pretrained model</h3><p>Let’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection.faster_rcnn import FastRCNNPredictor</span><br><span class="line"></span><br><span class="line"># load a model pre-trained pre-trained on COCO</span><br><span class="line">model &#x3D; torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained&#x3D;True)</span><br><span class="line"></span><br><span class="line"># replace the classifier with a new one, that has</span><br><span class="line"># num_classes which is user-defined</span><br><span class="line">num_classes &#x3D; 2  # 1 class (person) + background</span><br><span class="line"># get number of input features for the classifier</span><br><span class="line">in_features &#x3D; model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line"># replace the pre-trained head with a new one</span><br><span class="line">model.roi_heads.box_predictor &#x3D; FastRCNNPredictor(in_features, num_classes)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-Modifying-the-model-to-add-a-different-backbone"><a href="#2-Modifying-the-model-to-add-a-different-backbone" class="headerlink" title="2 - Modifying the model to add a different backbone"></a>2 - Modifying the model to add a different backbone</h3><p>Another common situation arises when the user wants to replace the backbone of a detection<br>model with a different one. For example, the current default backbone (ResNet-50) might be too big for some applications, and smaller models might be necessary.</p>
<p>Here is how we would go into leveraging the functions provided by torchvision to modify a backbone.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection import FasterRCNN</span><br><span class="line">from torchvision.models.detection.rpn import AnchorGenerator</span><br><span class="line"></span><br><span class="line"># load a pre-trained model for classification and return</span><br><span class="line"># only the features</span><br><span class="line">backbone &#x3D; torchvision.models.mobilenet_v2(pretrained&#x3D;True).features</span><br><span class="line"># FasterRCNN needs to know the number of</span><br><span class="line"># output channels in a backbone. For mobilenet_v2, it&#39;s 1280</span><br><span class="line"># so we need to add it here</span><br><span class="line">backbone.out_channels &#x3D; 1280</span><br><span class="line"></span><br><span class="line"># let&#39;s make the RPN generate 5 x 3 anchors per spatial</span><br><span class="line"># location, with 5 different sizes and 3 different aspect</span><br><span class="line"># ratios. We have a Tuple[Tuple[int]] because each feature</span><br><span class="line"># map could potentially have different sizes and</span><br><span class="line"># aspect ratios </span><br><span class="line">anchor_generator &#x3D; AnchorGenerator(sizes&#x3D;((32, 64, 128, 256, 512),),</span><br><span class="line">                                   aspect_ratios&#x3D;((0.5, 1.0, 2.0),))</span><br><span class="line"></span><br><span class="line"># let&#39;s define what are the feature maps that we will</span><br><span class="line"># use to perform the region of interest cropping, as well as</span><br><span class="line"># the size of the crop after rescaling.</span><br><span class="line"># if your backbone returns a Tensor, featmap_names is expected to</span><br><span class="line"># be [0]. More generally, the backbone should return an</span><br><span class="line"># OrderedDict[Tensor], and in featmap_names you can choose which</span><br><span class="line"># feature maps to use.</span><br><span class="line">roi_pooler &#x3D; torchvision.ops.MultiScaleRoIAlign(featmap_names&#x3D;[0],</span><br><span class="line">                                                output_size&#x3D;7,</span><br><span class="line">                                                sampling_ratio&#x3D;2)</span><br><span class="line"></span><br><span class="line"># put the pieces together inside a FasterRCNN model</span><br><span class="line">model &#x3D; FasterRCNN(backbone,</span><br><span class="line">                   num_classes&#x3D;2,</span><br><span class="line">                   rpn_anchor_generator&#x3D;anchor_generator,</span><br><span class="line">                   box_roi_pool&#x3D;roi_pooler)</span><br></pre></td></tr></table></figure>
<h3 id="An-Instance-segmentation-model-for-PennFudan-Dataset"><a href="#An-Instance-segmentation-model-for-PennFudan-Dataset" class="headerlink" title="An Instance segmentation model for PennFudan Dataset"></a>An Instance segmentation model for PennFudan Dataset</h3><p>In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.</p>
<p>Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> FastRCNNPredictor</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.mask_rcnn <span class="keyword">import</span> MaskRCNNPredictor</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_instance_segmentation_model</span><span class="params">(num_classes)</span>:</span></span><br><span class="line">    <span class="comment"># load an instance segmentation model pre-trained on COCO</span></span><br><span class="line">    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the number of input features for the classifier</span></span><br><span class="line">    in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    <span class="comment"># replace the pre-trained head with a new one</span></span><br><span class="line">    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now get the number of input features for the mask classifier</span></span><br><span class="line">    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels</span><br><span class="line">    hidden_layer = <span class="number">256</span></span><br><span class="line">    <span class="comment"># and replace the mask predictor with a new one</span></span><br><span class="line">    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,</span><br><span class="line">                                                       hidden_layer,</span><br><span class="line">                                                       num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>That’s it, this will make model be ready to be trained and evaluated on our custom dataset.</p>
<h2 id="Training-and-evaluation-functions"><a href="#Training-and-evaluation-functions" class="headerlink" title="Training and evaluation functions"></a>Training and evaluation functions</h2><p>In <code>references/detection/,</code> we have a number of helper functions to simplify training and evaluating detection models.<br>Here, we will use <code>references/detection/engine.py</code>, <code>references/detection/utils.py</code> and <code>references/detection/transforms.py</code>.</p>
<p>Let’s copy those files (and their dependencies) in here so that they are available in the notebook</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download TorchVision repo to use some files from</span></span><br><span class="line"><span class="comment"># references/detection</span></span><br><span class="line">git clone https://github.com/pytorch/vision.git</span><br><span class="line">cd vision</span><br><span class="line">git checkout v0<span class="number">.3</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">cp references/detection/utils.py ../</span><br><span class="line">cp references/detection/transforms.py ../</span><br><span class="line">cp references/detection/coco_eval.py ../</span><br><span class="line">cp references/detection/engine.py ../</span><br><span class="line">cp references/detection/coco_utils.py ../</span><br></pre></td></tr></table></figure>
<pre><code>Cloning into &#39;vision&#39;...
remote: Enumerating objects: 20, done.[K
remote: Counting objects: 100% (20/20), done.[K
remote: Compressing objects: 100% (20/20), done.[K
remote: Total 9278 (delta 7), reused 3 (delta 0), pack-reused 9258[K
Receiving objects: 100% (9278/9278), 11.24 MiB | 9.51 MiB/s, done.
Resolving deltas: 100% (6426/6426), done.
Note: checking out &#39;v0.3.0&#39;.

You are in &#39;detached HEAD&#39; state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b &lt;new-branch-name&gt;

HEAD is now at be37608 version check against PyTorch&#39;s CUDA version
</code></pre><p>Let’s write some helper functions for data augmentation / transformation, which leverages the functions in <code>refereces/detection</code> that we have just copied:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> engine <span class="keyword">import</span> train_one_epoch, evaluate</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(train)</span>:</span></span><br><span class="line">    transforms = []</span><br><span class="line">    <span class="comment"># converts the image, a PIL image, into a PyTorch Tensor</span></span><br><span class="line">    transforms.append(T.ToTensor())</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        <span class="comment"># during training, randomly flip the training images</span></span><br><span class="line">        <span class="comment"># and ground-truth for data augmentation</span></span><br><span class="line">        transforms.append(T.RandomHorizontalFlip(<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> T.Compose(transforms)</span><br></pre></td></tr></table></figure>
<h4 id="Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model"><a href="#Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model" class="headerlink" title="Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."></a>Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.</h4><h3 id="Putting-everything-together"><a href="#Putting-everything-together" class="headerlink" title="Putting everything together"></a>Putting everything together</h3><p>We now have the dataset class, the models and the data transforms. Let’s instantiate them</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use our dataset and defined transformations</span></span><br><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">True</span>))</span><br><span class="line">dataset_test = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split the dataset in train and test set</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">indices = torch.randperm(len(dataset)).tolist()</span><br><span class="line">dataset = torch.utils.data.Subset(dataset, indices[:<span class="number">-50</span>])</span><br><span class="line">dataset_test = torch.utils.data.Subset(dataset_test, indices[<span class="number">-50</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># define training and validation data loaders</span></span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br><span class="line"></span><br><span class="line">data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_test, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br></pre></td></tr></table></figure>
<p>Now let’s instantiate the model and the optimizer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># our dataset has two classes only - background and person</span></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get the model using our helper function</span></span><br><span class="line">model = get_instance_segmentation_model(num_classes)</span><br><span class="line"><span class="comment"># move model to the right device</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct an optimizer</span></span><br><span class="line">params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=<span class="number">0.005</span>,</span><br><span class="line">                            momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and a learning rate scheduler which decreases the learning rate by</span></span><br><span class="line"><span class="comment"># 10x every 3 epochs</span></span><br><span class="line">lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,</span><br><span class="line">                                               step_size=<span class="number">3</span>,</span><br><span class="line">                                               gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading: &quot;https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth&quot; to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth



HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value=&#39;&#39;)))
</code></pre><p>And now let’s train the model for 10 epochs, evaluating at the end of every epoch.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># let's train it for 10 epochs</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># train for one epoch, printing every 10 iterations</span></span><br><span class="line">    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># update the learning rate</span></span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    <span class="comment"># evaluate on the test dataset</span></span><br><span class="line">    evaluate(model, data_loader_test, device=device)</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:
    nonzero(Tensor input, *, Tensor out)
Consider using one of the following signatures instead:
    nonzero(Tensor input, *, bool as_tuple)


Epoch: [0]  [ 0/60]  eta: 0:02:18  lr: 0.000090  loss: 3.5827 (3.5827)  loss_classifier: 0.7385 (0.7385)  loss_box_reg: 0.1523 (0.1523)  loss_mask: 2.6620 (2.6620)  loss_objectness: 0.0224 (0.0224)  loss_rpn_box_reg: 0.0076 (0.0076)  time: 2.3152  data: 0.2933  max mem: 2303
Epoch: [0]  [10/60]  eta: 0:01:14  lr: 0.000936  loss: 1.5605 (2.1212)  loss_classifier: 0.4479 (0.4976)  loss_box_reg: 0.1826 (0.1906)  loss_mask: 0.9259 (1.4017)  loss_objectness: 0.0224 (0.0208)  loss_rpn_box_reg: 0.0090 (0.0105)  time: 1.4865  data: 0.0356  max mem: 2860
Epoch: [0]  [20/60]  eta: 0:00:57  lr: 0.001783  loss: 0.8700 (1.4312)  loss_classifier: 0.2338 (0.3409)  loss_box_reg: 0.1579 (0.1731)  loss_mask: 0.4010 (0.8836)  loss_objectness: 0.0191 (0.0216)  loss_rpn_box_reg: 0.0099 (0.0120)  time: 1.3888  data: 0.0096  max mem: 2861
Epoch: [0]  [30/60]  eta: 0:00:43  lr: 0.002629  loss: 0.5382 (1.1211)  loss_classifier: 0.0968 (0.2569)  loss_box_reg: 0.1155 (0.1598)  loss_mask: 0.2489 (0.6751)  loss_objectness: 0.0105 (0.0176)  loss_rpn_box_reg: 0.0099 (0.0117)  time: 1.4144  data: 0.0095  max mem: 3596
Epoch: [0]  [40/60]  eta: 0:00:28  lr: 0.003476  loss: 0.4041 (0.9495)  loss_classifier: 0.0690 (0.2099)  loss_box_reg: 0.1090 (0.1521)  loss_mask: 0.2121 (0.5609)  loss_objectness: 0.0038 (0.0142)  loss_rpn_box_reg: 0.0118 (0.0124)  time: 1.4593  data: 0.0098  max mem: 3596
Epoch: [0]  [50/60]  eta: 0:00:14  lr: 0.004323  loss: 0.3387 (0.8263)  loss_classifier: 0.0496 (0.1785)  loss_box_reg: 0.0833 (0.1393)  loss_mask: 0.1797 (0.4837)  loss_objectness: 0.0035 (0.0122)  loss_rpn_box_reg: 0.0118 (0.0128)  time: 1.4368  data: 0.0101  max mem: 3596
Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.2567 (0.7367)  loss_classifier: 0.0392 (0.1566)  loss_box_reg: 0.0545 (0.1240)  loss_mask: 0.1464 (0.4332)  loss_objectness: 0.0020 (0.0106)  loss_rpn_box_reg: 0.0109 (0.0122)  time: 1.4374  data: 0.0101  max mem: 3596
Epoch: [0] Total time: 0:01:26 (1.4425 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3444 (0.3444)  evaluator_time: 0.0059 (0.0059)  time: 0.4881  data: 0.1360  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)  time: 0.3262  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3309 s / it)
Averaged stats: model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.698
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.901
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.759
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.700
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.886
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.716
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.741
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.745
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.751
Epoch: [1]  [ 0/60]  eta: 0:01:39  lr: 0.005000  loss: 0.1716 (0.1716)  loss_classifier: 0.0231 (0.0231)  loss_box_reg: 0.0309 (0.0309)  loss_mask: 0.1041 (0.1041)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0124 (0.0124)  time: 1.6632  data: 0.3040  max mem: 3596
Epoch: [1]  [10/60]  eta: 0:01:14  lr: 0.005000  loss: 0.2137 (0.2460)  loss_classifier: 0.0314 (0.0385)  loss_box_reg: 0.0309 (0.0406)  loss_mask: 0.1438 (0.1540)  loss_objectness: 0.0011 (0.0017)  loss_rpn_box_reg: 0.0113 (0.0111)  time: 1.4996  data: 0.0356  max mem: 3596
Epoch: [1]  [20/60]  eta: 0:00:59  lr: 0.005000  loss: 0.2565 (0.2636)  loss_classifier: 0.0484 (0.0464)  loss_box_reg: 0.0338 (0.0442)  loss_mask: 0.1639 (0.1582)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0123 (0.0131)  time: 1.4682  data: 0.0102  max mem: 3596
Epoch: [1]  [30/60]  eta: 0:00:43  lr: 0.005000  loss: 0.2174 (0.2409)  loss_classifier: 0.0349 (0.0410)  loss_box_reg: 0.0266 (0.0365)  loss_mask: 0.1426 (0.1502)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0080 (0.0115)  time: 1.4462  data: 0.0105  max mem: 3596
Epoch: [1]  [40/60]  eta: 0:00:29  lr: 0.005000  loss: 0.1930 (0.2327)  loss_classifier: 0.0274 (0.0406)  loss_box_reg: 0.0189 (0.0334)  loss_mask: 0.1380 (0.1463)  loss_objectness: 0.0007 (0.0015)  loss_rpn_box_reg: 0.0075 (0.0109)  time: 1.4625  data: 0.0096  max mem: 3596
Epoch: [1]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.2011 (0.2291)  loss_classifier: 0.0344 (0.0409)  loss_box_reg: 0.0253 (0.0325)  loss_mask: 0.1287 (0.1427)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0079 (0.0115)  time: 1.5020  data: 0.0099  max mem: 3596
Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1680 (0.2220)  loss_classifier: 0.0294 (0.0398)  loss_box_reg: 0.0148 (0.0302)  loss_mask: 0.1265 (0.1394)  loss_objectness: 0.0011 (0.0016)  loss_rpn_box_reg: 0.0075 (0.0110)  time: 1.4470  data: 0.0098  max mem: 3596
Epoch: [1] Total time: 0:01:27 (1.4662 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3606 (0.3606)  evaluator_time: 0.0046 (0.0046)  time: 0.5041  data: 0.1374  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)  time: 0.3234  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3253 s / it)
Averaged stats: model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.772
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.747
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.891
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794
Epoch: [2]  [ 0/60]  eta: 0:01:38  lr: 0.005000  loss: 0.1239 (0.1239)  loss_classifier: 0.0123 (0.0123)  loss_box_reg: 0.0068 (0.0068)  loss_mask: 0.0971 (0.0971)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.6336  data: 0.2465  max mem: 3596
Epoch: [2]  [10/60]  eta: 0:01:12  lr: 0.005000  loss: 0.1866 (0.1751)  loss_classifier: 0.0283 (0.0308)  loss_box_reg: 0.0135 (0.0162)  loss_mask: 0.1129 (0.1189)  loss_objectness: 0.0007 (0.0011)  loss_rpn_box_reg: 0.0072 (0.0082)  time: 1.4478  data: 0.0310  max mem: 3596
Epoch: [2]  [20/60]  eta: 0:00:55  lr: 0.005000  loss: 0.1433 (0.1623)  loss_classifier: 0.0203 (0.0253)  loss_box_reg: 0.0094 (0.0129)  loss_mask: 0.1074 (0.1162)  loss_objectness: 0.0003 (0.0008)  loss_rpn_box_reg: 0.0046 (0.0071)  time: 1.3800  data: 0.0095  max mem: 3596
Epoch: [2]  [30/60]  eta: 0:00:42  lr: 0.005000  loss: 0.1621 (0.1821)  loss_classifier: 0.0218 (0.0294)  loss_box_reg: 0.0101 (0.0170)  loss_mask: 0.1160 (0.1257)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0077 (0.0088)  time: 1.4109  data: 0.0095  max mem: 3596
Epoch: [2]  [40/60]  eta: 0:00:28  lr: 0.005000  loss: 0.1841 (0.1834)  loss_classifier: 0.0286 (0.0291)  loss_box_reg: 0.0157 (0.0164)  loss_mask: 0.1288 (0.1278)  loss_objectness: 0.0005 (0.0012)  loss_rpn_box_reg: 0.0081 (0.0088)  time: 1.4780  data: 0.0099  max mem: 3596
Epoch: [2]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.1970 (0.1878)  loss_classifier: 0.0279 (0.0295)  loss_box_reg: 0.0173 (0.0175)  loss_mask: 0.1317 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0083 (0.0092)  time: 1.4749  data: 0.0099  max mem: 3596
Epoch: [2]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1872 (0.1894)  loss_classifier: 0.0279 (0.0307)  loss_box_reg: 0.0173 (0.0177)  loss_mask: 0.1296 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0094 (0.0095)  time: 1.5513  data: 0.0099  max mem: 3596
Epoch: [2] Total time: 0:01:28 (1.4738 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:21  model_time: 0.3020 (0.3020)  evaluator_time: 0.0047 (0.0047)  time: 0.4358  data: 0.1272  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)  time: 0.3183  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3203 s / it)
Averaged stats: model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.921
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798
Epoch: [3]  [ 0/60]  eta: 0:01:55  lr: 0.000500  loss: 0.1690 (0.1690)  loss_classifier: 0.0193 (0.0193)  loss_box_reg: 0.0098 (0.0098)  loss_mask: 0.1339 (0.1339)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0058 (0.0058)  time: 1.9331  data: 0.4201  max mem: 3596
Epoch: [3]  [10/60]  eta: 0:01:19  lr: 0.000500  loss: 0.1668 (0.1790)  loss_classifier: 0.0294 (0.0273)  loss_box_reg: 0.0102 (0.0148)  loss_mask: 0.1203 (0.1283)  loss_objectness: 0.0005 (0.0010)  loss_rpn_box_reg: 0.0058 (0.0076)  time: 1.5992  data: 0.0464  max mem: 3596
Epoch: [3]  [20/60]  eta: 0:01:01  lr: 0.000500  loss: 0.1635 (0.1723)  loss_classifier: 0.0225 (0.0257)  loss_box_reg: 0.0088 (0.0133)  loss_mask: 0.1203 (0.1243)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0061 (0.0079)  time: 1.5232  data: 0.0096  max mem: 3596
Epoch: [3]  [30/60]  eta: 0:00:44  lr: 0.000500  loss: 0.1603 (0.1683)  loss_classifier: 0.0212 (0.0251)  loss_box_reg: 0.0083 (0.0121)  loss_mask: 0.1198 (0.1228)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0060 (0.0073)  time: 1.4131  data: 0.0097  max mem: 3596
Epoch: [3]  [40/60]  eta: 0:00:29  lr: 0.000500  loss: 0.1603 (0.1725)  loss_classifier: 0.0266 (0.0268)  loss_box_reg: 0.0093 (0.0127)  loss_mask: 0.1150 (0.1239)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0069 (0.0082)  time: 1.4049  data: 0.0097  max mem: 3596
Epoch: [3]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1715 (0.1754)  loss_classifier: 0.0266 (0.0267)  loss_box_reg: 0.0109 (0.0134)  loss_mask: 0.1232 (0.1261)  loss_objectness: 0.0005 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0083)  time: 1.4872  data: 0.0099  max mem: 3596
Epoch: [3]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1509 (0.1709)  loss_classifier: 0.0256 (0.0263)  loss_box_reg: 0.0093 (0.0126)  loss_mask: 0.1055 (0.1231)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0081)  time: 1.4687  data: 0.0096  max mem: 3596
Epoch: [3] Total time: 0:01:28 (1.4791 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3690 (0.3690)  evaluator_time: 0.0046 (0.0046)  time: 0.5078  data: 0.1324  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)  time: 0.3199  data: 0.0051  max mem: 3596
Test: Total time: 0:00:16 (0.3224 s / it)
Averaged stats: model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.509
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.377
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.869
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.755
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.917
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.805
Epoch: [4]  [ 0/60]  eta: 0:01:27  lr: 0.000500  loss: 0.1045 (0.1045)  loss_classifier: 0.0070 (0.0070)  loss_box_reg: 0.0029 (0.0029)  loss_mask: 0.0902 (0.0902)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0043 (0.0043)  time: 1.4538  data: 0.2039  max mem: 3596
Epoch: [4]  [10/60]  eta: 0:01:12  lr: 0.000500  loss: 0.1510 (0.1583)  loss_classifier: 0.0209 (0.0197)  loss_box_reg: 0.0101 (0.0126)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0007)  loss_rpn_box_reg: 0.0071 (0.0075)  time: 1.4482  data: 0.0278  max mem: 3596
Epoch: [4]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1510 (0.1582)  loss_classifier: 0.0209 (0.0215)  loss_box_reg: 0.0073 (0.0110)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0060 (0.0066)  time: 1.3827  data: 0.0100  max mem: 3596
Epoch: [4]  [30/60]  eta: 0:00:41  lr: 0.000500  loss: 0.1427 (0.1624)  loss_classifier: 0.0227 (0.0243)  loss_box_reg: 0.0077 (0.0117)  loss_mask: 0.1005 (0.1176)  loss_objectness: 0.0005 (0.0014)  loss_rpn_box_reg: 0.0058 (0.0074)  time: 1.3627  data: 0.0097  max mem: 3596
Epoch: [4]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1472 (0.1611)  loss_classifier: 0.0255 (0.0253)  loss_box_reg: 0.0085 (0.0114)  loss_mask: 0.1079 (0.1161)  loss_objectness: 0.0004 (0.0013)  loss_rpn_box_reg: 0.0063 (0.0071)  time: 1.4537  data: 0.0094  max mem: 3596
Epoch: [4]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1548 (0.1612)  loss_classifier: 0.0250 (0.0249)  loss_box_reg: 0.0079 (0.0113)  loss_mask: 0.1106 (0.1161)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0068 (0.0077)  time: 1.4913  data: 0.0094  max mem: 3596
Epoch: [4]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1548 (0.1647)  loss_classifier: 0.0250 (0.0256)  loss_box_reg: 0.0079 (0.0121)  loss_mask: 0.1106 (0.1179)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0079 (0.0080)  time: 1.4724  data: 0.0097  max mem: 3596
Epoch: [4] Total time: 0:01:26 (1.4357 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3692 (0.3692)  evaluator_time: 0.0045 (0.0045)  time: 0.5054  data: 0.1301  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)  time: 0.3202  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3220 s / it)
Averaged stats: model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.813
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.944
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.520
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.825
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.868
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.920
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815
Epoch: [5]  [ 0/60]  eta: 0:02:13  lr: 0.000500  loss: 0.1545 (0.1545)  loss_classifier: 0.0223 (0.0223)  loss_box_reg: 0.0059 (0.0059)  loss_mask: 0.1200 (0.1200)  loss_objectness: 0.0004 (0.0004)  loss_rpn_box_reg: 0.0059 (0.0059)  time: 2.2323  data: 0.5519  max mem: 3596
Epoch: [5]  [10/60]  eta: 0:01:10  lr: 0.000500  loss: 0.1409 (0.1489)  loss_classifier: 0.0178 (0.0211)  loss_box_reg: 0.0076 (0.0094)  loss_mask: 0.1140 (0.1118)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0057 (0.0061)  time: 1.4098  data: 0.0540  max mem: 3596
Epoch: [5]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1379 (0.1454)  loss_classifier: 0.0189 (0.0208)  loss_box_reg: 0.0076 (0.0088)  loss_mask: 0.1032 (0.1091)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0054 (0.0061)  time: 1.3437  data: 0.0070  max mem: 3596
Epoch: [5]  [30/60]  eta: 0:00:42  lr: 0.000500  loss: 0.1430 (0.1597)  loss_classifier: 0.0240 (0.0246)  loss_box_reg: 0.0092 (0.0114)  loss_mask: 0.1032 (0.1154)  loss_objectness: 0.0005 (0.0008)  loss_rpn_box_reg: 0.0069 (0.0075)  time: 1.4147  data: 0.0100  max mem: 3596
Epoch: [5]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1503 (0.1609)  loss_classifier: 0.0242 (0.0243)  loss_box_reg: 0.0102 (0.0117)  loss_mask: 0.1148 (0.1163)  loss_objectness: 0.0004 (0.0008)  loss_rpn_box_reg: 0.0083 (0.0078)  time: 1.4826  data: 0.0101  max mem: 3596
Epoch: [5]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1397 (0.1571)  loss_classifier: 0.0198 (0.0237)  loss_box_reg: 0.0075 (0.0107)  loss_mask: 0.1017 (0.1144)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0066 (0.0075)  time: 1.4890  data: 0.0096  max mem: 3596
Epoch: [5]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1422 (0.1581)  loss_classifier: 0.0197 (0.0241)  loss_box_reg: 0.0066 (0.0107)  loss_mask: 0.1042 (0.1149)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0064 (0.0076)  time: 1.5030  data: 0.0094  max mem: 3596
Epoch: [5] Total time: 0:01:27 (1.4584 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3761 (0.3761)  evaluator_time: 0.0041 (0.0041)  time: 0.5475  data: 0.1655  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)  time: 0.3195  data: 0.0049  max mem: 3596
Test: Total time: 0:00:16 (0.3235 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [6]  [ 0/60]  eta: 0:01:53  lr: 0.000050  loss: 0.1645 (0.1645)  loss_classifier: 0.0255 (0.0255)  loss_box_reg: 0.0121 (0.0121)  loss_mask: 0.1195 (0.1195)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.8885  data: 0.2840  max mem: 3596
Epoch: [6]  [10/60]  eta: 0:01:17  lr: 0.000050  loss: 0.1414 (0.1454)  loss_classifier: 0.0206 (0.0221)  loss_box_reg: 0.0056 (0.0076)  loss_mask: 0.1039 (0.1099)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0058 (0.0053)  time: 1.5460  data: 0.0336  max mem: 3596
Epoch: [6]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1414 (0.1516)  loss_classifier: 0.0206 (0.0241)  loss_box_reg: 0.0065 (0.0101)  loss_mask: 0.1030 (0.1104)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0059 (0.0066)  time: 1.5057  data: 0.0092  max mem: 3596
Epoch: [6]  [30/60]  eta: 0:00:45  lr: 0.000050  loss: 0.1479 (0.1531)  loss_classifier: 0.0255 (0.0261)  loss_box_reg: 0.0087 (0.0099)  loss_mask: 0.1030 (0.1098)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0072 (0.0068)  time: 1.4797  data: 0.0104  max mem: 3596
Epoch: [6]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1493 (0.1593)  loss_classifier: 0.0255 (0.0267)  loss_box_reg: 0.0087 (0.0111)  loss_mask: 0.1043 (0.1137)  loss_objectness: 0.0005 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0072)  time: 1.4498  data: 0.0104  max mem: 3596
Epoch: [6]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1440 (0.1584)  loss_classifier: 0.0234 (0.0261)  loss_box_reg: 0.0088 (0.0110)  loss_mask: 0.1129 (0.1135)  loss_objectness: 0.0004 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0073)  time: 1.4308  data: 0.0097  max mem: 3596
Epoch: [6]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1440 (0.1588)  loss_classifier: 0.0216 (0.0260)  loss_box_reg: 0.0080 (0.0110)  loss_mask: 0.1118 (0.1140)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0070 (0.0073)  time: 1.4508  data: 0.0095  max mem: 3596
Epoch: [6] Total time: 0:01:28 (1.4739 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3615 (0.3615)  evaluator_time: 0.0038 (0.0038)  time: 0.5174  data: 0.1505  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)  time: 0.3199  data: 0.0057  max mem: 3596
Test: Total time: 0:00:16 (0.3225 s / it)
Averaged stats: model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.351
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [7]  [ 0/60]  eta: 0:01:47  lr: 0.000050  loss: 0.1122 (0.1122)  loss_classifier: 0.0151 (0.0151)  loss_box_reg: 0.0039 (0.0039)  loss_mask: 0.0920 (0.0920)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.7859  data: 0.4771  max mem: 3596
Epoch: [7]  [10/60]  eta: 0:01:11  lr: 0.000050  loss: 0.1316 (0.1467)  loss_classifier: 0.0157 (0.0221)  loss_box_reg: 0.0052 (0.0086)  loss_mask: 0.1004 (0.1097)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0037 (0.0053)  time: 1.4340  data: 0.0504  max mem: 3596
Epoch: [7]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1570 (0.1557)  loss_classifier: 0.0288 (0.0274)  loss_box_reg: 0.0085 (0.0100)  loss_mask: 0.1075 (0.1104)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0066 (0.0068)  time: 1.4943  data: 0.0092  max mem: 3596
Epoch: [7]  [30/60]  eta: 0:00:44  lr: 0.000050  loss: 0.1447 (0.1519)  loss_classifier: 0.0257 (0.0255)  loss_box_reg: 0.0076 (0.0093)  loss_mask: 0.1062 (0.1092)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0071 (0.0068)  time: 1.4985  data: 0.0104  max mem: 3596
Epoch: [7]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1418 (0.1546)  loss_classifier: 0.0222 (0.0251)  loss_box_reg: 0.0068 (0.0098)  loss_mask: 0.1095 (0.1120)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0054 (0.0069)  time: 1.4120  data: 0.0099  max mem: 3596
Epoch: [7]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1616 (0.1590)  loss_classifier: 0.0232 (0.0253)  loss_box_reg: 0.0082 (0.0107)  loss_mask: 0.1132 (0.1150)  loss_objectness: 0.0002 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0072)  time: 1.4197  data: 0.0096  max mem: 3596
Epoch: [7]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1474 (0.1592)  loss_classifier: 0.0230 (0.0256)  loss_box_reg: 0.0060 (0.0106)  loss_mask: 0.1101 (0.1150)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0057 (0.0072)  time: 1.4280  data: 0.0095  max mem: 3596
Epoch: [7] Total time: 0:01:27 (1.4505 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3761 (0.3761)  evaluator_time: 0.0044 (0.0044)  time: 0.5139  data: 0.1315  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)  time: 0.3181  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3223 s / it)
Averaged stats: model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.946
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.923
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.382
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.352
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [8]  [ 0/60]  eta: 0:01:59  lr: 0.000050  loss: 0.1533 (0.1533)  loss_classifier: 0.0187 (0.0187)  loss_box_reg: 0.0076 (0.0076)  loss_mask: 0.1242 (0.1242)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0027 (0.0027)  time: 1.9847  data: 0.7161  max mem: 3596
Epoch: [8]  [10/60]  eta: 0:01:14  lr: 0.000050  loss: 0.1533 (0.1537)  loss_classifier: 0.0227 (0.0245)  loss_box_reg: 0.0076 (0.0104)  loss_mask: 0.1094 (0.1121)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0047 (0.0061)  time: 1.4974  data: 0.0700  max mem: 3596
Epoch: [8]  [20/60]  eta: 0:00:58  lr: 0.000050  loss: 0.1512 (0.1592)  loss_classifier: 0.0217 (0.0230)  loss_box_reg: 0.0084 (0.0112)  loss_mask: 0.1071 (0.1168)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0070 (0.0072)  time: 1.4405  data: 0.0081  max mem: 3596
Epoch: [8]  [30/60]  eta: 0:00:43  lr: 0.000050  loss: 0.1390 (0.1557)  loss_classifier: 0.0217 (0.0237)  loss_box_reg: 0.0084 (0.0111)  loss_mask: 0.1021 (0.1130)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0071)  time: 1.4221  data: 0.0102  max mem: 3596
Epoch: [8]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1438 (0.1632)  loss_classifier: 0.0257 (0.0257)  loss_box_reg: 0.0086 (0.0117)  loss_mask: 0.1112 (0.1172)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0076 (0.0075)  time: 1.4616  data: 0.0096  max mem: 3596
Epoch: [8]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1613 (0.1628)  loss_classifier: 0.0265 (0.0265)  loss_box_reg: 0.0104 (0.0116)  loss_mask: 0.1130 (0.1157)  loss_objectness: 0.0005 (0.0011)  loss_rpn_box_reg: 0.0076 (0.0080)  time: 1.5084  data: 0.0096  max mem: 3596
Epoch: [8]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1426 (0.1593)  loss_classifier: 0.0209 (0.0254)  loss_box_reg: 0.0060 (0.0107)  loss_mask: 0.1046 (0.1148)  loss_objectness: 0.0003 (0.0011)  loss_rpn_box_reg: 0.0056 (0.0073)  time: 1.4303  data: 0.0097  max mem: 3596
Epoch: [8] Total time: 0:01:27 (1.4531 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3871 (0.3871)  evaluator_time: 0.0041 (0.0041)  time: 0.5413  data: 0.1481  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)  time: 0.3191  data: 0.0052  max mem: 3596
Test: Total time: 0:00:16 (0.3230 s / it)
Averaged stats: model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [9]  [ 0/60]  eta: 0:01:35  lr: 0.000005  loss: 0.1384 (0.1384)  loss_classifier: 0.0122 (0.0122)  loss_box_reg: 0.0035 (0.0035)  loss_mask: 0.1193 (0.1193)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0031 (0.0031)  time: 1.5976  data: 0.1940  max mem: 3596
Epoch: [9]  [10/60]  eta: 0:01:12  lr: 0.000005  loss: 0.1391 (0.1678)  loss_classifier: 0.0229 (0.0239)  loss_box_reg: 0.0087 (0.0123)  loss_mask: 0.1188 (0.1247)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0055 (0.0064)  time: 1.4416  data: 0.0261  max mem: 3596
Epoch: [9]  [20/60]  eta: 0:00:57  lr: 0.000005  loss: 0.1595 (0.1658)  loss_classifier: 0.0253 (0.0262)  loss_box_reg: 0.0106 (0.0121)  loss_mask: 0.1154 (0.1203)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0056 (0.0066)  time: 1.4367  data: 0.0094  max mem: 3596
Epoch: [9]  [30/60]  eta: 0:00:43  lr: 0.000005  loss: 0.1595 (0.1680)  loss_classifier: 0.0256 (0.0275)  loss_box_reg: 0.0088 (0.0125)  loss_mask: 0.1150 (0.1199)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0060 (0.0074)  time: 1.4554  data: 0.0095  max mem: 3596
Epoch: [9]  [40/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1449 (0.1605)  loss_classifier: 0.0212 (0.0258)  loss_box_reg: 0.0070 (0.0111)  loss_mask: 0.1049 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0064 (0.0069)  time: 1.4446  data: 0.0094  max mem: 3596
Epoch: [9]  [50/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1504 (0.1591)  loss_classifier: 0.0195 (0.0256)  loss_box_reg: 0.0083 (0.0109)  loss_mask: 0.1037 (0.1149)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0071)  time: 1.4887  data: 0.0095  max mem: 3596
Epoch: [9]  [59/60]  eta: 0:00:01  lr: 0.000005  loss: 0.1527 (0.1602)  loss_classifier: 0.0224 (0.0256)  loss_box_reg: 0.0083 (0.0108)  loss_mask: 0.1102 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0072)  time: 1.4984  data: 0.0097  max mem: 3596
Epoch: [9] Total time: 0:01:27 (1.4592 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3668 (0.3668)  evaluator_time: 0.0042 (0.0042)  time: 0.5024  data: 0.1296  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)  time: 0.3215  data: 0.0061  max mem: 3596
Test: Total time: 0:00:16 (0.3233 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.822
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.832
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.874
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.774
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
</code></pre><p>Now that training has finished, let’s have a look at what it actually predicts in a test image</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pick one image from the test set</span></span><br><span class="line">img, _ = dataset_test[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># put the model in evaluation mode</span></span><br><span class="line">model.eval()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    prediction = model([img.to(device)])</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
</code></pre><p>Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.<br>The dictionary contains the predictions for the image we passed. In this case, we can see that it contains <code>boxes</code>, <code>labels</code>, <code>masks</code> and <code>scores</code> as fields.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction</span><br></pre></td></tr></table></figure>
<pre><code>[{&#39;boxes&#39;: tensor([[ 59.6432,  41.9334, 195.6993, 327.8640],
          [276.4631,  22.6867, 290.8581,  73.6079]], device=&#39;cuda:0&#39;),
  &#39;labels&#39;: tensor([1, 1], device=&#39;cuda:0&#39;),
  &#39;masks&#39;: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]],


          [[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]]], device=&#39;cuda:0&#39;),
  &#39;scores&#39;: tensor([0.9991, 0.8170], device=&#39;cuda:0&#39;)}]
</code></pre><p>Let’s inspect the image and the predicted segmentation masks.</p>
<p>For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in <code>[C, H, W]</code> format.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image.fromarray(img.mul(<span class="number">255</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).byte().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_31_0.png" alt="png"></p>
<p>And let’s now visualize the top predicted segmentation mask. The masks are predicted as <code>[N, 1, H, W]</code>, where <code>N</code> is the number of predictions, and are probability maps between 0-1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image.fromarray(prediction[<span class="number">0</span>][<span class="string">'masks'</span>][<span class="number">0</span>, <span class="number">0</span>].mul(<span class="number">255</span>).byte().cpu().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_33_0.png" alt="png"></p>
<p>Looks pretty good!</p>
<h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset.<br>For that, you wrote a <code>torch.utils.data.Dataset</code> class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.</p>
<p>For a more complete example, which includes multi-machine / multi-gpu training, check <code>references/detection/train.py</code>, which is present in the <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision GitHub repo</a>. </p>
<p>#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">coordinates:坐标, 座标,协调, 配合, 接应</span><br><span class="line">segmentation:分割</span><br><span class="line">backbone:主干</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/uploads/weixin.png" alt="Zhangshuai WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/uploads/zhifubao.jpg" alt="Zhangshuai Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/pytorch/" rel="tag"># pytorch</a>
          
            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          
            <a href="/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" rel="tag"># pytorch1.5.1官网教程</a>
          
            <a href="/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" rel="tag"># Pytorch1.5.1官网教程-Image</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" rel="next" title="Pytorch-Learning-cifar10tutorial-visualizing">
                <i class="fa fa-chevron-left"></i> Pytorch-Learning-cifar10tutorial-visualizing
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" rel="prev" title="Pytorch-Image-计算机视觉迁移学习">
                Pytorch-Image-计算机视觉迁移学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar2.jpg"
                alt="Zhangshuai" />
            
              <p class="site-author-name" itemprop="name">Zhangshuai</p>
              <p class="site-description motion-element" itemprop="description">你刚才说了JOJO对吧</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7Carchive">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=486194129&auto=1&height=66"></iframe>
          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/GitHubzhangshuai" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="1802528291@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/ying-ying-ying-vue" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-globe"></i>知乎</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://music.163.com/#/user/home?id=377737250" target="_blank" title="网易云音乐">
                      
                        <i class="fa fa-fw fa-globe"></i>网易云音乐</a>
                  </span>
                
            </div>
          

          
          
            <div class="cc-license motion-element" itemprop="license">
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
                <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
              </a>
            </div>
          

          
          

          

        </div>
      </section>


      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#TorchVision-0-3-Object-Detection-finetuning-tutorial"><span class="nav-number">1.</span> <span class="nav-text">TorchVision 0.3 Object Detection finetuning tutorial</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Defining-the-Dataset"><span class="nav-number">1.1.</span> <span class="nav-text">Defining the Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-a-custom-dataset-for-Penn-Fudan"><span class="nav-number">1.1.1.</span> <span class="nav-text">Writing a custom dataset for Penn-Fudan</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Defining-your-model"><span class="nav-number">1.2.</span> <span class="nav-text">Defining your model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Finetuning-from-a-pretrained-model"><span class="nav-number">1.2.1.</span> <span class="nav-text">1 - Finetuning from a pretrained model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Modifying-the-model-to-add-a-different-backbone"><span class="nav-number">1.2.2.</span> <span class="nav-text">2 - Modifying the model to add a different backbone</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#An-Instance-segmentation-model-for-PennFudan-Dataset"><span class="nav-number">1.2.3.</span> <span class="nav-text">An Instance segmentation model for PennFudan Dataset</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-and-evaluation-functions"><span class="nav-number">1.3.</span> <span class="nav-text">Training and evaluation functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model"><span class="nav-number">1.3.0.1.</span> <span class="nav-text">Note that we do not need to add a mean&#x2F;std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Putting-everything-together"><span class="nav-number">1.3.1.</span> <span class="nav-text">Putting everything together</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wrapping-up"><span class="nav-number">1.4.</span> <span class="nav-text">Wrapping up</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script type="text/javascript" src="/js/src/busuanzi.js"></script>


<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-[object Object]"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张帅</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>









  <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
  <span class="post-meta-divider">|</span>
  <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>


<script>
setTimeout(function(){
document.getElementById('busuanzi_container_site_pv').style.display='inline-block'
document.getElementById('busuanzi_container_site_uv').style.display='inline-block'
},1000)
</script>
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  <!-- 页面点击小红心 -->


   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
   <script type="text/javascript" src="/js/src/fireworks.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body>
</html>
