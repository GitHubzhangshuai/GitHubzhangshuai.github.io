<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BP识别数字</title>
    <url>/2020/07/09/BP%E8%AF%86%E5%88%AB%E6%95%B0%E5%AD%97/</url>
    <content><![CDATA[<p>原生BP代码简单演示</p>
<a id="more"></a>



<p>BP(Back Propagation)神经网络<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dsigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeruralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,layers)</span>:</span></span><br><span class="line">        self.V=np.random.random((layers[<span class="number">0</span>]+<span class="number">1</span>,layers[<span class="number">1</span>]+<span class="number">1</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">        self.W=np.random.random((layers[<span class="number">1</span>]+<span class="number">1</span>,layers[<span class="number">2</span>]))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,X,y,lr=<span class="number">0.11</span>,epochs=<span class="number">100000</span>)</span>:</span></span><br><span class="line">        temp = np.ones([X.shape[<span class="number">0</span>],X.shape[<span class="number">1</span>]+<span class="number">1</span>])</span><br><span class="line">        temp[:,<span class="number">0</span>:<span class="number">-1</span>]=X</span><br><span class="line">        X=temp</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(epochs+<span class="number">1</span>):</span><br><span class="line">            i = np.random.randint(X.shape[<span class="number">0</span>])</span><br><span class="line">            x=[X[i]]</span><br><span class="line">            x=np.atleast_2d(x)</span><br><span class="line">            L1=sigmoid(np.dot(x,self.V))</span><br><span class="line">            L2=sigmoid(np.dot(L1,self.W))</span><br><span class="line">            L2_delta=(y[i]-L2)*dsigmoid(L2)</span><br><span class="line">            L1_delta=L2_delta.dot((self.W.T))*dsigmoid(L1)</span><br><span class="line">            self.W+=lr*L1.T.dot(L2_delta)</span><br><span class="line">            self.V+=lr*x.T.dot(L1_delta)</span><br><span class="line">            <span class="keyword">if</span> n%<span class="number">1000</span>==<span class="number">0</span>:</span><br><span class="line">                predictions=[]</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(X_test.shape[<span class="number">0</span>]):</span><br><span class="line">                    o=self.predict(X_test[j])</span><br><span class="line">                    predictions.append(np.argmax(o))</span><br><span class="line">                accuracy=np.mean(np.equal(predictions,y_test))</span><br><span class="line">                print(<span class="string">'epoch:'</span>,n,<span class="string">'accuracy:'</span>,accuracy)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        temp=np.ones(x.shape[<span class="number">0</span>]+<span class="number">1</span>)</span><br><span class="line">        temp[<span class="number">0</span>:<span class="number">-1</span>]=x</span><br><span class="line">        x=temp</span><br><span class="line">        x=np.atleast_2d(x)</span><br><span class="line">        L1=sigmoid(np.dot(x,self.V))</span><br><span class="line">        L2=sigmoid(np.dot(L1,self.W))</span><br><span class="line">        <span class="keyword">return</span> L2</span><br><span class="line">    </span><br><span class="line">digits =load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">X -= X.min()</span><br><span class="line">X/=X.max()</span><br><span class="line"></span><br><span class="line">nm = NeruralNetwork([<span class="number">64</span>,<span class="number">100</span>,<span class="number">10</span>])</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y)</span><br><span class="line">labels_train = LabelBinarizer().fit_transform(y_train)</span><br><span class="line">labels_test = LabelBinarizer().fit_transform(y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'start'</span>)</span><br><span class="line">nm.train(X_train,labels_train,epochs=<span class="number">20000</span>)</span><br><span class="line">print(<span class="string">'end'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>start
epoch: 0 accuracy: 0.09777777777777778
epoch: 1000 accuracy: 0.5288888888888889
epoch: 2000 accuracy: 0.7333333333333333
epoch: 3000 accuracy: 0.8444444444444444
epoch: 4000 accuracy: 0.8666666666666667
epoch: 5000 accuracy: 0.92
epoch: 6000 accuracy: 0.9444444444444444
epoch: 7000 accuracy: 0.9555555555555556
epoch: 8000 accuracy: 0.9577777777777777
epoch: 9000 accuracy: 0.9488888888888889
epoch: 10000 accuracy: 0.9644444444444444
epoch: 11000 accuracy: 0.9666666666666667
epoch: 12000 accuracy: 0.96
epoch: 13000 accuracy: 0.9688888888888889
epoch: 14000 accuracy: 0.96
epoch: 15000 accuracy: 0.9644444444444444
epoch: 16000 accuracy: 0.9644444444444444
epoch: 17000 accuracy: 0.9666666666666667
epoch: 18000 accuracy: 0.9711111111111111
epoch: 19000 accuracy: 0.9688888888888889
epoch: 20000 accuracy: 0.9777777777777777
end</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>K-Means</title>
    <url>/2020/07/10/K-Means/</url>
    <content><![CDATA[<p>K-Means代码简单演示</p>
<p>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/kmeans.txt" target="_blank" rel="noopener">kmeans.txt</a></p>
<a id="more"></a>



<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans7.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/kmeans.txt'</span>, delimiter=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclDistance</span><span class="params">(vector1,vector2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(sum((vector2-vector1)**<span class="number">2</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(data,k)</span>:</span></span><br><span class="line">    numSamples,dim = data.shape</span><br><span class="line">    centroids = np.zeros((k,dim))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        index = int(np.random.uniform(<span class="number">0</span>,numSamples))</span><br><span class="line">        centroids[i,:]=data[index,:]</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(data,k)</span>:</span></span><br><span class="line">    numSamples = data.shape[<span class="number">0</span>]</span><br><span class="line">    clusterData = np.array(np.zeros((numSamples,<span class="number">2</span>)))</span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    centroids = initCentroids(data,k)</span><br><span class="line">    <span class="keyword">while</span> clusterChanged:  </span><br><span class="line">        clusterChanged = <span class="literal">False</span>  </span><br><span class="line">        <span class="comment"># 循环每一个样本 </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  </span><br><span class="line">            <span class="comment"># 最小距离</span></span><br><span class="line">            minDist  = <span class="number">100000.0</span>  </span><br><span class="line">            <span class="comment"># 定义样本所属的簇</span></span><br><span class="line">            minIndex = <span class="number">0</span>  </span><br><span class="line">            <span class="comment"># 循环计算每一个质心与该样本的距离</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </span><br><span class="line">                <span class="comment"># 循环每一个质心和样本，计算距离</span></span><br><span class="line">                distance = euclDistance(centroids[j, :], data[i, :])  </span><br><span class="line">                <span class="comment"># 如果计算的距离小于最小距离，则更新最小距离</span></span><br><span class="line">                <span class="keyword">if</span> distance &lt; minDist:  </span><br><span class="line">                    minDist  = distance  </span><br><span class="line">                    <span class="comment"># 更新样本所属的簇</span></span><br><span class="line">                    minIndex = j  </span><br><span class="line">                    <span class="comment"># 更新最小距离</span></span><br><span class="line">                    clusterData[i, <span class="number">1</span>] = distance</span><br><span class="line">              </span><br><span class="line">            <span class="comment"># 如果样本的所属的簇发生了变化</span></span><br><span class="line">            <span class="keyword">if</span> clusterData[i, <span class="number">0</span>] != minIndex:  </span><br><span class="line">                <span class="comment"># 质心要重新计算</span></span><br><span class="line">                clusterChanged = <span class="literal">True</span></span><br><span class="line">                <span class="comment"># 更新样本的簇</span></span><br><span class="line">                clusterData[i, <span class="number">0</span>] = minIndex</span><br><span class="line">        <span class="comment"># 更新质心</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </span><br><span class="line">            <span class="comment"># 获取第j个簇所有的样本所在的索引</span></span><br><span class="line">            cluster_index = np.nonzero(clusterData[:, <span class="number">0</span>] == j)</span><br><span class="line">            <span class="comment"># 第j个簇所有的样本点</span></span><br><span class="line">            pointsInCluster = data[cluster_index]  </span><br><span class="line">            <span class="comment"># 计算质心</span></span><br><span class="line">            centroids[j, :] = np.mean(pointsInCluster, axis = <span class="number">0</span>) </span><br><span class="line"><span class="comment">#         showCluster(data, k, centroids, clusterData)</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterData  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(data, k, centroids, clusterData)</span>:</span>  </span><br><span class="line">    numSamples, dim = data.shape  </span><br><span class="line">    <span class="keyword">if</span> dim != <span class="number">2</span>:  </span><br><span class="line">        print(<span class="string">"dimension of your data is not 2!"</span>)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 用不同颜色形状来表示各个类别</span></span><br><span class="line">    mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]  </span><br><span class="line">    <span class="keyword">if</span> k &gt; len(mark):  </span><br><span class="line">        print(<span class="string">"Your k is too large!"</span>)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 画样本点  </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  </span><br><span class="line">        markIndex = int(clusterData[i, <span class="number">0</span>])  </span><br><span class="line">        plt.plot(data[i, <span class="number">0</span>], data[i, <span class="number">1</span>], mark[markIndex])  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 用不同颜色形状来表示各个类别</span></span><br><span class="line">    mark = [<span class="string">'*r'</span>, <span class="string">'*b'</span>, <span class="string">'*g'</span>, <span class="string">'*k'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]  </span><br><span class="line">    <span class="comment"># 画质心点 </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):  </span><br><span class="line">        plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize = <span class="number">20</span>)  </span><br><span class="line">  </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_lost=[]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">10</span>):</span><br><span class="line">    min_loss = <span class="number">10000</span></span><br><span class="line">    min_loss_centroids = np.array([])</span><br><span class="line">    min_loss_clusterData = np.array([])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        centroids,clusterData = kmeans(data,k)</span><br><span class="line">        loss = sum(clusterData[:,<span class="number">1</span>])/data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> loss&lt;min_loss:</span><br><span class="line">            min_loss = loss</span><br><span class="line">            min_loss_centroids = centroids</span><br><span class="line">            min_loss_clusterData = clusterData</span><br><span class="line">    list_lost.append(min_loss)</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\numpy\core\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\numpy\core\_methods.py:154: RuntimeWarning: invalid value encountered in true_divide
  ret, rcount, out=ret, casting=&apos;unsafe&apos;, subok=False)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_lost</span><br></pre></td></tr></table></figure>




<pre><code>[2.9811811738953176,
 1.9708559728104191,
 1.1675654672086735,
 1.0712368269135584,
 1.0019034362200374,
 0.9470283294527311,
 0.8835789709731454,
 0.8393052369848919]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(range(<span class="number">2</span>,<span class="number">10</span>),list_lost)</span><br><span class="line">plt.xlabel(<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/K-Means/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test = [<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k=<span class="number">6</span></span><br><span class="line">min_loss = <span class="number">10000</span></span><br><span class="line">min_loss_centroids = np.array([])</span><br><span class="line">min_loss_clusterData = np.array([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">    centroids,clusterData = kmeans(data,k)</span><br><span class="line">    loss = sum(clusterData[:,<span class="number">1</span>])/data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> loss&lt;min_loss:</span><br><span class="line">        min_loss = loss</span><br><span class="line">        min_loss_centroids = centroids</span><br><span class="line">        min_loss_clusterData = clusterData</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(datas)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array([np.argmin(((np.tile(data,(k,<span class="number">1</span>))-centroids)**<span class="number">2</span>).sum(axis=<span class="number">1</span>)) <span class="keyword">for</span> data <span class="keyword">in</span> datas])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取数据值所在的范围</span></span><br><span class="line">x_min, x_max = data[:, <span class="number">0</span>].min() - <span class="number">1</span>, data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = data[:, <span class="number">1</span>].min() - <span class="number">1</span>, data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成网格矩阵</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">z = predict(np.c_[xx.ravel(), yy.ravel()])<span class="comment"># ravel与flatten类似，多维数据转一维。flatten不会改变原始数据，ravel会改变原始数据</span></span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># 等高线图</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">showCluster(data, k, centroids, clusterData)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/K-Means/output_9_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title>BP解决异或问题</title>
    <url>/2020/07/09/BP%E8%A7%A3%E5%86%B3%E5%BC%82%E6%88%96%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>BP解决异或代码简单演示</p>
<a id="more"></a>


<p>BP(Back Propagation)神经网络<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line">V = np.random.random((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">W = np.random.random((<span class="number">4</span>,<span class="number">1</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">print(V)</span><br><span class="line">print(W)</span><br><span class="line">lr = <span class="number">0.11</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dsigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> X,Y,W,V,lr</span><br><span class="line">    L1 = sigmoid(np.dot(X,V))</span><br><span class="line">    L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">    L2_delta = (Y.T-L2)*dsigmoid(L2)</span><br><span class="line">    L1_delta = L2_delta.dot(W.T)*dsigmoid(L1)</span><br><span class="line">    W_C=lr*L1.T.dot(L2_delta)</span><br><span class="line">    V_C=lr*X.T.dot(L1_delta)</span><br><span class="line">    W=W+W_C</span><br><span class="line">    V=V+V_C</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0.34261592  0.95360188  0.04334655 -0.11015956]
 [ 0.12224398 -0.95540579  0.276879   -0.46803064]
 [-0.27212006  0.92409323  0.67230364 -0.63426355]]
[[ 0.6872099 ]
 [-0.05683569]
 [ 0.53593031]
 [ 0.6657038 ]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    update()</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">500</span>==<span class="number">0</span>:</span><br><span class="line">        L1 = sigmoid(np.dot(X,V))</span><br><span class="line">        L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">        print(<span class="string">'Error:'</span>, np.mean(np.abs(Y.T-L2)))</span><br><span class="line"></span><br><span class="line">L1 = sigmoid(np.dot(X,V))</span><br><span class="line">L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">print(L2)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">judge</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x&gt;=<span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> map(judge,L2):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>

<pre><code>Error: 0.5001898525337108
Error: 0.4934029320599436
Error: 0.476950391452333
Error: 0.43144945106972044
Error: 0.3591844985087166
Error: 0.2441743332168715
Error: 0.15809286523556107
Error: 0.11588525072232857
Error: 0.09306039789777351
Error: 0.0789263390463002
Error: 0.06928189244260918
Error: 0.06224125874868228
Error: 0.05684589474475504
Error: 0.05255899358953674
Error: 0.04905658260975497
Error: 0.04613132103488044
Error: 0.04364412894489313
Error: 0.04149806879631519
Error: 0.0396233922893207
Error: 0.03796855199862452
Error: 0.036494571680927006
Error: 0.03517139424513
Error: 0.033975439770446966
Error: 0.032887928957447604
Error: 0.031893705073012604
Error: 0.030980388990824857
Error: 0.030137761945244118
Error: 0.02935730716518244
Error: 0.028631864414259807
Error: 0.027955366108362894
Error: 0.027322633269370986
Error: 0.02672921597533191
Error: 0.026171267318935168
Error: 0.025645442893166143
Error: 0.025148819932565722
Error: 0.02467883173915275
Error: 0.024233214103394567
Error: 0.02380996121912868
Error: 0.02340728917277522
Error: 0.02302360552036342
[[0.01509335]
 [0.97846218]
 [0.97414206]
 [0.02814369]]
0
1
1
0</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>KNN</title>
    <url>/2020/07/10/KNN/</url>
    <content><![CDATA[<p>KNN代码简单演示</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line">x_data = np.array([[<span class="number">3</span>,<span class="number">104</span>],[<span class="number">2</span>,<span class="number">100</span>],[<span class="number">1</span>,<span class="number">81</span>],[<span class="number">101</span>,<span class="number">10</span>],[<span class="number">99</span>,<span class="number">5</span>],[<span class="number">81</span>,<span class="number">2</span>]])</span><br><span class="line">y_data = np.array([<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>])</span><br><span class="line">x_test = np.array([<span class="number">18</span>,<span class="number">90</span>])</span><br><span class="line"></span><br><span class="line">x_data_size = x_data.shape[<span class="number">0</span>]</span><br><span class="line">x_data_size</span><br></pre></td></tr></table></figure>




<pre><code>6</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.tile(x_test,(x_data_size,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[18, 90],
       [18, 90],
       [18, 90],
       [18, 90],
       [18, 90],
       [18, 90]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">diffMat = np.tile(x_test,(x_data_size,<span class="number">1</span>)) - x_data</span><br><span class="line">diffMat</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 15, -14],
       [ 16, -10],
       [ 17,   9],
       [-83,  80],
       [-81,  85],
       [-63,  88]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">sqDiffMat</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 225,  196],
       [ 256,  100],
       [ 289,   81],
       [6889, 6400],
       [6561, 7225],
       [3969, 7744]], dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">sqDistances</span><br></pre></td></tr></table></figure>




<pre><code>array([  421,   356,   370, 13289, 13786, 11713], dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distance = sqDistances**<span class="number">0.5</span></span><br><span class="line">distance</span><br></pre></td></tr></table></figure>




<pre><code>array([ 20.51828453,  18.86796226,  19.23538406, 115.27792503,
       117.41379817, 108.2266141 ])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">sortedDistances = distance.argsort()</span><br><span class="line">sortedDistances</span><br></pre></td></tr></table></figure>




<pre><code>array([1, 2, 0, 5, 3, 4], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classCount = &#123;&#125;</span><br><span class="line">k = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    votelabel = y_data[sortedDistances[i]]</span><br><span class="line">    classCount[votelabel] = classCount.get(votelabel,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">classCount</span><br></pre></td></tr></table></figure>




<pre><code>{&apos;A&apos;: 3, &apos;B&apos;: 2}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">classCount.items(),sortedClassCount</span><br></pre></td></tr></table></figure>




<pre><code>(dict_items([(&apos;A&apos;, 3), (&apos;B&apos;, 2)]), [(&apos;A&apos;, 3), (&apos;B&apos;, 2)])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knnclass = sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">knnclass</span><br></pre></td></tr></table></figure>




<pre><code>&apos;A&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report,confusion_matrix</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(x_test,x_data,y_data,k)</span>:</span></span><br><span class="line">    x_data_size = x_data.shape[<span class="number">0</span>]</span><br><span class="line">    np.tile(x_test,(x_data_size,<span class="number">1</span>))</span><br><span class="line">    diffMat = np.tile(x_test,(x_data_size,<span class="number">1</span>))-x_data</span><br><span class="line">    sqDiffMat=diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances=sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances=sqDistances**<span class="number">0.5</span></span><br><span class="line">    sortedDistances=distances.argsort()</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        votelabel = y_data[sortedDistances[i]]</span><br><span class="line">        classCount[votelabel]=classCount.get(votelabel,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">data_size=iris.data.shape[<span class="number">0</span>]</span><br><span class="line">index = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(data_size)]</span><br><span class="line">random.shuffle(index)</span><br><span class="line">iris.data = iris.data[index]</span><br><span class="line">iris.target = iris.target[index]</span><br><span class="line"></span><br><span class="line">test_size = <span class="number">40</span></span><br><span class="line">x_train = iris.data[test_size:]</span><br><span class="line">x_test = iris.data[:test_size]</span><br><span class="line">y_train = iris.target[test_size:]</span><br><span class="line">y_test = iris.target[:test_size]</span><br><span class="line"></span><br><span class="line">predictions = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x_test.shape[<span class="number">0</span>]):</span><br><span class="line">    predictions.append(knn(x_test[i],x_train,y_train,<span class="number">5</span>))</span><br><span class="line">    </span><br><span class="line">print(classification_report(y_test,predictions))</span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

           0       1.00      1.00      1.00        13
           1       1.00      1.00      1.00        11
           2       1.00      1.00      1.00        16

    accuracy                           1.00        40
   macro avg       1.00      1.00      1.00        40
weighted avg       1.00      1.00      1.00        40</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(confusion_matrix(y_test,predictions))</span><br></pre></td></tr></table></figure>

<pre><code>[[13  0  0]
 [ 0 11  0]
 [ 0  0 16]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>最近邻算法</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2020/07/11/PCA/</url>
    <content><![CDATA[<p>PCA简单代码展示:</p>
<a id="more"></a>

<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA7.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA9.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA10.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report,confusion_matrix</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">digits = load_digits()</span><br><span class="line">x_data = digits.data</span><br><span class="line">y_data = digits.target</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data.shape</span><br></pre></td></tr></table></figure>




<pre><code>(1797, 64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">100</span>,<span class="number">50</span>), max_iter=<span class="number">500</span>)</span><br><span class="line">mlp.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>MLPClassifier(activation=&apos;relu&apos;, alpha=0.0001, batch_size=&apos;auto&apos;, beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100, 50), learning_rate=&apos;constant&apos;,
              learning_rate_init=0.001, max_fun=15000, max_iter=500,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=None, shuffle=True, solver=&apos;adam&apos;,
              tol=0.0001, validation_fraction=0.1, verbose=False,
              warm_start=False)</code></pre><p>协方差代码形式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">函数原型：def cov(m, y&#x3D;None, rowvar&#x3D;True, bias&#x3D;False, ddof&#x3D;None, fweights&#x3D;None,aweights&#x3D;None)</span><br><span class="line">m:一维或则二维的数组，默认情况下每一行代表一个变量（属性），每一列代表一个观测</span><br><span class="line">y:与m具有一样的形式的一组数据</span><br><span class="line">rowvar:默认为True,此时每一行代表一个变量（属性），每一列代表一个观测；为False时，则反之</span><br><span class="line">bias:默认为False,此时标准化时除以n-1；反之为n。其中n为观测数</span><br><span class="line">ddof:类型是int，当其值非None时，bias参数作用将失效。当ddof&#x3D;1时，将会返回无偏估计（除以n-1），即使指定了fweights和aweights参数；当ddof&#x3D;0时，则返回简单平均值。</span><br><span class="line">frequency weights:一维数组，代表每个观测要重复的次数（相当于给观测赋予权重）</span><br><span class="line">analytic weights:一维数组，代表观测矢量权重。对于被认为“重要”的观察,这些相对权重通常很大,而对于被认为不太重要的观察,这些相对权重较小。如果ddof &#x3D; 0,则可以使用权重数组将概率分配给观测向量。</span><br></pre></td></tr></table></figure>
<p>代码示例<br>基本使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 计算协方差的时候，一行代表一个特征</span><br><span class="line"># 下面计算cov(T, S, M)</span><br><span class="line">T &#x3D; np.array([9, 15, 25, 14, 10, 18, 0, 16, 5, 19, 16, 20])</span><br><span class="line">S &#x3D; np.array([39, 56, 93, 61, 50, 75, 32, 85, 42, 70, 66, 80])</span><br><span class="line">M &#x3D; np.asarray([38, 56, 90, 63, 56, 77, 30, 80, 41, 79, 64, 88])</span><br><span class="line">X &#x3D; np.vstack((T, S, M))</span><br><span class="line"># X每行代表一个属性</span><br><span class="line">#  每列代表一个示例，或者说观测</span><br><span class="line">print(np.cov(X))</span><br><span class="line"></span><br><span class="line"># [[ 47.71969697 122.9469697  129.59090909]</span><br><span class="line">#  [122.9469697  370.08333333 374.59090909]</span><br><span class="line">#  [129.59090909 374.59090909 399.        ]]</span><br></pre></td></tr></table></figure>
<p>重点：协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间。拿到一个样本矩阵，首先要明确的就是行代表什么，列代表什么。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zeroMean</span><span class="params">(dataMat)</span>:</span></span><br><span class="line">    meanVal = np.mean(dataMat, axis=<span class="number">0</span>)</span><br><span class="line">    newData = dataMat - meanVal</span><br><span class="line">    <span class="keyword">return</span> newData, meanVal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(dataMat,top)</span>:</span></span><br><span class="line">    newData, meanVal = zeroMean(dataMat)</span><br><span class="line">    covMat = np.cov(newData,rowvar=<span class="number">0</span>) <span class="comment"># 求样本的协方差矩阵。</span></span><br><span class="line">    eigVals,eigVects = np.linalg.eig(np.mat(covMat)) <span class="comment"># 对协方差1m 𝑋𝑋𝑇矩阵做特征值分解。</span></span><br><span class="line">    eigValIndice = np.argsort(eigVals)</span><br><span class="line">    n_eigValIndice = eigValIndice[<span class="number">-1</span>:-(top+<span class="number">1</span>):<span class="number">-1</span>] <span class="comment"># 选出最大的k个特征值对应的k个特征向量。</span></span><br><span class="line">    n_eigVect = eigVects[:,n_eigValIndice]</span><br><span class="line">    lowDDataMat = newData*n_eigVect <span class="comment"># 将原始数据投影到选取的特征向量上。</span></span><br><span class="line">    reconMat = (lowDDataMat*n_eigVect.T)+meanVal</span><br><span class="line">    <span class="keyword">return</span> lowDDataMat,reconMat</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowDDataMat,reconMat = pca(x_data,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x,y,c=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/PCA/output_8_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = mlp.predict(x_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x,y,c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/PCA/output_10_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowDDataMat,reconMat = pca(x_data,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">z = np.array(lowDDataMat)[:,<span class="number">2</span>]</span><br><span class="line">ax = plt.figure().add_subplot(<span class="number">111</span>,projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(x,y,z,c=y_data,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/PCA/output_12_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>主成分分析</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo开发博客</title>
    <url>/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">从0搭建</a></li>
<li>2.<a href="#header2">遇到的坑</a></li>
<li>3.<a href="#header3">图片放置问题</a></li>
<li>4.<a href="#header4">更换主题</a></li>
<li>5.<a href="#header5">支持Latex</a></li>
</ul>
<a id="more"></a>
<h1 id="视频手把手教你从0开始搭建自己的个人博客hexo参考"><a href="#视频手把手教你从0开始搭建自己的个人博客hexo参考" class="headerlink" title="视频手把手教你从0开始搭建自己的个人博客hexo参考"></a><span id="header1">视频手把手教你从0开始搭建自己的个人博客hexo</span><a href="https://www.bilibili.com/video/BV1Yb411a7ty/?spm_id_from=333.788.videocard.4" target="_blank" rel="noopener">参考</a></h1><ul>
<li>安装node<br><a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">node官网</a>可以直接下载node,可以换源</li>
<li>npm换源(可选)<br>默认的npm源 —— <a href="https://registry.npmjs.org" target="_blank" rel="noopener">https://registry.npmjs.org</a> 比较慢,可以换成淘宝镜像 —— <a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure>
换回来同理<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npmjs.org</span><br></pre></td></tr></table></figure></li>
<li>全局安装hexo-cli(前提是安装了node,可以用npm命令)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm i -g hexo-cli</span><br></pre></td></tr></table></figure></li>
<li>初始化<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo init (文件夹名)</span><br></pre></td></tr></table></figure>
不加文件夹名就是在当前文件夹下</li>
<li><p>新建一篇文章</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo n 文章名</span><br><span class="line">或者</span><br><span class="line">hexo new 文章名</span><br></pre></td></tr></table></figure>
<p>文章内容使用<a href="https://githubzhangshuai.github.io/2020/07/08/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/" target="_blank" rel="noopener">Markdown</a>语法进行书写即可</p>
</li>
<li><p>在线预览</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server</span><br><span class="line">或者</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>浏览器窗口输入localhost:4000即可预览</p>
</li>
<li><p>部署</p>
<ul>
<li>先手动安装hexo-deployer-git<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm i --save hexo-deployer-git</span><br></pre></td></tr></table></figure></li>
<li>然后github新建仓库,仓库名为xxx.github.io,xxx为github账号名,如图,我的xxx为GitHubzhangshuai故仓库名为GitHubzhangshuai.github.io.git<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/hexo%E6%88%AA%E5%9B%BE.png" alt="示例图片"></li>
<li>修改配置文件<br>找到_config.yml文件,修改deploy部分(100行左右)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;deployment.html</span><br><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo: https:&#x2F;&#x2F;github.com&#x2F;GitHubzhangshuai&#x2F;GitHubzhangshuai.github.io.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure></li>
<li>部署到github上<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br><span class="line">或者</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></li>
<li>访问<br>浏览器地址栏输入xxx.github.io</li>
</ul>
</li>
</ul>
<h1 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a><span id="header2">遇到的坑</span></h1><ul>
<li>npm安装失败<br>检查网络问题并且换淘宝源<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li>
<li>部署时出现错误,<a href="https://blog.csdn.net/HTL2018/article/details/106876940?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">参考</a></li>
</ul>
<ul>
<li>1.错误:<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">error：spawn failed...</span><br><span class="line">或者:</span><br><span class="line">fatal: cannot lock ref &#39;HEAD&#39;: unable to resolve reference HEAD: Invalid argument error: src refspec</span><br><span class="line">或者:</span><br><span class="line">error: src refspec HEAD does not match any.</span><br></pre></td></tr></table></figure></li>
<li><ol>
<li>总结一下<br>问题大多是因为git进行push或者hexo d的时候改变了一些.deploy_git文件下的内容。</li>
</ol>
</li>
<li><ol>
<li>解决办法<ul>
<li>3-1.删除.deploy_git文件夹;</li>
<li>3-2.全局设置git的core.autocrlf为false;<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入git config --global core.autocrlf false</span><br></pre></td></tr></table></figure></li>
<li>3-2.然后，依次执行：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h1 id="hexo图片问题"><a href="#hexo图片问题" class="headerlink" title="hexo图片问题"></a><span id="header3">hexo图片问题</span></h1><ul>
<li>1.放在OSS上,我用的阿里云的oss(记得设置bucket的公共读权限)<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/2.jpg" alt="xx"></li>
<li>2.使用base64格式</li>
<li>3.使用插件hexo-asset-image<br>安装插件<br>注意:<br>使用 npm install <a href="https://github.com/CodeFalling/hexo-asset-image" target="_blank" rel="noopener">https://github.com/CodeFalling/hexo-asset-image</a> —save 安装0.0.5版本的hexo-asset-image插件。 使用 npm install hexo-asset-image —save 安装的是1.0.0版本的hexo-asset-image插件。 两者最直接的区别是映射关系不同。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install https:&#x2F;&#x2F;github.com&#x2F;CodeFalling&#x2F;hexo-asset-image --save (有效)</span><br><span class="line">npm install hexo-asset-image --save (无效)</span><br></pre></td></tr></table></figure>
在_config.yml配置文件中(44行左右)，修改为 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>
记得重启服务器(VS code的MD预览插件下无效)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a><span id="#header4">更换主题</span></h1><p><a href="https://www.jianshu.com/p/33bc0a0a6e90" target="_blank" rel="noopener">参考</a></p>
<ul>
<li>1.下载 <a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">NexT</a> 主题,将主题克隆到 themes 目录下，以下截图就是 clone 之后的结果。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &lt;博客存放的目录&gt;</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;iissnan&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure></li>
<li>2.打开 _config.yml 文件，该文件为站点配置文件,将主题修改为 next<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">theme: landscape</span><br><span class="line">改为</span><br><span class="line">theme: next</span><br></pre></td></tr></table></figure></li>
<li>3.重启服务器发现所有的点击后边都带了%20,如图<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/3.png" alt="x"><br><a href="https://blog.csdn.net/weixin_43790779/article/details/104854588" target="_blank" rel="noopener">解决</a>:<br>找到next的配置文件themes\next_config.yml,将配置文件里 ||之前所有的空格删掉,改为以下即可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: &#x2F;||home</span><br><span class="line">  #about: &#x2F;about&#x2F;||user</span><br><span class="line">  #tags: &#x2F;tags&#x2F;||tags</span><br><span class="line">  #categories: &#x2F;categories&#x2F;||th</span><br><span class="line">  archives: &#x2F;archives&#x2F;||archive</span><br><span class="line">  #schedule: &#x2F;schedule&#x2F;||calendar</span><br><span class="line">  #sitemap: &#x2F;sitemap.xml||sitemap</span><br><span class="line">  #commonweal: &#x2F;404&#x2F;||heartbeat</span><br></pre></td></tr></table></figure></li>
<li>4.设置菜单<br>菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标<ul>
<li>4.1设定菜单项的名称和链接<br>找到next的配置文件themes\next_config.yml<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">home: &#x2F;</span><br><span class="line">archives: &#x2F;archives</span><br><span class="line">about: &#x2F;about</span><br><span class="line">categories: &#x2F;categories</span><br><span class="line">tags: &#x2F;tags</span><br></pre></td></tr></table></figure></li>
<li>4.2设定菜单项的显示文本<br>在设置 菜单项的名称和链接中的名称并不会直接显示在网页上，而是会通过 NexT 主题目录下的 languages/{language}.yml 找到对应的显示文本。<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/5.jpg" alt="x"></li>
<li>4.3 设定菜单项的图标<br>对应的字段是 menu_icons。 此设定格式是 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">item name: icon name</span><br></pre></td></tr></table></figure>
，其中 item name 与上一步所配置的菜单名字对应，icon name 是 Font Awesome 图标的 名字。而 enable 可用于控制是否显示图标，你可以设置成 false 来去掉图标。</li>
</ul>
</li>
<li>5.生成子页面<br>命令行输入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new page xxx</span><br></pre></td></tr></table></figure>
然后在source里会生成和_posts同级的文件夹xxx,进入xxx编辑index.md,修改成以下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2020-07-09 00:58:42</span><br><span class="line">type: &quot;xxx&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
其中xxx可为about/categories/tags/schedule/sitemap/404</li>
<li><p>6.next选择 Scheme<br>Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是：<br>找到next的配置文件themes\next_config.yml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</span><br><span class="line">Mist - Muse 的紧凑版本，整洁有序的单栏外观</span><br><span class="line">Pisces - 双栏 Scheme，小家碧玉似的清新</span><br></pre></td></tr></table></figure>
<p>scheme: Muse<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme1.png" alt="x"><br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme1-2.png" alt="x"><br>scheme: Mist<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme2.png" alt="x"><br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme2-1.png" alt="x"><br>scheme: Pisces<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme3.png" alt="x"><br>scheme: Gemini<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme4.png" alt="x"></p>
</li>
<li><p>7.添加百度/谷歌/本地 自定义站点内容搜索<br>安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
<p>编辑 站点配置文件_config.yml，新增以下内容到任意位置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
<p>编辑 主题配置文件themes\next_config.yml，启用本地搜索功能：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Local search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>8.侧边栏社交链接<br>侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 主题配置文件themes\next_config.yml 中。</p>
</li>
</ul>
<p>链接放置在 social 字段下，一行一个链接。其键值格式是 显示文本: 链接地址。</p>
<p>配置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Social links</span><br><span class="line">social:</span><br><span class="line">  GitHub: https:&#x2F;&#x2F;github.com&#x2F;your-user-name</span><br><span class="line">  Twitter: https:&#x2F;&#x2F;twitter.com&#x2F;your-user-name</span><br><span class="line">  微博: http:&#x2F;&#x2F;weibo.com&#x2F;your-user-name</span><br><span class="line">  豆瓣: http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;your-user-name</span><br><span class="line">  知乎: http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;your-user-name</span><br><span class="line">  # 等等</span><br></pre></td></tr></table></figure><br>设定链接的图标，对应的字段是 social_icons。其键值格式是 匹配键: Font Awesome 图标名称， 匹配键 与上一步所配置的链接的 显示文本 相同（大小写严格匹配），图标名称 是 Font Awesome 图标的名字（不必带 fa- 前缀）。 enable 选项用于控制是否显示图标，你可以设置成 false 来去掉图标。</p>
<p>配置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Social Icons</span><br><span class="line">social_icons:</span><br><span class="line">  enable: true</span><br><span class="line">  # Icon Mappings</span><br><span class="line">  GitHub: github</span><br><span class="line">  Twitter: twitter</span><br><span class="line">  微博: weibo</span><br></pre></td></tr></table></figure></p>
<ul>
<li>9.设置代码高亮主题<br>NexT 使用 Tomorrow Theme 作为代码高亮，共有5款主题供你选择。 NexT 默认使用的是 白色的 normal 主题，可选的值有 normal，night， night blue， night bright， night eighties：</li>
</ul>
<p>themes\next_config.yml更改 highlight_theme 字段，将其值设定成你所喜爱的高亮主题，例如：</p>
<p>高亮主题设置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Code Highlight theme</span><br><span class="line"># Available value: normal | night | night eighties | night blue | night bright</span><br><span class="line"># https:&#x2F;&#x2F;github.com&#x2F;chriskempson&#x2F;tomorrow-theme</span><br><span class="line">highlight_theme: normal</span><br></pre></td></tr></table></figure></p>
<ul>
<li>10.开启打赏功能 由 habren 贡献<br>越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 主题配置文件themes\next_config.yml 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能。</li>
</ul>
<p>打赏功能配置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！</span><br><span class="line">wechatpay: &#x2F;path&#x2F;to&#x2F;wechat-reward-image</span><br><span class="line">alipay: &#x2F;path&#x2F;to&#x2F;alipay-reward-image</span><br></pre></td></tr></table></figure></p>
<ul>
<li>11.鼠标点击特效<br>小红心效果<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/click1.gif" alt><br>具体步骤如下：<br>在/themes/next/source/js/src下新建文件 clicklove.js ，接着把下面的代码拷贝粘贴到 clicklove.js 文件中<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &#39;&#39;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e&#x3D;0;e&lt;d.length;e++)d[e].alpha&lt;&#x3D;0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+&#x3D;.004,d[e].alpha-&#x3D;.013,d[e].el.style.cssText&#x3D;&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t&#x3D;&quot;function&quot;&#x3D;&#x3D;typeof e.onclick&amp;&amp;e.onclick;e.onclick&#x3D;function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a&#x3D;t.createElement(&quot;div&quot;);a.className&#x3D;&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a&#x3D;t.createElement(&quot;style&quot;);a.type&#x3D;&quot;text&#x2F;css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText&#x3D;e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d&#x3D;[];e.requestAnimationFrame&#x3D;function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3&#x2F;60)&#125;&#125;(),n()&#125;(window,document);</span><br></pre></td></tr></table></figure>
在\themes\next\layout_layout.swig文件末尾添加：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 页面点击小红心 --&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;src&#x2F;clicklove.js&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>另一种点击效果如图所示<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/click2.png" alt><br>在themes/next/source/js/src里面建一个叫fireworks.js的文件，代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;;</span><br></pre></td></tr></table></figure><br>打开themes/next/layout/_layout.swig,在&lt;/body&gt;上面写下如下代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% if theme.fireworks %&#125;</span><br><span class="line">   &lt;canvas class&#x3D;&quot;fireworks&quot; style&#x3D;&quot;position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;&quot; &gt;&lt;&#x2F;canvas&gt; </span><br><span class="line">   &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;&#x2F;cdn.bootcss.com&#x2F;animejs&#x2F;2.2.0&#x2F;anime.min.js&quot;&gt;&lt;&#x2F;script&gt; </span><br><span class="line">   &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;src&#x2F;fireworks.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><br>打开主题配置文件themes\next_config.yml，在里面最后写下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Fireworks</span><br><span class="line">fireworks: true</span><br></pre></td></tr></table></figure></p>
<ul>
<li>12.动态背景<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/bg.gif" alt><br>上面这种只是其中一种动态背景，新版的Next主题集成了该功能，只需要在主题配置themes\next_config.yml中设置如下即可，下面每个模块只设置其中一个为true，具体效果如何可自己尝试：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Canvas-nest</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-nest</span><br><span class="line">canvas_nest: # 网络背景</span><br><span class="line">  enable: true</span><br><span class="line">  onmobile: true # display on mobile or not</span><br><span class="line">  color: &#39;0,0,0&#39; # RGB values, use &#39;,&#39; to separate</span><br><span class="line">  opacity: 0.5 # the opacity of line: 0~1</span><br><span class="line">  zIndex: -1 # z-index property of the background</span><br><span class="line">  count: 150 # the number of lines</span><br><span class="line"></span><br><span class="line"># JavaScript 3D library.</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-three</span><br><span class="line"># three_waves</span><br><span class="line">three_waves: false</span><br><span class="line"># canvas_lines</span><br><span class="line">canvas_lines: false</span><br><span class="line"># canvas_sphere</span><br><span class="line">canvas_sphere: false</span><br><span class="line"></span><br><span class="line"># Canvas-ribbon</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-ribbon</span><br><span class="line"># size: The width of the ribbon.</span><br><span class="line"># alpha: The transparency of the ribbon.</span><br><span class="line"># zIndex: The display level of the ribbon.</span><br><span class="line">canvas_ribbon:</span><br><span class="line">  enable: false</span><br><span class="line">  size: 300</span><br><span class="line">  alpha: 0.6</span><br><span class="line">  zIndex: -1</span><br></pre></td></tr></table></figure>
另外需要在blog中下载相应资源包，具体见上面的链接，下面我给出canvas_nest的下载方式：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-nest themes&#x2F;next&#x2F;source&#x2F;lib&#x2F;canvas-nest</span><br></pre></td></tr></table></figure></li>
<li><p>13.文章结束标志<br>在路径 \themes\next\layout_macro 中新建 passage-end-tag.swig 文件,并添加以下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">        &lt;div style&#x3D;&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class&#x3D;&quot;fa fa-paw&quot;&gt;&lt;&#x2F;i&gt;感谢您的阅读-------------&lt;&#x2F;div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>
<p>接着打开\themes\next\layout_macro\post.swig文件，在post-body 之后(END POST BODY)， post-footer 之前添加如代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &#39;passage-end-tag.swig&#39; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>
<p>然后打开主题配置文件（_config.yml),在末尾添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 文章末尾添加“本文结束”标记</span><br><span class="line">passage_end_tag:</span><br><span class="line">  enabled: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>14.文字阴影效果<br>打开\themes\next\source\css_custom\custom.styl,向里面加入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 主页文章添加阴影效果</span><br><span class="line"> .post &#123;</span><br><span class="line">   margin-top: 60px;</span><br><span class="line">   margin-bottom: 60px;</span><br><span class="line">   padding: 25px;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>15.博客底部布局<br>对应主题配置文件themes\next_config.yml中的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  since: 2020 #建站时间</span><br><span class="line"></span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  icon:</span><br><span class="line">    # Icon name in fontawesome, see: https:&#x2F;&#x2F;fontawesome.com&#x2F;v4.7.0&#x2F;icons&#x2F;</span><br><span class="line">    # &#96;heart&#96; is recommended with animation in red (#ff0000).</span><br><span class="line">    name: heart  #作者图标（默认是author人像)</span><br><span class="line">    # If you want to animate the icon, set it to true.</span><br><span class="line">    animated: true #图标是否闪动</span><br><span class="line">    # Change the color of icon, using Hex Code.</span><br><span class="line">    color: &quot;#808080&quot; #图标颜色</span><br><span class="line"></span><br><span class="line">  # If not defined, &#96;author&#96; from Hexo main config will be used.</span><br><span class="line">  copyright: 张帅 #别填bool型，最后显示的东西是copyright || author，即左边没有设置的话就显示作者</span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  powered:</span><br><span class="line">    # Hexo link (Powered by Hexo).</span><br><span class="line">    enable: false #是否显示 Powered by hexo</span><br><span class="line">    # Version info of Hexo after Hexo link (vX.X.X).</span><br><span class="line">    version: false #是否显示Hexo版本</span><br><span class="line"></span><br><span class="line">  theme:</span><br><span class="line">    # Theme &amp; scheme info link (Theme - NexT.scheme).</span><br><span class="line">    enable: false #是否显示主题信息</span><br><span class="line">    # Version info of NexT after scheme info (vX.X.X).</span><br><span class="line">    version: false #是否显示主题版本</span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  # Beian icp information for Chinese users. In China, every legal website should have a beian icp in website footer.</span><br><span class="line">  # http:&#x2F;&#x2F;www.miitbeian.gov.cn</span><br><span class="line">  beian:</span><br><span class="line">    enable: false #是否显示网站备案信息</span><br><span class="line">    icp:</span><br><span class="line"></span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  # Any custom text can be defined here.</span><br><span class="line">  #custom_text: Hosted by &lt;a href&#x3D;&quot;https:&#x2F;&#x2F;pages.coding.me&quot; class&#x3D;&quot;theme-link&quot; rel&#x3D;&quot;noopener&quot; target&#x3D;&quot;_blank&quot;&gt;Coding Pages&lt;&#x2F;a&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>16.添加页面宠物<br>首先在博客目录下执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -save hexo-helper-live2d</span><br></pre></td></tr></table></figure>
<p>然后在站点配置文件中加入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  scriptFrom: local</span><br><span class="line">  pluginRootPath: live2dw&#x2F;</span><br><span class="line">  pluginJsPath: lib&#x2F;</span><br><span class="line">  pluginModelPath: assets&#x2F;</span><br><span class="line">  tagMode: false</span><br><span class="line">  model:</span><br><span class="line">    use: live2d-widget-model-wanko  #选择哪种模型</span><br><span class="line">  display: #放置位置和大小</span><br><span class="line">    position: right</span><br><span class="line">    width: 150</span><br><span class="line">    height: 300</span><br><span class="line">  mobile:</span><br><span class="line">    show: false #是否在手机端显示</span><br></pre></td></tr></table></figure>
<p>上面模型的选择可在lived2d中选择，并下载相应的模型：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install live2d-widget-model-wanko</span><br></pre></td></tr></table></figure>
</li>
<li><p>17.设置博客摘要显示<br>对于摘要显示，首先我们需要开启摘要功能，修改主题配置文件themes\next_config.yml：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Automatically scroll page to section which is under &lt;!-- more --&gt; mark.</span><br><span class="line">scroll_to_more: true #选取博客正文&lt;!--more--&gt;前的内容</span><br><span class="line"></span><br><span class="line"># Automatically saving scroll position on each post&#x2F;page in cookies.</span><br><span class="line">save_scroll: false</span><br><span class="line"></span><br><span class="line"># Automatically excerpt description in homepage as preamble text.</span><br><span class="line">excerpt_description: true #自动截取摘要</span><br><span class="line"></span><br><span class="line"># Automatically Excerpt. Not recommend.</span><br><span class="line"># Use &lt;!-- more --&gt; in the post to control excerpt accurately.</span><br><span class="line">auto_excerpt: </span><br><span class="line">  enable: false #自动截取一定程度的摘要</span><br><span class="line">  length: 150</span><br><span class="line"></span><br><span class="line"># Read more button</span><br><span class="line"># If true, the read more button would be displayed in excerpt section.</span><br><span class="line">read_more_btn: true #显示阅读全文按钮</span><br></pre></td></tr></table></figure>
</li>
<li><p>18.设置RSS订阅<br>在博客主目录下执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure>
<p>在站点配置文件中修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Extensions</span><br><span class="line">## Plugins: http:&#x2F;&#x2F;hexo.io&#x2F;plugins&#x2F;</span><br><span class="line">plugins: hexo-generate-feed</span><br></pre></td></tr></table></figure>
<p>然后设置主题配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set rss to false to disable feed link.</span><br><span class="line"># Leave rss as empty to use site&#39;s feed link.</span><br><span class="line"># Set rss to specific value if you have burned your feed already.</span><br><span class="line">rss: &#x2F;atom.xml</span><br></pre></td></tr></table></figure>
</li>
<li><p>19.修改文章链接样式<br>修改文件 themes\next\source\css_common\components\post\post.styl，在末尾添加如下css样式，：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 文章内链接文本样式</span><br><span class="line">.post-body p a&#123;</span><br><span class="line">  color: #0593d3;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  border-bottom: 1px solid #0593d3;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #fc6423;</span><br><span class="line">    border-bottom: none;</span><br><span class="line">    border-bottom: 1px solid #fc6423;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="支持Latex"><a href="#支持Latex" class="headerlink" title="支持Latex"></a><span id="header5">支持Latex</span></h1><h2 id="第一步：-安装Kramed"><a href="#第一步：-安装Kramed" class="headerlink" title="第一步： 安装Kramed"></a>第一步： 安装Kramed</h2><p>更换Hexo的默认的hexo-renderer-marked渲染引擎，改为hexo-renderer-kramed。在终端输入命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p>
<h2 id="第二步：更改文件配置"><a href="#第二步：更改文件配置" class="headerlink" title="第二步：更改文件配置"></a>第二步：更改文件配置</h2><p>打开/node_modules/hexo-renderer-kramed/lib/renderer.js,更改：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function (text) &#123;</span><br><span class="line">    &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + 1 + $$</span><br><span class="line">    return text.replace(&#x2F;&#96;$(.*?)$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>为，直接返回text<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function (text) &#123;</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="第三步-停止使用-hexo-math，并安装mathjax包"><a href="#第三步-停止使用-hexo-math，并安装mathjax包" class="headerlink" title="第三步: 停止使用 hexo-math，并安装mathjax包"></a>第三步: 停止使用 hexo-math，并安装mathjax包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure>
<h2 id="第四步-更新-Mathjax-的-配置文件"><a href="#第四步-更新-Mathjax-的-配置文件" class="headerlink" title="第四步: 更新 Mathjax 的 配置文件"></a>第四步: 更新 Mathjax 的 配置文件</h2><p>打开/node_modules/hexo-renderer-mathjax/mathjax.html ，注释掉第二个<code>&lt;script&gt;</code></p>
<h2 id="第五步-更改默认转义规则"><a href="#第五步-更改默认转义规则" class="headerlink" title="第五步: 更改默认转义规则"></a>第五步: 更改默认转义规则</h2><p>打开/node_modules/kramed/lib/rules/inline.js<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">escape: &#x2F;^\([\&#96;*&#123;&#125;[]()#$+-.!_&gt;])&#x2F;,</span><br><span class="line">更改为</span><br><span class="line">escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">    </span><br><span class="line">em: &#x2F;^b_((?:__|[sS])+?)_b|^*((?:**|[sS])+?)*(?!*)&#x2F;,</span><br><span class="line">更改为</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure></p>
<h2 id="第六步-开启mathjax"><a href="#第六步-开启mathjax" class="headerlink" title="第六步: 开启mathjax"></a>第六步: 开启mathjax</h2><p>打开你所使用主题的_config.yml文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure></p>
<h2 id="最后的最后"><a href="#最后的最后" class="headerlink" title="最后的最后"></a>最后的最后</h2><p>在每个文章的开头添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown基础语法</title>
    <url>/2020/07/08/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul>
<li>1.<a href="#header1">标题</a></li>
<li>2.<a href="#header2">字体</a></li>
<li>3.<a href="#header3">图片</a></li>
<li>4.<a href="#header4">超链接</a></li>
<li>5.<a href="#header5">锚点</a></li>
<li>6.<a href="#header6">引用</a></li>
<li>7.<a href="#header7">代码</a></li>
<li>8.<a href="#header8">列表</a></li>
<li>9.<a href="#header9">分割线</a></li>
<li>10.<a href="#header10">表格</a></li>
<li>11.<a href="#header11">换行</a></li>
<li>12.<a href="#header12">流程图</a></li>
</ul>
<a id="more"></a>

<h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a><span id="header1">标题</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 标题1</span><br><span class="line">## 标题2</span><br></pre></td></tr></table></figure>
<p>效果</p>
<h1 id="标题1"><a href="#标题1" class="headerlink" title="标题1"></a>标题1</h1><h2 id="标题2"><a href="#标题2" class="headerlink" title="标题2"></a>标题2</h2><h1 id="字体"><a href="#字体" class="headerlink" title="字体"></a><span id="header2">字体</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**这是加粗的文字**</span><br><span class="line">*这是倾斜的文字*&#96;</span><br><span class="line">***这是斜体加粗的文字***</span><br><span class="line">~~这是加删除线的文字~~</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><strong>这是加粗的文字</strong><br><em>这是倾斜的文字</em>`<br><strong><em>这是斜体加粗的文字</em></strong><br><del>这是加删除线的文字</del></p>
<h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a><span id="header3">图片</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![我是ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif)</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><img src="https://www.baidu.com/img/dong_54209c0ff3da32eecc31f340c08a18f6.gif" alt="我是ALT"></p>
<h1 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a><span id="header4">超链接</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[我是百度](www.baidu.com)</span><br><span class="line">[![我是ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif)](![我是ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif))</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><a href="http://www.baidu.com" target="_blank" rel="noopener">点我或者图片均可跳转到百度</a><br><a href="http://www.baidu.com" target="_blank" rel="noopener"><img src="https://www.baidu.com/img/dong_54209c0ff3da32eecc31f340c08a18f6.gif" alt="我是ALT"></a></p>
<h1 id="锚点"><a href="#锚点" class="headerlink" title="锚点"></a><span id="header5">锚点</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[点我跳转](#mao)</span><br><span class="line">&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</span><br><span class="line">&lt;span id&#x3D;&quot;ma0&quot;&gt;跳到了我&lt;&#x2F;span&gt;</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><a href="#mao">点我跳转</a><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><span id="mao">跳到了我</span></p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a><span id="header6">引用</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;我是引用1</span><br><span class="line">&gt;&gt;我是引用2</span><br><span class="line">&gt;&gt;&gt;我是引用3</span><br><span class="line">&gt;&gt;&gt;&gt;我是引用4</span><br></pre></td></tr></table></figure>
<p>效果</p>
<blockquote>
<p>我是引用1</p>
<blockquote>
<p>我是引用2</p>
<blockquote>
<p>我是引用3</p>
<blockquote>
<p>我是引用4</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a><span id="header7">代码</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#96;&#96;&#96;</span><br><span class="line">console.log(1)</span><br><span class="line">&#96;&#96;&#96;</span><br></pre></td></tr></table></figure>
<p>效果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">console.log(1)</span><br></pre></td></tr></table></figure>

<h1 id="列表"><a href="#列表" class="headerlink" title="列表"></a><span id="header8">列表</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* 无序列表1</span><br><span class="line">    * 无序列表1-1</span><br><span class="line">    * 无序列表1-2</span><br><span class="line">    * 无序列表1-3</span><br><span class="line">* 无序列表2</span><br><span class="line">* 无序列表3</span><br><span class="line">* 无序列表4</span><br><span class="line">* 无序列表5</span><br><span class="line">+ 无序列表6</span><br><span class="line">    + 无序列表6-1</span><br><span class="line">        + 无序列表6-1-1</span><br><span class="line">            + 无序列表6-1-1-1</span><br><span class="line">                + 无序列表6-1-1-1-1</span><br><span class="line">+ 无序列表7</span><br><span class="line">- 无序列表8</span><br><span class="line">- 无序列表9</span><br><span class="line">    * 无序列表9-1</span><br><span class="line">    * 无序列表9-2</span><br><span class="line">    * 无序列表9-3</span><br><span class="line">    * 无序列表9-4</span><br><span class="line">- 无序列表10</span><br><span class="line">- 1.有序列表</span><br><span class="line">    - 1.1有序列表</span><br><span class="line">    - 1.2有序列表</span><br><span class="line">    - 1.3有序列表</span><br><span class="line">- 2.有序列表</span><br><span class="line">- 3.有序列表</span><br><span class="line">- 4.有序列表</span><br><span class="line">- 5.有序列表</span><br></pre></td></tr></table></figure>
<p>效果</p>
<ul>
<li>无序列表1<ul>
<li>无序列表1-1</li>
<li>无序列表1-2</li>
<li>无序列表1-3</li>
</ul>
</li>
<li>无序列表2</li>
<li>无序列表3</li>
<li>无序列表4</li>
<li>无序列表5</li>
</ul>
<ul>
<li>无序列表6<ul>
<li>无序列表6-1<ul>
<li>无序列表6-1-1<ul>
<li>无序列表6-1-1-1<ul>
<li>无序列表6-1-1-1-1</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>无序列表7</li>
</ul>
<ul>
<li>无序列表8</li>
<li>无序列表9<ul>
<li>无序列表9-1</li>
<li>无序列表9-2</li>
<li>无序列表9-3</li>
<li>无序列表9-4</li>
</ul>
</li>
<li>无序列表10</li>
<li>1.有序列表<ul>
<li>1.1有序列表</li>
<li>1.2有序列表</li>
<li>1.3有序列表</li>
</ul>
</li>
<li>2.有序列表</li>
<li>3.有序列表</li>
<li>4.有序列表</li>
<li>5.有序列表</li>
</ul>
<h1 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a><span id="header9">分割线</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">****</span><br><span class="line">*****</span><br><span class="line">******</span><br><span class="line">----</span><br><span class="line">-----</span><br><span class="line">------</span><br></pre></td></tr></table></figure>
<p>效果</p>
<hr>
<hr>
<hr>
<hr>
<hr>
<hr>
<h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a><span id="header10">表格</span></h1><ul>
<li>第二行分割表头和内容。</li>
<li>- 有一个就行，为了对齐，多加了几个</li>
<li>文字默认居左</li>
<li>-两边加：表示文字居中</li>
<li>-右边加：表示文字居右</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">姓名|性别|年龄|战斗力</span><br><span class="line">:-:|:-|-:|-</span><br><span class="line">张飞123|男boy|100|2001</span><br><span class="line">刘备45|男girl|10|101</span><br><span class="line">关羽67890|男啊啊啊|1000|5001</span><br></pre></td></tr></table></figure>
<p>效果</p>
<table>
<thead>
<tr>
<th align="center">姓名</th>
<th align="left">性别</th>
<th align="right">年龄</th>
<th>战斗力</th>
</tr>
</thead>
<tbody><tr>
<td align="center">张飞123</td>
<td align="left">男boy</td>
<td align="right">100</td>
<td>2001</td>
</tr>
<tr>
<td align="center">刘备45</td>
<td align="left">男girl</td>
<td align="right">10</td>
<td>101</td>
</tr>
<tr>
<td align="center">关羽67890</td>
<td align="left">男啊啊啊</td>
<td align="right">1000</td>
<td>5001</td>
</tr>
</tbody></table>
<h1 id="换行"><a href="#换行" class="headerlink" title="换行"></a><span id="header11">换行</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">abcd</span><br><span class="line">efg</span><br><span class="line">&lt;br&gt;</span><br><span class="line">abcd</span><br><span class="line"></span><br><span class="line">efg</span><br><span class="line">&lt;br&gt;</span><br><span class="line">abcd&lt;br&gt;</span><br><span class="line">efg</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p>abcd<br>efg<br><br><br>abcd</p>
<p>efg<br><br><br>abcd<br><br>efg</p>
<h1 id="流程图-部分网站如github不支持-VS-code的MD预览插件不支持"><a href="#流程图-部分网站如github不支持-VS-code的MD预览插件不支持" class="headerlink" title="流程图(部分网站如github不支持,VS code的MD预览插件不支持)"></a><span id="header12">流程图(部分网站如github不支持,VS code的MD预览插件不支持)</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flow</span><br><span class="line">st&#x3D;&gt;start: Start</span><br><span class="line">op&#x3D;&gt;operation: Your Operation</span><br><span class="line">cond&#x3D;&gt;condition: Yes or No?</span><br><span class="line">e&#x3D;&gt;end</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>


<p>flow<br>st=&gt;start: Start<br>op=&gt;operation: Your Operation<br>cond=&gt;condition: Yes or No?<br>e=&gt;end<br>st-&gt;op-&gt;cond<br>cond(yes)-&gt;e<br>cond(no)-&gt;op</p>
]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理1</title>
    <url>/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">改变颜色空间</a></li>
<li>2.<a href="#header2">图像几何变换</a></li>
<li>3.<a href="#header3">图像阈值</a></li>
<li>4.<a href="#header4">图像平滑</a><a id="more"></a>

</li>
</ul>
<h1 id="改变颜色空间"><a href="#改变颜色空间" class="headerlink" title="改变颜色空间"></a><span id="header1">改变颜色空间</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>在本教程中，你将学习如何将图像从一个色彩空间转换到另一个，像BGR↔灰色，BGR↔HSV等</li>
<li>除此之外，我们还将创建一个应用程序，以提取视频中的彩色对象</li>
<li>你将学习以下功能：cv.cvtColor，<strong>cv.inRange</strong>等。</li>
</ul>
<h2 id="改变颜色空间-1"><a href="#改变颜色空间-1" class="headerlink" title="改变颜色空间"></a>改变颜色空间</h2><p>OpenCV中有超过150种颜色空间转换方法。但是我们将研究只有两个最广泛使用的,BGR↔灰色和BGR↔HSV。</p>
<p>对于颜色转换，我们使用cv函数。cvtColor(input_image, flag)，其中flag决定转换的类型。</p>
<p>对于BGR→灰度转换，我们使用标志cv.COLOR_BGR2GRAY。类似地，对于BGR→HSV，我们使用标志cv.COLOR_BGR2HSV。</p>
<p>要获取其他标记，只需在Python终端中运行以下命令</p>
<p>注意 HSV的色相范围为[0,179]，饱和度范围为[0,255]，值范围为[0,255]。不同的软件使用不同的规模。</p>
<p>因此，如果你要将OpenCV值和它们比较，你需要将这些范围标准化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">flags = [i <span class="keyword">for</span> i <span class="keyword">in</span> dir(cv) <span class="keyword">if</span> i.startswith(<span class="string">'COLOR_'</span>)]</span><br><span class="line">flags</span><br></pre></td></tr></table></figure>




<pre><code>[&apos;COLOR_BAYER_BG2BGR&apos;,
 &apos;COLOR_BAYER_BG2BGRA&apos;,
 &apos;COLOR_BAYER_BG2BGR_EA&apos;,
 &apos;COLOR_BAYER_BG2BGR_VNG&apos;,
 &apos;COLOR_BAYER_BG2GRAY&apos;,
 &apos;COLOR_BAYER_BG2RGB&apos;,
 &apos;COLOR_BAYER_BG2RGBA&apos;,
 &apos;COLOR_BAYER_BG2RGB_EA&apos;,
 &apos;COLOR_BAYER_BG2RGB_VNG&apos;,
 &apos;COLOR_BAYER_GB2BGR&apos;,
 &apos;COLOR_BAYER_GB2BGRA&apos;,
 &apos;COLOR_BAYER_GB2BGR_EA&apos;,
 &apos;COLOR_BAYER_GB2BGR_VNG&apos;,
 &apos;COLOR_BAYER_GB2GRAY&apos;,
 &apos;COLOR_BAYER_GB2RGB&apos;,
 &apos;COLOR_BAYER_GB2RGBA&apos;,
 &apos;COLOR_BAYER_GB2RGB_EA&apos;,
 &apos;COLOR_BAYER_GB2RGB_VNG&apos;,
 &apos;COLOR_BAYER_GR2BGR&apos;,
 &apos;COLOR_BAYER_GR2BGRA&apos;,
 &apos;COLOR_BAYER_GR2BGR_EA&apos;,
 &apos;COLOR_BAYER_GR2BGR_VNG&apos;,
 &apos;COLOR_BAYER_GR2GRAY&apos;,
 &apos;COLOR_BAYER_GR2RGB&apos;,
 &apos;COLOR_BAYER_GR2RGBA&apos;,
 &apos;COLOR_BAYER_GR2RGB_EA&apos;,
 &apos;COLOR_BAYER_GR2RGB_VNG&apos;,
 &apos;COLOR_BAYER_RG2BGR&apos;,
 &apos;COLOR_BAYER_RG2BGRA&apos;,
 &apos;COLOR_BAYER_RG2BGR_EA&apos;,
 &apos;COLOR_BAYER_RG2BGR_VNG&apos;,
 &apos;COLOR_BAYER_RG2GRAY&apos;,
 &apos;COLOR_BAYER_RG2RGB&apos;,
 &apos;COLOR_BAYER_RG2RGBA&apos;,
 &apos;COLOR_BAYER_RG2RGB_EA&apos;,
 &apos;COLOR_BAYER_RG2RGB_VNG&apos;,
 &apos;COLOR_BGR2BGR555&apos;,
 &apos;COLOR_BGR2BGR565&apos;,
 &apos;COLOR_BGR2BGRA&apos;,
 &apos;COLOR_BGR2GRAY&apos;,
 &apos;COLOR_BGR2HLS&apos;,
 &apos;COLOR_BGR2HLS_FULL&apos;,
 &apos;COLOR_BGR2HSV&apos;,
 &apos;COLOR_BGR2HSV_FULL&apos;,
 &apos;COLOR_BGR2LAB&apos;,
 &apos;COLOR_BGR2LUV&apos;,
 &apos;COLOR_BGR2Lab&apos;,
 &apos;COLOR_BGR2Luv&apos;,
 &apos;COLOR_BGR2RGB&apos;,
 &apos;COLOR_BGR2RGBA&apos;,
 &apos;COLOR_BGR2XYZ&apos;,
 &apos;COLOR_BGR2YCR_CB&apos;,
 &apos;COLOR_BGR2YCrCb&apos;,
 &apos;COLOR_BGR2YUV&apos;,
 &apos;COLOR_BGR2YUV_I420&apos;,
 &apos;COLOR_BGR2YUV_IYUV&apos;,
 &apos;COLOR_BGR2YUV_YV12&apos;,
 &apos;COLOR_BGR5552BGR&apos;,
 &apos;COLOR_BGR5552BGRA&apos;,
 &apos;COLOR_BGR5552GRAY&apos;,
 &apos;COLOR_BGR5552RGB&apos;,
 &apos;COLOR_BGR5552RGBA&apos;,
 &apos;COLOR_BGR5652BGR&apos;,
 &apos;COLOR_BGR5652BGRA&apos;,
 &apos;COLOR_BGR5652GRAY&apos;,
 &apos;COLOR_BGR5652RGB&apos;,
 &apos;COLOR_BGR5652RGBA&apos;,
 &apos;COLOR_BGRA2BGR&apos;,
 &apos;COLOR_BGRA2BGR555&apos;,
 &apos;COLOR_BGRA2BGR565&apos;,
 &apos;COLOR_BGRA2GRAY&apos;,
 &apos;COLOR_BGRA2RGB&apos;,
 &apos;COLOR_BGRA2RGBA&apos;,
 &apos;COLOR_BGRA2YUV_I420&apos;,
 &apos;COLOR_BGRA2YUV_IYUV&apos;,
 &apos;COLOR_BGRA2YUV_YV12&apos;,
 &apos;COLOR_BayerBG2BGR&apos;,
 &apos;COLOR_BayerBG2BGRA&apos;,
 &apos;COLOR_BayerBG2BGR_EA&apos;,
 &apos;COLOR_BayerBG2BGR_VNG&apos;,
 &apos;COLOR_BayerBG2GRAY&apos;,
 &apos;COLOR_BayerBG2RGB&apos;,
 &apos;COLOR_BayerBG2RGBA&apos;,
 &apos;COLOR_BayerBG2RGB_EA&apos;,
 &apos;COLOR_BayerBG2RGB_VNG&apos;,
 &apos;COLOR_BayerGB2BGR&apos;,
 &apos;COLOR_BayerGB2BGRA&apos;,
 &apos;COLOR_BayerGB2BGR_EA&apos;,
 &apos;COLOR_BayerGB2BGR_VNG&apos;,
 &apos;COLOR_BayerGB2GRAY&apos;,
 &apos;COLOR_BayerGB2RGB&apos;,
 &apos;COLOR_BayerGB2RGBA&apos;,
 &apos;COLOR_BayerGB2RGB_EA&apos;,
 &apos;COLOR_BayerGB2RGB_VNG&apos;,
 &apos;COLOR_BayerGR2BGR&apos;,
 &apos;COLOR_BayerGR2BGRA&apos;,
 &apos;COLOR_BayerGR2BGR_EA&apos;,
 &apos;COLOR_BayerGR2BGR_VNG&apos;,
 &apos;COLOR_BayerGR2GRAY&apos;,
 &apos;COLOR_BayerGR2RGB&apos;,
 &apos;COLOR_BayerGR2RGBA&apos;,
 &apos;COLOR_BayerGR2RGB_EA&apos;,
 &apos;COLOR_BayerGR2RGB_VNG&apos;,
 &apos;COLOR_BayerRG2BGR&apos;,
 &apos;COLOR_BayerRG2BGRA&apos;,
 &apos;COLOR_BayerRG2BGR_EA&apos;,
 &apos;COLOR_BayerRG2BGR_VNG&apos;,
 &apos;COLOR_BayerRG2GRAY&apos;,
 &apos;COLOR_BayerRG2RGB&apos;,
 &apos;COLOR_BayerRG2RGBA&apos;,
 &apos;COLOR_BayerRG2RGB_EA&apos;,
 &apos;COLOR_BayerRG2RGB_VNG&apos;,
 &apos;COLOR_COLORCVT_MAX&apos;,
 &apos;COLOR_GRAY2BGR&apos;,
 &apos;COLOR_GRAY2BGR555&apos;,
 &apos;COLOR_GRAY2BGR565&apos;,
 &apos;COLOR_GRAY2BGRA&apos;,
 &apos;COLOR_GRAY2RGB&apos;,
 &apos;COLOR_GRAY2RGBA&apos;,
 &apos;COLOR_HLS2BGR&apos;,
 &apos;COLOR_HLS2BGR_FULL&apos;,
 &apos;COLOR_HLS2RGB&apos;,
 &apos;COLOR_HLS2RGB_FULL&apos;,
 &apos;COLOR_HSV2BGR&apos;,
 &apos;COLOR_HSV2BGR_FULL&apos;,
 &apos;COLOR_HSV2RGB&apos;,
 &apos;COLOR_HSV2RGB_FULL&apos;,
 &apos;COLOR_LAB2BGR&apos;,
 &apos;COLOR_LAB2LBGR&apos;,
 &apos;COLOR_LAB2LRGB&apos;,
 &apos;COLOR_LAB2RGB&apos;,
 &apos;COLOR_LBGR2LAB&apos;,
 &apos;COLOR_LBGR2LUV&apos;,
 &apos;COLOR_LBGR2Lab&apos;,
 &apos;COLOR_LBGR2Luv&apos;,
 &apos;COLOR_LRGB2LAB&apos;,
 &apos;COLOR_LRGB2LUV&apos;,
 &apos;COLOR_LRGB2Lab&apos;,
 &apos;COLOR_LRGB2Luv&apos;,
 &apos;COLOR_LUV2BGR&apos;,
 &apos;COLOR_LUV2LBGR&apos;,
 &apos;COLOR_LUV2LRGB&apos;,
 &apos;COLOR_LUV2RGB&apos;,
 &apos;COLOR_Lab2BGR&apos;,
 &apos;COLOR_Lab2LBGR&apos;,
 &apos;COLOR_Lab2LRGB&apos;,
 &apos;COLOR_Lab2RGB&apos;,
 &apos;COLOR_Luv2BGR&apos;,
 &apos;COLOR_Luv2LBGR&apos;,
 &apos;COLOR_Luv2LRGB&apos;,
 &apos;COLOR_Luv2RGB&apos;,
 &apos;COLOR_M_RGBA2RGBA&apos;,
 &apos;COLOR_RGB2BGR&apos;,
 &apos;COLOR_RGB2BGR555&apos;,
 &apos;COLOR_RGB2BGR565&apos;,
 &apos;COLOR_RGB2BGRA&apos;,
 &apos;COLOR_RGB2GRAY&apos;,
 &apos;COLOR_RGB2HLS&apos;,
 &apos;COLOR_RGB2HLS_FULL&apos;,
 &apos;COLOR_RGB2HSV&apos;,
 &apos;COLOR_RGB2HSV_FULL&apos;,
 &apos;COLOR_RGB2LAB&apos;,
 &apos;COLOR_RGB2LUV&apos;,
 &apos;COLOR_RGB2Lab&apos;,
 &apos;COLOR_RGB2Luv&apos;,
 &apos;COLOR_RGB2RGBA&apos;,
 &apos;COLOR_RGB2XYZ&apos;,
 &apos;COLOR_RGB2YCR_CB&apos;,
 &apos;COLOR_RGB2YCrCb&apos;,
 &apos;COLOR_RGB2YUV&apos;,
 &apos;COLOR_RGB2YUV_I420&apos;,
 &apos;COLOR_RGB2YUV_IYUV&apos;,
 &apos;COLOR_RGB2YUV_YV12&apos;,
 &apos;COLOR_RGBA2BGR&apos;,
 &apos;COLOR_RGBA2BGR555&apos;,
 &apos;COLOR_RGBA2BGR565&apos;,
 &apos;COLOR_RGBA2BGRA&apos;,
 &apos;COLOR_RGBA2GRAY&apos;,
 &apos;COLOR_RGBA2M_RGBA&apos;,
 &apos;COLOR_RGBA2RGB&apos;,
 &apos;COLOR_RGBA2YUV_I420&apos;,
 &apos;COLOR_RGBA2YUV_IYUV&apos;,
 &apos;COLOR_RGBA2YUV_YV12&apos;,
 &apos;COLOR_RGBA2mRGBA&apos;,
 &apos;COLOR_XYZ2BGR&apos;,
 &apos;COLOR_XYZ2RGB&apos;,
 &apos;COLOR_YCR_CB2BGR&apos;,
 &apos;COLOR_YCR_CB2RGB&apos;,
 &apos;COLOR_YCrCb2BGR&apos;,
 &apos;COLOR_YCrCb2RGB&apos;,
 &apos;COLOR_YUV2BGR&apos;,
 &apos;COLOR_YUV2BGRA_I420&apos;,
 &apos;COLOR_YUV2BGRA_IYUV&apos;,
 &apos;COLOR_YUV2BGRA_NV12&apos;,
 &apos;COLOR_YUV2BGRA_NV21&apos;,
 &apos;COLOR_YUV2BGRA_UYNV&apos;,
 &apos;COLOR_YUV2BGRA_UYVY&apos;,
 &apos;COLOR_YUV2BGRA_Y422&apos;,
 &apos;COLOR_YUV2BGRA_YUNV&apos;,
 &apos;COLOR_YUV2BGRA_YUY2&apos;,
 &apos;COLOR_YUV2BGRA_YUYV&apos;,
 &apos;COLOR_YUV2BGRA_YV12&apos;,
 &apos;COLOR_YUV2BGRA_YVYU&apos;,
 &apos;COLOR_YUV2BGR_I420&apos;,
 &apos;COLOR_YUV2BGR_IYUV&apos;,
 &apos;COLOR_YUV2BGR_NV12&apos;,
 &apos;COLOR_YUV2BGR_NV21&apos;,
 &apos;COLOR_YUV2BGR_UYNV&apos;,
 &apos;COLOR_YUV2BGR_UYVY&apos;,
 &apos;COLOR_YUV2BGR_Y422&apos;,
 &apos;COLOR_YUV2BGR_YUNV&apos;,
 &apos;COLOR_YUV2BGR_YUY2&apos;,
 &apos;COLOR_YUV2BGR_YUYV&apos;,
 &apos;COLOR_YUV2BGR_YV12&apos;,
 &apos;COLOR_YUV2BGR_YVYU&apos;,
 &apos;COLOR_YUV2GRAY_420&apos;,
 &apos;COLOR_YUV2GRAY_I420&apos;,
 &apos;COLOR_YUV2GRAY_IYUV&apos;,
 &apos;COLOR_YUV2GRAY_NV12&apos;,
 &apos;COLOR_YUV2GRAY_NV21&apos;,
 &apos;COLOR_YUV2GRAY_UYNV&apos;,
 &apos;COLOR_YUV2GRAY_UYVY&apos;,
 &apos;COLOR_YUV2GRAY_Y422&apos;,
 &apos;COLOR_YUV2GRAY_YUNV&apos;,
 &apos;COLOR_YUV2GRAY_YUY2&apos;,
 &apos;COLOR_YUV2GRAY_YUYV&apos;,
 &apos;COLOR_YUV2GRAY_YV12&apos;,
 &apos;COLOR_YUV2GRAY_YVYU&apos;,
 &apos;COLOR_YUV2RGB&apos;,
 &apos;COLOR_YUV2RGBA_I420&apos;,
 &apos;COLOR_YUV2RGBA_IYUV&apos;,
 &apos;COLOR_YUV2RGBA_NV12&apos;,
 &apos;COLOR_YUV2RGBA_NV21&apos;,
 &apos;COLOR_YUV2RGBA_UYNV&apos;,
 &apos;COLOR_YUV2RGBA_UYVY&apos;,
 &apos;COLOR_YUV2RGBA_Y422&apos;,
 &apos;COLOR_YUV2RGBA_YUNV&apos;,
 &apos;COLOR_YUV2RGBA_YUY2&apos;,
 &apos;COLOR_YUV2RGBA_YUYV&apos;,
 &apos;COLOR_YUV2RGBA_YV12&apos;,
 &apos;COLOR_YUV2RGBA_YVYU&apos;,
 &apos;COLOR_YUV2RGB_I420&apos;,
 &apos;COLOR_YUV2RGB_IYUV&apos;,
 &apos;COLOR_YUV2RGB_NV12&apos;,
 &apos;COLOR_YUV2RGB_NV21&apos;,
 &apos;COLOR_YUV2RGB_UYNV&apos;,
 &apos;COLOR_YUV2RGB_UYVY&apos;,
 &apos;COLOR_YUV2RGB_Y422&apos;,
 &apos;COLOR_YUV2RGB_YUNV&apos;,
 &apos;COLOR_YUV2RGB_YUY2&apos;,
 &apos;COLOR_YUV2RGB_YUYV&apos;,
 &apos;COLOR_YUV2RGB_YV12&apos;,
 &apos;COLOR_YUV2RGB_YVYU&apos;,
 &apos;COLOR_YUV420P2BGR&apos;,
 &apos;COLOR_YUV420P2BGRA&apos;,
 &apos;COLOR_YUV420P2GRAY&apos;,
 &apos;COLOR_YUV420P2RGB&apos;,
 &apos;COLOR_YUV420P2RGBA&apos;,
 &apos;COLOR_YUV420SP2BGR&apos;,
 &apos;COLOR_YUV420SP2BGRA&apos;,
 &apos;COLOR_YUV420SP2GRAY&apos;,
 &apos;COLOR_YUV420SP2RGB&apos;,
 &apos;COLOR_YUV420SP2RGBA&apos;,
 &apos;COLOR_YUV420p2BGR&apos;,
 &apos;COLOR_YUV420p2BGRA&apos;,
 &apos;COLOR_YUV420p2GRAY&apos;,
 &apos;COLOR_YUV420p2RGB&apos;,
 &apos;COLOR_YUV420p2RGBA&apos;,
 &apos;COLOR_YUV420sp2BGR&apos;,
 &apos;COLOR_YUV420sp2BGRA&apos;,
 &apos;COLOR_YUV420sp2GRAY&apos;,
 &apos;COLOR_YUV420sp2RGB&apos;,
 &apos;COLOR_YUV420sp2RGBA&apos;,
 &apos;COLOR_mRGBA2RGBA&apos;]</code></pre><h2 id="如何找到要追踪的HSV值？"><a href="#如何找到要追踪的HSV值？" class="headerlink" title="如何找到要追踪的HSV值？"></a>如何找到要追踪的HSV值？</h2><p>这是在stackoverflow.com上发现的一个常见问题。它非常简单，你可以使用相同的函数<strong>cv.cvtColor()</strong>。</p>
<p>你只需传递你想要的BGR值，而不是传递图像。例如，要查找绿色的HSV值，请在Python终端中尝试以下命令</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">green = np.uint8([[[<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>]]])</span><br><span class="line">hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)</span><br><span class="line">hsv_green</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 60, 255, 255]]], dtype=uint8)</code></pre><h1 id="图像的几何变换"><a href="#图像的几何变换" class="headerlink" title="图像的几何变换"></a><span id="header2">图像的几何变换</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>学习将不同的几何变换应用到图像上，如平移、旋转、仿射变换等。</p>
<p>你会看到这些函数: cv.getPerspectiveTransform</p>
<h2 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h2><p>OpenCV提供了两个转换函数<strong>cv.warpAffine</strong>和<strong>cv.warpPerspective</strong>，您可以使用它们进行各种转换。</p>
<p><strong>cv.warpAffine</strong>采用2x3转换矩阵，而<strong>cv.warpPerspective</strong>采用3x3转换矩阵作为输入。</p>
<h2 id="缩放"><a href="#缩放" class="headerlink" title="缩放"></a>缩放</h2><p>缩放只是调整图像的大小。为此，OpenCV带有一个函数**cv.resize()。图像的大小可以手动指定，也可以指定缩放比例。</p>
<p>也可使用不同的插值方法。首选的插值方法是<strong>cv.INTER_AREA</strong>用于缩小，<strong>cv.INTER_CUBIC（慢）和</strong>cv.INTER_LINEAR**用于缩放。</p>
<p>默认情况下，出于所有调整大小的目的，使用的插值方法为<strong>cv.INTER_LINEAR</strong>。您可以使用以下方法调整输入图像的大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">res = cv.resize(img,<span class="literal">None</span>,fx=<span class="number">2</span>, fy=<span class="number">2</span>, interpolation = cv.INTER_AREA)</span><br><span class="line">cv.imshow(<span class="string">'res1'</span>,res)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">height, width = img.shape[:<span class="number">2</span>]</span><br><span class="line">res = cv.resize(img,(<span class="number">2</span>*width, <span class="number">2</span>*height), interpolation = cv.INTER_CUBIC)</span><br><span class="line">cv.imshow(<span class="string">'res2'</span>,res)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="平移"><a href="#平移" class="headerlink" title="平移"></a>平移</h2><p>平移是物体位置的移动。如果您知道在(x,y)方向上的位移，则将其设为(tx,ty)，你可以创建转换矩阵M，如下所示：</p>
<p>M = $$<br> \left[<br> \begin{matrix}<br>   1 &amp; 0 &amp; tx \<br>   0 &amp; 1 &amp; ty<br>  \end{matrix}<br>  \right] <br>$$<br>您可以将其放入<strong>np.float32</strong>类型的Numpy数组中，并将其传递给<strong>cv.warpAffine</strong>函数。</p>
<p>参见下面偏移为(100, 50)的示例：</p>
<p><strong>cv.warpAffine</strong>函数的第三个参数是输出图像的大小，其形式应为(width，height)。记住width =列数，height =行数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">rows,cols = img.shape</span><br><span class="line">M = np.float32([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">100</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">50</span>]])</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h2><p>图像旋转角度为θ是通过以下形式的变换矩阵实现的：</p>
<p>M=$$<br> \left[<br> \begin{matrix}<br>   \cos \theta &amp; \sin \theta  \<br>   \sin \theta &amp; \cos \theta<br>  \end{matrix}<br>  \right] <br>$$<br>但是OpenCV提供了可缩放的旋转以及可调整的旋转中心，因此您可以在自己喜欢的任何位置旋转。修改后的变换矩阵为</p>
<p>$$<br> \left[<br> \begin{matrix}<br>   \alpha &amp; \beta &amp; (1-\alpha)·center·x-\beta·center·y \<br>   -\beta &amp; \alpha &amp; \beta·center·x+(1-\alpha)·center·y<br>  \end{matrix}<br>  \right] <br>$$<br>其中：</p>
<p>$$α=scale⋅\cos \theta,β=scale⋅\sin \theta $$</p>
<p>为了找到此转换矩阵，OpenCV提供了一个函数<strong>cv.getRotationMatrix2D</strong>。</p>
<p>请检查以下示例，该示例将图像相对于中心旋转90度而没有任何缩放比例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">rows,cols = img.shape</span><br><span class="line"><span class="comment"># cols-1 和 rows-1 是坐标限制</span></span><br><span class="line">M = cv.getRotationMatrix2D(((cols<span class="number">-1</span>)/<span class="number">2.0</span>,(rows<span class="number">-1</span>)/<span class="number">2.0</span>),<span class="number">90</span>,<span class="number">2</span>)</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h2><p>在仿射变换中，原始图像中的所有平行线在输出图像中仍将平行。</p>
<p>为了找到变换矩阵，我们需要输入图像中的三个点及其在输出图像中的对应位置。</p>
<p>然后<strong>cv.getAffineTransform</strong>将创建一个2x3矩阵，该矩阵将传递给<strong>cv.warpAffine</strong>。</p>
<p>查看以下示例，并查看我选择的点（以绿色标记）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'drawing.png'</span>)</span><br><span class="line">rows,cols,ch = img.shape</span><br><span class="line">pts1 = np.float32([[<span class="number">50</span>,<span class="number">50</span>],[<span class="number">200</span>,<span class="number">50</span>],[<span class="number">50</span>,<span class="number">200</span>]])</span><br><span class="line">pts2 = np.float32([[<span class="number">10</span>,<span class="number">100</span>],[<span class="number">200</span>,<span class="number">50</span>],[<span class="number">100</span>,<span class="number">250</span>]])</span><br><span class="line">M = cv.getAffineTransform(pts1,pts2)</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Input'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Output'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(&lt;matplotlib.axes._subplots.AxesSubplot at 0x1bb7651dcc0&gt;,
 &lt;matplotlib.image.AxesImage at 0x1bb76bc5c18&gt;,
 Text(0.5, 1.0, &apos;Output&apos;))</code></pre><p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_9_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>)</span><br><span class="line">rows,cols,ch = img.shape</span><br><span class="line">pts1 = np.float32([[<span class="number">56</span>,<span class="number">65</span>],[<span class="number">368</span>,<span class="number">52</span>],[<span class="number">28</span>,<span class="number">387</span>],[<span class="number">389</span>,<span class="number">390</span>]])</span><br><span class="line">pts2 = np.float32([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">300</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">300</span>],[<span class="number">300</span>,<span class="number">300</span>]])</span><br><span class="line">M = cv.getPerspectiveTransform(pts1,pts2)</span><br><span class="line">dst = cv.warpPerspective(img,M,(<span class="number">300</span>,<span class="number">300</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Input'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Output'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_10_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="图像阈值"><a href="#图像阈值" class="headerlink" title="图像阈值"></a><span id="header3">图像阈值</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在本教程中，您将学习简单阈值，自适应阈值和Otsu阈值。</p>
<p>你将学习函数<strong>cv.threshold</strong>和<strong>cv.adaptiveThreshold</strong>。</p>
<h2 id="简单阈值"><a href="#简单阈值" class="headerlink" title="简单阈值"></a>简单阈值</h2><p>在这里，问题直截了当。对于每个像素，应用相同的阈值。</p>
<p>如果像素值小于阈值，则将其设置为0，否则将其设置为最大值。函数<strong>cv.threshold</strong>用于应用阈值。</p>
<ul>
<li>第一个参数是源图像，它<strong>应该是灰度图像</strong>。</li>
<li>第二个参数是阈值，用于对像素值进行分类。</li>
<li>第三个参数是分配给超过阈值的像素值的最大值。</li>
<li>第四个参数OpenCV提供了不同类型的阈值,通过使用<strong>cv.THRESH_BINARY</strong>类型。所有简单的阈值类型为：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.THRESH_BINARY</span><br><span class="line">cv.THRESH_BINARY_INV</span><br><span class="line">cv.THRESH_TRUNC</span><br><span class="line">cv.THRESH_TOZERO</span><br><span class="line">cv.THRESH_TOZERO_INV</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>该方法返回两个输出。第一个是使用的阈值，第二个输出是<strong>阈值后的图像</strong>。</p>
<p>此代码比较了不同的简单阈值类型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'gradient.png'</span>,<span class="number">0</span>)</span><br><span class="line">ret,thresh1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line">ret,thresh2 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">ret,thresh3 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TRUNC)</span><br><span class="line">ret,thresh4 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO)</span><br><span class="line">ret,thresh5 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO_INV)</span><br><span class="line">titles = [<span class="string">'Original Image'</span>,<span class="string">'BINARY'</span>,<span class="string">'BINARY_INV'</span>,<span class="string">'TRUNC'</span>,<span class="string">'TOZERO'</span>,<span class="string">'TOZERO_INV'</span>]</span><br><span class="line">images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_2_1.png" alt="png"></p>
<h2 id="自适应阈值"><a href="#自适应阈值" class="headerlink" title="自适应阈值"></a>自适应阈值</h2><p>在上一节中，我们使用一个全局值作为阈值。但这可能并非在所有情况下都很好，例如，如果图像在不同区域具有不同的光照条件。在这种情况下，自适应阈值阈值化可以提供帮助。在此，算法基于像素周围的小区域确定像素的阈值。因此，对于同一图像的不同区域，我们获得了不同的阈值，这为光照度变化的图像提供了更好的结果。</p>
<p>除上述参数外，方法<strong>cv.adaptiveThreshold</strong>还包含三个输入参数：</p>
<p>该<strong>adaptiveMethod</strong>决定阈值是如何计算的：</p>
<p>cv.ADAPTIVE_THRESH_MEAN_C::阈值是邻近区域的平均值减去常数<strong>C</strong>。 </p>
<p>cv.ADAPTIVE_THRESH_GAUSSIAN_C:阈值是邻域值的高斯加权总和减去常数<strong>C</strong>。</p>
<p>该<strong>BLOCKSIZE</strong>确定附近区域的大小，<strong>C</strong>是从邻域像素的平均或加权总和中减去的一个常数。</p>
<p>下面的代码比较了光照变化的图像的全局阈值和自适应阈值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line">ret,th1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line">th2 = cv.adaptiveThreshold(img,<span class="number">255</span>,cv.ADAPTIVE_THRESH_MEAN_C,\</span><br><span class="line">            cv.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line">th3 = cv.adaptiveThreshold(img,<span class="number">255</span>,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\</span><br><span class="line">            cv.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line">titles = [<span class="string">'Original Image'</span>, <span class="string">'Global Thresholding (v = 127)'</span>,</span><br><span class="line">            <span class="string">'Adaptive Mean Thresholding'</span>, <span class="string">'Adaptive Gaussian Thresholding'</span>]</span><br><span class="line">images = [img, th1, th2, th3]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">2</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_4_0.png" alt="png"></p>
<h2 id="Otsu的二值化"><a href="#Otsu的二值化" class="headerlink" title="Otsu的二值化"></a>Otsu的二值化</h2><p>在全局阈值化中，我们使用任意选择的值作为阈值。相反，Otsu的方法避免了必须选择一个值并自动确定它的情况。</p>
<p>考虑仅具有两个不同图像值的图像（双峰图像），其中直方图将仅包含两个峰。一个好的阈值应该在这两个值的中间。类似地，Otsu的方法从图像直方图中确定最佳全局阈值。</p>
<p>为此，使用了<strong>cv.threshold</strong>作为附加标志传递。阈值可以任意选择。然后，算法找到最佳阈值，该阈值作为第一输出返回。</p>
<p>查看以下示例。输入图像为噪点图像。</p>
<p>在第一种情况下，采用值为127的全局阈值。</p>
<p>在第二种情况下，直接采用Otsu阈值法。</p>
<p>在第三种情况下，首先使用5x5高斯核对图像进行滤波以去除噪声，然后应用Otsu阈值处理。了解噪声滤波如何改善结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 全局阈值</span></span><br><span class="line">ret1,th1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line"><span class="comment"># Otsu阈值</span></span><br><span class="line">ret2,th2 = cv.threshold(img,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line"><span class="comment"># 高斯滤波后再采用Otsu阈值</span></span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line">ret3,th3 = cv.threshold(blur,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line"><span class="comment"># 绘制所有图像及其直方图</span></span><br><span class="line">images = [img, <span class="number">0</span>, th1,</span><br><span class="line">          img, <span class="number">0</span>, th2,</span><br><span class="line">          blur, <span class="number">0</span>, th3]</span><br><span class="line">titles = [<span class="string">'Original Noisy Image'</span>,<span class="string">'Histogram'</span>,<span class="string">'Global Thresholding (v=127)'</span>,</span><br><span class="line">          <span class="string">'Original Noisy Image'</span>,<span class="string">'Histogram'</span>,<span class="string">"Otsu's Thresholding"</span>,</span><br><span class="line">          <span class="string">'Gaussian filtered Image'</span>,<span class="string">'Histogram'</span>,<span class="string">"Otsu's Thresholding"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">1</span>),plt.imshow(images[i*<span class="number">3</span>],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">2</span>),plt.hist(images[i*<span class="number">3</span>].ravel(),<span class="number">256</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>+<span class="number">1</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">3</span>),plt.imshow(images[i*<span class="number">3</span>+<span class="number">2</span>],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>+<span class="number">2</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_6_0.png" alt="png"></p>
<h2 id="Otsu的二值化如何实现？"><a href="#Otsu的二值化如何实现？" class="headerlink" title="Otsu的二值化如何实现？"></a>Otsu的二值化如何实现？</h2><p>本节演示了Otsu二值化的Python实现，以展示其实际工作方式。</p>
<p>由于我们正在处理双峰图像，因此Otsu的算法尝试找到一个阈值(t)，该阈值将由关系式给出的<strong>加权类内方差</strong>最小化：</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Otsu.png" alt></p>
<p>实际上，它找到位于两个峰值之间的t值，以使两个类别的差异最小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 寻找归一化直方图和对应的累积分布函数</span></span><br><span class="line">hist = cv.calcHist([blur],[<span class="number">0</span>],<span class="literal">None</span>,[<span class="number">256</span>],[<span class="number">0</span>,<span class="number">256</span>])</span><br><span class="line">hist_norm = hist.ravel()/hist.max()</span><br><span class="line">Q = hist_norm.cumsum()</span><br><span class="line">bins = np.arange(<span class="number">256</span>)</span><br><span class="line">fn_min = np.inf</span><br><span class="line">thresh = <span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">256</span>):</span><br><span class="line">    p1,p2 = np.hsplit(hist_norm,[i]) <span class="comment"># 概率</span></span><br><span class="line">    q1,q2 = Q[i],Q[<span class="number">255</span>]-Q[i] <span class="comment"># 对类求和</span></span><br><span class="line">    b1,b2 = np.hsplit(bins,[i]) <span class="comment"># 权重</span></span><br><span class="line">    <span class="comment"># 寻找均值和方差</span></span><br><span class="line">    m1,m2 = np.sum(p1*b1)/q1, np.sum(p2*b2)/q2</span><br><span class="line">    v1,v2 = np.sum(((b1-m1)**<span class="number">2</span>)*p1)/q1,np.sum(((b2-m2)**<span class="number">2</span>)*p2)/q2</span><br><span class="line">    <span class="comment"># 计算最小化函数</span></span><br><span class="line">    fn = v1*q1 + v2*q2</span><br><span class="line">    <span class="keyword">if</span> fn &lt; fn_min:</span><br><span class="line">        fn_min = fn</span><br><span class="line">        thresh = i</span><br><span class="line"><span class="comment"># 使用OpenCV函数找到otsu的阈值</span></span><br><span class="line">ret, otsu = cv.threshold(blur,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line">print( <span class="string">"&#123;&#125; &#123;&#125;"</span>.format(thresh,ret) )</span><br></pre></td></tr></table></figure>

<pre><code>101 100.0


c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars
  from ipykernel import kernelapp as app
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in double_scalars
  from ipykernel import kernelapp as app
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply
  app.launch_new_instance()</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="图像平滑"><a href="#图像平滑" class="headerlink" title="图像平滑"></a><span id="header4">图像平滑</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>学会： - 使用各种低通滤镜模糊图像 - 将定制的滤镜应用于图像（2D卷积）</p>
<h2 id="2D卷积（图像过滤）"><a href="#2D卷积（图像过滤）" class="headerlink" title="2D卷积（图像过滤）"></a>2D卷积（图像过滤）</h2><p>与一维信号一样，还可以使用各种低通滤波器（LPF），高通滤波器（HPF）等对图像进行滤波。LPF有助于消除噪声，使图像模糊等。HPF滤波器有助于在图像中找到边缘。</p>
<p>OpenCV提供了一个函数<strong>cv.filter2D</strong>来将内核与图像进行卷积。例如，我们将尝试对图像进行平均滤波。5x5平均滤波器内核如下所示：</p>
<p>K=$$\frac{1}{25}<br> \left[<br> \begin{matrix}<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1<br>  \end{matrix}<br>  \right] <br>$$<br>操作如下:保持这个内核在一个像素上，将所有低于这个内核的25个像素相加，取其平均值，然后用新的平均值替换中心像素。它将对图像中的所有像素继续此操作。试试这个代码，并检查结果:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.float32)/<span class="number">25</span></span><br><span class="line">dst = cv.filter2D(img,<span class="number">-1</span>,kernel)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Averaging'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_2_0.png" alt="png"></p>
<h2 id="图像模糊（图像平滑）"><a href="#图像模糊（图像平滑）" class="headerlink" title="图像模糊（图像平滑）"></a>图像模糊（图像平滑）</h2><p>通过将图像与低通滤波器内核进行卷积来实现图像模糊。这对于消除噪音很有用。它实际上从图像中消除了高频部分（例如噪声，边缘）。</p>
<p>因此，在此操作中边缘有些模糊。（有一些模糊技术也可以不模糊边缘）。OpenCV主要提供四种类型的模糊技术。</p>
<h3 id="1-平均"><a href="#1-平均" class="headerlink" title="1.平均"></a>1.平均</h3><p>这是通过将图像与归一化框滤镜进行卷积来完成的。它仅获取内核区域下所有像素的平均值，并替换中心元素。这是通过功能<strong>cv.blur()或</strong>cv.boxFilter()完成的。检查文档以获取有关内核的更多详细信息。我们应该指定内核的宽度和高度。3x3归一化框式过滤器如下所示：</p>
<p>K=$$\frac{1}{9}<br> \left[<br> \begin{matrix}<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1<br>  \end{matrix}<br>  \right] <br>$$<br>注意 如果您不想使用标准化的框式过滤器，请使用<strong>cv.boxFilter()</strong>。将参数normalize = False传递给函数。</p>
<p>查看下面的示例演示，其内核大小为5x5：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.blur(img,(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_5_0.png" alt="png"></p>
<h3 id="2-高斯模糊"><a href="#2-高斯模糊" class="headerlink" title="2.高斯模糊"></a>2.高斯模糊</h3><p>在这种情况下，代替盒式滤波器，使用了高斯核。</p>
<p>这是通过功能<strong>cv.GaussianBlur()</strong> 完成的。我们应指定内核的宽度和高度，该宽度和高度应为正数和奇数。</p>
<p>我们还应指定X和Y方向的标准偏差，分别为sigmaX和sigmaY。如果仅指定sigmaX，则将sigmaY与sigmaX相同。</p>
<p>如果两个都为零，则根据内核大小进行计算。高斯模糊对于从图像中去除高斯噪声非常有效。</p>
<p>如果需要，可以使用函数<strong>cv.getGaussianKernel()</strong> 创建高斯内核。</p>
<p>可以修改以上代码以实现高斯模糊：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_7_0.png" alt="png"></p>
<h3 id="3-中位模糊"><a href="#3-中位模糊" class="headerlink" title="3.中位模糊"></a>3.中位模糊</h3><p>在这里，函数<strong>cv.medianBlur()</strong> 提取内核区域下所有像素的中值，并将中心元素替换为该中值。</p>
<p>这对于消除图像中的椒盐噪声非常有效。有趣的是，在上述过滤器中，中心元素是新计算的值，该值可以是图像中的像素值或新值。</p>
<p>但是在中值模糊中，中心元素总是被图像中的某些像素值代替。有效降低噪音。其内核大小应为正奇数整数。</p>
<p>在此演示中，我向原始图像添加了50％的噪声并应用了中值模糊。检查结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">median = cv.medianBlur(img,<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(median),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_9_0.png" alt="png"></p>
<h3 id="4-双边滤波"><a href="#4-双边滤波" class="headerlink" title="4.双边滤波"></a>4.双边滤波</h3><p>cv.bilateralFilter() 在去除噪声的同时保持边缘清晰锐利非常有效。</p>
<p>但是，与其他过滤器相比，该操作速度较慢。我们已经看到，高斯滤波器采用像素周围的邻域并找到其高斯加权平均值。</p>
<p>高斯滤波器仅是空间的函数，也就是说，滤波时会考虑附近的像素。它不考虑像素是否具有几乎相同的强度。它不考虑像素是否是边缘像素。因此它也模糊了边缘，这是我们不想做的。</p>
<p>双边滤波器在空间中也采用高斯滤波器，但是又有一个高斯滤波器，它是像素差的函数。</p>
<p>空间的高斯函数确保仅考虑附近像素的模糊，而强度差的高斯函数确保仅考虑强度与中心像素相似的那些像素的模糊。由于边缘的像素强度变化较大，因此可以保留边缘。</p>
<p>以下示例显示了使用双边过滤器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.bilateralFilter(img,<span class="number">9</span>,<span class="number">75</span>,<span class="number">75</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_11_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理2</title>
    <url>/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/</url>
    <content><![CDATA[<ul>
<li>5.<a href="#header1">形态转换</a></li>
<li>6.<a href="#header2">图像梯度</a></li>
<li>7.<a href="#header3">Canny边缘检测</a></li>
<li>8.<a href="#header4">图像金字塔</a><a id="more"></a>


</li>
</ul>
<h1 id="形态学转换"><a href="#形态学转换" class="headerlink" title="形态学转换"></a><span id="header1">形态学转换</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>在这一章当中， 我们将学习不同的形态学操作，例如侵蚀，膨胀，开运算，闭运算等。<br>我们将看到不同的功能，</p>
<p>例如：cv.erode(),cv.dilate(), cv.morphologyEx()等。</p>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>形态变换是一些基于图像形状的简单操作。通常在二进制图像上执行。</p>
<p>它需要两个输入，一个是我们的原始图像，第二个是决定<strong>操作性质的结构元素</strong>或<strong>内核</strong>。</p>
<p>两种基本的形态学算子是侵蚀和膨胀。</p>
<p>然后，它的变体形式（如“打开”，“关闭”，“渐变”等）也开始起作用。在下图的帮助下，我们将一一看到它们：</p>
<h2 id="1-侵蚀"><a href="#1-侵蚀" class="headerlink" title="1. 侵蚀"></a>1. 侵蚀</h2><p>侵蚀的基本思想就像土壤侵蚀一样，它侵蚀前景物体的边界(尽量使前景保持白色)。</p>
<p>它是做什么的呢?内核滑动通过图像(在2D卷积中)。</p>
<p>原始图像中的一个像素(无论是1还是0)只有当内核下的所有像素都是1时才被认为是1，否则它就会被侵蚀(变成0)。</p>
<p>结果是，根据内核的大小，边界附近的所有像素都会被丢弃。</p>
<p>因此，前景物体的厚度或大小减小，或只是图像中的白色区域减小。</p>
<p>它有助于去除小的白色噪声(正如我们在颜色空间章节中看到的)，分离两个连接的对象等。</p>
<p>在这里，作为一个例子，我将使用一个5x5内核，它包含了所有的1。</p>
<h2 id="2-扩张"><a href="#2-扩张" class="headerlink" title="2. 扩张"></a>2. 扩张</h2><p>它与侵蚀正好相反。如果内核下的至少一个像素为“ 1”，则像素元素为“ 1”。</p>
<p>因此，它会增加图像中的白色区域或增加前景对象的大小。</p>
<p>通常，在消除噪音的情况下，腐蚀后会膨胀。因为腐蚀会消除白噪声，但也会缩小物体。</p>
<p>因此，我们对其进行了扩展。由于噪音消失了，它们不会回来，但是我们的目标区域增加了。在连接对象的损坏部分时也很有用。</p>
<h2 id="3-开运算"><a href="#3-开运算" class="headerlink" title="3. 开运算"></a>3. 开运算</h2><p>开放只是<strong>侵蚀然后扩张</strong>的另一个名称。</p>
<p>如上文所述，它对于消除噪音很有用。在这里，我们使用函数<strong>cv.morphologyEx</strong>()</p>
<h2 id="4-闭运算"><a href="#4-闭运算" class="headerlink" title="4. 闭运算"></a>4. 闭运算</h2><p>闭运算与开运算相反，先扩张然后再侵蚀。</p>
<p>在关闭前景对象内部的小孔或对象上的小黑点时很有用。</p>
<h2 id="5-形态学梯度"><a href="#5-形态学梯度" class="headerlink" title="5. 形态学梯度"></a>5. 形态学梯度</h2><p>这是图像扩张和侵蚀之间的区别。</p>
<p>结果将看起来像对象的轮廓。</p>
<h2 id="6-顶帽"><a href="#6-顶帽" class="headerlink" title="6. 顶帽"></a>6. 顶帽</h2><p>它是输入图像和图像开运算之差。下面的示例针对9x9内核完成。</p>
<h2 id="7-黑帽"><a href="#7-黑帽" class="headerlink" title="7. 黑帽"></a>7. 黑帽</h2><p>这是输入图像和图像闭运算之差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl </span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>] <span class="comment"># 指定默认字体 </span></span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span> <span class="comment"># 解决保存图像是负号'-'显示为方块的问题</span></span><br><span class="line">img = cv.imread(<span class="string">'j.png'</span>,<span class="number">0</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)</span><br><span class="line">erosion = cv.erode(img,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">dilation = cv.dilate(img,kernel,iterations = <span class="number">1</span>) </span><br><span class="line">opening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel) </span><br><span class="line">closing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel) </span><br><span class="line">gradient = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel) </span><br><span class="line">tophat = cv.morphologyEx(img, cv.MORPH_TOPHAT, kernel) </span><br><span class="line">blackhat = cv.morphologyEx(img, cv.MORPH_BLACKHAT, kernel) </span><br><span class="line">imgs = [img,erosion,dilation,opening,closing,gradient,tophat,blackhat]</span><br><span class="line">titles = [<span class="string">'原图'</span>,<span class="string">'侵蚀'</span>,<span class="string">'膨胀'</span>,<span class="string">'开运算'</span>,<span class="string">'闭运算'</span>,<span class="string">'形态学梯度'</span>,<span class="string">'顶帽'</span>,<span class="string">'黑帽'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(imgs)):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">4</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(imgs[i])</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_9_0.png" alt="png"></p>
<h2 id="结构元素"><a href="#结构元素" class="headerlink" title="结构元素"></a>结构元素</h2><p>在Numpy的帮助下，我们在前面的示例中手动创建了一个结构元素。</p>
<p>它是矩形。但是在某些情况下，您可能需要椭圆形/圆形的内核。</p>
<p>因此，为此，OpenCV具有一个函数<strong>cv.getStructuringElement</strong>()。</p>
<p>您只需传递内核的形状和大小，即可获得所需的内核</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩形</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_RECT,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1]], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 椭圆内核</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_ELLIPSE,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0, 1, 0, 0],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [0, 0, 1, 0, 0]], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 十字内核</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_CROSS,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0],
       [1, 1, 1, 1, 1],
       [0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0]], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="图像梯度"><a href="#图像梯度" class="headerlink" title="图像梯度"></a><span id="header2">图像梯度</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习： - 查找图像梯度，边缘等 - </p>
<p>我们将看到以下函数：cv.Sobel()，cv.Scharr()，cv.Laplacian()等</p>
<h2 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h2><p>OpenCV提供三种类型的梯度滤波器或高通滤波器，即Sobel，Scharr和Laplacian。我们将看到他们每一种。</p>
<h2 id="1-Sobel-和-Scharr-算子"><a href="#1-Sobel-和-Scharr-算子" class="headerlink" title="1. Sobel 和 Scharr 算子"></a>1. Sobel 和 Scharr 算子</h2><p>Sobel算子是高斯平滑加微分运算的联合运算，因此它更抗噪声。逆可以指定要采用的导数方向，垂直或水平（分别通过参数yorder和xorder）。逆还可以通过参数ksize指定内核的大小。如果ksize = -1，则使用3x3 Scharr滤波器，比3x3 Sobel滤波器具有更好的结果。请参阅文档以了解所使用的内核。</p>
<h2 id="2-Laplacian-算子"><a href="#2-Laplacian-算子" class="headerlink" title="2. Laplacian 算子"></a>2. Laplacian 算子</h2><p>它计算了由关系Δsrc=$\frac{\delta^2 src}{\delta x^2}+\frac{\delta^2 src}{\delta y^2}$给出的图像的拉普拉斯图,它是每一阶导数通过Sobel算子计算。如果ksize = 1,然后使用以下内核用于过滤:</p>
<p>kernel=$$<br> \left[<br> \begin{matrix}<br>   0 &amp; 1 &amp; 0 \<br>   1 &amp; -4 &amp; 1 \<br>   0 &amp; 1 &amp; 0<br>  \end{matrix}<br>  \right] <br>$$<br>代码<br>下面的代码显示了单个图表中的所有算子。所有内核都是5x5大小。输出图像的深度通过-1得到结果的np.uint8型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'dave.png'</span>,<span class="number">0</span>)</span><br><span class="line">laplacian = cv.Laplacian(img,cv.CV_64F)</span><br><span class="line">sobelx = cv.Sobel(img,cv.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize=<span class="number">5</span>)</span><br><span class="line">sobely = cv.Sobel(img,cv.CV_64F,<span class="number">0</span>,<span class="number">1</span>,ksize=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),plt.imshow(img,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Original'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>),plt.imshow(laplacian,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Laplacian'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>),plt.imshow(sobelx,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Sobel X'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),plt.imshow(sobely,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Sobel Y'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="Canny边缘检测"><a href="#Canny边缘检测" class="headerlink" title="Canny边缘检测"></a><span id="header3">Canny边缘检测</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习 - Canny边缘检测的概念 - OpenCV函数: cv.Canny()</p>
<h2 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h2><p>Canny Edge Detection是一种流行的边缘检测算法。它由John F. Canny发明</p>
<p>这是一个多阶段算法，我们将经历每个阶段。</p>
<h2 id="降噪"><a href="#降噪" class="headerlink" title="降噪"></a>降噪</h2><p>由于边缘检测容易受到图像中噪声的影响，因此第一步是使用5x5高斯滤波器消除图像中的噪声。我们已经在前面的章节中看到了这一点。</p>
<p>查找图像的强度梯度<br>然后使用Sobel核在水平和垂直方向上对平滑的图像进行滤波，以在水平方向(Gx)和垂直方向(Gy)上获得一阶导数。从这两张图片中，我们可以找到每个像素的边缘渐变和方向，如下所示：</p>
<p>$$<br>Edge_Gradient ; (G) = \sqrt{G_x^2 + G_y^2} \ Angle ; (\theta) = \tan^{-1} \bigg(\frac{G_y}{G_x}\bigg)<br>$$<br>渐变方向始终垂直于边缘。将其舍入为代表垂直，水平和两个对角线方向的四个角度之一。</p>
<p>非极大值抑制 在获得梯度大小和方向后，将对图像进行全面扫描，以去除可能不构成边缘的所有不需要的像素。</p>
<p>为此，在每个像素处，检查像素是否是其在梯度方向上附近的局部最大值。查看下面的图片：<br><img src="http://qiniu.aihubs.net/nms.jpg" alt></p>
<p>点A在边缘（垂直方向）上。渐变方向垂直于边缘。点B和C在梯度方向上。因此，将A点与B点和C点进行检查，看是否形成局部最大值。如果是这样，则考虑将其用于下一阶段，否则将其抑制（置为零）。 简而言之，你得到的结果是带有“细边”的二进制图像。</p>
<h2 id="磁滞阈值"><a href="#磁滞阈值" class="headerlink" title="磁滞阈值"></a>磁滞阈值</h2><p>该阶段确定哪些边缘全部是真正的边缘，哪些不是。为此，我们需要两个阈值minVal和maxVal。强度梯度大于maxVal的任何边缘必定是边缘，而小于minVal的那些边缘必定是非边缘，因此将其丢弃。介于这两个阈值之间的对象根据其连通性被分类为边缘或非边缘。如果将它们连接到“边缘”像素，则将它们视为边缘的一部分。否则，它们也将被丢弃。见下图：</p>
<p><img src="http://qiniu.aihubs.net/hysteresis.jpg" alt><br>边缘A在maxVal之上，因此被视为“确定边缘”。尽管边C低于maxVal，但它连接到边A，因此也被视为有效边，我们得到了完整的曲线。但是边缘B尽管在minVal之上并且与边缘C处于同一区域，但是它没有连接到任何“确保边缘”，因此被丢弃。因此，非常重要的一点是我们必须相应地选择minVal和maxVal以获得正确的结果。</p>
<p>在边缘为长线的假设下，该阶段还消除了小像素噪声。</p>
<p>因此，我们最终得到的是图像中的强边缘。</p>
<h2 id="OpenCV中的Canny-Edge检测"><a href="#OpenCV中的Canny-Edge检测" class="headerlink" title="OpenCV中的Canny Edge检测"></a>OpenCV中的Canny Edge检测</h2><p>OpenCV将以上所有内容放在单个函数<strong>cv.Canny</strong>()中。我们将看到如何使用它。</p>
<p>第一个参数是我们的输入图像。</p>
<p>第二个和第三个参数分别是我们的minVal和maxVal。</p>
<p>第三个参数是perture_size。它是用于查找图像渐变的Sobel内核的大小。默认情况下为3。</p>
<p>最后一个参数是L2gradient，它指定用于查找梯度幅度的方程式。</p>
<p>如果为True，则使用上面提到的更精确的公式，否则使用以下函数：Edge_Gradient(G)=|Gx|+|Gy|。默认情况下，它为False。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">edges = cv.Canny(img,<span class="number">100</span>,<span class="number">200</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Original Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(edges,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Edge Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="图像金字塔"><a href="#图像金字塔" class="headerlink" title="图像金字塔"></a><span id="header4">图像金字塔</span></h1><h1 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h1><p>在本章中， - 我们将学习图像金字塔 - 我们将使用图像金字塔创建一个新的水果“Orapple” - </p>
<p>我们将看到以下功能：cv.pyrUp()，cv.pyrDown()</p>
<h2 id="理论-3"><a href="#理论-3" class="headerlink" title="理论"></a>理论</h2><p>通常，我们过去使用的是恒定大小的图像。但是在某些情况下，我们需要使用不同分辨率的（相同）图像。</p>
<p>例如，当在图像中搜索某些东西（例如人脸）时，我们不确定对象将以多大的尺寸显示在图像中。</p>
<p>在这种情况下，我们将需要创建一组具有不同分辨率的相同图像，并在所有图像中搜索对象。</p>
<p>这些具有不同分辨率的图像集称为“图像金字塔”（因为当它们堆叠在底部时，最高分辨率的图像位于底部，最低分辨率的图像位于顶部时，看起来像金字塔）。</p>
<p>有两种图像金字塔。1）高斯金字塔<strong>和2）</strong>拉普拉斯金字塔</p>
<p>高斯金字塔中的较高级别（低分辨率）是通过删除较低级别（较高分辨率）图像中的连续行和列而形成的。</p>
<p>然后，较高级别的每个像素由基础级别的5个像素的贡献与高斯权重形成。</p>
<p>通过这样做，M×N图像变成M/2×N/2图像。因此面积减少到原始面积的四分之一。</p>
<p>它称为Octave。当我们在金字塔中越靠上时（即分辨率下降），这种模式就会继续。</p>
<p>同样，在扩展时，每个级别的面积变为4倍。</p>
<p>我们可以使用<strong>cv.pyrDown</strong>()和<strong>cv.pyrUp</strong>()函数找到高斯金字塔</p>
<p>以下是图像金字塔中的4个级别。<br><img src="http://qiniu.aihubs.net/messipyr.jpg" alt><br>现在，您可以使用<strong>cv.pyrUp</strong>()函数查看图像金字塔。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">higher_reso2 &#x3D; cv.pyrUp(lower_reso)</span><br></pre></td></tr></table></figure>
<p>记住，higher_reso2不等于higher_reso，因为一旦降低了分辨率，就会丢失信息。下面的图像是3层的金字塔从最小的图像在前面的情况下创建。与原图对比:<br><img src="http://qiniu.aihubs.net/messiup.jpg" alt></p>
<p>拉普拉斯金字塔由高斯金字塔形成。没有专用功能。</p>
<p>拉普拉斯金字塔图像仅像边缘图像。它的大多数元素为零。它们用于图像压缩。</p>
<p>拉普拉斯金字塔的层由高斯金字塔的层与高斯金字塔的高层的扩展版本之间的差形成。</p>
<p>拉普拉斯等级的三个等级如下所示（调整对比度以增强内容）：<br><img src="http://qiniu.aihubs.net/lap.jpg" alt></p>
<h2 id="使用金字塔进行图像融合"><a href="#使用金字塔进行图像融合" class="headerlink" title="使用金字塔进行图像融合"></a>使用金字塔进行图像融合</h2><p>金字塔的一种应用是图像融合。</p>
<p>例如，在图像拼接中，您需要将两个图像堆叠在一起，但是由于图像之间的不连续性，可能看起来不太好。</p>
<p>在这种情况下，使用金字塔混合图像可以无缝混合，而不会在图像中保留大量数据。</p>
<p>一个经典的例子是将两种水果，橙和苹果混合在一起</p>
<p>只需完成以下步骤即可：</p>
<ul>
<li>加载苹果和橙子的两个图像</li>
<li>查找苹果和橙子的高斯金字塔（在此示例中， 级别数为6）</li>
<li>在高斯金字塔中，找到其拉普拉斯金字塔</li>
<li>然后在每个拉普拉斯金字塔级别中加入苹果的左半部分和橙子的右半部分</li>
<li>最后从此联合图像金字塔中重建原始图像。<br><img src="http://qiniu.aihubs.net/orapple.jpg" alt></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np,sys</span><br><span class="line">A = cv.imread(<span class="string">'apple.png'</span>)</span><br><span class="line">B = cv.imread(<span class="string">'orange.png'</span>)</span><br><span class="line"><span class="comment"># 生成A的高斯金字塔</span></span><br><span class="line">G = A.copy()</span><br><span class="line">gpA = [G]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    G = cv.pyrDown(G)</span><br><span class="line">    gpA.append(G)</span><br><span class="line"><span class="comment"># 生成B的高斯金字塔</span></span><br><span class="line">G = B.copy()</span><br><span class="line">gpB = [G]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    G = cv.pyrDown(G)</span><br><span class="line">    gpB.append(G)</span><br><span class="line"><span class="comment"># 生成A的拉普拉斯金字塔</span></span><br><span class="line">lpA = [gpA[<span class="number">5</span>]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    GE = cv.pyrUp(gpA[i])</span><br><span class="line">    L = cv.subtract(gpA[i<span class="number">-1</span>],GE)</span><br><span class="line">    lpA.append(L)</span><br><span class="line"><span class="comment"># 生成B的拉普拉斯金字塔</span></span><br><span class="line">lpB = [gpB[<span class="number">5</span>]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    GE = cv.pyrUp(gpB[i])</span><br><span class="line">    L = cv.subtract(gpB[i<span class="number">-1</span>],GE)</span><br><span class="line">    lpB.append(L)</span><br><span class="line"><span class="comment"># 现在在每个级别中添加左右两半图像 </span></span><br><span class="line">LS = []</span><br><span class="line"><span class="keyword">for</span> la,lb <span class="keyword">in</span> zip(lpA,lpB):</span><br><span class="line">    rows,cols,dpt = la.shape</span><br><span class="line">    ls = np.hstack((la[:,<span class="number">0</span>:cols/<span class="number">2</span>], lb[:,cols/<span class="number">2</span>:]))</span><br><span class="line">    LS.append(ls)</span><br><span class="line"><span class="comment"># 现在重建</span></span><br><span class="line">ls_ = LS[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    ls_ = cv.pyrUp(ls_)</span><br><span class="line">    ls_ = cv.add(ls_, LS[i])</span><br><span class="line"><span class="comment"># 图像与直接连接的每一半</span></span><br><span class="line">real = np.hstack((A[:,:cols/<span class="number">2</span>],B[:,cols/<span class="number">2</span>:]))</span><br><span class="line">cv.imwrite(<span class="string">'Pyramid_blending2.jpg'</span>,ls_)</span><br><span class="line">cv.imwrite(<span class="string">'Direct_blending.jpg'</span>,real)</span><br><span class="line"><span class="comment">##</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理3-轮廓</title>
    <url>/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%863-%E8%BD%AE%E5%BB%93/</url>
    <content><![CDATA[<ul>
<li>9.1.<a href="#header1">轮廓入门</a></li>
<li>9.2.<a href="#header2">轮廓特征</a></li>
<li>9.3.<a href="#header3">轮廓属性</a></li>
<li>9.4.<a href="#header4">轮廓更多属性</a></li>
<li>9.5.<a href="#header5">轮廓分层</a><a id="more"></a>

</li>
</ul>
<h1 id="轮廓：入门"><a href="#轮廓：入门" class="headerlink" title="轮廓：入门"></a><span id="header1">轮廓：入门</span></h1><p>##目标<br>了解轮廓是什么。</p>
<p>学习查找轮廓，绘制轮廓等。</p>
<p>你将看到以下功能：cv.findContours()，cv.drawContours()</p>
<h2 id="什么是轮廓"><a href="#什么是轮廓" class="headerlink" title="什么是轮廓?"></a>什么是轮廓?</h2><p>轮廓可以简单地解释为连接具有相同颜色或强度的所有连续点（沿边界）的曲线。</p>
<p>轮廓是用于形状分析以及对象检测和识别的有用工具。</p>
<p>为了获得更高的准确性，请使用二进制图像。因此，在找到轮廓之前，请应用阈值或canny边缘检测。</p>
<p>从OpenCV 3.2开始，findContours()不再修改源图像。</p>
<p>在OpenCV中，找到轮廓就像从黑色背景中找到白色物体。因此请记住，要找到的对象应该是白色，背景应该是黑色。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv.threshold(imgray, <span class="number">127</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">unknown,contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)</span><br></pre></td></tr></table></figure>

<p>findcontour()函数中有三个参数，</p>
<p>第一个是源图像，</p>
<p>第二个是轮廓检索模式，</p>
<p>第三个是轮廓逼近方法。</p>
<p>输出未知量,等高线和层次结构。</p>
<p>轮廓是图像中所有轮廓的Python列表。</p>
<p>每个单独的轮廓是一个(x,y)坐标的Numpy数组的边界点的对象。</p>
<p>注意 稍后我们将详细讨论第二和第三个参数以及有关层次结构。</p>
<p>在此之前，代码示例中赋予它们的值将适用于所有图像。</p>
<h2 id="如何绘制轮廓"><a href="#如何绘制轮廓" class="headerlink" title="如何绘制轮廓?"></a>如何绘制轮廓?</h2><p>要绘制轮廓，请使用<strong>cv.drawContours</strong>函数。只要有边界点，它也可以用来绘制任何形状。</p>
<p>它的第一个参数是源图像，</p>
<p>第二个参数是应该作为Python列表传递的轮廓，</p>
<p>第三个参数是轮廓的索引（在绘制单个轮廓时有用。要绘制所有轮廓，请传递-1），其余参数是颜色，厚度等等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在图像中绘制所有轮廓：</span></span><br><span class="line">cv.drawContours(img, contours, <span class="number">-1</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制单个轮廓，如第四个轮廓：</span></span><br><span class="line">cnt = contours[<span class="number">4</span>]</span><br><span class="line">cv.drawContours(img, [cnt], <span class="number">0</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 但是在大多数情况下，以下方法会很有用：</span></span><br><span class="line">cnt = contours[<span class="number">4</span>]</span><br><span class="line">cv.drawContours(img, [cnt], <span class="number">0</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<p>轮廓近似方法<br>这是<strong>cv.findContours</strong>函数中的第三个参数。它实际上表示什么？</p>
<p>上面我们告诉我们轮廓是强度相同的形状的边界。</p>
<p>它存储形状边界的(x,y)坐标。但是它存储所有坐标吗？这是通过这种轮廓近似方法指定的。</p>
<p>如果传递<strong>cv.CHAIN_APPROX_NONE</strong>，则将存储所有边界点。</p>
<p>但是实际上我们需要所有这些要点吗？</p>
<p>例如，您找到了一条直线的轮廓。您是否需要线上的所有点来代表该线？</p>
<p>不，我们只需要该线的两个端点即可。</p>
<p>这就是<strong>cv.CHAIN_APPROX_SIMPLE</strong>所做的。它删除所有冗余点并压缩轮廓，从而节省内存。</p>
<p>下面的矩形图像演示了此技术。</p>
<p>只需在轮廓数组中的所有坐标上绘制一个圆（以蓝色绘制）。</p>
<p>第一幅图像显示了我用<strong>cv.CHAIN_APPROX_NONE</strong>获得的积分（734个点），</p>
<p>第二幅图像显示了我用<strong>cv.CHAIN_APPROX_SIMPLE</strong>获得的效果（只有4个点）。</p>
<p>看，它可以节省多少内存！！！</p>
<p><img src="http://qiniu.aihubs.net/none.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="轮廓特征"><a href="#轮廓特征" class="headerlink" title="轮廓特征"></a><span id="header2">轮廓特征</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>在本文中，我们将学习 - 如何找到轮廓的不同特征，</p>
<p>例如面积，周长，质心，边界框等。 - 您将看到大量与轮廓有关的功能。</p>
<h2 id="1-特征矩"><a href="#1-特征矩" class="headerlink" title="1. 特征矩"></a>1. 特征矩</h2><p>特征矩可以帮助您计算一些特征，例如物体的质心，物体的面积等。</p>
<p>请查看特征矩上的维基百科页面。</p>
<p>函数<strong>cv.moments</strong>()提供了所有计算出的矩值的字典。见下文：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">ret,thresh = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line">unknown,contours,hierarchy = cv.findContours(thresh, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line">M = cv.moments(cnt)</span><br><span class="line">print( M )</span><br></pre></td></tr></table></figure>

<pre><code>{&apos;m00&apos;: 2.0, &apos;m10&apos;: 536.0, &apos;m01&apos;: 1272.0, &apos;m20&apos;: 143648.3333333333, &apos;m11&apos;: 340896.0, &apos;m02&apos;: 808992.3333333333, &apos;m30&apos;: 38497932.0, &apos;m21&apos;: 91360340.0, &apos;m12&apos;: 216809945.33333334, &apos;m03&apos;: 514519548.0, &apos;mu20&apos;: 0.3333333333139308, &apos;mu11&apos;: 0.0, &apos;mu02&apos;: 0.3333333332557231, &apos;mu30&apos;: 1.4901161193847656e-08, &apos;mu21&apos;: 1.234002411365509e-08, &apos;mu12&apos;: 3.073364496231079e-08, &apos;mu03&apos;: 1.1920928955078125e-07, &apos;nu20&apos;: 0.0833333333284827, &apos;nu11&apos;: 0.0, &apos;nu02&apos;: 0.08333333331393078, &apos;nu30&apos;: 2.634178031930877e-09, &apos;nu21&apos;: 2.1814286826927578e-09, &apos;nu12&apos;: 5.432992190857434e-09, &apos;nu03&apos;: 2.1073424255447017e-08}</code></pre><p>从这一刻起，您可以提取有用的数据，<br>例如面积，质心等。质心由关系给出，$C_x\frac{M_{10}}{M_{00}}$ 和 $C_y\frac{M_{01}}{M_{00}}$。可以按照以下步骤进行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cx = int(M[<span class="string">'m10'</span>]/M[<span class="string">'m00'</span>])</span><br><span class="line">cy = int(M[<span class="string">'m01'</span>]/M[<span class="string">'m00'</span>])</span><br></pre></td></tr></table></figure>

<h2 id="2-轮廓面积"><a href="#2-轮廓面积" class="headerlink" title="2. 轮廓面积"></a>2. 轮廓面积</h2><p>轮廓区域由函数<strong>cv.contourArea</strong>()或从矩M[‘m00’]中给出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">area = cv.contourArea(cnt) </span><br><span class="line">print(area)</span><br><span class="line">print(M[<span class="string">'m00'</span>])</span><br></pre></td></tr></table></figure>

<pre><code>2.0
2.0</code></pre><h2 id="3-轮廓周长"><a href="#3-轮廓周长" class="headerlink" title="3. 轮廓周长"></a>3. 轮廓周长</h2><p>也称为弧长。可以使用<strong>cv.arcLength</strong>()函数找到它。第二个参数指定形状是闭合轮廓(True)还是曲线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">perimeter = cv.arcLength(cnt,<span class="literal">True</span>)</span><br><span class="line">perimeter</span><br></pre></td></tr></table></figure>




<pre><code>5.656854152679443</code></pre><h2 id="4-轮廓近似"><a href="#4-轮廓近似" class="headerlink" title="4. 轮廓近似"></a>4. 轮廓近似</h2><p>根据我们指定的精度，它可以将轮廓形状近似为顶点数量较少的其他形状。</p>
<p>它是Douglas-Peucker算法的实现。检查维基百科页面上的算法和演示。</p>
<p>为了理解这一点，假设您试图在图像中找到一个正方形，但是由于图像中的某些问题，您没有得到一个完美的正方形，而是一个“坏形状”（如下图所示）。</p>
<p>现在，您可以使用此功能来近似形状。在这种情况下，第二个参数称为epsilon，它是从轮廓到近似轮廓的最大距离。</p>
<p>它是一个精度参数。需要正确选择epsilon才能获得正确的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">0.1</span>*cv.arcLength(cnt,<span class="literal">True</span>) </span><br><span class="line">approx = cv.approxPolyDP(cnt,epsilon,<span class="literal">True</span>)</span><br><span class="line">print(epsilon)</span><br><span class="line">print(approx)</span><br></pre></td></tr></table></figure>

<pre><code>0.5656854152679444
[[[267 636]]

 [[268 635]]

 [[269 636]]

 [[268 637]]]</code></pre><p>下面，在第二张图片中，绿线显示了ε=弧长的10％时的近似曲线。第三幅图显示了ε=弧长度的1％时的情况。第三个参数指定曲线是否闭合。<br><img src="http://qiniu.aihubs.net/approx.jpg" alt></p>
<h2 id="5-轮廓凸包"><a href="#5-轮廓凸包" class="headerlink" title="5. 轮廓凸包"></a>5. 轮廓凸包</h2><p>凸包外观看起来与轮廓逼近相似，但不相似（在某些情况下两者可能提供相同的结果）。</p>
<p>在这里，cv.convexHull()函数检查曲线是否存在凸凹缺陷并对其进行校正。</p>
<p>一般而言，凸曲线是始终凸出或至少平坦的曲线。如果在内部凸出，则称为凸度缺陷。</p>
<p>例如，检查下面的手的图像。红线显示手的凸包。双向箭头标记显示凸度缺陷，这是凸包与轮廓线之间的局部最大偏差。</p>
<p><img src="http://qiniu.aihubs.net/convexitydefects.jpg" alt></p>
<p>关于它的语法，有一些需要讨论：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hull &#x3D; cv.convexHull(points[, hull[, clockwise[, returnPoints]]</span><br></pre></td></tr></table></figure>
<p>参数详细信息： </p>
<ul>
<li>点**是我们传递到的轮廓。 </li>
<li><strong>凸包</strong>是输出，通常我们忽略它。 </li>
<li>**顺时针方向：方向标记。如果为True，则输出凸包为顺时针方向。否则，其方向为逆时针方向。 </li>
<li>returnPoints：默认情况下为True。然后返回凸包的坐标。如果为False，则返回与凸包点相对应的轮廓点的索引。</li>
</ul>
<p>因此，要获得如上图所示的凸包，以下内容就足够了：<br>``<br>hull = cv.convexHull(cnt) </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">但是，如果要查找凸度缺陷，则需要传递returnPoints &#x3D; False。</span><br><span class="line"></span><br><span class="line">为了理解它，我们将拍摄上面的矩形图像。</span><br><span class="line"></span><br><span class="line">首先，我发现它的轮廓为cnt。现在，我发现它的带有returnPoints &#x3D; True的凸包，</span><br><span class="line"></span><br><span class="line">得到以下值：[[[234 202]]，[[51 202]]，[[51 79]]，[[234 79]]]，它们是四个角 矩形的点。</span><br><span class="line"></span><br><span class="line">现在，如果对returnPoints &#x3D; False执行相同的操作，</span><br><span class="line"></span><br><span class="line">则会得到以下结果：[[129]，[67]，[0]，[142]]。这些是轮廓中相应点的索引。</span><br><span class="line"></span><br><span class="line">例如，检查第一个值：cnt [129] &#x3D; [[234，202]]与第一个结果相同（对于其他结果依此类推）。</span><br><span class="line"></span><br><span class="line">当我们讨论凸度缺</span><br><span class="line"></span><br><span class="line">## 6. 检查凸度</span><br><span class="line">cv.isContourConvex()具有检查曲线是否凸出的功能。它只是返回True还是False。没什么大不了的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">k &#x3D; cv.isContourConvex(cnt) </span><br><span class="line">k</span><br></pre></td></tr></table></figure>




<pre><code>True</code></pre><h2 id="7-边界矩形"><a href="#7-边界矩形" class="headerlink" title="7. 边界矩形"></a>7. 边界矩形</h2><p>有两种类型的边界矩形。</p>
<h3 id="7-a-直角矩形"><a href="#7-a-直角矩形" class="headerlink" title="7.a.直角矩形"></a>7.a.直角矩形</h3><p>它是一个矩形，不考虑物体的旋转。所以边界矩形的面积不是最小的。</p>
<p>它是由函数<strong>cv.boundingRect</strong>()找到的。</p>
<p>令(x，y)为矩形的左上角坐标，而(w，h)为矩形的宽度和高度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x,y,w,h &#x3D; cv.boundingRect(cnt)</span><br><span class="line">cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)</span><br></pre></td></tr></table></figure>
<h3 id="7-b-旋转矩形"><a href="#7-b-旋转矩形" class="headerlink" title="7.b. 旋转矩形"></a>7.b. 旋转矩形</h3><p>这里，边界矩形是用最小面积绘制的，所以它也考虑了旋转。</p>
<p>使用的函数是<strong>cv.minAreaRect</strong>()。</p>
<p>它返回一个Box2D结构，其中包含以下细节 -(中心(x,y)，(宽度，高度)，旋转角度)。</p>
<p>但要画出这个矩形，我们需要矩形的四个角。</p>
<p>它由函数<strong>cv.boxPoints</strong>()获得</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rect &#x3D; cv.minAreaRect(cnt)</span><br><span class="line">box &#x3D; cv.boxPoints(rect)</span><br><span class="line">box &#x3D; np.int0(box)</span><br><span class="line">cv.drawContours(img,[box],0,(0,0,255),2)</span><br></pre></td></tr></table></figure>
<p>两个矩形都显示在一张单独的图像中。绿色矩形显示正常的边界矩形。红色矩形是旋转后的矩形。</p>
<p><img src="http://qiniu.aihubs.net/boundingrect.png" alt></p>
<h2 id="8-最小闭合圈"><a href="#8-最小闭合圈" class="headerlink" title="8. 最小闭合圈"></a>8. 最小闭合圈</h2><p>接下来，使用函数<em>*cv.minEnclosingCircle(</em>()查找对象的圆周。它是一个以最小面积完全覆盖物体的圆。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(x,y),radius &#x3D; cv.minEnclosingCircle(cnt)</span><br><span class="line">center &#x3D; (int(x),int(y))</span><br><span class="line">radius &#x3D; int(radius)</span><br><span class="line">cv.circle(img,center,radius,(0,255,0),2)</span><br><span class="line">&#96;&#96;&#96;                            </span><br><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;circumcircle.png)</span><br><span class="line"></span><br><span class="line">## 9. 拟合一个椭圆</span><br><span class="line">下一个是把一个椭圆拟合到一个物体上。它返回内接椭圆的旋转矩形。</span><br></pre></td></tr></table></figure>
<p>ellipse = cv.fitEllipse(cnt)<br>cv.ellipse(img,ellipse,(0,255,0),2)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;fitellipse.png)</span><br><span class="line"></span><br><span class="line">## 10. 拟合直线</span><br><span class="line">同样，我们可以将一条直线拟合到一组点。下图包含一组白点。我们可以近似一条直线。</span><br></pre></td></tr></table></figure>
<p>rows,cols = img.shape[:2]<br>[vx,vy,x,y] = cv.fitLine(cnt, cv.DIST_L2,0,0.01,0.01)<br>lefty = int((-x<em>vy/vx) + y)<br>righty = int(((cols-x)</em>vy/vx)+y)<br>cv.line(img,(cols-1,righty),(0,lefty),(0,255,0),2)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># &lt;span id&#x3D;&quot;header3&quot;&gt;轮廓属性&lt;&#x2F;span&gt;</span><br><span class="line">在这里，我们将学习提取一些常用的物体属性，</span><br><span class="line"></span><br><span class="line">如坚实度，等效直径，掩模图像，平均强度等。更多的功能可以在Matlab regionprops文档中找到。</span><br><span class="line"></span><br><span class="line">(注:质心、面积、周长等也属于这一类，但我们在上一章已经见过)</span><br><span class="line"></span><br><span class="line">## 1. 长宽比</span><br><span class="line">它是对象边界矩形的宽度与高度的比值。</span><br><span class="line"></span><br><span class="line">Aspect Ratio&#x3D;$\frac&#123;Width&#125;&#123;Height&#125;$</span><br></pre></td></tr></table></figure>
<p>x,y,w,h = cv.boundingRect(cnt)<br>aspect_ratio = float(w)/h</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 2. 范围</span><br><span class="line">范围是轮廓区域与边界矩形区域的比值。</span><br><span class="line"></span><br><span class="line">Extent&#x3D;$\frac&#123;Object Area&#125;&#123;Bounding Rectangle Area&#125;$</span><br></pre></td></tr></table></figure>
<p>area = cv.contourArea(cnt)<br>x,y,w,h = cv.boundingRect(cnt)<br>rect_area = w*h<br>extent = float(area)/rect_area</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 3. 坚实度</span><br><span class="line">坚实度是等高线面积与其凸包面积之比。</span><br><span class="line"></span><br><span class="line">Solidity&#x3D;$\frac&#123;Contour Area&#125;&#123;ConvexHull Area&#125;$</span><br></pre></td></tr></table></figure>
<p>area = cv.contourArea(cnt)<br>hull = cv.convexHull(cnt)<br>hull_area = cv.contourArea(hull)<br>solidity = float(area)/hull_area</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 4. 等效直径</span><br><span class="line">等效直径是面积与轮廓面积相同的圆的直径。</span><br><span class="line"></span><br><span class="line">EquivalentDiameter&#x3D;$\sqrt&#123;\frac&#123;4×ContourArea&#125;&#123;\Pi&#125;&#125;$</span><br></pre></td></tr></table></figure>
<p>area = cv.contourArea(cnt)<br>equi_diameter = np.sqrt(4*area/np.pi)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 5. 取向</span><br><span class="line">取向是物体指向的角度。以下方法还给出了主轴和副轴的长度。</span><br></pre></td></tr></table></figure>
<p>(x,y),(MA,ma),angle = cv.fitEllipse(cnt)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 6. 掩码和像素点</span><br><span class="line">在某些情况下，我们可能需要构成该对象的所有点。可以按照以下步骤完成：</span><br></pre></td></tr></table></figure>
<p>mask = np.zeros(imgray.shape,np.uint8)<br>cv.drawContours(mask,[cnt],0,255,-1)<br>pixelpoints = np.transpose(np.nonzero(mask))<br>#pixelpoints = cv.findNonZero(mask)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这里提供了两个方法，一个使用Numpy函数，另一个使用OpenCV函数(最后的注释行)。结果也是一样的，只是略有不同。</span><br><span class="line"></span><br><span class="line">Numpy给出的坐标是(行、列)格式，</span><br><span class="line"></span><br><span class="line">而OpenCV给出的坐标是(x,y)格式。所以基本上答案是可以互换的。注意，row &#x3D; x, column &#x3D; y。</span><br><span class="line"></span><br><span class="line">## 7. 最大值，最小值和它们的位置</span><br><span class="line">我们可以使用掩码图像找到这些参数。</span><br></pre></td></tr></table></figure>
<p>min_val, max_val, min_loc, max_loc = cv.minMaxLoc(imgray,mask = mask)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 8. 平均颜色或平均强度</span><br><span class="line">在这里，我们可以找到对象的平均颜色。或者可以是灰度模式下物体的平均强度。我们再次使用相同的掩码进行此操作。</span><br></pre></td></tr></table></figure>
<p>mean_val = cv.mean(im,mask = mask)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 9. 极端点</span><br><span class="line">极点是指对象的最顶部，最底部，最右侧和最左侧的点。</span><br></pre></td></tr></table></figure>
<p>leftmost = tuple(cnt[cnt[:,:,0].argmin()][0])<br>rightmost = tuple(cnt[cnt[:,:,0].argmax()][0])<br>topmost = tuple(cnt[cnt[:,:,1].argmin()][0])<br>bottommost = tuple(cnt[cnt[:,:,1].argmax()][0])</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">例如，如果我将其应用于印度地图，则会得到以下结果：</span><br><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;extremepoints.jpg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br></pre></td></tr></table></figure>


<h1 id="轮廓：更多属性"><a href="#轮廓：更多属性" class="headerlink" title="轮廓：更多属性"></a><span id="header4">轮廓：更多属性</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习 - 凸性缺陷以及如何找到它们 - 查找点到多边形的最短距离 - 匹配不同的形状</p>
<h2 id="理论和代码"><a href="#理论和代码" class="headerlink" title="理论和代码"></a>理论和代码</h2><h3 id="1-凸性缺陷"><a href="#1-凸性缺陷" class="headerlink" title="1. 凸性缺陷"></a>1. 凸性缺陷</h3><p>我们看到了关于轮廓的第二章的凸包。从这个凸包上的任何偏差都可以被认为是凸性缺陷。 OpenCV有一个函数来找到这个,cv.convexityDefects()。一个基本的函数调用如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hull &#x3D; cv.convexHull(cnt,returnPoints &#x3D; False)</span><br><span class="line">defects &#x3D; cv.convexityDefects(cnt,hull)</span><br></pre></td></tr></table></figure>
<p>注意 记住,我们必须在发现凸包时,传递returnPoints= False,以找到凸性缺陷。</p>
<p>它返回一个数组，其中每行包含这些值—[起点、终点、最远点、到最远点的近似距离]。我们可以用图像把它形象化。我们画一条连接起点和终点的线，然后在最远处画一个圆。记住，返回的前三个值是cnt的索引。所以我们必须从cnt中获取这些值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(&#39;star.jpg&#39;)</span><br><span class="line">img_gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,thresh &#x3D; cv.threshold(img_gray, 127, 255,0)</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh,2,1)</span><br><span class="line">cnt &#x3D; contours[0]</span><br><span class="line">hull &#x3D; cv.convexHull(cnt,returnPoints &#x3D; False)</span><br><span class="line">defects &#x3D; cv.convexityDefects(cnt,hull)</span><br><span class="line">for i in range(defects.shape[0]):</span><br><span class="line">    s,e,f,d &#x3D; defects[i,0]</span><br><span class="line">    start &#x3D; tuple(cnt[s][0])</span><br><span class="line">    end &#x3D; tuple(cnt[e][0])</span><br><span class="line">    far &#x3D; tuple(cnt[f][0])</span><br><span class="line">    cv.line(img,start,end,[0,255,0],2)</span><br><span class="line">    cv.circle(img,far,5,[0,0,255],-1)</span><br><span class="line">cv.imshow(&#39;img&#39;,img)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>查看结果：<br><img src="http://qiniu.aihubs.net/defects.jpg" alt></p>
<h3 id="2-点多边形测试"><a href="#2-点多边形测试" class="headerlink" title="2. 点多边形测试"></a>2. 点多边形测试</h3><p>这个函数找出图像中一点到轮廓线的最短距离。它返回的距离，点在轮廓线外时为负，点在轮廓线内时为正，点在轮廓线上时为零。</p>
<p>例如，我们可以检查点(50,50)如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dist &#x3D; cv.pointPolygonTest(cnt,(50,50),True)</span><br></pre></td></tr></table></figure>
<p>在函数中，第三个参数是measureDist。如果它是真的，它会找到有符号的距离。如果为假，则查找该点是在轮廓线内部还是外部(分别返回+1、-1和0)。</p>
<p>注意 如果您不想找到距离，请确保第三个参数为False，因为这是一个耗时的过程。因此，将其设置为False可使速度提高2-3倍。</p>
<h3 id="3-形状匹配"><a href="#3-形状匹配" class="headerlink" title="3. 形状匹配"></a>3. 形状匹配</h3><p>OpenCV附带一个函数<strong>cv.matchShapes</strong>()，该函数使我们能够比较两个形状或两个轮廓，并返回一个显示相似性的度量。结果越低，匹配越好。它是根据矩值计算出来的。不同的测量方法在文档中有解释。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img1 &#x3D; cv.imread(&#39;star.jpg&#39;,0)</span><br><span class="line">img2 &#x3D; cv.imread(&#39;star2.jpg&#39;,0)</span><br><span class="line">ret, thresh &#x3D; cv.threshold(img1, 127, 255,0)</span><br><span class="line">ret, thresh2 &#x3D; cv.threshold(img2, 127, 255,0)</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh,2,1)</span><br><span class="line">cnt1 &#x3D; contours[0]</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh2,2,1)</span><br><span class="line">cnt2 &#x3D; contours[0]</span><br><span class="line">ret &#x3D; cv.matchShapes(cnt1,cnt2,1,0.0)</span><br><span class="line">print( ret )</span><br></pre></td></tr></table></figure>
<p>我尝试过匹配下面给出的不同形状的形状：<br><img src="http://qiniu.aihubs.net/matchshapes.jpg" alt></p>
<p>我得到以下结果: - 匹配的图像A与本身= 0.0 - 匹配图像A与图像B = 0.001946 - 匹配图像A与图像C = 0.326911</p>
<p>看,即使是图像旋转也不会对这个比较产生很大的影响。</p>
<p>参考 Hu矩是平移、旋转和比例不变的七个矩。第七个是无偏斜量。这些值可以使用<strong>cpu.HuMoments</strong>()函数找到。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="轮廓分层"><a href="#轮廓分层" class="headerlink" title="轮廓分层"></a><span id="header5">轮廓分层</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>这次我们学习轮廓的层次，即轮廓中的父子关系。</p>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>在前几篇关于轮廓的文章中，我们已经讨论了与OpenCV提供的轮廓相关的几个函数。</p>
<p>但是当我们使用<strong>cv.findcontour</strong>()函数在图像中找到轮廓时，我们已经传递了一个参数，轮廓检索模式。</p>
<p>我们通常通过了<strong>cv.RETR_LIST</strong>或<strong>cv.RETR_TREE</strong>，效果很好。但这到底意味着什么呢?</p>
<p>另外，在输出中，我们得到了三个数组，第一个是图像，第二个是轮廓，还有一个我们命名为<strong>hierarchy</strong>的输出(请检查前面文章中的代码)。</p>
<p>但我们从未在任何地方使用过这种层次结构。那么这个层级是什么?它是用来做什么的?它与前面提到的函数参数有什么关系?</p>
<p>这就是我们在本文中要讨论的内容。</p>
<h2 id="层次结构是什么？"><a href="#层次结构是什么？" class="headerlink" title="层次结构是什么？"></a>层次结构是什么？</h2><p>通常我们使用<strong>cv.findcontour</strong>()函数来检测图像中的对象，对吧？</p>
<p>有时对象在不同的位置。但在某些情况下，某些形状在其他形状中。</p>
<p>就像嵌套的图形一样。在这种情况下，我们把外部的称为<strong>父类</strong>，把内部的称为<strong>子类</strong>。</p>
<p>这样，图像中的轮廓就有了一定的相互关系。</p>
<p>我们可以指定一个轮廓是如何相互连接的，比如，它是另一个轮廓的子轮廓，还是父轮廓等等。这种关系的表示称为<strong>层次结构</strong>。</p>
<p><img src="http://qiniu.aihubs.net/hierarchy.png" alt><br>在这张图中，有一些形状我已经从<strong>0-5</strong>开始编号。<em>2</em>和<em>2a</em>表示最外层盒子的外部和内部轮廓。</p>
<p>这里，等高线0,1,2在<strong>外部或最外面</strong>。我们可以说，它们在<strong>层级-0</strong>中，或者简单地说，它们在<strong>同一个层级</strong>中。</p>
<p>其次是<strong>contour-2a</strong>。它可以被认为是<strong>contour-2的子级</strong>(或者反过来，contour-2是contour-2a的父级)。</p>
<p>假设它在<strong>层级-1</strong>中。类似地，contour-3是contour-2的子级，它位于下一个层次结构中。</p>
<p>最后，轮廓4,5是contour-3a的子级，他们在最后一个层级。</p>
<p>从对方框的编号来看，我认为contour-4是contour-3a的第一个子级(它也可以是contour-5)。</p>
<p>我提到这些是为了理解一些术语，比如<strong>相同层级</strong>，外部轮廓，子轮廓，父轮廓，<strong>第一个子轮廓</strong>等等。现在让我们进入OpenCV。</p>
<h2 id="OpenCV中的分级表示"><a href="#OpenCV中的分级表示" class="headerlink" title="OpenCV中的分级表示"></a>OpenCV中的分级表示</h2><p>所以每个轮廓都有它自己的信息关于它是什么层次，谁是它的孩子，谁是它的父母等等。</p>
<p>OpenCV将它表示为一个包含四个值的数组:[Next, Previous, First_Child, Parent]</p>
<p>“Next表示同一层次的下一个轮廓。”</p>
<p>例如，在我们的图片中取contour-0。谁是下一个同级别的等高线?这是contour-1。</p>
<p>简单地令Next = 1。类似地，Contour-1也是contour-2。所以Next = 2。 contour-2呢?同一水平线上没有下一条等高线。</p>
<p>简单地，让Next = -1。contour-4呢?它与contour-5处于同一级别。它的下一条等高线是contour-5，所以next = 5。</p>
<p>“Previous表示同一层次上的先前轮廓。”</p>
<p>和上面一样。contour-1之前的等值线为同级别的contour-0。</p>
<p>类似地，contour-2也是contour-1。对于contour-0，没有前项，所以设为-1。</p>
<p>“First_Child表示它的第一个子轮廓。”</p>
<p>没有必要作任何解释。对于contour-2, child是contour-2a。从而得到contour-2a对应的指标值。</p>
<p>contour-3a呢?它有两个孩子。但我们只关注第一个孩子。它是contour-4。那么First_Child = 4 对contour-3a而言。</p>
<p>“Parent表示其父轮廓的索引。”</p>
<p>它与<strong>First_Child</strong>相反。对于轮廓线-4和轮廓线-5，父轮廓线都是轮廓线-3a。对于轮廓3a，它是轮廓-3，以此类推。</p>
<p>注意 如果没有子元素或父元素，则该字段被视为-1</p>
<p>现在我们已经了解了OpenCV中使用的层次样式，我们可以借助上面给出的相同图像来检查OpenCV中的轮廓检索模式。</p>
<p>一些标志如 cv.RETR_LIST, cv.RETR_TREE,cv.RETR_CCOMP, <strong>cv.RETR_EXTERNAL</strong>等等的含义。</p>
<h2 id="轮廓检索模式"><a href="#轮廓检索模式" class="headerlink" title="轮廓检索模式"></a>轮廓检索模式</h2><h3 id="1-RETR-LIST"><a href="#1-RETR-LIST" class="headerlink" title="1. RETR_LIST"></a>1. RETR_LIST</h3><p>这是四个标志中最简单的一个(从解释的角度来看)。它只是检索所有的轮廓，但不创建任何亲子关系。</p>
<p>在这个规则下，父轮廓和子轮廓是平等的，他们只是轮廓。他们都属于同一层级。</p>
<p>这里，第3和第4项总是-1。但是很明显，下一项和上一项都有对应的值。你自己检查一下就可以了。</p>
<p>下面是我得到的结果，每一行是对应轮廓的层次细节。例如，第一行对应于轮廓0。下一条轮廓是轮廓1。所以Next = 1。</p>
<p>没有先前的轮廓，所以Previous=-1。剩下的两个，如前所述，是-1。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 1, -1, -1, -1],</span><br><span class="line">        [ 2,  0, -1, -1],</span><br><span class="line">        [ 3,  1, -1, -1],</span><br><span class="line">        [ 4,  2, -1, -1],</span><br><span class="line">        [ 5,  3, -1, -1],</span><br><span class="line">        [ 6,  4, -1, -1],</span><br><span class="line">        [ 7,  5, -1, -1],</span><br><span class="line">        [-1,  6, -1, -1]]])</span><br></pre></td></tr></table></figure>
<p>如果您没有使用任何层次结构特性，那么这是在您的代码中使用的最佳选择。</p>
<h3 id="2-RETR-EXTERNAL"><a href="#2-RETR-EXTERNAL" class="headerlink" title="2. RETR_EXTERNAL"></a>2. RETR_EXTERNAL</h3><p>如果使用此标志，它只返回极端外部标志。所有孩子的轮廓都被留下了。</p>
<p>我们可以说，根据这项规则，每个家庭只有长子得到关注。它不关心家庭的其他成员:)。</p>
<p>所以在我们的图像中，有多少个极端的外轮廓?在等级0级?有3个，即等值线是0 1 2，对吧?</p>
<p>现在试着用这个标志找出等高线。这里，给每个元素的值与上面相同。并与上述结果进行了比较。以下是我得到的:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 1, -1, -1, -1],</span><br><span class="line">        [ 2,  0, -1, -1],</span><br><span class="line">        [-1,  1, -1, -1]]])</span><br></pre></td></tr></table></figure>
<p>如果只想提取外部轮廓，可以使用此标志。它在某些情况下可能有用。</p>
<h3 id="3-RETR-CCOMP"><a href="#3-RETR-CCOMP" class="headerlink" title="3. RETR_CCOMP"></a>3. RETR_CCOMP</h3><p>此标志检索所有轮廓并将其排列为2级层次结构。物体的外部轮廓(即物体的边界)放在层次结构-1中。</p>
<p>对象内部孔洞的轮廓(如果有)放在层次结构-2中。如果其中有任何对象，则其轮廓仅在层次结构1中重新放置。以及它在层级2中的漏洞等等。</p>
<p>只需考虑在黑色背景上的“白色的零”图像。零的外圆属于第一级，零的内圆属于第二级。</p>
<p>我们可以用一个简单的图像来解释它。这里我用红色标注了等高线的顺序和它们所属的层次，用绿色标注(1或2)，顺序与OpenCV检测等高线的顺序相同。<br><img src="http://qiniu.aihubs.net/ccomp_hierarchy.png" alt><br>考虑第一个轮廓，即contour-0。这是hierarchy-1。它有两个孔，分别是等高线1和2，属于第二级。</p>
<p>因此，对于轮廓-0，在同一层次的下一个轮廓是轮廓-3。previous也没有。在hierarchy-2中，它的第一个子结点是contour-1。</p>
<p>它没有父类，因为它在hierarchy-1中。所以它的层次数组是[3，-1,1，-1]</p>
<p>现在contour-1。它在层级-2中。相同层次结构中的下一个(在contour-1的父母关系下)是contour-2。</p>
<p>没有previous。没有child，但是parent是contour-0。所以数组是[2，-1，-1,0]</p>
<p>类似的contour-2:它在hierarchy-2中。在contour-0下，同一层次结构中没有下一个轮廓。</p>
<p>所以没有Next。previous是contour-1。没有child，parent是contour0。所以数组是[-1,1，-1,0]</p>
<p>contour-3:层次-1的下一个是轮廓-5。以前是contour-0。child是contour4，没有parent。所以数组是[5,0,4，-1]</p>
<p>contour-4:它在contour-3下的层次结构2中，它没有兄弟姐妹。没有next，没有previous，没有child，parent是contour-3。</p>
<p>所以数组是[-1，-1，-1,3]</p>
<p>剩下的你可以补充。这是我得到的最终答案:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 3, -1,  1, -1],</span><br><span class="line">        [ 2, -1, -1,  0],</span><br><span class="line">        [-1,  1, -1,  0],</span><br><span class="line">        [ 5,  0,  4, -1],</span><br><span class="line">        [-1, -1, -1,  3],</span><br><span class="line">        [ 7,  3,  6, -1],</span><br><span class="line">        [-1, -1, -1,  5],</span><br><span class="line">        [ 8,  5, -1, -1],</span><br><span class="line">        [-1,  7, -1, -1]]])</span><br></pre></td></tr></table></figure>
<h3 id="4-RETR-TREE"><a href="#4-RETR-TREE" class="headerlink" title="4. RETR_TREE"></a>4. RETR_TREE</h3><p>这是最后一个家伙，完美先生。它检索所有的轮廓并创建一个完整的家族层次结构列表。它甚至告诉，谁是爷爷，父亲，儿子，孙子，甚至更多…:)。</p>
<p>例如，我拿上面的图片，重写了cv的代码。RETR_TREE，根据OpenCV给出的结果重新排序等高线并进行分析。</p>
<p>同样，红色的字母表示轮廓数，绿色的字母表示层次顺序。<br><img src="http://qiniu.aihubs.net/tree_hierarchy.png" alt><br>取contour-0:它在hierarchy-0中。同一层次结构的next轮廓是轮廓-7。没有previous的轮廓。child是contour-1，没有parent。所以数组是[7，-1,1，-1]</p>
<p>以contour-2为例:它在hierarchy-1中。没有轮廓在同一水平。没有previous。child是contour-3。父母是contour-1。所以数组是[-1，-1,3,1]</p>
<p>剩下的，你自己试试。以下是完整答案:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 7, -1,  1, -1],</span><br><span class="line">        [-1, -1,  2,  0],</span><br><span class="line">        [-1, -1,  3,  1],</span><br><span class="line">        [-1, -1,  4,  2],</span><br><span class="line">        [-1, -1,  5,  3],</span><br><span class="line">        [ 6, -1, -1,  4],</span><br><span class="line">        [-1,  5, -1,  4],</span><br><span class="line">        [ 8,  0, -1, -1],</span><br><span class="line">        [-1,  7, -1, -1]]])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>notebook使用</title>
    <url>/2020/07/08/notebook%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">主题修改</a></li>
<li>2.<a href="#header2">密码修改</a></li>
<li>3.<a href="#header3">添加内核</a></li>
</ul>
<a id="more"></a>

<h1 id="主题修改参考"><a href="#主题修改参考" class="headerlink" title="主题修改参考"></a><span id="header1">主题修改<a href="https://blog.csdn.net/wh8514/article/details/81532286/" target="_blank" rel="noopener">参考</a></span></h1><p>安装Jupyter主题：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install  jupyterthemes</span><br></pre></td></tr></table></figure>
<p>更新Jupyter主题：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install --upgrade jupyterthemes</span><br></pre></td></tr></table></figure>
<p>装和更新成功以后，可以查看可用主题：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jt -l</span><br></pre></td></tr></table></figure>
<p>如图<img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/2.jpg" alt="x"><br>参数含义如下<br><img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/3.png" alt="x"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jt -t monokai -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T -N</span><br><span class="line">-f(字体)  -fs(字体大小) -cellw(占屏比或宽度)  -ofs(输出段的字号)  -T(显示工具栏)  -N(显示自己主机名)</span><br></pre></td></tr></table></figure>
<p>解决输出显示不全的问题<br><br>在C:\Users\XXX.jupyter\custom\custom.css里,找到div.output_area</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">div.output_area&#123;</span><br><span class="line">    display:-webkit-box;</span><br><span class="line">&#125;</span><br><span class="line">改为</span><br><span class="line">div.output_area&#123;</span><br><span class="line">    display:-webkit-box;</span><br><span class="line">    padding:13px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="密码修改"><a href="#密码修改" class="headerlink" title="密码修改"></a><span id="header2">密码修改</span></h1><p>终端运行，输入新的密码即可<a href="https://blog.csdn.net/qq_36950604/article/details/103848631" target="_blank" rel="noopener">参考</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/1.jpg" alt="x"></p>
<h1 id="添加内核"><a href="#添加内核" class="headerlink" title="添加内核"></a><span id="header3">添加内核</span></h1><ul>
<li>首先添加虚拟环境,比如我要添加py版本为2.7,环境名为py2的环境<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n py2 python&#x3D;2.7</span><br></pre></td></tr></table></figure></li>
<li>然后进入当前虚拟环境下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">activate py2</span><br></pre></td></tr></table></figure></li>
<li>在虚拟环境中安装ipykernel<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install ipykernel</span><br></pre></td></tr></table></figure></li>
<li>添加内核到jupyter里:python -m ipykernel install –user –name 环境名称 –display-name “在jupyter中显示的环境名称”，注意不要忘记了双引号<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m ipykernel install --user --name py2 --display-name &quot;py2&quot;</span><br></pre></td></tr></table></figure>
成功<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/py2-1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/py2.png" alt></li>
</ul>
]]></content>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv图像核心操作</title>
    <url>/2020/07/12/opencv%E5%9B%BE%E5%83%8F%E6%A0%B8%E5%BF%83%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">图片的基本操作</a></li>
<li>2.<a href="#header2">图片的算法操作</a></li>
<li>3.<a href="#header3">性能衡量和提升技术</a><a id="more"></a>

</li>
</ul>
<h1 id="图像的基本操作"><a href="#图像的基本操作" class="headerlink" title="图像的基本操作"></a><span id="header1">图像的基本操作</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>学会： - 访问像素值并修改它们 - 访问图像属性 - 设置感兴趣区域(ROI) - 分割和合并图像</p>
<p>本节中的几乎所有操作都主要与Numpy相关，而不是与OpenCV相关。要使用OpenCV编写更好的优化代码，需要Numpy的丰富知识。</p>
<h2 id="访问和修改像素值"><a href="#访问和修改像素值" class="headerlink" title="访问和修改像素值"></a>访问和修改像素值</h2><p>对于 BGR 图像，它返回一个由蓝色、绿色和红色值组成的数组。对于灰度图像，只返回相应的灰度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>) <span class="comment"># 载入彩色图像</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">px = img[<span class="number">210</span>,<span class="number">490</span>] <span class="comment"># 访问210,490点处的全部元素</span></span><br><span class="line">px</span><br></pre></td></tr></table></figure>




<pre><code>array([237, 189, 147], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blue = img[<span class="number">210</span>,<span class="number">490</span>,<span class="number">0</span>] <span class="comment"># 仅访问蓝色元素</span></span><br><span class="line">blue</span><br></pre></td></tr></table></figure>




<pre><code>237</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img[<span class="number">100</span>,<span class="number">100</span>] = [<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>]<span class="comment"># 修改像素值</span></span><br><span class="line">img[<span class="number">100</span>,<span class="number">100</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([255, 255, 255], dtype=uint8)</code></pre><h3 id="警告"><a href="#警告" class="headerlink" title="警告"></a>警告</h3><p>Numpy是用于快速数组计算的优化库。因此，简单地访问每个像素值并对其进行修改将非常缓慢，因此不建议使用。</p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>上面的方法通常用于选择数组的区域，例如前5行和后3列。</p>
<p>对于单个像素访问，Numpy数组方法array.item()和array.itemset())被认为更好，但是它们始终返回标量。</p>
<p>如果要访问所有B，G，R值，则需要分别调用所有的array.item()。</p>
<p>下面是更好的像素访问和编辑方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">red = img.item(<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">red</span><br></pre></td></tr></table></figure>




<pre><code>255</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.itemset((<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>),<span class="number">222</span>)</span><br><span class="line">red = img.item(<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">red</span><br></pre></td></tr></table></figure>




<pre><code>222</code></pre><h2 id="访问图像属性"><a href="#访问图像属性" class="headerlink" title="访问图像属性"></a>访问图像属性</h2><p>图像属性包括行数，列数和通道数，图像数据类型，像素数等。</p>
<p>图像的形状可通过img.shape访问。它返回行，列和通道数的元组（如果图像是彩色的）：</p>
<p>注意 如果图像是灰度的，则返回的元组仅包含行数和列数，因此这是检查加载的图像是灰度还是彩色的好方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.shape</span><br></pre></td></tr></table></figure>




<pre><code>(640, 640, 3)</code></pre><p>像素总数可通过访问img.size：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.size</span><br></pre></td></tr></table></figure>




<pre><code>1228800</code></pre><p>图像数据类型通过img.dtype获得：</p>
<p>注意 img.dtype在调试时非常重要，因为OpenCV-Python代码中的大量错误是由无效的数据类型引起的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.dtype</span><br></pre></td></tr></table></figure>




<pre><code>dtype(&apos;uint8&apos;)</code></pre><h2 id="图像感兴趣区域ROI"><a href="#图像感兴趣区域ROI" class="headerlink" title="图像感兴趣区域ROI"></a>图像感兴趣区域ROI</h2><p>有时候，你不得不处理一些特定区域的图像。</p>
<p>对于图像中的眼睛检测，首先对整个图像进行人脸检测。</p>
<p>在获取人脸图像时，我们只选择人脸区域，搜索其中的眼睛，而不是搜索整个图像。</p>
<p>它提高了准确性(因为眼睛总是在面部上:D )和性能(因为我们搜索的区域很小)。</p>
<p>使用Numpy索引再次获得ROI。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ball = img[<span class="number">180</span>:<span class="number">240</span>, <span class="number">230</span>:<span class="number">290</span>]</span><br><span class="line">img[<span class="number">273</span>:<span class="number">333</span>, <span class="number">100</span>:<span class="number">160</span>] = ball </span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="拆分和合并图像通道"><a href="#拆分和合并图像通道" class="headerlink" title="拆分和合并图像通道"></a>拆分和合并图像通道</h2><p>有时你需要分别处理图像的B，G，R通道。在这种情况下，你需要将BGR图像拆分为单个通道。</p>
<p>在其他情况下，你可能需要将这些单独的频道加入BGR图片。你可以通过以下方式简单地做到这一点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b,g,r = cv.split(img)</span><br><span class="line">img = cv.merge((b,g,r))</span><br></pre></td></tr></table></figure>

<p>假设你要将所有红色像素都设置为零，则无需先拆分通道。numpy索引更快</p>
<p>警告</p>
<p>cv.split()是一项耗时的操作（就时间而言）。因此，仅在必要时才这样做。否则请进行Numpy索引。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img [:, :, <span class="number">2</span>] = <span class="number">0</span></span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="为图像设置边框（填充）"><a href="#为图像设置边框（填充）" class="headerlink" title="为图像设置边框（填充）"></a>为图像设置边框（填充）</h2><p>如果要在图像周围创建边框（如相框），则可以使用cv.copyMakeBorder()。但是它在卷积运算，零填充等方面有更多应用。此函数采用以下参数：</p>
<p>src - 输入图像</p>
<p>top，bottom，left，right 边界宽度（以相应方向上的像素数为单位）</p>
<p>borderType - 定义要添加哪种边框的标志。它可以是以下类型：</p>
<ul>
<li>cv.BORDER_CONSTANT - 添加恒定的彩色边框。该值应作为下一个参数给出。</li>
<li>cv.BORDER_REFLECT - 边框将是边框元素的镜像，如下所示： fedcba | abcdefgh | hgfedcb</li>
<li>cv.BORDER_REFLECT_101或 cv.BORDER_DEFAULT与上述相同，但略有变化，例如： gfedcb | abcdefgh | gfedcba</li>
<li>cv.BORDER_REPLICATE最后一个元素被复制，像这样： aaaaaa | abcdefgh | hhhhhhh</li>
<li>cv.BORDER_WRAP难以解释，它看起来像这样： cdefgh | abcdefgh | abcdefg</li>
</ul>
<p>value -边框的颜色，如果边框类型为<strong>cv.BORDER_CONSTANT</strong></p>
<p>下面是一个示例代码，演示了所有这些边框类型，以便更好地理解：<br>(图像与matplotlib一起显示。因此红色和蓝色通道将互换)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">BLUE = [<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">img1 = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">replicate = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REPLICATE)</span><br><span class="line">reflect = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REFLECT)</span><br><span class="line">reflect101 = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REFLECT_101)</span><br><span class="line">wrap = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_WRAP)</span><br><span class="line">constant= cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_CONSTANT,value=BLUE)</span><br><span class="line">plt.subplot(<span class="number">231</span>),plt.imshow(img1,<span class="string">'gray'</span>),plt.title(<span class="string">'ORIGINAL'</span>)</span><br><span class="line">plt.subplot(<span class="number">232</span>),plt.imshow(replicate,<span class="string">'gray'</span>),plt.title(<span class="string">'REPLICATE'</span>)</span><br><span class="line">plt.subplot(<span class="number">233</span>),plt.imshow(reflect,<span class="string">'gray'</span>),plt.title(<span class="string">'REFLECT'</span>)</span><br><span class="line">plt.subplot(<span class="number">234</span>),plt.imshow(reflect101,<span class="string">'gray'</span>),plt.title(<span class="string">'REFLECT_101'</span>)</span><br><span class="line">plt.subplot(<span class="number">235</span>),plt.imshow(wrap,<span class="string">'gray'</span>),plt.title(<span class="string">'WRAP'</span>)</span><br><span class="line">plt.subplot(<span class="number">236</span>),plt.imshow(constant,<span class="string">'gray'</span>),plt.title(<span class="string">'CONSTANT'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E5%9B%BE%E5%83%8F%E6%A0%B8%E5%BF%83%E6%93%8D%E4%BD%9C/output_21_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="图像上的算术运算"><a href="#图像上的算术运算" class="headerlink" title="图像上的算术运算"></a><span id="header2">图像上的算术运算</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>学习图像的几种算术运算，例如加法，减法，按位运算等。</p>
<p>您将学习以下功能：cv.add，<strong>cv.addWeighted</strong>等。</p>
<h2 id="图像加法"><a href="#图像加法" class="headerlink" title="图像加法"></a>图像加法</h2><p>您可以通过OpenCV函数cv.add()或仅通过numpy操作res = img1 + img2添加两个图像。</p>
<p>两个图像应具有相同的深度和类型，或者第二个图像可以只是一个标量值。</p>
<p>注意 OpenCV加法和Numpy加法之间有区别。OpenCV加法是饱和运算，而Numpy加法是模运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.uint8([<span class="number">250</span>])</span><br><span class="line">y = np.uint8([<span class="number">10</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x+y  <span class="comment"># 250+10 = 260 % 256 = 4</span></span><br></pre></td></tr></table></figure>




<pre><code>array([4], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.add(x,y) <span class="comment"># 250+10 = 260 =&gt; 255</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[255]], dtype=uint8)</code></pre><h2 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h2><p>这也是图像加法，但是对图像赋予不同的权重，以使其具有融合或透明的感觉。根据以下等式添加图像：</p>
<p>G(x)=(1−α)f0(x)+αf1(x)<br>通过从 α 从 0→1 更改，您可以在一个图像到另一个图像之间执行很酷的过渡。</p>
<p>将两幅图像合在一起。第一幅图像的权重为0.7，第二幅图像的权重为0.3。</p>
<p>cv.addWeighted()在图像上应用以下公式。</p>
<p>dst=α⋅img1+β⋅img2+γ<br>在这里，γ 被视为零。</p>
<p>先保存一个和avatar1大小一样上下相反的图像avatar2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">newImg = img.copy() <span class="comment"># 深拷贝</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(img.shape[<span class="number">0</span>]):</span><br><span class="line">    newImg[img.shape[<span class="number">0</span>]<span class="number">-1</span>-i] = img[i]</span><br><span class="line">cv.imshow(<span class="string">'x'</span>,newImg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imwrite(<span class="string">'avatar2.jpg'</span>,newImg)</span><br></pre></td></tr></table></figure>




<pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img1 = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">'avatar2.jpg'</span>)</span><br><span class="line">dst = cv.addWeighted(img1,<span class="number">0.7</span>,img2,<span class="number">0.3</span>,<span class="number">0</span>)</span><br><span class="line">cv.imshow(<span class="string">'dst'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="按位运算"><a href="#按位运算" class="headerlink" title="按位运算"></a>按位运算</h2><p>这包括按位 AND、 OR、NOT 和 XOR 操作。它们在提取图像的任何部分(我们将在后面的章节中看到)、定义和处理非矩形 ROI 等方面非常有用。 </p>
<p>下面我们将看到一个例子，如何改变一个图像的特定区域。 </p>
<p>我想把 OpenCV 的标志放在一个图像上面。如果我添加两个图像，它会改变颜色。如果我混合它，我得到一个透明的效果。</p>
<p>但我希望它是不透明的。如果是一个矩形区域，</p>
<p>我可以使用 ROI，就像我们在上一章中所做的那样。</p>
<p>但是 OpenCV 的 logo 不是长方形的。所以你可以使用如下的按位操作来实现:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载两张图片</span></span><br><span class="line">img1 = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line"><span class="comment"># 我想把logo放在左上角，所以我创建了ROI</span></span><br><span class="line">rows,cols,channels = img2.shape</span><br><span class="line">roi = img1[<span class="number">0</span>:rows, <span class="number">0</span>:cols ]</span><br><span class="line"><span class="comment"># 现在创建logo的掩码，并同时创建其相反掩码</span></span><br><span class="line">img2gray = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, mask = cv.threshold(img2gray, <span class="number">10</span>, <span class="number">255</span>, cv.THRESH_BINARY)</span><br><span class="line">mask_inv = cv.bitwise_not(mask)</span><br><span class="line">cv.imshow(<span class="string">'img2gray'</span>,img2gray)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'mask'</span>,mask)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'mask_inv'</span>,mask_inv)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"><span class="comment"># 现在将ROI中logo的区域涂黑</span></span><br><span class="line">img1_bg = cv.bitwise_and(roi,roi,mask = mask_inv)</span><br><span class="line"><span class="comment"># 仅从logo图像中提取logo区域</span></span><br><span class="line">img2_fg = cv.bitwise_and(img2,img2,mask = mask)</span><br><span class="line"><span class="comment"># 将logo放入ROI并修改主图像</span></span><br><span class="line">cv.imshow(<span class="string">'img1_bg'</span>,img1_bg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'img2_bg'</span>,img2_fg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">dst = cv.add(img1_bg,img2_fg)</span><br><span class="line">img1[<span class="number">0</span>:rows, <span class="number">0</span>:cols ] = dst</span><br><span class="line">cv.imshow(<span class="string">'res'</span>,img1)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<h1 id="性能衡量和提升技术"><a href="#性能衡量和提升技术" class="headerlink" title="性能衡量和提升技术"></a><span id="header3">性能衡量和提升技术</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在图像处理中，由于每秒要处理大量操作，因此必须使代码不仅提供正确的解决方案，而且还必须以最快的方式提供。</p>
<p>将学习衡量代码的性能,一些提高代码性能的技巧。</p>
<p>你将看到以下功能：cv.getTickCount，<strong>cv.getTickFrequency</strong>等。</p>
<p>除了OpenCV，Python还提供了一个模块<strong>time</strong>，这有助于衡量执行时间。</p>
<p>另一个模块<strong>profile</strong>有助于获取有关代码的详细报告，例如代码中每个函数花费了多少时间，调用了函数的次数等。</p>
<p>但是，如果你使用的是IPython，则所有这些功能都集成在用户友好的界面中方式。</p>
<h2 id="使用OpenCV衡量性能"><a href="#使用OpenCV衡量性能" class="headerlink" title="使用OpenCV衡量性能"></a>使用OpenCV衡量性能</h2><p><strong>cv.getTickCount</strong>函数返回从参考事件（如打开机器的那一刻）到调用此函数那一刻之间的时钟周期数。因此，如果在函数执行之前和之后调用它，则会获得用于执行函数的时钟周期数。</p>
<p><strong>cv.getTickFrequency</strong>函数返回时钟周期的频率或每秒的时钟周期数。因此，要找到执行时间（以秒为单位），你可以执行以下操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img1 = cv.imread(<span class="string">'avatar2gray.jpg.jpg'</span>)</span><br><span class="line">e1 = cv.getTickCount()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">49</span>,<span class="number">2</span>):</span><br><span class="line">    img1 = cv.medianBlur(img1,i)</span><br><span class="line">e2 = cv.getTickCount()</span><br><span class="line">t = (e2 - e1)/cv.getTickFrequency()</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>0.0002413</code></pre><h2 id="OpenCV中的默认优化"><a href="#OpenCV中的默认优化" class="headerlink" title="OpenCV中的默认优化"></a>OpenCV中的默认优化</h2><p>许多 OpenCV 函数都是使用 SSE2、 AVX 等进行优化的。 它还包含未优化的代码。</p>
<p>因此，如果我们的系统支持这些特性，我们就应该利用它们(几乎所有现代的处理器都支持它们)。</p>
<p>在编译时默认启用它。因此，如果启用了 OpenCV，它将运行优化的代码，否则它将运行未优化的代码。</p>
<p>你可以使用 cvUseoptimized 检查是否启用 / 禁用和 cvSetuseoptimized 以启用 / 禁用它。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.useOptimized() <span class="comment"># 检查是否启用了优化</span></span><br></pre></td></tr></table></figure>




<pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit res = cv.medianBlur(img1,<span class="number">49</span>)</span><br></pre></td></tr></table></figure>

<pre><code>748 ns ± 45.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.setUseOptimized(<span class="literal">False</span>)</span><br><span class="line">print(cv.useOptimized())</span><br></pre></td></tr></table></figure>

<pre><code>False</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit res = cv.medianBlur(img1,<span class="number">49</span>)</span><br></pre></td></tr></table></figure>

<pre><code>752 ns ± 34.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.setUseOptimized(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="在IPython中衡量性能"><a href="#在IPython中衡量性能" class="headerlink" title="在IPython中衡量性能"></a>在IPython中衡量性能</h2><p>有时你可能需要比较两个类似操作的性能。</p>
<p>IPython为你提供了一个神奇的命令计时器来执行此操作。它</p>
<p>会多次运行代码以获得更准确的结果。同样，它们适用于测量单行代码。</p>
<p>例如，你知道以下哪个加法运算更好，</p>
<p>x = 5 y = x**2, </p>
<p>x = 5  y = x*x, </p>
<p>x = np.uint8([5]) y = x*x或y = np.square(x)?我们将在IPython shell中使用timeit</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="number">5</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=x**<span class="number">2</span></span><br></pre></td></tr></table></figure>

<pre><code>515 ns ± 17 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=x*x</span><br></pre></td></tr></table></figure>

<pre><code>121 ns ± 5.63 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">z = np.uint8([<span class="number">5</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=z*z</span><br></pre></td></tr></table></figure>

<pre><code>1.04 µs ± 61.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=np.square(z)</span><br></pre></td></tr></table></figure>

<pre><code>1.04 µs ± 57.5 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><p>可以看到x = 5; y = x * x最快，比Numpy快20倍左右。如果你还考虑阵列的创建，它可能会快100倍。</p>
<p>注意 Python标量操作比Numpy标量操作快。</p>
<p>因此，对于包含一两个元素的运算，Python标量比Numpy数组好。</p>
<p>当数组大小稍大时，Numpy会占优势。</p>
<p>我们将比较<strong>cv.countNonZero</strong>和<strong>np.count_nonzero</strong>对于同一张图片的性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit z = np.count_nonzero(img1)</span><br></pre></td></tr></table></figure>

<pre><code>2.58 µs ± 321 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit z = cv.countNonZero(img1)</span><br></pre></td></tr></table></figure>

<pre><code>722 ns ± 25.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><p>OpenCV 函数比 Numpy 函数快近25倍。</p>
<p>注意 </p>
<p>通常，OpenCV函数比Numpy函数要快。因此，对于相同的操作，首选OpenCV功能。</p>
<p>但是，可能会有例外，尤其是当Numpy处理视图而不是副本时。</p>
<h2 id="性能优化技术"><a href="#性能优化技术" class="headerlink" title="性能优化技术"></a>性能优化技术</h2><p>有几种技术和编码方法可以充分利用 Python 和 Numpy 的最大性能。</p>
<p>这里要注意的主要事情是，首先尝试以一种简单的方式实现算法。</p>
<p>一旦它运行起来，分析它，找到瓶颈并优化它们。</p>
<ul>
<li>尽量避免在Python中使用循环，尤其是双/三重循环等。它们本来就很慢。</li>
<li>由于Numpy和OpenCV已针对向量运算进行了优化，因此将算法/代码向量化到最大程度。</li>
<li>利用缓存一致性。</li>
<li>除非需要，否则切勿创建数组的副本。尝试改用视图。数组复制是一项昂贵的操作。</li>
<li>即使执行了所有这些操作后，如果你的代码仍然很慢，或者不可避免地需要使用大循环，请使用Cython等其他库来使其更快。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>单层感知器</title>
    <url>/2020/07/09/%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8/</url>
    <content><![CDATA[<p>单层感知器代码简单演示</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">-1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">-1</span>]])</span><br><span class="line">W = (np.random.random([<span class="number">3</span>,<span class="number">1</span>])<span class="number">-0.5</span>)*<span class="number">2</span></span><br><span class="line">print(W)</span><br><span class="line">lr = <span class="number">0.11</span></span><br><span class="line">O = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> X,Y,W,lr</span><br><span class="line">    O = np.sign(np.dot(X,W))</span><br><span class="line">    W_C = lr*(X.T.dot(Y-O))/int(X.shape[<span class="number">0</span>])</span><br><span class="line">    W = W + W_C</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0.23015384]
 [-0.72847367]
 [ 0.52092108]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    update()</span><br><span class="line">    O = np.sign(np.dot(X,W))</span><br><span class="line">    <span class="keyword">if</span>(O==Y).all():</span><br><span class="line">        print(<span class="string">'Finished'</span>)</span><br><span class="line">        print(<span class="string">'epoch:'</span>,i)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">x1 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">y1 = [<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">x2 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">y2 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">k = -W[<span class="number">1</span>]/W[<span class="number">2</span>]</span><br><span class="line">d = -W[<span class="number">0</span>]/W[<span class="number">2</span>]</span><br><span class="line">print(<span class="string">'k='</span>,k)</span><br><span class="line">print(<span class="string">'d='</span>,d)</span><br><span class="line">xdata = (<span class="number">-2</span>,<span class="number">3</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(xdata,xdata*k+d,<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(x1,y1,c=<span class="string">'b'</span>)</span><br><span class="line">plt.scatter(x2,y2,c=<span class="string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>k= [0.16650386]
d= [-0.80515292]</code></pre><p><img src="/2020/07/09/%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8/output_2_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv-GUI特性</title>
    <url>/2020/07/12/opencv-GUI%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">图像入门</a></li>
<li>2.<a href="#header2">视频入门</a></li>
<li>3.<a href="#header3">绘图</a></li>
<li>4.<a href="#header4">鼠标作为画笔</a></li>
<li>5.<a href="#header5">轨迹栏作为调色板</a></li>
</ul>
<h1 id="图像入门"><a href="#图像入门" class="headerlink" title="图像入门"></a><span id="header1">图像入门</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>在这里，你将学习如何读取图像，如何显示图像以及如何将其保存回去</li>
<li>你将学习以下功能：cv.imread()，cv.imshow()，cv.imwrite()</li>
<li>(可选)你将学习如何使用Matplotlib显示图像<a id="more"></a>


</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h2 id="读取图像"><a href="#读取图像" class="headerlink" title="读取图像"></a>读取图像</h2><ul>
<li>使用<strong>cv.imread</strong>()函数读取图像。</li>
<li>图像应该在工作目录或图像的完整路径应给出。<br>第二个参数是一个标志，它指定了读取图像的方式。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.IMREAD_COLOR： 加载彩色图像。任何图像的透明度都会被忽视。它是默认标志。</span><br><span class="line">cv.IMREAD_GRAYSCALE：以灰度模式加载图像</span><br><span class="line">cv.IMREAD_UNCHANGED：加载图像，包括alpha通道</span><br></pre></td></tr></table></figure>
注意 除了这三个标志，你可以分别简单地传递整数1、0或-1。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img=cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img</span><br></pre></td></tr></table></figure>




<pre><code>array([[181, 247, 251, ..., 203, 177, 170],
       [217, 255, 250, ..., 211, 178, 167],
       [247, 255, 245, ..., 218, 180, 164],
       ...,
       [ 13,  13,  12, ...,   0,  16,   0],
       [ 11,  10,  10, ...,   0,   0,   0],
       [ 17,  17,  16, ...,   0,   4,   0]], dtype=uint8)</code></pre><h2 id="显示图像"><a href="#显示图像" class="headerlink" title="显示图像"></a>显示图像</h2><ul>
<li>使用函数<strong>cv.imshow()</strong>在窗口中显示图像。窗口自动适合图像尺寸。</li>
<li>第一个参数是窗口名称，它是一个字符串。</li>
<li>第二个参数是我们的对象。你可以根据需要创建任意多个窗口，但可以使用不同的窗口名称。</li>
</ul>
<p>cv.waitKey()是一个键盘绑定函数。其参数是以毫秒为单位的时间。</p>
<p>该函数等待任何键盘事件指定的毫秒。如果您在这段时间内按下任何键，程序将继续运行。</p>
<p>如果<strong>0</strong>被传递，它将无限期地等待一次敲击键。</p>
<p>它也可以设置为检测特定的按键，例如，如果按下键 a 等</p>
<p>cv.destroyAllWindows()只会破坏我们创建的所有窗口。</p>
<p>如果要销毁任何特定的窗口，请使用函数 cv.destroyWindow()在其中传递确切的窗口名称作为参数。</p>
<p>在特殊情况下，你可以创建一个空窗口，然后再将图像加载到该窗口。在这种情况下，你可以指定窗口是否可调整大小。</p>
<p>这是通过功能<strong>cv.namedWindow</strong>()完成的。默认情况下，该标志为<strong>cv.WINDOW_AUTOSIZE</strong>。</p>
<p>但是，如果将标志指定为<strong>cv.WINDOW_NORMAL</strong>，则可以调整窗口大小。当图像尺寸过大以及向窗口添加跟踪栏时，这将很有帮助。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.namedWindow(<span class="string">'image'</span>,cv.WINDOW_NORMAL)</span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="写入图像"><a href="#写入图像" class="headerlink" title="写入图像"></a>写入图像</h2><p>使用函数<strong>cv.imwrite</strong>()保存图像。</p>
<p>第一个参数是文件名，第二个参数是要保存的图像。</p>
<p>cv.imwrite(‘messigray.png’，img)这会将图像以PNG格式保存在工作目录中。</p>
<p>在下面的程序中，以灰度加载图像，显示图像，按s保存图像并退出，或者按ESC键直接退出而不保存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">k =cv.waitKey(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> k == <span class="number">27</span>: <span class="comment"># 按下esc时</span></span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"><span class="keyword">elif</span> k == ord(<span class="string">'s'</span>): <span class="comment"># 按下s时</span></span><br><span class="line">    cv.imwrite(<span class="string">'copy1.jpg'</span>,img)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">plt.imshow(img, interpolation = <span class="string">'bicubic'</span>)</span><br><span class="line"><span class="comment"># plt.imshow(img)</span></span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv-GUI%E7%89%B9%E6%80%A7/output_13_0.png" alt="png"></p>
<h1 id="视频入门"><a href="#视频入门" class="headerlink" title="视频入门"></a><span id="header2">视频入门</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><ul>
<li>学习读取视频，显示视频和保存视频。</li>
<li>学习从相机捕捉并显示它。</li>
<li>你将学习以下功能：cv.VideoCapture()，cv.VideoWriter()<!--more-->

</li>
</ul>
<p>要捕获视频，你需要创建一个 VideoCapture 对象。</p>
<p>它的参数可以是设备索引或视频文件的名称。设备索引就是指定哪个摄像头的数字。</p>
<p>正常情况下，一个摄像头会被连接(就像我的情况一样)。所以我简单地传0(或-1)。你可以通过传递1来选择第二个相机，以此类推。</p>
<p>在此之后，你可以逐帧捕获。但是在最后，不要忘记释放俘虏。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> cap.isOpened():</span><br><span class="line">    print(<span class="string">"Cannot open camera"</span>)</span><br><span class="line">    exit()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 逐帧捕获</span></span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="comment"># 如果正确读取帧，ret为True</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 我们在框架上的操作到这里</span></span><br><span class="line">    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># 显示结果帧e</span></span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, gray)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment"># 完成所有操作后，释放捕获器</span></span><br><span class="line">cap.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<pre><code>Cannot open camera
Can&apos;t receive frame (stream end?). Exiting ...</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="string">'video1.flv'</span>)</span><br><span class="line"><span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="comment"># 如果正确读取帧，ret为True</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, gray)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cap.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="string">'video1.flv'</span>)</span><br><span class="line"><span class="comment"># 定义编解码器并创建VideoWriter对象</span></span><br><span class="line">fourcc = cv.VideoWriter_fourcc(*<span class="string">'DIVX'</span>)</span><br><span class="line">out = cv.VideoWriter(<span class="string">'output.flv'</span>, fourcc, <span class="number">20.0</span>, (<span class="number">640</span>,  <span class="number">480</span>))</span><br><span class="line"><span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    frame = cv.flip(frame, <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 写翻转的框架</span></span><br><span class="line">    out.write(frame)</span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, frame)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment"># 完成工作后释放所有内容</span></span><br><span class="line">cap.release()</span><br><span class="line">out.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<pre><code>Can&apos;t receive frame (stream end?). Exiting ...</code></pre><h1 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a><span id="header3">绘图</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><ul>
<li>学习使用OpenCV绘制不同的几何形状</li>
<li>您将学习以下功能：cv.line()，cv.circle()，cv.rectangle()，cv.ellipse()，cv.putText()等。</li>
<li>在上述所有功能中，您将看到一些常见的参数，如下所示：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img：您要绘制形状的图像</span><br><span class="line">color：形状的颜色。对于BGR，将其作为元组传递，例如：(255,0,0)对于蓝色。对于灰度，只需传递标量值即可。</span><br><span class="line">厚度：线或圆等的粗细。如果对闭合图形（如圆）传递-1 ，它将填充形状。默认厚度&#x3D; 1</span><br><span class="line">lineType：线的类型，是否为8连接线，抗锯齿线等。默认情况下，为8连接线。**cv.LINE_AA**给出了抗锯齿的线条，看起来非常适合曲线。</span><br></pre></td></tr></table></figure>


</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制一条线，您需要传递线的开始和结束坐标。我们将创建一个黑色图像，并从左上角到右下角在其上绘制一条蓝线。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line"><span class="comment"># 绘制一条厚度为5的蓝色对角线</span></span><br><span class="line">cv.line(img,(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">511</span>,<span class="number">511</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">5</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制矩形，您需要矩形的左上角和右下角。这次，我们将在图像的右上角绘制一个绿色矩形。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.rectangle(img,(<span class="number">384</span>,<span class="number">0</span>),(<span class="number">510</span>,<span class="number">128</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制一个圆，需要其中心坐标和半径。我们将在上面绘制的矩形内绘制一个圆。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.circle(img,(<span class="number">447</span>,<span class="number">63</span>), <span class="number">63</span>, (<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="number">-1</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制椭圆，我们需要传递几个参数。一个参数是中心位置（x，y）。</span></span><br><span class="line"><span class="comment"># 下一个参数是轴长度（长轴长度，短轴长度）。</span></span><br><span class="line"><span class="comment"># angle是椭圆沿逆时针方向旋转的角度。</span></span><br><span class="line"><span class="comment"># startAngle和endAngle表示从主轴沿顺时针方向测量的椭圆弧的开始和结束。即给出0和360给出完整的椭圆</span></span><br><span class="line"><span class="comment"># 下面的示例在图像的中心绘制一个椭圆形。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.ellipse(img,(<span class="number">256</span>,<span class="number">256</span>),(<span class="number">100</span>,<span class="number">50</span>),<span class="number">0</span>,<span class="number">0</span>,<span class="number">180</span>,<span class="number">255</span>,<span class="number">-1</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制多边形，首先需要顶点的坐标。将这些点组成形状为ROWSx1x2的数组，其中ROWS是顶点数，并且其类型应为int32。</span></span><br><span class="line"><span class="comment"># 在这里，我们绘制了一个带有四个顶点的黄色小多边形。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">pts = np.array([[<span class="number">10</span>,<span class="number">5</span>],[<span class="number">20</span>,<span class="number">30</span>],[<span class="number">270</span>,<span class="number">20</span>],[<span class="number">150</span>,<span class="number">5</span>]], np.int32)</span><br><span class="line">pts = pts.reshape((<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">cv.polylines(img,[pts],<span class="literal">True</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果第三个参数为False，您将获得一条连接所有点的折线，而不是闭合形状。 </span></span><br><span class="line"><span class="comment"># cv.polylines()可用于绘制多条线。只需创建要绘制的所有线条的列表，然后将其传递给函数即可。</span></span><br><span class="line"><span class="comment"># 所有线条将单独绘制。与为每条线调用**cv.line**相比，绘制一组线是一种更好，更快的方法。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">pts = np.array([[<span class="number">10</span>,<span class="number">5</span>],[<span class="number">20</span>,<span class="number">30</span>],[<span class="number">270</span>,<span class="number">20</span>],[<span class="number">150</span>,<span class="number">5</span>]], np.int32)</span><br><span class="line">pts = pts.reshape((<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">cv.polylines(img,[pts],<span class="literal">False</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要将文本放入图像中，需要指定以下内容。 </span></span><br><span class="line"><span class="comment"># - 您要写入的文字数据 </span></span><br><span class="line"><span class="comment"># - 您要放置它的位置坐标（即数据开始的左下角）。 </span></span><br><span class="line"><span class="comment"># - 字体类型（检查**cv.putText**文档以获取受支持的字体）</span></span><br><span class="line"><span class="comment"># - 字体比例（指定字体大小） </span></span><br><span class="line"><span class="comment"># - 常规的内容，例如颜色，厚度，线条类型等。为了获得更好的外观，建议使用lineType = cv.LINE_AA。</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">font = cv.FONT_HERSHEY_SIMPLEX</span><br><span class="line">cv.putText(img,<span class="string">'OpenCV'</span>,(<span class="number">10</span>,<span class="number">500</span>), font, <span class="number">4</span>,(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>),<span class="number">2</span>,cv.LINE_AA)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<h1 id="鼠标作为画笔"><a href="#鼠标作为画笔" class="headerlink" title="鼠标作为画笔"></a><span id="header4">鼠标作为画笔</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><ul>
<li>了解如何在OpenCV中处理鼠标事件</li>
<li>您将学习以下功能：cv.setMouseCallback()</li>
</ul>
<p>简单演示<br>在这里，我们创建一个简单的应用程序，无论我们在哪里双击它，都可以在图像上绘制一个圆。</p>
<p>首先，我们创建一个鼠标回调函数，该函数在发生鼠标事件时执行。</p>
<p>鼠标事件可以是与鼠标相关的任何事物，例如左键按下，左键按下，左键双击等。</p>
<p>它为我们提供了每个鼠标事件的坐标(x，y)。通过此活动和地点，我们可以做任何我们喜欢的事情。</p>
<p>要列出所有可用的可用事件，请在Python终端中运行以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">events = [i <span class="keyword">for</span> i <span class="keyword">in</span> dir(cv) <span class="keyword">if</span> <span class="string">'EVENT'</span> <span class="keyword">in</span> i]</span><br><span class="line">print( events )</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;EVENT_FLAG_ALTKEY&apos;, &apos;EVENT_FLAG_CTRLKEY&apos;, &apos;EVENT_FLAG_LBUTTON&apos;, &apos;EVENT_FLAG_MBUTTON&apos;, &apos;EVENT_FLAG_RBUTTON&apos;, &apos;EVENT_FLAG_SHIFTKEY&apos;, &apos;EVENT_LBUTTONDBLCLK&apos;, &apos;EVENT_LBUTTONDOWN&apos;, &apos;EVENT_LBUTTONUP&apos;, &apos;EVENT_MBUTTONDBLCLK&apos;, &apos;EVENT_MBUTTONDOWN&apos;, &apos;EVENT_MBUTTONUP&apos;, &apos;EVENT_MOUSEHWHEEL&apos;, &apos;EVENT_MOUSEMOVE&apos;, &apos;EVENT_MOUSEWHEEL&apos;, &apos;EVENT_RBUTTONDBLCLK&apos;, &apos;EVENT_RBUTTONDOWN&apos;, &apos;EVENT_RBUTTONUP&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在我们双击的地方绘制一个圆圈</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># 鼠标回调函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_circle</span><span class="params">(event,x,y,flags,param)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> event == cv.EVENT_LBUTTONDBLCLK:</span><br><span class="line">        cv.circle(img,(x,y),<span class="number">100</span>,(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># 创建一个黑色的图像，一个窗口，并绑定到窗口的功能</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line">cv.setMouseCallback(<span class="string">'image'</span>,draw_circle)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按m之前拖拽画青色矩形</span></span><br><span class="line"><span class="comment"># 按m之后拖拽画红色直线</span></span><br><span class="line"><span class="comment"># 按esc退出</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">drawing = <span class="literal">False</span> <span class="comment"># 如果按下鼠标，则为真</span></span><br><span class="line">mode = <span class="literal">True</span> <span class="comment"># 如果为真，绘制矩形。按 m 键可以切换到曲线</span></span><br><span class="line">ix,iy = <span class="number">-1</span>,<span class="number">-1</span></span><br><span class="line"><span class="comment"># 鼠标回调函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_circle</span><span class="params">(event,x,y,flags,param)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ix,iy,drawing,mode</span><br><span class="line">    <span class="keyword">if</span> event == cv.EVENT_LBUTTONDOWN:</span><br><span class="line">        drawing = <span class="literal">True</span></span><br><span class="line">        ix,iy = x,y</span><br><span class="line">    <span class="keyword">elif</span> event == cv.EVENT_MOUSEMOVE:</span><br><span class="line">        <span class="keyword">if</span> drawing == <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="literal">True</span>:</span><br><span class="line">                cv.rectangle(img,(ix,iy),(x,y),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cv.circle(img,(x,y),<span class="number">25</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">elif</span> event == cv.EVENT_LBUTTONUP:</span><br><span class="line">        drawing = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="literal">True</span>:</span><br><span class="line">            cv.rectangle(img,(ix,iy),(x,y),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cv.circle(img,(x,y),<span class="number">25</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">-1</span>)</span><br><span class="line">            </span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line">cv.setMouseCallback(<span class="string">'image'</span>,draw_circle)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; cv.waitKey(<span class="number">20</span>) == ord(<span class="string">'m'</span>):</span><br><span class="line">        mode = ~mode</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h1 id="轨迹栏作为调色板"><a href="#轨迹栏作为调色板" class="headerlink" title="轨迹栏作为调色板"></a><span id="header5">轨迹栏作为调色板</span></h1><h2 id="目标-4"><a href="#目标-4" class="headerlink" title="目标"></a>目标</h2><ul>
<li>了解将轨迹栏固定到OpenCV窗口</li>
<li>您将学习以下功能：cv.getTrackbarPos，<strong>cv.createTrackbar</strong>等。</li>
</ul>
<p>对于cv.getTrackbarPos()函数，</p>
<p>第一个参数是轨迹栏名称，</p>
<p>第二个参数是它附加到的窗口名称，</p>
<p>第三个参数是默认值，</p>
<p>第四个参数是最大值，</p>
<p>第五个是执行的回调函数每次跟踪栏值更改。</p>
<p>回调函数始终具有默认参数，即轨迹栏位置。</p>
<p>在我们的例子中，函数什么都不做，所以我们简单地通过。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在这里，我们将创建一个简单的应用程序，以显示您指定的颜色。</span></span><br><span class="line"><span class="comment"># 您有一个显示颜色的窗口，以及三个用于指定B、G、R颜色的跟踪栏。滑动轨迹栏，并相应地更改窗口颜色。</span></span><br><span class="line"><span class="comment"># 默认情况下，初始颜色将设置为黑色。只有在该开关为ON的情况下，该应用程序才能在其中运行，否则屏幕始终为黑色。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nothing</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># 创建一个黑色的图像，一个窗口</span></span><br><span class="line">img = np.zeros((<span class="number">300</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line"><span class="comment"># 创建颜色变化的轨迹栏</span></span><br><span class="line">cv.createTrackbar(<span class="string">'R'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line">cv.createTrackbar(<span class="string">'G'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line">cv.createTrackbar(<span class="string">'B'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line"><span class="comment"># 为 ON/OFF 功能创建开关</span></span><br><span class="line">switch = <span class="string">'0 : OFF \n1 : ON'</span></span><br><span class="line">cv.createTrackbar(switch, <span class="string">'image'</span>,<span class="number">0</span>,<span class="number">1</span>,nothing)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    k = cv.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 得到四条轨迹的当前位置</span></span><br><span class="line">    r = cv.getTrackbarPos(<span class="string">'R'</span>,<span class="string">'image'</span>)</span><br><span class="line">    g = cv.getTrackbarPos(<span class="string">'G'</span>,<span class="string">'image'</span>)</span><br><span class="line">    b = cv.getTrackbarPos(<span class="string">'B'</span>,<span class="string">'image'</span>)</span><br><span class="line">    s = cv.getTrackbarPos(switch,<span class="string">'image'</span>)</span><br><span class="line">    <span class="keyword">if</span> s == <span class="number">0</span>:</span><br><span class="line">        img[:] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img[:] = [b,g,r]</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>岭回归</title>
    <url>/2020/07/11/%E5%B2%AD%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p><a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/longley.csv" target="_blank" rel="noopener">longley.csv</a></p>
<a id="more"></a>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%991.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%992.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%993.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%994.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%995.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%996.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%997.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%998.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%999.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9910.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9911.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9912.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9913.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9914.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = genfromtxt(<span class="string">'./data/longley.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>

<pre><code>[[     nan      nan      nan      nan      nan      nan      nan      nan]
 [     nan   83.     234.289  235.6    159.     107.608 1947.      60.323]
 [     nan   88.5    259.426  232.5    145.6    108.632 1948.      61.122]
 [     nan   88.2    258.054  368.2    161.6    109.773 1949.      60.171]
 [     nan   89.5    284.599  335.1    165.     110.929 1950.      61.187]
 [     nan   96.2    328.975  209.9    309.9    112.075 1951.      63.221]
 [     nan   98.1    346.999  193.2    359.4    113.27  1952.      63.639]
 [     nan   99.     365.385  187.     354.7    115.094 1953.      64.989]
 [     nan  100.     363.112  357.8    335.     116.219 1954.      63.761]
 [     nan  101.2    397.469  290.4    304.8    117.388 1955.      66.019]
 [     nan  104.6    419.18   282.2    285.7    118.734 1956.      67.857]
 [     nan  108.4    442.769  293.6    279.8    120.445 1957.      68.169]
 [     nan  110.8    444.546  468.1    263.7    121.95  1958.      66.513]
 [     nan  112.6    482.704  381.3    255.2    123.366 1959.      68.655]
 [     nan  114.2    502.601  393.1    251.4    125.368 1960.      69.564]
 [     nan  115.7    518.173  480.6    257.2    127.852 1961.      69.331]
 [     nan  116.9    554.894  400.7    282.7    130.081 1962.      70.551]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = data[<span class="number">1</span>:,<span class="number">2</span>:]</span><br><span class="line">y_data = data[<span class="number">1</span>:,<span class="number">1</span>,np.newaxis]</span><br><span class="line">print(x_data)</span><br><span class="line">print(y_data)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 234.289  235.6    159.     107.608 1947.      60.323]
 [ 259.426  232.5    145.6    108.632 1948.      61.122]
 [ 258.054  368.2    161.6    109.773 1949.      60.171]
 [ 284.599  335.1    165.     110.929 1950.      61.187]
 [ 328.975  209.9    309.9    112.075 1951.      63.221]
 [ 346.999  193.2    359.4    113.27  1952.      63.639]
 [ 365.385  187.     354.7    115.094 1953.      64.989]
 [ 363.112  357.8    335.     116.219 1954.      63.761]
 [ 397.469  290.4    304.8    117.388 1955.      66.019]
 [ 419.18   282.2    285.7    118.734 1956.      67.857]
 [ 442.769  293.6    279.8    120.445 1957.      68.169]
 [ 444.546  468.1    263.7    121.95  1958.      66.513]
 [ 482.704  381.3    255.2    123.366 1959.      68.655]
 [ 502.601  393.1    251.4    125.368 1960.      69.564]
 [ 518.173  480.6    257.2    127.852 1961.      69.331]
 [ 554.894  400.7    282.7    130.081 1962.      70.551]]
[[ 83. ]
 [ 88.5]
 [ 88.2]
 [ 89.5]
 [ 96.2]
 [ 98.1]
 [ 99. ]
 [100. ]
 [101.2]
 [104.6]
 [108.4]
 [110.8]
 [112.6]
 [114.2]
 [115.7]
 [116.9]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.mat(x_data).shape)</span><br><span class="line">print(np.mat(y_data).shape)</span><br><span class="line">X_data = np.concatenate((np.ones((<span class="number">16</span>,<span class="number">1</span>)),x_data),axis=<span class="number">1</span>)</span><br><span class="line">print(X_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(16, 6)
(16, 1)
(16, 7)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(X_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[[1.00000e+00 2.34289e+02 2.35600e+02 1.59000e+02 1.07608e+02 1.94700e+03
  6.03230e+01]
 [1.00000e+00 2.59426e+02 2.32500e+02 1.45600e+02 1.08632e+02 1.94800e+03
  6.11220e+01]
 [1.00000e+00 2.58054e+02 3.68200e+02 1.61600e+02 1.09773e+02 1.94900e+03
  6.01710e+01]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights</span><span class="params">(xArr,yArr,lam=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    rxTx = xTx +np.eye(xMat.shape[<span class="number">1</span>])*lam</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(rxTx) == <span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'This martix cannot do inverse'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = rxTx.I*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = weights(X_data,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 7.38107363e-04]
 [ 2.07703836e-01]
 [ 2.10076376e-02]
 [ 5.05385441e-03]
 [-1.59173066e+00]
 [ 1.10442920e-01]
 [-2.42280461e-01]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.mat(X_data)* np.mat(ws)</span><br></pre></td></tr></table></figure>




<pre><code>matrix([[ 83.55075226],
        [ 86.92588689],
        [ 88.09720227],
        [ 90.95677622],
        [ 96.06951002],
        [ 97.81955375],
        [ 98.36444357],
        [ 99.99814266],
        [103.26832266],
        [105.03165135],
        [107.45224671],
        [109.52190685],
        [112.91863666],
        [113.98357055],
        [115.29845063],
        [117.64279933]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降法</title>
    <url>/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</url>
    <content><![CDATA[<p>线性回归梯度下降代码简单演示</p>
<p>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/data.csv" target="_blank" rel="noopener">data.csv</a></p>
<a id="more"></a>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%951.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%952.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%953.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%954.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%955.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%956.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%957.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%958.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%959.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9510.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9511.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9512.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9513.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9514.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9515.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9516.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9517.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/data.csv'</span>,delimiter=<span class="string">","</span>)</span><br><span class="line">x_data = data[:,<span class="number">0</span>]</span><br><span class="line">y_data = data[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">k = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span><span class="params">(b,k,x_data,y_data)</span>:</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        totalError += (y_data[i]-(k*x_data[i]+b))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/float(len(x_data))/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span><span class="params">(x_data,y_data,b,k,lr,epochs)</span>:</span></span><br><span class="line">    m = float(len(x_data))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        b_grad = <span class="number">0</span></span><br><span class="line">        k_grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,len(x_data)):</span><br><span class="line">            b_grad += (<span class="number">1</span>/m)*(((k*x_data[j])+b)-y_data[j])</span><br><span class="line">            k_grad += (<span class="number">1</span>/m)*x_data[j]*(((k*x_data[j])+b)-y_data[j])</span><br><span class="line">        b = b-(lr*b_grad)</span><br><span class="line">        k = k-(lr*k_grad)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epochs:"</span>,i)</span><br><span class="line">            plt.plot(x_data,y_data,<span class="string">'b.'</span>)</span><br><span class="line">            plt.plot(x_data, k*x_data+b, <span class="string">'r'</span>)</span><br><span class="line">            plt.show()</span><br><span class="line">    <span class="keyword">return</span> b,k</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Starting b = &#123;0&#125;, k = &#123;1&#125;, error = &#123;2&#125;"</span>.format(b, k, compute_error(b, k, x_data, y_data)))</span><br><span class="line">print(<span class="string">"Running..."</span>)</span><br><span class="line">b, k = gradient_descent_runner(x_data, y_data, b, k, lr, epochs)</span><br><span class="line">print(<span class="string">"After &#123;0&#125; iterations b = &#123;1&#125;, k = &#123;2&#125;, error = &#123;3&#125;"</span>.format(epochs, b, k, compute_error(b, k, x_data, y_data)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.plot(x_data, y_data, <span class="string">'b.'</span>)</span><br><span class="line">plt.plot(x_data, k*x_data + b, <span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Starting b = 0, k = 0, error = 2782.5539172416056
Running...
epochs: 0</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_1.png" alt="png"></p>
<pre><code>epochs: 10</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_3.png" alt="png"></p>
<pre><code>epochs: 20</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_5.png" alt="png"></p>
<pre><code>epochs: 30</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_7.png" alt="png"></p>
<pre><code>epochs: 40</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_9.png" alt="png"></p>
<pre><code>After 50 iterations b = 0.030569950649287983, k = 1.4788903781318357, error = 56.32488184238028</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_11.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x0,x1 = np.meshgrid([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x0</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3, 4],
       [1, 2, 3, 4]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1</span><br></pre></td></tr></table></figure>




<pre><code>array([[4, 4, 4, 4],
       [5, 5, 5, 5]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>标准方程法</title>
    <url>/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/</url>
    <content><![CDATA[<p>标准方程法求线性回归代码展示:<br>数据集下载<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/data.csv" target="_blank" rel="noopener">data</a></p>
<a id="more"></a>

<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%951.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%952.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%953.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%954.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%955.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%956.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%957.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%958.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data =np.genfromtxt(<span class="string">'./data/data.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">x_data = data[:,<span class="number">0</span>,np.newaxis]</span><br><span class="line">y_data = data[:,<span class="number">1</span>,np.newaxis]</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.mat(x_data).shape)</span><br><span class="line">print(np.mat(y_data).shape)</span><br><span class="line">X_data = np.concatenate((np.ones((<span class="number">100</span>,<span class="number">1</span>)),x_data),axis=<span class="number">1</span>)</span><br><span class="line">print(X_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(100, 1)
(100, 1)
(100, 2)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(X_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[[ 1.         32.50234527]
 [ 1.         53.42680403]
 [ 1.         61.53035803]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) ==<span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'This martix cannot do inverse'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = xTx.I*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = weights(X_data,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>

<pre><code>[[7.99102098]
 [1.32243102]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test = np.array([[<span class="number">20</span>],[<span class="number">80</span>]])</span><br><span class="line">y_test = ws[<span class="number">0</span>]+x_test*ws[<span class="number">1</span>]</span><br><span class="line">plt.plot(x_data,y_data,<span class="string">'b.'</span>)</span><br><span class="line">plt.plot(x_test,y_test,<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/output_7_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降法———多元</title>
    <url>/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E2%80%94%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83/</url>
    <content><![CDATA[<p>线性回归多元梯度下降代码简单演示</p>
<p>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/Delivery.csv" target="_blank" rel="noopener">Delivery.csv</a></p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = genfromtxt(<span class="string">'./data/Delivery.csv'</span>, delimiter=<span class="string">","</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>

<pre><code>[[100.    4.    9.3]
 [ 50.    3.    4.8]
 [100.    4.    8.9]
 [100.    2.    6.5]
 [ 50.    2.    4.2]
 [ 80.    2.    6.2]
 [ 75.    3.    7.4]
 [ 65.    4.    6. ]
 [ 90.    3.    7.6]
 [ 90.    2.    6.1]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = data[:,:<span class="number">-1</span>]</span><br><span class="line">y_data = data[:,<span class="number">-1</span>]</span><br><span class="line">print(x_data)</span><br><span class="line">print(y_data)</span><br></pre></td></tr></table></figure>

<pre><code>[[100.   4.]
 [ 50.   3.]
 [100.   4.]
 [100.   2.]
 [ 50.   2.]
 [ 80.   2.]
 [ 75.   3.]
 [ 65.   4.]
 [ 90.   3.]
 [ 90.   2.]]
[9.3 4.8 8.9 6.5 4.2 6.2 7.4 6.  7.6 6.1]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">theta0 = <span class="number">0</span></span><br><span class="line">theta1 = <span class="number">0</span></span><br><span class="line">theta2 = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span><span class="params">(theta0,theta1,theta2,x_data,y_data)</span>:</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        totalError += (y_data[i]-(theta1*x_data[i,<span class="number">0</span>]+theta2*x_data[i,<span class="number">1</span>]+theta0))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/float(len(x_data))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span><span class="params">(x_data,y_data,theta0,theta1,theta2,lr,epochs)</span>:</span></span><br><span class="line">    m = float(len(x_data))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        theta0_grad = <span class="number">0</span></span><br><span class="line">        theta1_grad = <span class="number">0</span></span><br><span class="line">        theta2_grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,len(x_data)):</span><br><span class="line">            theta0_grad+=(<span class="number">1</span>/m)*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]))</span><br><span class="line">            theta1_grad+=(<span class="number">1</span>/m)*x_data[j,<span class="number">0</span>]*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]+theta0)-y_data[j])</span><br><span class="line">            theta2_grad+=(<span class="number">1</span>/m)*x_data[j,<span class="number">1</span>]*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]+theta0)-y_data[j])</span><br><span class="line">            theta0 = theta0-(lr*theta0_grad)</span><br><span class="line">            theta1 = theta1-(lr*theta1_grad)</span><br><span class="line">            theta2 = theta2-(lr*theta2_grad)</span><br><span class="line">    <span class="keyword">return</span> theta0,theta1,theta2</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = plt.figure().add_subplot(<span class="number">111</span>,projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],y_data,c=<span class="string">'r'</span>,marker=<span class="string">'o'</span>,s=<span class="number">100</span>)</span><br><span class="line">x0=x_data[:,<span class="number">0</span>]</span><br><span class="line">x1=x_data[:,<span class="number">1</span>]</span><br><span class="line">x0,x1 = np.meshgrid(x0,x1)</span><br><span class="line">z = theta0+x0*theta1+x1*theta2</span><br><span class="line"></span><br><span class="line">ax.plot_surface(x0,x1,z)</span><br><span class="line">ax.set_xlabel(<span class="string">'Miles'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Num of Deliveries'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E2%80%94%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯单词拼写器</title>
    <url>/2020/07/11/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8D%95%E8%AF%8D%E6%8B%BC%E5%86%99%E5%99%A8/</url>
    <content><![CDATA[<p>贝叶斯单词拼写器代码:<br>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/big.txt" target="_blank" rel="noopener">big.txt</a></p>
<a id="more"></a>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF7.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF9.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF10.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF11.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF12.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF13.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF14.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF15.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF16.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF17.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF18.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF19.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = open(<span class="string">'./data/big.txt'</span>).read()</span><br><span class="line">text = re.findall(<span class="string">'[a-z]+'</span>, text.lower())</span><br><span class="line">dic_words = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> text:</span><br><span class="line">    dic_words[t]=dic_words.get(t,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">dic_words</span><br></pre></td></tr></table></figure>




<pre><code>{&apos;the&apos;: 80030,
 &apos;project&apos;: 288,
 &apos;gutenberg&apos;: 263,
 &apos;ebook&apos;: 87,
 &apos;of&apos;: 40025,
 &apos;adventures&apos;: 17,
 &apos;sherlock&apos;: 101,
 &apos;holmes&apos;: 467,
 &apos;by&apos;: 6738,
 &apos;sir&apos;: 177,
 &apos;arthur&apos;: 34,
 &apos;conan&apos;: 4,
 &apos;doyle&apos;: 5,
 &apos;in&apos;: 22047,
 &apos;our&apos;: 1066,
 &apos;series&apos;: 128,
 &apos;copyright&apos;: 69,
 &apos;laws&apos;: 233,
 &apos;are&apos;: 3630,
 &apos;changing&apos;: 44,
 &apos;all&apos;: 4144,
 &apos;over&apos;: 1282,
 &apos;world&apos;: 362,
 &apos;be&apos;: 6155,
 &apos;sure&apos;: 123,
 &apos;to&apos;: 28766,
 &apos;check&apos;: 38,
 &apos;for&apos;: 6939,
 &apos;your&apos;: 1279,
 &apos;country&apos;: 423,
 &apos;before&apos;: 1363,
 &apos;downloading&apos;: 5,
 &apos;or&apos;: 5352,
 &apos;redistributing&apos;: 7,
 &apos;this&apos;: 4063,
 &apos;any&apos;: 1204,
 &apos;other&apos;: 1502,
 &apos;header&apos;: 7,
 &apos;should&apos;: 1297,
 &apos;first&apos;: 1177,
 &apos;thing&apos;: 303,
 &apos;seen&apos;: 444,
 &apos;when&apos;: 2923,
 &apos;viewing&apos;: 7,
 &apos;file&apos;: 21,
 &apos;please&apos;: 172,
 &apos;do&apos;: 1503,
 &apos;not&apos;: 6625,
 &apos;remove&apos;: 53,
 &apos;it&apos;: 10681,
 &apos;change&apos;: 150,
 &apos;edit&apos;: 4,
 &apos;without&apos;: 1015,
 &apos;written&apos;: 117,
 &apos;permission&apos;: 52,
 &apos;read&apos;: 218,
 &apos;legal&apos;: 52,
 &apos;small&apos;: 527,
 &apos;print&apos;: 47,
 &apos;and&apos;: 38312,
 &apos;information&apos;: 73,
 &apos;about&apos;: 1497,
 &apos;at&apos;: 6791,
 &apos;bottom&apos;: 42,
 &apos;included&apos;: 43,
 &apos;is&apos;: 9774,
 &apos;important&apos;: 285,
 &apos;specific&apos;: 37,
 &apos;rights&apos;: 168,
 &apos;restrictions&apos;: 23,
 &apos;how&apos;: 1315,
 &apos;may&apos;: 2551,
 &apos;used&apos;: 276,
 &apos;you&apos;: 5622,
 &apos;can&apos;: 1095,
 &apos;also&apos;: 778,
 &apos;find&apos;: 294,
 &apos;out&apos;: 1987,
 &apos;make&apos;: 504,
 &apos;a&apos;: 21155,
 &apos;donation&apos;: 10,
 &apos;get&apos;: 468,
 &apos;involved&apos;: 107,
 &apos;welcome&apos;: 18,
 &apos;free&apos;: 421,
 &apos;plain&apos;: 108,
 &apos;vanilla&apos;: 6,
 &apos;electronic&apos;: 58,
 &apos;texts&apos;: 7,
 &apos;ebooks&apos;: 54,
 &apos;readable&apos;: 13,
 &apos;both&apos;: 529,
 &apos;humans&apos;: 2,
 &apos;computers&apos;: 7,
 &apos;since&apos;: 260,
 &apos;these&apos;: 1231,
 &apos;were&apos;: 4289,
 &apos;prepared&apos;: 138,
 &apos;thousands&apos;: 93,
 &apos;volunteers&apos;: 22,
 &apos;title&apos;: 39,
 &apos;author&apos;: 29,
 &apos;release&apos;: 28,
 &apos;date&apos;: 48,
 &apos;march&apos;: 135,
 &apos;most&apos;: 908,
 &apos;recently&apos;: 30,
 &apos;updated&apos;: 4,
 &apos;november&apos;: 41,
 &apos;edition&apos;: 21,
 &apos;language&apos;: 61,
 &apos;english&apos;: 211,
 &apos;character&apos;: 174,
 &apos;set&apos;: 324,
 &apos;encoding&apos;: 5,
 &apos;ascii&apos;: 11,
 &apos;start&apos;: 67,
 &apos;additional&apos;: 30,
 &apos;editing&apos;: 6,
 &apos;jose&apos;: 1,
 &apos;menendez&apos;: 1,
 &apos;contents&apos;: 50,
 &apos;i&apos;: 7682,
 &apos;scandal&apos;: 19,
 &apos;bohemia&apos;: 15,
 &apos;ii&apos;: 77,
 &apos;red&apos;: 288,
 &apos;headed&apos;: 37,
 &apos;league&apos;: 53,
 &apos;iii&apos;: 91,
 &apos;case&apos;: 438,
 &apos;identity&apos;: 11,
 &apos;iv&apos;: 55,
 &apos;boscombe&apos;: 16,
 &apos;valley&apos;: 78,
 &apos;mystery&apos;: 39,
 &apos;v&apos;: 51,
 &apos;five&apos;: 279,
 &apos;orange&apos;: 23,
 &apos;pips&apos;: 12,
 &apos;vi&apos;: 37,
 &apos;man&apos;: 1652,
 &apos;with&apos;: 9740,
 &apos;twisted&apos;: 21,
 &apos;lip&apos;: 56,
 &apos;vii&apos;: 34,
 &apos;adventure&apos;: 34,
 &apos;blue&apos;: 143,
 &apos;carbuncle&apos;: 17,
 &apos;viii&apos;: 39,
 &apos;speckled&apos;: 5,
 &apos;band&apos;: 54,
 &apos;ix&apos;: 28,
 &apos;engineer&apos;: 12,
 &apos;s&apos;: 5631,
 &apos;thumb&apos;: 51,
 &apos;x&apos;: 136,
 &apos;noble&apos;: 48,
 &apos;bachelor&apos;: 18,
 &apos;xi&apos;: 28,
 &apos;beryl&apos;: 4,
 &apos;coronet&apos;: 29,
 &apos;xii&apos;: 28,
 &apos;copper&apos;: 26,
 &apos;beeches&apos;: 12,
 &apos;she&apos;: 3946,
 &apos;always&apos;: 608,
 &apos;woman&apos;: 325,
 &apos;have&apos;: 3493,
 &apos;seldom&apos;: 76,
 &apos;heard&apos;: 636,
 &apos;him&apos;: 5230,
 &apos;mention&apos;: 46,
 &apos;her&apos;: 5284,
 &apos;under&apos;: 963,
 &apos;name&apos;: 262,
 &apos;his&apos;: 10034,
 &apos;eyes&apos;: 939,
 &apos;eclipses&apos;: 2,
 &apos;predominates&apos;: 3,
 &apos;whole&apos;: 744,
 &apos;sex&apos;: 11,
 &apos;was&apos;: 11410,
 &apos;that&apos;: 12512,
 &apos;he&apos;: 12401,
 &apos;felt&apos;: 697,
 &apos;emotion&apos;: 36,
 &apos;akin&apos;: 14,
 &apos;love&apos;: 484,
 &apos;irene&apos;: 18,
 &apos;adler&apos;: 16,
 &apos;emotions&apos;: 10,
 &apos;one&apos;: 3371,
 &apos;particularly&apos;: 174,
 &apos;abhorrent&apos;: 1,
 &apos;cold&apos;: 257,
 &apos;precise&apos;: 13,
 &apos;but&apos;: 5653,
 &apos;admirably&apos;: 7,
 &apos;balanced&apos;: 6,
 &apos;mind&apos;: 341,
 &apos;take&apos;: 616,
 &apos;perfect&apos;: 39,
 &apos;reasoning&apos;: 41,
 &apos;observing&apos;: 21,
 &apos;machine&apos;: 39,
 &apos;has&apos;: 1603,
 &apos;as&apos;: 8064,
 &apos;lover&apos;: 26,
 &apos;would&apos;: 1953,
 &apos;placed&apos;: 182,
 &apos;himself&apos;: 1158,
 &apos;false&apos;: 64,
 &apos;position&apos;: 432,
 &apos;never&apos;: 593,
 &apos;spoke&apos;: 218,
 &apos;softer&apos;: 10,
 &apos;passions&apos;: 29,
 &apos;save&apos;: 110,
 &apos;gibe&apos;: 2,
 &apos;sneer&apos;: 6,
 &apos;they&apos;: 3938,
 &apos;admirable&apos;: 14,
 &apos;things&apos;: 321,
 &apos;observer&apos;: 13,
 &apos;excellent&apos;: 62,
 &apos;drawing&apos;: 240,
 &apos;veil&apos;: 16,
 &apos;from&apos;: 5709,
 &apos;men&apos;: 1145,
 &apos;motives&apos;: 14,
 &apos;actions&apos;: 77,
 &apos;trained&apos;: 23,
 &apos;reasoner&apos;: 6,
 &apos;admit&apos;: 65,
 &apos;such&apos;: 1436,
 &apos;intrusions&apos;: 1,
 &apos;into&apos;: 2124,
 &apos;own&apos;: 785,
 &apos;delicate&apos;: 54,
 &apos;finely&apos;: 11,
 &apos;adjusted&apos;: 16,
 &apos;temperament&apos;: 5,
 &apos;introduce&apos;: 23,
 &apos;distracting&apos;: 1,
 &apos;factor&apos;: 41,
 &apos;which&apos;: 4842,
 &apos;might&apos;: 536,
 &apos;throw&apos;: 48,
 &apos;doubt&apos;: 152,
 &apos;upon&apos;: 1111,
 &apos;mental&apos;: 37,
 &apos;results&apos;: 229,
 &apos;grit&apos;: 1,
 &apos;sensitive&apos;: 35,
 &apos;instrument&apos;: 35,
 &apos;crack&apos;: 20,
 &apos;high&apos;: 290,
 &apos;power&apos;: 548,
 &apos;lenses&apos;: 1,
 &apos;more&apos;: 1997,
 &apos;disturbing&apos;: 9,
 &apos;than&apos;: 1206,
 &apos;strong&apos;: 168,
 &apos;nature&apos;: 170,
 &apos;yet&apos;: 488,
 &apos;there&apos;: 2972,
 &apos;late&apos;: 165,
 &apos;dubious&apos;: 1,
 &apos;questionable&apos;: 3,
 &apos;memory&apos;: 55,
 &apos;had&apos;: 7383,
 &apos;little&apos;: 1001,
 &apos;lately&apos;: 22,
 &apos;my&apos;: 2249,
 &apos;marriage&apos;: 96,
 &apos;drifted&apos;: 5,
 &apos;us&apos;: 684,
 &apos;away&apos;: 838,
 &apos;each&apos;: 411,
 &apos;complete&apos;: 145,
 &apos;happiness&apos;: 143,
 &apos;home&apos;: 295,
 &apos;centred&apos;: 2,
 &apos;interests&apos;: 118,
 &apos;rise&apos;: 240,
 &apos;up&apos;: 2284,
 &apos;around&apos;: 271,
 &apos;who&apos;: 3050,
 &apos;finds&apos;: 23,
 &apos;master&apos;: 141,
 &apos;establishment&apos;: 40,
 &apos;sufficient&apos;: 75,
 &apos;absorb&apos;: 4,
 &apos;attention&apos;: 191,
 &apos;while&apos;: 768,
 &apos;loathed&apos;: 1,
 &apos;every&apos;: 650,
 &apos;form&apos;: 507,
 &apos;society&apos;: 169,
 &apos;bohemian&apos;: 8,
 &apos;soul&apos;: 168,
 &apos;remained&apos;: 231,
 &apos;lodgings&apos;: 11,
 &apos;baker&apos;: 49,
 &apos;street&apos;: 180,
 &apos;buried&apos;: 21,
 &apos;among&apos;: 451,
 &apos;old&apos;: 1180,
 &apos;books&apos;: 59,
 &apos;alternating&apos;: 2,
 &apos;week&apos;: 95,
 &apos;between&apos;: 654,
 &apos;cocaine&apos;: 4,
 &apos;ambition&apos;: 13,
 &apos;drowsiness&apos;: 4,
 &apos;drug&apos;: 21,
 &apos;fierce&apos;: 12,
 &apos;energy&apos;: 45,
 &apos;keen&apos;: 32,
 &apos;still&apos;: 922,
 &apos;ever&apos;: 274,
 &apos;deeply&apos;: 77,
 &apos;attracted&apos;: 36,
 &apos;study&apos;: 144,
 &apos;crime&apos;: 61,
 &apos;occupied&apos;: 116,
 &apos;immense&apos;: 77,
 &apos;faculties&apos;: 8,
 &apos;extraordinary&apos;: 74,
 &apos;powers&apos;: 149,
 &apos;observation&apos;: 39,
 &apos;following&apos;: 208,
 &apos;those&apos;: 1201,
 &apos;clues&apos;: 3,
 &apos;clearing&apos;: 29,
 &apos;mysteries&apos;: 9,
 &apos;been&apos;: 2599,
 &apos;abandoned&apos;: 72,
 &apos;hopeless&apos;: 17,
 &apos;official&apos;: 91,
 &apos;police&apos;: 94,
 &apos;time&apos;: 1529,
 &apos;some&apos;: 1536,
 &apos;vague&apos;: 39,
 &apos;account&apos;: 177,
 &apos;doings&apos;: 11,
 &apos;summons&apos;: 11,
 &apos;odessa&apos;: 3,
 &apos;trepoff&apos;: 1,
 &apos;murder&apos;: 30,
 &apos;singular&apos;: 36,
 &apos;tragedy&apos;: 9,
 &apos;atkinson&apos;: 1,
 &apos;brothers&apos;: 50,
 &apos;trincomalee&apos;: 1,
 &apos;finally&apos;: 156,
 &apos;mission&apos;: 34,
 &apos;accomplished&apos;: 39,
 &apos;so&apos;: 3017,
 &apos;delicately&apos;: 3,
 &apos;successfully&apos;: 25,
 &apos;reigning&apos;: 3,
 &apos;family&apos;: 210,
 &apos;holland&apos;: 12,
 &apos;beyond&apos;: 225,
 &apos;signs&apos;: 98,
 &apos;activity&apos;: 131,
 &apos;however&apos;: 430,
 &apos;merely&apos;: 189,
 &apos;shared&apos;: 25,
 &apos;readers&apos;: 11,
 &apos;daily&apos;: 44,
 &apos;press&apos;: 81,
 &apos;knew&apos;: 496,
 &apos;former&apos;: 177,
 &apos;friend&apos;: 283,
 &apos;companion&apos;: 81,
 &apos;night&apos;: 385,
 &apos;on&apos;: 6643,
 &apos;twentieth&apos;: 19,
 &apos;returning&apos;: 68,
 &apos;journey&apos;: 69,
 &apos;patient&apos;: 383,
 &apos;now&apos;: 1697,
 &apos;returned&apos;: 194,
 &apos;civil&apos;: 177,
 &apos;practice&apos;: 95,
 &apos;way&apos;: 859,
 &apos;led&apos;: 196,
 &apos;me&apos;: 1920,
 &apos;through&apos;: 815,
 &apos;passed&apos;: 367,
 &apos;well&apos;: 1198,
 &apos;remembered&apos;: 120,
 &apos;door&apos;: 498,
 &apos;must&apos;: 955,
 &apos;associated&apos;: 196,
 &apos;wooing&apos;: 2,
 &apos;dark&apos;: 181,
 &apos;incidents&apos;: 14,
 &apos;scarlet&apos;: 22,
 &apos;seized&apos;: 114,
 &apos;desire&apos;: 96,
 &apos;see&apos;: 1101,
 &apos;again&apos;: 866,
 &apos;know&apos;: 1048,
 &apos;employing&apos;: 7,
 &apos;rooms&apos;: 86,
 &apos;brilliantly&apos;: 5,
 &apos;lit&apos;: 74,
 &apos;even&apos;: 946,
 &apos;looked&apos;: 760,
 &apos;saw&apos;: 599,
 &apos;tall&apos;: 74,
 &apos;spare&apos;: 27,
 &apos;figure&apos;: 103,
 &apos;pass&apos;: 154,
 &apos;twice&apos;: 84,
 &apos;silhouette&apos;: 1,
 &apos;against&apos;: 660,
 &apos;blind&apos;: 23,
 &apos;pacing&apos;: 26,
 &apos;room&apos;: 960,
 &apos;swiftly&apos;: 38,
 &apos;eagerly&apos;: 39,
 &apos;head&apos;: 725,
 &apos;sunk&apos;: 27,
 &apos;chest&apos;: 81,
 &apos;hands&apos;: 455,
 &apos;clasped&apos;: 11,
 &apos;behind&apos;: 401,
 &apos;mood&apos;: 51,
 &apos;habit&apos;: 55,
 &apos;attitude&apos;: 72,
 &apos;manner&apos;: 135,
 &apos;told&apos;: 490,
 &apos;their&apos;: 2955,
 &apos;story&apos;: 133,
 &apos;work&apos;: 382,
 &apos;risen&apos;: 30,
 &apos;created&apos;: 62,
 &apos;dreams&apos;: 16,
 &apos;hot&apos;: 119,
 &apos;scent&apos;: 17,
 &apos;new&apos;: 1211,
 &apos;problem&apos;: 76,
 &apos;rang&apos;: 29,
 &apos;bell&apos;: 65,
 &apos;shown&apos;: 113,
 &apos;chamber&apos;: 35,
 &apos;formerly&apos;: 77,
 &apos;part&apos;: 704,
 &apos;effusive&apos;: 2,
 &apos;glad&apos;: 150,
 &apos;think&apos;: 557,
 &apos;hardly&apos;: 173,
 &apos;word&apos;: 298,
 &apos;spoken&apos;: 92,
 &apos;kindly&apos;: 86,
 &apos;eye&apos;: 110,
 &apos;waved&apos;: 29,
 &apos;an&apos;: 3423,
 &apos;armchair&apos;: 49,
 &apos;threw&apos;: 96,
 &apos;across&apos;: 222,
 &apos;cigars&apos;: 7,
 &apos;indicated&apos;: 88,
 &apos;spirit&apos;: 167,
 &apos;gasogene&apos;: 1,
 &apos;corner&apos;: 128,
 &apos;then&apos;: 1558,
 &apos;stood&apos;: 383,
 &apos;fire&apos;: 274,
 &apos;introspective&apos;: 3,
 &apos;fashion&apos;: 49,
 &apos;wedlock&apos;: 1,
 &apos;suits&apos;: 8,
 &apos;remarked&apos;: 169,
 &apos;watson&apos;: 83,
 &apos;put&apos;: 435,
 &apos;seven&apos;: 132,
 &apos;half&apos;: 318,
 &apos;pounds&apos;: 26,
 &apos;answered&apos;: 226,
 &apos;indeed&apos;: 139,
 &apos;thought&apos;: 902,
 &apos;just&apos;: 767,
 &apos;trifle&apos;: 11,
 &apos;fancy&apos;: 50,
 &apos;observe&apos;: 37,
 &apos;did&apos;: 1875,
 &apos;tell&apos;: 492,
 &apos;intended&apos;: 58,
 &apos;go&apos;: 905,
 &apos;harness&apos;: 27,
 &apos;deduce&apos;: 14,
 &apos;getting&apos;: 92,
 &apos;yourself&apos;: 162,
 &apos;very&apos;: 1340,
 &apos;wet&apos;: 60,
 &apos;clumsy&apos;: 8,
 &apos;careless&apos;: 14,
 &apos;servant&apos;: 46,
 &apos;girl&apos;: 166,
 &apos;dear&apos;: 449,
 &apos;said&apos;: 3464,
 &apos;too&apos;: 548,
 &apos;much&apos;: 671,
 &apos;certainly&apos;: 119,
 &apos;burned&apos;: 77,
 &apos;lived&apos;: 113,
 &apos;few&apos;: 458,
 &apos;centuries&apos;: 12,
 &apos;ago&apos;: 108,
 &apos;true&apos;: 205,
 &apos;walk&apos;: 75,
 &apos;thursday&apos;: 7,
 &apos;came&apos;: 979,
 &apos;dreadful&apos;: 68,
 &apos;mess&apos;: 10,
 &apos;changed&apos;: 134,
 &apos;clothes&apos;: 62,
 &apos;t&apos;: 1318,
 &apos;imagine&apos;: 96,
 &apos;mary&apos;: 705,
 &apos;jane&apos;: 2,
 &apos;incorrigible&apos;: 2,
 &apos;wife&apos;: 367,
 &apos;given&apos;: 364,
 &apos;notice&apos;: 98,
 &apos;fail&apos;: 40,
 &apos;chuckled&apos;: 7,
 &apos;rubbed&apos;: 32,
 &apos;long&apos;: 991,
 &apos;nervous&apos;: 54,
 &apos;together&apos;: 260,
 &apos;simplicity&apos;: 30,
 &apos;itself&apos;: 273,
 &apos;inside&apos;: 43,
 &apos;left&apos;: 834,
 &apos;shoe&apos;: 11,
 &apos;where&apos;: 977,
 &apos;firelight&apos;: 2,
 &apos;strikes&apos;: 19,
 &apos;leather&apos;: 35,
 &apos;scored&apos;: 4,
 &apos;six&apos;: 176,
 &apos;almost&apos;: 325,
 &apos;parallel&apos;: 17,
 &apos;cuts&apos;: 5,
 &apos;obviously&apos;: 38,
 &apos;caused&apos;: 102,
 &apos;someone&apos;: 160,
 &apos;carelessly&apos;: 14,
 &apos;scraped&apos;: 21,
 &apos;round&apos;: 556,
 &apos;edges&apos;: 70,
 &apos;sole&apos;: 70,
 &apos;order&apos;: 404,
 &apos;crusted&apos;: 2,
 &apos;mud&apos;: 36,
 &apos;hence&apos;: 32,
 &apos;double&apos;: 49,
 &apos;deduction&apos;: 12,
 &apos;vile&apos;: 16,
 &apos;weather&apos;: 42,
 &apos;malignant&apos;: 88,
 &apos;boot&apos;: 22,
 &apos;slitting&apos;: 2,
 &apos;specimen&apos;: 14,
 &apos;london&apos;: 76,
 &apos;slavey&apos;: 1,
 &apos;if&apos;: 2372,
 &apos;gentleman&apos;: 99,
 &apos;walks&apos;: 10,
 &apos;smelling&apos;: 5,
 &apos;iodoform&apos;: 43,
 &apos;black&apos;: 235,
 &apos;mark&apos;: 38,
 &apos;nitrate&apos;: 7,
 &apos;silver&apos;: 128,
 &apos;right&apos;: 710,
 &apos;forefinger&apos;: 7,
 &apos;bulge&apos;: 2,
 &apos;side&apos;: 511,
 &apos;top&apos;: 42,
 &apos;hat&apos;: 105,
 &apos;show&apos;: 213,
 &apos;secreted&apos;: 2,
 &apos;stethoscope&apos;: 2,
 &apos;dull&apos;: 74,
 &apos;pronounce&apos;: 9,
 &apos;active&apos;: 96,
 &apos;member&apos;: 50,
 &apos;medical&apos;: 22,
 &apos;profession&apos;: 22,
 &apos;could&apos;: 1700,
 &apos;help&apos;: 230,
 &apos;laughing&apos;: 115,
 &apos;ease&apos;: 44,
 &apos;explained&apos;: 60,
 &apos;process&apos;: 219,
 &apos;hear&apos;: 183,
 &apos;give&apos;: 523,
 &apos;reasons&apos;: 64,
 &apos;appears&apos;: 108,
 &apos;ridiculously&apos;: 1,
 &apos;simple&apos;: 139,
 &apos;easily&apos;: 114,
 &apos;myself&apos;: 227,
 &apos;though&apos;: 650,
 &apos;successive&apos;: 17,
 &apos;instance&apos;: 50,
 &apos;am&apos;: 746,
 &apos;baffled&apos;: 8,
 &apos;until&apos;: 325,
 &apos;explain&apos;: 123,
 &apos;believe&apos;: 183,
 &apos;good&apos;: 744,
 &apos;yours&apos;: 46,
 &apos;quite&apos;: 502,
 &apos;lighting&apos;: 16,
 &apos;cigarette&apos;: 6,
 &apos;throwing&apos;: 46,
 &apos;down&apos;: 1128,
 &apos;distinction&apos;: 19,
 &apos;clear&apos;: 233,
 &apos;example&apos;: 286,
 &apos;frequently&apos;: 218,
 &apos;steps&apos;: 188,
 &apos;lead&apos;: 137,
 &apos;hall&apos;: 83,
 &apos;often&apos;: 443,
 &apos;hundreds&apos;: 48,
 &apos;times&apos;: 236,
 &apos;many&apos;: 609,
 &apos;don&apos;: 581,
 &apos;observed&apos;: 131,
 &apos;point&apos;: 223,
 &apos;seventeen&apos;: 10,
 &apos;because&apos;: 630,
 &apos;interested&apos;: 65,
 &apos;problems&apos;: 78,
 &apos;enough&apos;: 175,
 &apos;chronicle&apos;: 7,
 &apos;two&apos;: 1138,
 &apos;trifling&apos;: 12,
 &apos;experiences&apos;: 11,
 &apos;sheet&apos;: 29,
 &apos;thick&apos;: 77,
 &apos;pink&apos;: 27,
 &apos;tinted&apos;: 9,
 &apos;notepaper&apos;: 2,
 &apos;lying&apos;: 118,
 &apos;open&apos;: 325,
 &apos;table&apos;: 296,
 &apos;last&apos;: 565,
 &apos;post&apos;: 117,
 &apos;aloud&apos;: 28,
 &apos;note&apos;: 115,
 &apos;undated&apos;: 1,
 &apos;either&apos;: 293,
 &apos;signature&apos;: 9,
 &apos;address&apos;: 76,
 &apos;will&apos;: 1577,
 &apos;call&apos;: 197,
 &apos;quarter&apos;: 46,
 &apos;eight&apos;: 128,
 &apos;o&apos;: 257,
 &apos;clock&apos;: 120,
 &apos;desires&apos;: 22,
 &apos;consult&apos;: 19,
 &apos;matter&apos;: 365,
 &apos;deepest&apos;: 15,
 &apos;moment&apos;: 487,
 &apos;recent&apos;: 54,
 &apos;services&apos;: 38,
 &apos;royal&apos;: 111,
 &apos;houses&apos;: 117,
 &apos;europe&apos;: 153,
 &apos;safely&apos;: 11,
 &apos;trusted&apos;: 16,
 &apos;matters&apos;: 136,
 &apos;importance&apos;: 117,
 &apos;exaggerated&apos;: 28,
 &apos;we&apos;: 1906,
 &apos;quarters&apos;: 72,
 &apos;received&apos;: 280,
 &apos;hour&apos;: 157,
 &apos;amiss&apos;: 6,
 &apos;visitor&apos;: 74,
 &apos;wear&apos;: 30,
 &apos;mask&apos;: 12,
 &apos;what&apos;: 3011,
 &apos;means&apos;: 253,
 &apos;no&apos;: 2348,
 &apos;data&apos;: 17,
 &apos;capital&apos;: 144,
 &apos;mistake&apos;: 39,
 &apos;theorise&apos;: 1,
 &apos;insensibly&apos;: 2,
 &apos;begins&apos;: 47,
 &apos;twist&apos;: 14,
 &apos;facts&apos;: 72,
 &apos;suit&apos;: 25,
 &apos;theories&apos;: 21,
 &apos;instead&apos;: 137,
 &apos;carefully&apos;: 72,
 &apos;examined&apos;: 49,
 &apos;writing&apos;: 69,
 &apos;paper&apos;: 177,
 &apos;wrote&apos;: 149,
 &apos;presumably&apos;: 8,
 &apos;endeavouring&apos;: 8,
 &apos;imitate&apos;: 7,
 &apos;processes&apos;: 35,
 &apos;bought&apos;: 55,
 &apos;crown&apos;: 61,
 &apos;packet&apos;: 11,
 &apos;peculiarly&apos;: 14,
 &apos;stiff&apos;: 20,
 &apos;peculiar&apos;: 84,
 &apos;hold&apos;: 114,
 &apos;light&apos;: 278,
 &apos;large&apos;: 483,
 &apos;e&apos;: 136,
 &apos;g&apos;: 55,
 &apos;p&apos;: 66,
 &apos;woven&apos;: 5,
 &apos;texture&apos;: 6,
 &apos;asked&apos;: 777,
 &apos;maker&apos;: 4,
 &apos;monogram&apos;: 4,
 &apos;rather&apos;: 219,
 &apos;stands&apos;: 19,
 &apos;gesellschaft&apos;: 1,
 &apos;german&apos;: 196,
 &apos;company&apos;: 192,
 &apos;customary&apos;: 19,
 &apos;contraction&apos;: 61,
 &apos;like&apos;: 1080,
 &apos;co&apos;: 30,
 &apos;course&apos;: 389,
 &apos;papier&apos;: 1,
 &apos;eg&apos;: 1,
 &apos;let&apos;: 506,
 &apos;glance&apos;: 91,
 &apos;continental&apos;: 46,
 &apos;gazetteer&apos;: 1,
 &apos;took&apos;: 573,
 &apos;heavy&apos;: 139,
 &apos;brown&apos;: 71,
 &apos;volume&apos;: 30,
 &apos;shelves&apos;: 3,
 &apos;eglow&apos;: 1,
 &apos;eglonitz&apos;: 1,
 &apos;here&apos;: 691,
 &apos;egria&apos;: 1,
 &apos;speaking&apos;: 185,
 &apos;far&apos;: 408,
 &apos;carlsbad&apos;: 1,
 &apos;remarkable&apos;: 77,
 &apos;being&apos;: 918,
 &apos;scene&apos;: 49,
 &apos;death&apos;: 330,
 &apos;wallenstein&apos;: 1,
 &apos;its&apos;: 1635,
 &apos;numerous&apos;: 50,
 &apos;glass&apos;: 116,
 &apos;factories&apos;: 29,
 &apos;mills&apos;: 39,
 &apos;ha&apos;: 75,
 &apos;boy&apos;: 169,
 &apos;sparkled&apos;: 5,
 &apos;sent&apos;: 319,
 &apos;great&apos;: 792,
 &apos;triumphant&apos;: 16,
 &apos;cloud&apos;: 30,
 &apos;made&apos;: 1007,
 &apos;precisely&apos;: 24,
 &apos;construction&apos;: 25,
 &apos;sentence&apos;: 26,
 &apos;frenchman&apos;: 102,
 &apos;russian&apos;: 461,
 &apos;uncourteous&apos;: 1,
 &apos;verbs&apos;: 1,
 &apos;only&apos;: 1873,
 &apos;remains&apos;: 73,
 &apos;therefore&apos;: 186,
 &apos;discover&apos;: 28,
 &apos;wanted&apos;: 213,
 &apos;writes&apos;: 20,
 &apos;prefers&apos;: 2,
 &apos;wearing&apos;: 87,
 &apos;showing&apos;: 104,
 &apos;face&apos;: 1125,
 &apos;comes&apos;: 91,
 &apos;mistaken&apos;: 59,
 &apos;resolve&apos;: 14,
 &apos;doubts&apos;: 39,
 &apos;sharp&apos;: 83,
 &apos;sound&apos;: 219,
 &apos;horses&apos;: 262,
 &apos;hoofs&apos;: 24,
 &apos;grating&apos;: 10,
 &apos;wheels&apos;: 47,
 &apos;curb&apos;: 4,
 &apos;followed&apos;: 329,
 &apos;pull&apos;: 23,
 &apos;whistled&apos;: 13,
 &apos;pair&apos;: 40,
 &apos;yes&apos;: 688,
 &apos;continued&apos;: 291,
 &apos;glancing&apos;: 98,
 &apos;window&apos;: 186,
 &apos;nice&apos;: 53,
 &apos;brougham&apos;: 4,
 &apos;beauties&apos;: 2,
 &apos;hundred&apos;: 229,
 &apos;fifty&apos;: 94,
 &apos;guineas&apos;: 3,
 &apos;apiece&apos;: 7,
 &apos;money&apos;: 326,
 &apos;nothing&apos;: 646,
 &apos;else&apos;: 201,
 &apos;better&apos;: 266,
 &apos;bit&apos;: 63,
 &apos;doctor&apos;: 183,
 &apos;stay&apos;: 74,
 &apos;lost&apos;: 224,
 &apos;boswell&apos;: 1,
 &apos;promises&apos;: 15,
 &apos;interesting&apos;: 71,
 &apos;pity&apos;: 75,
 &apos;miss&apos;: 112,
 &apos;client&apos;: 33,
 &apos;want&apos;: 323,
 &apos;sit&apos;: 89,
 &apos;best&apos;: 268,
 &apos;slow&apos;: 65,
 &apos;step&apos;: 139,
 &apos;stairs&apos;: 31,
 &apos;passage&apos;: 110,
 &apos;paused&apos;: 79,
 &apos;immediately&apos;: 182,
 &apos;outside&apos;: 110,
 &apos;loud&apos;: 64,
 &apos;authoritative&apos;: 2,
 &apos;tap&apos;: 10,
 &apos;come&apos;: 934,
 &apos;entered&apos;: 282,
 &apos;less&apos;: 367,
 &apos;feet&apos;: 179,
 &apos;inches&apos;: 16,
 &apos;height&apos;: 36,
 &apos;limbs&apos;: 67,
 &apos;hercules&apos;: 4,
 &apos;dress&apos;: 138,
 &apos;rich&apos;: 92,
 &apos;richness&apos;: 2,
 &apos;england&apos;: 311,
 &apos;bad&apos;: 155,
 &apos;taste&apos;: 23,
 &apos;bands&apos;: 27,
 &apos;astrakhan&apos;: 1,
 &apos;slashed&apos;: 3,
 &apos;sleeves&apos;: 30,
 &apos;fronts&apos;: 1,
 &apos;breasted&apos;: 1,
 &apos;coat&apos;: 172,
 &apos;deep&apos;: 215,
 &apos;cloak&apos;: 62,
 &apos;thrown&apos;: 92,
 &apos;shoulders&apos;: 125,
 &apos;lined&apos;: 32,
 &apos;flame&apos;: 15,
 &apos;coloured&apos;: 21,
 &apos;silk&apos;: 50,
 &apos;secured&apos;: 48,
 &apos;neck&apos;: 203,
 &apos;brooch&apos;: 1,
 &apos;consisted&apos;: 38,
 &apos;single&apos;: 173,
 &apos;flaming&apos;: 8,
 &apos;boots&apos;: 91,
 &apos;extended&apos;: 75,
 &apos;halfway&apos;: 19,
 &apos;calves&apos;: 3,
 &apos;trimmed&apos;: 8,
 &apos;tops&apos;: 3,
 &apos;fur&apos;: 38,
 &apos;completed&apos;: 25,
 &apos;impression&apos;: 67,
 &apos;barbaric&apos;: 2,
 &apos;opulence&apos;: 3,
 &apos;suggested&apos;: 69,
 &apos;appearance&apos;: 135,
 &apos;carried&apos;: 282,
 &apos;broad&apos;: 92,
 &apos;brimmed&apos;: 4,
 &apos;hand&apos;: 834,
 &apos;wore&apos;: 58,
 &apos;upper&apos;: 130,
 &apos;extending&apos;: 35,
 &apos;past&apos;: 223,
 &apos;cheekbones&apos;: 4,
 &apos;vizard&apos;: 1,
 &apos;apparently&apos;: 68,
 &apos;raised&apos;: 212,
 &apos;lower&apos;: 196,
 &apos;appeared&apos;: 197,
 &apos;hanging&apos;: 42,
 &apos;straight&apos;: 124,
 &apos;chin&apos;: 30,
 &apos;suggestive&apos;: 11,
 &apos;resolution&apos;: 57,
 &apos;pushed&apos;: 81,
 &apos;length&apos;: 63,
 &apos;obstinacy&apos;: 7,
 &apos;harsh&apos;: 22,
 &apos;voice&apos;: 462,
 &apos;strongly&apos;: 41,
 &apos;marked&apos;: 138,
 &apos;accent&apos;: 18,
 &apos;uncertain&apos;: 30,
 &apos;pray&apos;: 79,
 &apos;seat&apos;: 170,
 &apos;colleague&apos;: 7,
 &apos;dr&apos;: 48,
 &apos;occasionally&apos;: 89,
 &apos;cases&apos;: 453,
 &apos;whom&apos;: 489,
 &apos;honour&apos;: 16,
 &apos;count&apos;: 748,
 &apos;von&apos;: 11,
 &apos;kramm&apos;: 2,
 &apos;nobleman&apos;: 11,
 &apos;understand&apos;: 412,
 &apos;discretion&apos;: 13,
 &apos;trust&apos;: 68,
 &apos;extreme&apos;: 72,
 &apos;prefer&apos;: 21,
 &apos;communicate&apos;: 15,
 &apos;alone&apos;: 337,
 &apos;rose&apos;: 243,
 &apos;caught&apos;: 90,
 &apos;wrist&apos;: 68,
 &apos;back&apos;: 746,
 &apos;chair&apos;: 135,
 &apos;none&apos;: 110,
 &apos;say&apos;: 755,
 &apos;anything&apos;: 379,
 &apos;shrugged&apos;: 35,
 &apos;begin&apos;: 97,
 &apos;binding&apos;: 18,
 &apos;absolute&apos;: 56,
 &apos;secrecy&apos;: 18,
 &apos;years&apos;: 571,
 &apos;end&apos;: 465,
 &apos;present&apos;: 329,
 &apos;weight&apos;: 70,
 &apos;influence&apos;: 138,
 &apos;european&apos;: 99,
 &apos;history&apos;: 439,
 &apos;promise&apos;: 67,
 &apos;excuse&apos;: 53,
 &apos;strange&apos;: 220,
 &apos;august&apos;: 70,
 &apos;person&apos;: 185,
 &apos;employs&apos;: 2,
 &apos;wishes&apos;: 42,
 &apos;agent&apos;: 25,
 &apos;unknown&apos;: 87,
 &apos;confess&apos;: 36,
 &apos;once&apos;: 569,
 &apos;called&apos;: 450,
 &apos;exactly&apos;: 47,
 &apos;aware&apos;: 52,
 &apos;dryly&apos;: 5,
 &apos;circumstances&apos;: 107,
 &apos;delicacy&apos;: 11,
 &apos;precaution&apos;: 9,
 &apos;taken&apos;: 438,
 &apos;quench&apos;: 3,
 &apos;grow&apos;: 74,
 &apos;seriously&apos;: 63,
 &apos;compromise&apos;: 71,
 &apos;families&apos;: 45,
 &apos;speak&apos;: 255,
 &apos;plainly&apos;: 39,
 &apos;implicates&apos;: 5,
 &apos;house&apos;: 661,
 &apos;ormstein&apos;: 2,
 &apos;hereditary&apos;: 14,
 &apos;kings&apos;: 27,
 &apos;murmured&apos;: 18,
 &apos;settling&apos;: 16,
 &apos;closing&apos;: 35,
 &apos;glanced&apos;: 176,
 ...}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alphabet = <span class="string">'abcdefghigklmnopqrstuvwxyz'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span><span class="params">(word)</span>:</span></span><br><span class="line">    n = len(word)</span><br><span class="line">    <span class="keyword">return</span> set([word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]+</span><br><span class="line">              [word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>]+word[i]+word[i+<span class="number">2</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>)]+</span><br><span class="line">              [word[<span class="number">0</span>:i]+c+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet] +</span><br><span class="line">              [word[<span class="number">0</span>:i]+c+word[i:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> set(e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">e1 = edits1(<span class="string">'hello'</span>)</span><br><span class="line">e2 = edits2(<span class="string">'hello'</span>)</span><br><span class="line">len(e1)+len(e2)</span><br></pre></td></tr></table></figure>




<pre><code>33328</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span><span class="params">(words)</span>:</span></span><br><span class="line">    w = set()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dic_words:</span><br><span class="line">            w.add(word)</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correct</span><span class="params">(word)</span>:</span></span><br><span class="line">    candidates = known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known(edits2(word)) <span class="keyword">or</span> word</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> word == candidates:</span><br><span class="line">        <span class="keyword">return</span> word</span><br><span class="line">    max_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> candidates:</span><br><span class="line">        <span class="keyword">if</span> dic_words[c]&gt;=max_num:</span><br><span class="line">            max_num=dic_words[c]</span><br><span class="line">            candidate = c</span><br><span class="line">    <span class="keyword">return</span> candidate</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'ehell'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;shell&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'hel'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;he&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'heade'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;head&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'haved'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;have&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'adevnues'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;avenues&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯算法</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习bagging</title>
    <url>/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/</url>
    <content><![CDATA[<p>bagging代码简单演示</p>
<a id="more"></a>


<p>个体学习器之间不存在强依赖关系，装袋（bagging）<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data=iris.data[:,:<span class="number">2</span>]</span><br><span class="line">y_data=iris.target</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = neighbors.KNeighborsClassifier()</span><br><span class="line">knn.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据值所在的范围</span></span><br><span class="line">    x_min, x_max = x_data[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = x_data[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成网格矩阵</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                         np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line">    </span><br><span class="line">    z=model.predict(np.c_[xx.ravel(),yy.ravel()])</span><br><span class="line">    z = z.reshape(xx.shape)</span><br><span class="line">    cs = plt.contourf(xx,yy,z)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot(knn)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">knn.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_5_0.png" alt="png"></p>
<pre><code>0.631578947368421</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dtree = tree.DecisionTreeClassifier()</span><br><span class="line">dtree.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&apos;gini&apos;,
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=&apos;deprecated&apos;,
                       random_state=None, splitter=&apos;best&apos;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot(dtree)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">dtree.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_7_0.png" alt="png"></p>
<pre><code>0.6052631578947368</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bagging_knn = BaggingClassifier(knn,n_estimators=<span class="number">100</span>)</span><br><span class="line">bagging_knn.fit(x_train,y_train)</span><br><span class="line">plot(bagging_knn)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">bagging_knn.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_8_0.png" alt="png"></p>
<pre><code>0.631578947368421</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bagging_tree = BaggingClassifier(dtree,n_estimators=<span class="number">100</span>)</span><br><span class="line">bagging_tree.fit(x_train,y_train)</span><br><span class="line">plot(bagging_tree)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">-0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">bagging_tree.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_9_0.png" alt="png"></p>
<pre><code>0.6578947368421053</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习stacking</title>
    <url>/2020/07/11/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0stacking/</url>
    <content><![CDATA[<p>stacking代码简单演示</p>
<a id="more"></a>

<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data,y_data=iris.data[:,<span class="number">1</span>:<span class="number">3</span>],iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = DecisionTreeClassifier()</span><br><span class="line">clf3 = LogisticRegression()</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1,clf2,clf3],meta_classifier=lr)</span><br><span class="line"><span class="keyword">for</span> clf,label <span class="keyword">in</span> zip([clf1,clf2,clf3,sclf],</span><br><span class="line">                    [<span class="string">'KNN'</span>,<span class="string">'Decision Tree'</span>,<span class="string">'LogisticRegression'</span>,<span class="string">'StackingClassifier'</span>]):</span><br><span class="line">    scores = model_selection.cross_val_score(clf,x_data,y_data,cv=<span class="number">3</span>,scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">'Accuracy:%0.2f [%s]'</span> %(scores.mean(),label))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy:0.91 [KNN]
Accuracy:0.93 [Decision Tree]
Accuracy:0.95 [LogisticRegression]
Accuracy:0.93 [StackingClassifier]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习boosting</title>
    <url>/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/</url>
    <content><![CDATA[<p>boosting代码简单演示</p>
<a id="more"></a>


<p>个体学习器之间存在强依赖关系，提升（boosting）<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting7.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_gaussian_quantiles</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1,y1 = make_gaussian_quantiles(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,n_classes=<span class="number">2</span>)</span><br><span class="line">x2,y2 = make_gaussian_quantiles(mean=(<span class="number">3</span>,<span class="number">3</span>),n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,n_classes=<span class="number">2</span>)</span><br><span class="line">x_data = np.concatenate((x1,x2))</span><br><span class="line">y_data = np.concatenate((y1,<span class="number">1</span>-y2))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x_data[:,<span class="number">-0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_3_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tree.DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line">model.fit(x_data,y_data)</span><br><span class="line">x_min,x_max = x_data[:,<span class="number">0</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">0</span>].max()+<span class="number">1</span></span><br><span class="line">y_min,y_max = x_data[:,<span class="number">1</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">1</span>].max()+<span class="number">1</span></span><br><span class="line"><span class="comment"># 生成网格矩阵</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">z = model.predict(np.c_[xx.ravel(), yy.ravel()])<span class="comment"># ravel与flatten类似，多维数据转一维。flatten不会改变原始数据，ravel会改变原始数据</span></span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># 等高线图</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># 样本散点图</span></span><br><span class="line">plt.scatter(x_data[:, <span class="number">0</span>], x_data[:, <span class="number">1</span>], c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.score(x_data,y_data)</span><br></pre></td></tr></table></figure>




<pre><code>0.705</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=<span class="number">3</span>),n_estimators=<span class="number">10</span>)</span><br><span class="line">model.fit(x_data,y_data)</span><br><span class="line"><span class="comment"># 获取数据值所在的范围</span></span><br><span class="line">x_min, x_max = x_data[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = x_data[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成网格矩阵</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取预测值</span></span><br><span class="line">z = model.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># 等高线图</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># 样本散点图</span></span><br><span class="line">plt.scatter(x_data[:, <span class="number">0</span>], x_data[:, <span class="number">1</span>], c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.score(x_data,y_data)</span><br></pre></td></tr></table></figure>




<pre><code>0.988</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习voting</title>
    <url>/2020/07/11/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0voting/</url>
    <content><![CDATA[<p>voting代码简单演示</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data,y_data = iris.data[:,<span class="number">1</span>:<span class="number">3</span>],iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = DecisionTreeClassifier()</span><br><span class="line">clf3 = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = VotingClassifier([(<span class="string">'knn'</span>,clf1),(<span class="string">'dtree'</span>,clf2),(<span class="string">'lr'</span>,clf3)])</span><br><span class="line"><span class="keyword">for</span> clf,label <span class="keyword">in</span> zip([clf1,clf2,clf3,sclf],</span><br><span class="line">                    [<span class="string">'KNN'</span>,<span class="string">'Decistion Tree'</span>,<span class="string">'LogisticRegression'</span>,<span class="string">'VotingClassifier'</span>]):</span><br><span class="line">    scores = model_selection.cross_val_score(clf,x_data,y_data,cv=<span class="number">3</span>,scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">'Accuracy: %0.2f[%s]'</span> %(scores.mean(),label))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy: 0.91[KNN]
Accuracy: 0.90[Decistion Tree]
Accuracy: 0.91[LogisticRegression]
Accuracy: 0.92[VotingClassifier]


C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>非线性逻辑回归</title>
    <url>/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>非线性逻辑回归代码演示<br>数据下载<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/LR-testSet2.txt" target="_blank" rel="noopener">LR-testSet2.txt</a></p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">scale = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<ul>
<li>np.newaxis的功能——插入新维度</li>
</ul>
<hr>
<p>举个简单的例子介绍一下吧。</p>
<ul>
<li><p>栗子1：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">print(a.shape)</span><br><span class="line">print (a)</span><br></pre></td></tr></table></figure></li>
<li><p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(5,)</span><br><span class="line">[1 2 3 4 5]</span><br></pre></td></tr></table></figure>
</li>
<li><p>栗子2：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">aa&#x3D;a[:,np.newaxis]</span><br><span class="line">print(aa.shape)</span><br><span class="line">print (aa)</span><br></pre></td></tr></table></figure>
</li>
<li><p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(5, 1)</span><br><span class="line">[[1]</span><br><span class="line">[2]</span><br><span class="line">[3]</span><br><span class="line">[4]</span><br><span class="line">[5]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>栗子3：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">aa&#x3D;a[np.newaxis,:]</span><br><span class="line">print(aa.shape)</span><br><span class="line">print (aa)</span><br></pre></td></tr></table></figure></li>
<li><p>输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1, 5)</span><br><span class="line">[[1 2 3 4 5]]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<p>看明白了吧，原来np.newaxis的作用是增加一个维度。</p>
<p>对于[: , np.newaxis] 和 [np.newaxis，：]<br>是在np.newaxis这里增加1维。</p>
<p>这样改变维度的作用往往是将一维的数据转变成一个矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/LR-testSet2.txt'</span>,delimiter=<span class="string">','</span>)</span><br><span class="line">x_data = data[:,:<span class="number">-1</span>] <span class="comment">#shape(118, 2)</span></span><br><span class="line">y_data = data[:,<span class="number">-1</span>,np.newaxis] <span class="comment">#shape(118, 1)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></span><br><span class="line">    x0 = []</span><br><span class="line">    x1 = []</span><br><span class="line">    y0 = []</span><br><span class="line">    y1 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        <span class="keyword">if</span> y_data[i]==<span class="number">0</span>:</span><br><span class="line">            x0.append(x_data[i,<span class="number">0</span>])</span><br><span class="line">            y0.append(x_data[i,<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x1.append(x_data[i,<span class="number">0</span>])</span><br><span class="line">            y1.append(x_data[i,<span class="number">1</span>])</span><br><span class="line">    scatter0 = plt.scatter(x0,y0,c=<span class="string">'b'</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">    scatter1 = plt.scatter(x1,y1,c=<span class="string">'r'</span>,marker=<span class="string">'x'</span>)</span><br><span class="line">    plt.legend(handles=[scatter0,scatter1],labels=[<span class="string">'labell0'</span>,<span class="string">'label1'</span>],loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plot()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/output_2_0.png" alt="png"></p>
<p>使用sklearn.preprocessing.PolynomialFeatures来进行特征的构造。</p>
<p>它是使用多项式的方法来进行的，如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2），这个多项式的形式是使用poly的效果。</p>
<p>PolynomialFeatures有三个参数</p>
<ul>
<li>degree：控制多项式的度</li>
<li>interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，上面的二次项中没有a^2和b^2。</li>
<li>include_bias：默认为True。如果为True的话，那么就会有上面的 1那一项。</li>
</ul>
<p>例子1，interaction_only为默认的False时</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;[[5,10]]    #c&#x3D;[[a,b]],这里要注意a的shape，如果是list形式，则将a.shape&#x3D;-1,1</span><br><span class="line">pl&#x3D;PolynomialFeatures()</span><br><span class="line">b&#x3D;pl.fit_transform(c)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[  1.,   5.,  10.,  25.,  50., 100.]]) </span><br><span class="line">#符合（1,a,b,a^2,ab, b^2）</span><br></pre></td></tr></table></figure>

<p>例子2，interaction_only=True时</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;[[5,10]]</span><br><span class="line">pl&#x3D;PolynomialFeatures(interaction_only&#x3D;True)</span><br><span class="line">b&#x3D;pl.fit_transform(c)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 1.,  5., 10., 50.]])   </span><br><span class="line">#输出中不包含a^2和b^2项</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly_reg = PolynomialFeatures(degree=<span class="number">3</span>)</span><br><span class="line">x_poly = poly_reg.fit_transform(x_data)</span><br><span class="line">x_poly</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 1.00000000e+00,  5.12670000e-02,  6.99560000e-01, ...,
         1.83865725e-03,  2.50892595e-02,  3.42353606e-01],
       [ 1.00000000e+00, -9.27420000e-02,  6.84940000e-01, ...,
         5.89122275e-03, -4.35092419e-02,  3.21334672e-01],
       [ 1.00000000e+00, -2.13710000e-01,  6.92250000e-01, ...,
         3.16164171e-02, -1.02411982e-01,  3.31733166e-01],
       ...,
       [ 1.00000000e+00, -4.84450000e-01,  9.99270000e-01, ...,
         2.34520477e-01, -4.83742961e-01,  9.97811598e-01],
       [ 1.00000000e+00, -6.33640000e-03,  9.99270000e-01, ...,
         4.01206555e-05, -6.32715223e-03,  9.97811598e-01],
       [ 1.00000000e+00,  6.32650000e-01, -3.06120000e-02, ...,
        -1.22523312e-02,  5.92852863e-04, -2.86863382e-05]])</code></pre><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%921.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%922.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%923.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%924.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(xMat,yMat,ws)</span>:</span></span><br><span class="line">    left = np.multiply(yMat,np.log(sigmoid(xMat*ws)))</span><br><span class="line">    right = np.multiply(<span class="number">1</span>-yMat,np.log(<span class="number">1</span>-sigmoid(xMat*ws)))</span><br><span class="line">    <span class="keyword">return</span> np.sum(left+right)/-(len(xMat))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scale == <span class="literal">True</span>:</span><br><span class="line">        xArr = preprocessing.scale(xArr)</span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    lr = <span class="number">0.03</span></span><br><span class="line">    epochs = <span class="number">50000</span></span><br><span class="line">    costList = []</span><br><span class="line">    m,n = np.shape(xMat)</span><br><span class="line">    ws = np.mat(np.ones((n,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs+<span class="number">1</span>):</span><br><span class="line">        h = sigmoid(xMat*ws)</span><br><span class="line">        ws_grad = xMat.T*(h-yMat)/m</span><br><span class="line">        ws = ws - lr*ws_grad</span><br><span class="line">        <span class="keyword">if</span> i %<span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            costList.append(cost(xMat,yMat,ws))</span><br><span class="line">    <span class="keyword">return</span> ws,costList</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws, costList = gradAscent(x_poly,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 4.16787292]
 [ 2.72213524]
 [ 4.55120018]
 [-9.76109006]
 [-5.34880198]
 [-8.51458023]
 [-0.55950401]
 [-1.55418165]
 [-0.75929829]
 [-2.88573877]]</code></pre><p>np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; np.array([[1, 2, 3],[7,8,9]])</span><br><span class="line"> </span><br><span class="line">b&#x3D;np.array([[4,5,6],[1,2,3]])</span><br><span class="line"> </span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<p>Out[4]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1, 2, 3],</span><br><span class="line">       [7, 8, 9]])</span><br></pre></td></tr></table></figure>

<p>b<br>Out[5]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[4, 5, 6],</span><br><span class="line">       [1, 2, 3]])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;np.c_[a,b]</span><br><span class="line"> </span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<p>Out[7]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1, 2, 3, 4, 5, 6],</span><br><span class="line">       [7, 8, 9, 1, 2, 3]])</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">d&#x3D; np.array([7,8,9])</span><br><span class="line"> </span><br><span class="line">e&#x3D;np.array([1, 2, 3])</span><br><span class="line"> </span><br><span class="line">f&#x3D;np.c_[d,e]</span><br><span class="line"> </span><br><span class="line">f</span><br></pre></td></tr></table></figure>
<p>Out[12]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[7, 1],</span><br><span class="line">       [8, 2],</span><br><span class="line">       [9, 3]])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_min,x_max = x_data[:,<span class="number">0</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">0</span>].max()+<span class="number">1</span></span><br><span class="line">y_min,y_max = x_data[:,<span class="number">1</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">1</span>].max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">xx,yy = np.meshgrid(np.arange(x_min,y_max,<span class="number">0.02</span>),</span><br><span class="line">                   np.arange(y_min,y_max,<span class="number">0.02</span>))</span><br><span class="line"><span class="comment"># raval多维转一维</span></span><br><span class="line"><span class="comment"># dot矩阵乘积</span></span><br><span class="line"><span class="comment"># np.c_左右拼接两个矩阵</span></span><br><span class="line">z = sigmoid(poly_reg.fit_transform(np.c_[xx.ravel(),yy.ravel()]).dot(np.array(ws)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">    <span class="keyword">if</span> z[i]&gt;<span class="number">0.5</span>:</span><br><span class="line">        z[i]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        z[i]=<span class="number">0</span></span><br><span class="line">z=z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">cs = plt.contourf(xx,yy,z)</span><br><span class="line">plot()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/output_9_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x_data,ws)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scale == <span class="literal">True</span>:</span><br><span class="line">        x_data = preprocessing.scale(x_data)</span><br><span class="line">    xMat = np.mat(x_data)</span><br><span class="line">    ws = np.mat(ws)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x&gt;=<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> sigmoid(xMat*ws)] </span><br><span class="line"></span><br><span class="line">predictions = predict(x_poly,ws)</span><br><span class="line">print(classification_report(y_data,predictions))</span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

         0.0       0.86      0.83      0.85        60
         1.0       0.83      0.86      0.85        58

    accuracy                           0.85       118
   macro avg       0.85      0.85      0.85       118
weighted avg       0.85      0.85      0.85       118</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理4-直方图</title>
    <url>/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%864-%E7%9B%B4%E6%96%B9%E5%9B%BE/</url>
    <content><![CDATA[<ul>
<li>10.1.<a href="#header1">直方图-1：查找、绘制和分析</a></li>
<li>10.2.<a href="#header2">直方图-2：直方图均衡</a></li>
<li>10.3.<a href="#header3">直方图-3：二维直方图</a></li>
<li>10.4.<a href="#header4">直方图4：直方图反投影</a><a id="more"></a>

</li>
</ul>
<h1 id="直方图-1：查找、绘制和分析"><a href="#直方图-1：查找、绘制和分析" class="headerlink" title="直方图-1：查找、绘制和分析"></a><span id="header1">直方图-1：查找、绘制和分析</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>学会 </p>
<ul>
<li>使用OpenCV和Numpy函数查找直方图 </li>
<li>使用OpenCV和Matplotlib函数绘制直方图 </li>
<li>你将看到以下函数：cv.calcHist()，np.histogram()等。</li>
</ul>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>那么直方图是什么？您可以将直方图视为图形或绘图，从而可以总体了解图像的强度分布。</p>
<p>它是在X轴上具有像素值（不总是从0到255的范围），在Y轴上具有图像中相应像素数的图。</p>
<p>这只是理解图像的另一种方式。通过查看图像的直方图，您可以直观地了解该图像的对比度，亮度，强度分布等。</p>
<p>当今几乎所有图像处理工具都提供直方图功能。以下是剑桥彩色网站的图片，我建议您访问该网站以获取更多详细信息。</p>
<p><img src="http://qiniu.aihubs.net/histogram_sample.jpg" alt></p>
<p>您可以看到图像及其直方图。（请记住，此直方图是针对灰度图像而非彩色图像绘制的）。</p>
<p>直方图的左侧区域显示图像中较暗像素的数量，而右侧区域则显示明亮像素的数量。</p>
<p>从直方图中，您可以看到暗区域多于亮区域，而中间调的数量（中间值的像素值，例如127附近）则非常少。</p>
<h2 id="寻找直方图"><a href="#寻找直方图" class="headerlink" title="寻找直方图"></a>寻找直方图</h2><p>现在我们有了一个关于直方图的想法，我们可以研究如何找到它。OpenCV和Numpy都为此内置了功能。</p>
<p>在使用这些功能之前，我们需要了解一些与直方图有关的术语。</p>
<p>BINS：上面的直方图显示每个像素值的像素数，即从0到255。即，您需要256个值来显示上面的直方图。但是考虑一下，如果您不需要分别找到所有像素值的像素数，而是找到像素值间隔中的像素数怎么办？ 例如，您需要找到介于0到15之间的像素数，然后找到16到31之间，…，240到255之间的像素数。只需要16个值即可表示直方图。这就是在OpenCV教程中有关直方图的示例中显示的内容。</p>
<p>因此，您要做的就是将整个直方图分成16个子部分，每个子部分的值就是其中所有像素数的总和。 每个子部分都称为“ BIN”。在第一种情况下，bin的数量为256个（每个像素一个），而在第二种情况下，bin的数量仅为16个。BINS由OpenCV文档中的<strong>histSize</strong>术语表示。</p>
<p>DIMS：这是我们为其收集数据的参数的数量。在这种情况下，我们仅收集关于强度值的一件事的数据。所以这里是1。</p>
<p>RANGE：这是您要测量的强度值的范围。通常，它是[0,256]，即所有强度值。</p>
<h2 id="1-OpenCV中的直方图计算"><a href="#1-OpenCV中的直方图计算" class="headerlink" title="1. OpenCV中的直方图计算"></a>1. OpenCV中的直方图计算</h2><p>因此，现在我们使用<strong>cv.calcHist</strong>()函数查找直方图。让我们熟悉一下该函数及其参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.calcHist（images，channels，mask，histSize，ranges [，hist [，accumulate]]）</span><br><span class="line">images：它是uint8或float32类型的源图像。它应该放在方括号中，即“ [img]”。</span><br><span class="line">channels：也以方括号给出。它是我们计算直方图的通道的索引。例如，如果输入为灰度图像，则其值为[0]。对于彩色图像，您可以传递[0]，[1]或[2]分别计算蓝色，绿色或红色通道的直方图。</span><br><span class="line">mask：图像掩码。为了找到完整图像的直方图，将其指定为“无”。但是，如果要查找图像特定区域的直方图，则必须为此创建一个掩码图像并将其作为掩码。（我将在后面显示一个示例。）</span><br><span class="line">histSize：这表示我们的BIN计数。需要放在方括号中。对于全尺寸，我们通过[256]。</span><br><span class="line">ranges：这是我们的RANGE。通常为[0,256]。</span><br></pre></td></tr></table></figure>
<p>因此，让我们从示例图像开始。只需以灰度模式加载图像并找到其完整直方图即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line">hist &#x3D; cv.calcHist([img],[0],None,[256],[0,256])</span><br></pre></td></tr></table></figure>
<p>hist是256x1的数组，每个值对应于该图像中具有相应像素值的像素数。</p>
<h2 id="2-numpy的直方图计算"><a href="#2-numpy的直方图计算" class="headerlink" title="2. numpy的直方图计算"></a>2. numpy的直方图计算</h2><p>Numpy还为您提供了一个函数<strong>np.histogram</strong>()。因此，除了<strong>calcHist</strong>()函数外，您可以尝试下面的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hist,bins &#x3D; np.histogram(img.ravel(),256,[0,256])</span><br></pre></td></tr></table></figure>
<p>hist与我们之前计算的相同。但是bin将具有257个元素，因为Numpy计算出bin的范围为0-0.99、1-1.99、2-2.99等。因此最终范围为255-255.99。为了表示这一点，他们还在最后添加了256。但是我们不需要256。最多255就足够了。</p>
<p>另外 Numpy还有另一个函数<strong>np.bincount</strong>()，它比np.histogram()快10倍左右。因此，对于一维直方图，您可以更好地尝试一下。不要忘记在np.bincount中设置minlength = 256。例如，hist = np.bincount(img.ravel()，minlength = 256)<br>注意 OpenCV函数比np.histogram()快大约40倍。因此，尽可能使用OpenCV函数。</p>
<p>现在我们应该绘制直方图，但是怎么绘制？</p>
<h2 id="绘制直方图"><a href="#绘制直方图" class="headerlink" title="绘制直方图"></a>绘制直方图</h2><p>有两种方法， 1. 简短的方法：使用Matplotlib绘图功能 2. 稍长的方法：使用OpenCV绘图功能</p>
<h2 id="1-使用Matplotlib"><a href="#1-使用Matplotlib" class="headerlink" title="1. 使用Matplotlib"></a>1. 使用Matplotlib</h2><p>Matplotlib带有直方图绘图功能：matplotlib.pyplot.hist() 它直接找到直方图并将其绘制。</p>
<p>您无需使用<strong>calcHist</strong>()或np.histogram()函数来查找直方图。请参见下面的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line">plt.hist(img.ravel(),256,[0,256]); plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/histogram_matplotlib.jpg" alt></p>
<p>或者，您可以使用matplotlib的法线图，这对于BGR图是很好的。为此，您需要首先找到直方图数据。试试下面的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">color &#x3D; (&#39;b&#39;,&#39;g&#39;,&#39;r&#39;)</span><br><span class="line">for i,col in enumerate(color):</span><br><span class="line">    histr &#x3D; cv.calcHist([img],[i],None,[256],[0,256])</span><br><span class="line">    plt.plot(histr,color &#x3D; col)</span><br><span class="line">    plt.xlim([0,256])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/histogram_rgb_plot.jpg" alt><br>您可以从上图中得出，蓝色在图像中具有一些高值域（显然这应该是由于天空）</p>
<h2 id="2-使用-OpenCV"><a href="#2-使用-OpenCV" class="headerlink" title="2. 使用 OpenCV"></a>2. 使用 OpenCV</h2><p>好吧，在这里您可以调整直方图的值及其bin值，使其看起来像x，y坐标，</p>
<p>以便您可以使用<strong>cv.line</strong>()或cv.polyline()函数绘制它以生成与上述相同的图像。</p>
<p>OpenCV-Python2官方示例已经提供了此功能。检查示例/python/hist.py中的代码。</p>
<p>掩码的应用<br>我们使用了cv.calcHist()来查找整个图像的直方图。如果你想找到图像某些区域的直方图呢?只需创建一个掩码图像，在你要找到直方图为白色，否则黑色。然后把这个作为掩码传递。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line"># create a mask</span><br><span class="line">mask &#x3D; np.zeros(img.shape[:2], np.uint8)</span><br><span class="line">mask[100:300, 100:400] &#x3D; 255</span><br><span class="line">masked_img &#x3D; cv.bitwise_and(img,img,mask &#x3D; mask)</span><br><span class="line"># 计算掩码区域和非掩码区域的直方图</span><br><span class="line"># 检查作为掩码的第三个参数</span><br><span class="line">hist_full &#x3D; cv.calcHist([img],[0],None,[256],[0,256])</span><br><span class="line">hist_mask &#x3D; cv.calcHist([img],[0],mask,[256],[0,256])</span><br><span class="line">plt.subplot(221), plt.imshow(img, &#39;gray&#39;)</span><br><span class="line">plt.subplot(222), plt.imshow(mask,&#39;gray&#39;)</span><br><span class="line">plt.subplot(223), plt.imshow(masked_img, &#39;gray&#39;)</span><br><span class="line">plt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)</span><br><span class="line">plt.xlim([0,256])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>查看结果。在直方图中，蓝线表示完整图像的直方图，绿线表示掩码区域的直方图。<br><img src="http://qiniu.aihubs.net/histogram_masking.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="直方图-2：直方图均衡"><a href="#直方图-2：直方图均衡" class="headerlink" title="直方图-2：直方图均衡"></a><span id="header2">直方图-2：直方图均衡</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本节中, - 我们将学习直方图均衡化的概念,并利用它来提高图像的对比度。</p>
<h2 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h2><p>考虑这样一个图像，它的像素值仅局限于某个特定的值范围。</p>
<p>例如，较亮的图像将把所有像素限制在高值上。但是一幅好的图像会有来自图像所有区域的像素。</p>
<p>因此，您需要将这个直方图拉伸到两端(如下图所示，来自wikipedia)，这就是直方图均衡化的作用(简单来说)。这通常会提高图像的对比度。<br><img src="http://qiniu.aihubs.net/histogram_equalization.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;wiki.jpg&#39;,0)</span><br><span class="line">hist,bins &#x3D; np.histogram(img.flatten(),256,[0,256])</span><br><span class="line">cdf &#x3D; hist.cumsum()</span><br><span class="line">cdf_normalized &#x3D; cdf * float(hist.max()) &#x2F; cdf.max()</span><br><span class="line">plt.plot(cdf_normalized, color &#x3D; &#39;b&#39;)</span><br><span class="line">plt.hist(img.flatten(),256,[0,256], color &#x3D; &#39;r&#39;)</span><br><span class="line">plt.xlim([0,256])</span><br><span class="line">plt.legend((&#39;cdf&#39;,&#39;histogram&#39;), loc &#x3D; &#39;upper left&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/histeq_numpy1.jpg" alt></p>
<p>你可以看到直方图位于较亮的区域。我们需要全频谱。</p>
<p>为此，我们需要一个转换函数，将亮区域的输入像素映射到整个区域的输出像素。这就是直方图均衡化的作用。</p>
<p>现在我们找到最小的直方图值(不包括0)，并应用wiki页面中给出的直方图均衡化方程。</p>
<p>但我在这里用过，来自Numpy的掩码数组概念数组。对于掩码数组，所有操作都在非掩码元素上执行。您可以从Numpy文档中了解更多关于掩码数组的信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cdf_m &#x3D; np.ma.masked_equal(cdf,0)</span><br><span class="line">cdf_m &#x3D; (cdf_m - cdf_m.min())*255&#x2F;(cdf_m.max()-cdf_m.min())</span><br><span class="line">cdf &#x3D; np.ma.filled(cdf_m,0).astype(&#39;uint8&#39;)</span><br></pre></td></tr></table></figure>
<p>现在我们有了查找表，该表为我们提供了有关每个输入像素值的输出像素值是什么的信息。因此，我们仅应用变换。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img2 &#x3D; cdf[img]</span><br></pre></td></tr></table></figure>
<p>现在，我们像以前一样计算其直方图和cdf（您这样做），结果如下所示：<br><img src="http://qiniu.aihubs.net/histeq_numpy2.jpg" alt><br>另一个重要的特征是，即使图像是一个较暗的图像(而不是我们使用的一个较亮的图像)，经过均衡后，我们将得到几乎相同的图像。因此，这是作为一个“参考工具”，使所有的图像具有相同的照明条件。这在很多情况下都很有用。</p>
<p>例如，在人脸识别中，在对人脸数据进行训练之前，对人脸图像进行直方图均衡化处理，使其具有相同的光照条件。</p>
<h2 id="OpenCV中的直方图均衡"><a href="#OpenCV中的直方图均衡" class="headerlink" title="OpenCV中的直方图均衡"></a>OpenCV中的直方图均衡</h2><p>OpenCV具有执行此操作的功能cv.equalizeHist（）。</p>
<p>它的输入只是灰度图像，输出是我们的直方图均衡图像。 下面是一个简单的代码片段，显示了它与我们使用的同一图像的用法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;wiki.jpg&#39;,0)</span><br><span class="line">equ &#x3D; cv.equalizeHist(img)</span><br><span class="line">res &#x3D; np.hstack((img,equ)) #stacking images side-by-side</span><br><span class="line">cv.imwrite(&#39;res.png&#39;,res)</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/equalization_opencv.jpg" alt><br>因此，现在您可以在不同的光照条件下拍摄不同的图像，对其进行均衡并检查结果。</p>
<p>当图像的直方图限制在特定区域时，直方图均衡化效果很好。在直方图覆盖较大区域（即同时存在亮像素和暗像素）的强度变化较大的地方，效果不好。请检查其他资源中的SOF链接。</p>
<h2 id="CLAHE（对比度受限的自适应直方图均衡）"><a href="#CLAHE（对比度受限的自适应直方图均衡）" class="headerlink" title="CLAHE（对比度受限的自适应直方图均衡）"></a>CLAHE（对比度受限的自适应直方图均衡）</h2><p>我们刚刚看到的第一个直方图均衡化考虑了图像的整体对比度。</p>
<p>在许多情况下，这不是一个好主意。例如，下图显示了输入图像及其在全局直方图均衡后的结果。<br><img src="http://qiniu.aihubs.net/clahe_1.jpg" alt></p>
<p>直方图均衡后，背景对比度确实得到了改善。但是在两个图像中比较雕像的脸。</p>
<p>由于亮度过高，我们在那里丢失了大多数信息。</p>
<p>这是因为它的直方图不像我们在前面的案例中所看到的那样局限于特定区域（尝试绘制输入图像的直方图，您将获得更多的直觉）。</p>
<p>因此，为了解决这个问题，使用了<strong>自适应直方图均衡</strong>。</p>
<p>在这种情况下，图像被分成称为“tiles”的小块（在OpenCV中，tileSize默认为8x8）。</p>
<p>然后，像往常一样对这些块中的每一个进行直方图均衡。</p>
<p>因此，在较小的区域中，直方图将限制在一个较小的区域中（除非存在噪声）。</p>
<p>如果有噪音，它将被放大。为了避免这种情况，应用了对比度限制。</p>
<p>如果任何直方图bin超出指定的对比度限制（在OpenCV中默认为40），则在应用直方图均衡之前，将这些像素裁剪并均匀地分布到其他bin。</p>
<p>均衡后，要消除图块边界中的伪影，请应用双线性插值。</p>
<p>下面的代码片段显示了如何在OpenCV中应用CLAHE：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;tsukuba_l.png&#39;,0)</span><br><span class="line"># create a CLAHE object (Arguments are optional).</span><br><span class="line">clahe &#x3D; cv.createCLAHE(clipLimit&#x3D;2.0, tileGridSize&#x3D;(8,8))</span><br><span class="line">cl1 &#x3D; clahe.apply(img)</span><br><span class="line">cv.imwrite(&#39;clahe_2.jpg&#39;,cl1)</span><br></pre></td></tr></table></figure>
<p>查看下面的结果，并将其与上面的结果进行比较，尤其是雕像区域：<br><img src="http://qiniu.aihubs.net/clahe_2.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="直方图-3：二维直方图"><a href="#直方图-3：二维直方图" class="headerlink" title="直方图-3：二维直方图"></a><span id="header3">直方图-3：二维直方图</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习查找和绘制2D直方图。这将在以后的章节中有所帮助。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在第一篇文章中，我们计算并绘制了一维直方图。 之所以称为一维，是因为我们仅考虑一个特征，即像素的灰度强度值。 </p>
<p>但是在二维直方图中，您要考虑两个特征。 通常，它用于查找颜色直方图，其中两个特征是每个像素的色相和饱和度值。</p>
<p>我们将尝试了解如何创建这种颜色直方图，这对于理解诸如直方图反向投影之类的更多主题将很有用。</p>
<h2 id="OpenCV中的二维直方图"><a href="#OpenCV中的二维直方图" class="headerlink" title="OpenCV中的二维直方图"></a>OpenCV中的二维直方图</h2><p>它非常简单，并且使用相同的函数<strong>cv.calcHist</strong>()进行计算。 </p>
<p>对于颜色直方图，我们需要将图像从BGR转换为HSV。（请记住，对于一维直方图，我们从BGR转换为灰度）。对于二维直方图，其参数将进行如下修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">channel &#x3D; [0,1]，因为我们需要同时处理H和S平面。</span><br><span class="line">bins &#x3D; [180,256] 对于H平面为180，对于S平面为256。</span><br><span class="line">range &#x3D; [0,180,0,256] 色相值介于0和180之间，饱和度介于0和256之间。</span><br></pre></td></tr></table></figure>
<p>现在检查以下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist &#x3D; cv.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])</span><br></pre></td></tr></table></figure>
<p>就是这样。</p>
<h2 id="Numpy中的二维直方图"><a href="#Numpy中的二维直方图" class="headerlink" title="Numpy中的二维直方图"></a>Numpy中的二维直方图</h2><p>Numpy还为此提供了一个特定的函数:np.histogram2d()。(记住，对于一维直方图我们使用了<strong>np.histogram</strong>())。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist, xbins, ybins &#x3D; np.histogram2d(h.ravel(),s.ravel(),[180,256],[[0,180],[0,256]])</span><br></pre></td></tr></table></figure>
<p>第一个参数是H平面，第二个是S平面，第三个是每个箱子的数量，第四个是它们的范围。</p>
<p>现在我们可以检查如何绘制这个颜色直方图。</p>
<h2 id="绘制二维直方图"><a href="#绘制二维直方图" class="headerlink" title="绘制二维直方图"></a>绘制二维直方图</h2><h2 id="方法1：使用-cv-imshow"><a href="#方法1：使用-cv-imshow" class="headerlink" title="方法1：使用 cv.imshow()"></a>方法1：使用 cv.imshow()</h2><p>我们得到的结果是尺寸为80x256的二维数组。因此，可以使用<strong>cv.imshow</strong>()函数像平常一样显示它们。</p>
<p>它将是一幅灰度图像，除非您知道不同颜色的色相值，否则不会对其中的颜色有太多了解。</p>
<h2 id="方法2：使用Matplotlib"><a href="#方法2：使用Matplotlib" class="headerlink" title="方法2：使用Matplotlib"></a>方法2：使用Matplotlib</h2><p>我们可以使用matplotlib.pyplot.imshow()函数绘制具有不同颜色图的2D直方图。</p>
<p>它使我们对不同的像素密度有了更好的了解。但是，除非您知道不同颜色的色相值，否则乍一看并不能使我们知道到底是什么颜色。</p>
<p>注意 使用此功能时，请记住，插值法应采用最近邻以获得更好的结果。</p>
<p>考虑下面的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist &#x3D; cv.calcHist( [hsv], [0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line">plt.imshow(hist,interpolation &#x3D; &#39;nearest&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>下面是输入图像及其颜色直方图。X轴显示S值，Y轴显示色相。<br><img src="http://qiniu.aihubs.net/2dhist_matplotlib.jpg" alt></p>
<p>在直方图中，您可以在H = 100和S = 200附近看到一些较高的值。</p>
<p>它对应于天空的蓝色。同样，在H = 25和S = 100附近可以看到另一个峰值。它对应于宫殿的黄色。您可以使用GIMP等任何图像编辑工具进行验证。</p>
<h2 id="方法3：OpenCV示例样式"><a href="#方法3：OpenCV示例样式" class="headerlink" title="方法3：OpenCV示例样式"></a>方法3：OpenCV示例样式</h2><p>OpenCV-Python2示例中有一个颜色直方图的示例代码(samples / python / color_histogram.py)。</p>
<p>如果运行代码，则可以看到直方图也显示了相应的颜色。或者简单地，它输出颜色编码的直方图。其结果非常好（尽管您需要添加额外的线束）。</p>
<p>在该代码中，作者在HSV中创建了一个颜色图。然后将其转换为BGR。将所得的直方图图像与此颜色图相乘。他还使用一些预处理步骤来删除小的孤立像素，从而获得良好的直方图。</p>
<p>我将其留给读者来运行代码，对其进行分析并拥有自己的解决方法。下面是与上面相同的图像的代码输出：<br><img src="http://qiniu.aihubs.net/2dhist_opencv.jpg" alt></p>
<p>您可以在直方图中清楚地看到存在什么颜色，那里是蓝色，那里是黄色，并且由于棋盘的存在而有些白色。很好！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="直方图4：直方图反投影"><a href="#直方图4：直方图反投影" class="headerlink" title="直方图4：直方图反投影"></a><span id="header4">直方图4：直方图反投影</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习直方图反投影。</p>
<h2 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h2><p>这是由<strong>Michael J. Swain</strong>和<strong>Dana H. Ballard</strong>在他们的论文《通过颜色直方图索引》中提出的。</p>
<p>用简单的话说是什么意思？它用于图像分割或在图像中查找感兴趣的对象。</p>
<p>简而言之，它创建的图像大小与输入图像相同（但只有一个通道），其中每个像素对应于该像素属于我们物体的概率。</p>
<p>用更简单的话来说，与其余部分相比，输出图像将在可能有对象的区域具有更多的白色值。</p>
<p>好吧，这是一个直观的解释。（我无法使其更简单）。直方图反投影与camshift算法等配合使用。</p>
<p>我们该怎么做呢？我们创建一个图像的直方图，其中包含我们感兴趣的对象（在我们的示例中是背景，离开播放器等）。</p>
<p>对象应尽可能填充图像以获得更好的效果。而且颜色直方图比灰度直方图更可取，因为对象的颜色对比灰度强度是定义对象的好方法。</p>
<p>然后，我们将该直方图“反投影”到需要找到对象的测试图像上，</p>
<p>换句话说，我们计算出属于背景的每个像素的概率并将其显示出来。在适当的阈值下产生的输出使我们仅获得背景。</p>
<h2 id="Numpy中的算法"><a href="#Numpy中的算法" class="headerlink" title="Numpy中的算法"></a>Numpy中的算法</h2><p>首先，我们需要计算我们要查找的对象（使其为“ M”）和要搜索的图像（使其为“ I”）的颜色直方图。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cvfrom matplotlib import pyplot as plt</span><br><span class="line">#roi是我们需要找到的对象或对象区域</span><br><span class="line">roi &#x3D; cv.imread(&#39;rose_red.png&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(roi,cv.COLOR_BGR2HSV)</span><br><span class="line">#目标是我们搜索的图像</span><br><span class="line">target &#x3D; cv.imread(&#39;rose.png&#39;)</span><br><span class="line">hsvt &#x3D; cv.cvtColor(target,cv.COLOR_BGR2HSV)</span><br><span class="line"># 使用calcHist查找直方图。也可以使用np.histogram2d完成</span><br><span class="line">M &#x3D; cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line">I &#x3D; cv.calcHist([hsvt],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br></pre></td></tr></table></figure>

<p>求出比值R=$\frac{M}{I}$。然后反向投影R，即使用R作为调色板，并以每个像素作为其对应的目标概率创建一个新图像。即B(x,y) = R[h(x,y),s(x,y)] 其中h是色调，s是像素在(x，y)的饱和度。之后，应用条件B(x,y)=min[B(x,y),1]。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">h,s,v &#x3D; cv.split(hsvt)</span><br><span class="line">B &#x3D; R[h.ravel(),s.ravel()]</span><br><span class="line">B &#x3D; np.minimum(B,1)</span><br><span class="line">B &#x3D; B.reshape(hsvt.shape[:2])</span><br></pre></td></tr></table></figure>
<p>现在对圆盘应用卷积，B=D∗B，其中D是圆盘内核。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">disc &#x3D; cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))</span><br><span class="line">cv.filter2D(B,-1,disc,B)</span><br><span class="line">B &#x3D; np.uint8(B)</span><br><span class="line">cv.normalize(B,B,0,255,cv.NORM_MINMAX)</span><br></pre></td></tr></table></figure>
<p>现在最大强度的位置给了我们物体的位置。如果我们期望图像中有一个区域，则对合适的值进行阈值处理将获得不错的结果。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ret,thresh &#x3D; cv.threshold(B,50,255,0)</span><br></pre></td></tr></table></figure>
<p>就是这样</p>
<h2 id="OpenCV的反投影"><a href="#OpenCV的反投影" class="headerlink" title="OpenCV的反投影"></a>OpenCV的反投影</h2><p>OpenCV提供了一个内建的函数<strong>cv.calcBackProject</strong>()。</p>
<p>它的参数几乎与<strong>cv.calchist</strong>()函数相同。它的一个参数是直方图，也就是物体的直方图，我们必须找到它。</p>
<p>另外，在传递给backproject函数之前，应该对对象直方图进行归一化。它返回概率图像。</p>
<p>然后我们用圆盘内核对图像进行卷积并应用阈值。下面是我的代码和结果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">roi &#x3D; cv.imread(&#39;rose_red.png&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(roi,cv.COLOR_BGR2HSV)</span><br><span class="line">target &#x3D; cv.imread(&#39;rose.png&#39;)</span><br><span class="line">hsvt &#x3D; cv.cvtColor(target,cv.COLOR_BGR2HSV)</span><br><span class="line"># 计算对象的直方图</span><br><span class="line">roihist &#x3D; cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line"># 直方图归一化并利用反传算法</span><br><span class="line">cv.normalize(roihist,roihist,0,255,cv.NORM_MINMAX)</span><br><span class="line">dst &#x3D; cv.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)</span><br><span class="line"># 用圆盘进行卷积</span><br><span class="line">disc &#x3D; cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))</span><br><span class="line">cv.filter2D(dst,-1,disc,dst)</span><br><span class="line"># 应用阈值作与操作</span><br><span class="line">ret,thresh &#x3D; cv.threshold(dst,50,255,0)</span><br><span class="line">thresh &#x3D; cv.merge((thresh,thresh,thresh))</span><br><span class="line">res &#x3D; cv.bitwise_and(target,thresh)</span><br><span class="line">res &#x3D; np.vstack((target,thresh,res))</span><br><span class="line">cv.imwrite(&#39;res.jpg&#39;,res)</span><br></pre></td></tr></table></figure>
<p>以下是我处理过的一个示例。我将蓝色矩形内的区域用作示例对象，我想提取整个地面。<br><img src="http://qiniu.aihubs.net/backproject_opencv.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理5</title>
    <url>/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%865/</url>
    <content><![CDATA[<ul>
<li>11.<a href="#header1">傅里叶变换</a></li>
<li>12.<a href="#header2">模板匹配</a></li>
<li>13.<a href="#header3">霍夫线变换</a></li>
<li>14.<a href="#header4">霍夫圈变换</a></li>
<li>15.<a href="#header5">图像分割与Watershed算法</a></li>
<li>16.<a href="#header6">交互式前景提取使用GrabCut算法</a><a id="more"></a>

</li>
</ul>
<h1 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a><span id="header1">傅里叶变换</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>在本节中，我们将学习 </p>
<ul>
<li>使用OpenCV查找图像的傅立叶变换 </li>
<li>利用Numpy中可用的FFT函数 </li>
<li>傅立叶变换的某些应用程序 </li>
<li>我们将看到以下函数：cv.dft()，cv.idft()等</li>
</ul>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>傅立叶变换用于分析各种滤波器的频率特性。</p>
<p>对于图像，使用<strong>2D离散傅里叶变换</strong>(DFT)查找频域。</p>
<p>一种称为<strong>快速傅立叶变换</strong>(FFT)的快速算法用于DFT的计算。</p>
<p>关于这些的详细信息可以在任何图像处理或信号处理教科书中找到。请参阅其他资源部分。</p>
<p>对于正弦信号x(t)=Asin(2πft)，我们可以说f是信号的频率，如果采用其频域，则可以看到f的尖峰。</p>
<p>如果对信号进行采样以形成离散信号，我们将获得相同的频域，但是在[−π，π]或[0,2π]范围内（对于N点DFT为[0，N]）是周期性的。</p>
<p>您可以将图像视为在两个方向上采样的信号。因此，在X和Y方向都进行傅立叶变换，可以得到图像的频率表示。</p>
<p>更直观地说，对于正弦信号，如果幅度在短时间内变化如此之快，则可以说它是高频信号。</p>
<p>如果变化缓慢，则为低频信号。您可以将相同的想法扩展到图像。</p>
<p>图像中的振幅在哪里急剧变化？在边缘点或噪声。因此，可以说边缘和噪声是图像中的高频内容。</p>
<p>如果幅度没有太大变化，则它是低频分量。（一些链接已添加到“其他资源”，其中通过示例直观地说明了频率变换）。</p>
<p>现在，我们将看到如何找到傅立叶变换。</p>
<h2 id="Numpy中的傅里叶变换"><a href="#Numpy中的傅里叶变换" class="headerlink" title="Numpy中的傅里叶变换"></a>Numpy中的傅里叶变换</h2><p>首先，我们将看到如何使用Numpy查找傅立叶变换。</p>
<p>Numpy具有FFT软件包来执行此操作。np.fft.fft2()为我们提供了频率转换，它将是一个复杂的数组。</p>
<ul>
<li>它的第一个参数是输入图像，即灰度图像。</li>
<li>第二个参数是可选的，它决定输出数组的大小。如果它大于输入图像的大小，则在计算FFT之前用零填充输入图像。如果小于输入图像，将裁切输入图像。如果未传递任何参数，则输出数组的大小将与输入的大小相同。</li>
</ul>
<p>现在，一旦获得结果，零频率分量（DC分量）将位于左上角。</p>
<p>如果要使其居中，则需要在两个方向上将结果都移动N2。</p>
<p>只需通过函数<strong>np.fft.fftshift</strong>()即可完成。（它更容易分析）。找到频率变换后，就可以找到幅度谱。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">f &#x3D; np.fft.fft2(img)</span><br><span class="line">fshift &#x3D; np.fft.fftshift(f)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(np.abs(fshift))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Result look like below:<br>结果看起来像下面这样:<br><img src="http://qiniu.aihubs.net/fft1.jpg" alt></p>
<p>看，您可以在中心看到更多白色区域，这表明低频内容更多。</p>
<p>因此，您发现了频率变换现在，您可以在频域中进行一些操作，例如高通滤波和重建图像，即找到逆DFT。</p>
<p>为此，您只需用尺寸为60x60的矩形窗口遮罩即可消除低频。</p>
<p>然后，使用<strong>np.fft.ifftshift</strong>()应用反向移位，以使DC分量再次出现在左上角。</p>
<p>然后使用<strong>np.ifft2</strong>()函数找到逆FFT。同样，结果将是一个复数。您可以采用其绝对值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;&#x2F;2 , cols&#x2F;&#x2F;2</span><br><span class="line">fshift[crow-30:crow+31, ccol-30:ccol+31] &#x3D; 0</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; np.fft.ifft2(f_ishift)</span><br><span class="line">img_back &#x3D; np.real(img_back)</span><br><span class="line">plt.subplot(131),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(132),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Image after HPF&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(133),plt.imshow(img_back)</span><br><span class="line">plt.title(&#39;Result in JET&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果看起来像下面这样：<br><img src="http://qiniu.aihubs.net/fft2.jpg" alt></p>
<p>结果表明高通滤波是边缘检测操作。</p>
<p>这就是我们在“图像渐变”一章中看到的。</p>
<p>这也表明大多数图像数据都存在于频谱的低频区域。</p>
<p>无论如何，我们已经看到了如何在Numpy中找到DFT，IDFT等。</p>
<p>现在，让我们看看如何在OpenCV中进行操作。 </p>
<p>如果您仔细观察结果，尤其是最后一张JET颜色的图像，您会看到一些伪像（我用红色箭头标记的一个实例）。</p>
<p>它在那里显示出一些波纹状结构，称为<strong>振铃效应</strong>。</p>
<p>这是由我们用于遮罩的矩形窗口引起的。此掩码转换为正弦形状，从而导致此问题。</p>
<p>因此，矩形窗口不用于过滤。更好的选择是高斯窗口。</p>
<h2 id="OpenCV中的傅里叶变换"><a href="#OpenCV中的傅里叶变换" class="headerlink" title="OpenCV中的傅里叶变换"></a>OpenCV中的傅里叶变换</h2><p>OpenCV为此提供了<strong>cv.dft</strong>()和<strong>cv.idft</strong>()函数。它返回与前一个相同的结果，但是有两个通道。</p>
<p>第一个通道是结果的实部，第二个通道是结果的虚部。输入图像首先应转换为np.float32。我们来看看怎么做。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">dft &#x3D; cv.dft(np.float32(img),flags &#x3D; cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">dft_shift &#x3D; np.fft.fftshift(dft)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(cv.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>注意 您还可以使用<strong>cv.cartToPolar</strong>()，它在单个镜头中同时返回幅值和相位</p>
<p>现在我们要做DFT的逆变换。在上一节中，我们创建了一个HPF，这次我们将看到如何删除图像中的高频内容，即我们将LPF应用到图像中。</p>
<p>它实际上模糊了图像。为此，我们首先创建一个高值(1)在低频部分，即我们过滤低频内容，0在高频区。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;2 , cols&#x2F;2</span><br><span class="line"># 首先创建一个掩码，中心正方形为1，其余全为零</span><br><span class="line">mask &#x3D; np.zeros((rows,cols,2),np.uint8)</span><br><span class="line">mask[crow-30:crow+30, ccol-30:ccol+30] &#x3D; 1</span><br><span class="line"># 应用掩码和逆DFT</span><br><span class="line">fshift &#x3D; dft_shift*mask</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; cv.idft(f_ishift)</span><br><span class="line">img_back &#x3D; cv.magnitude(img_back[:,:,0],img_back[:,:,1])</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/fft4.jpg" alt></p>
<p>注意 通常，OpenCV函数<strong>cv.dft</strong>()和<strong>cv.idft</strong>()比Numpy函数更快。但是Numpy函数更容易使用。有关性能问题的更多细节，请参见下面的部分。</p>
<h2 id="DFT的性能优化"><a href="#DFT的性能优化" class="headerlink" title="DFT的性能优化"></a>DFT的性能优化</h2><p>对于某些数组尺寸，DFT的计算性能较好。当数组大小为2的幂时，速度最快。</p>
<p>对于大小为2、3和5的乘积的数组，也可以非常有效地进行处理。</p>
<p>因此，如果您担心代码的性能，可以在找到DFT之前将数组的大小修改为任何最佳大小(通过填充零)。对</p>
<p>于OpenCV，您必须手动填充零。但是对于Numpy，您指定FFT计算的新大小，它将自动为您填充零。</p>
<p>那么如何找到最优的大小呢?OpenCV为此提供了一个函数，cv.getOptimalDFTSize()。它同时适用于<strong>cv.dft</strong>()和<strong>np.fft.fft2</strong>()。让我们使用IPython魔术命令timeit来检查它们的性能。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [16]: img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">In [17]: rows,cols &#x3D; img.shape</span><br><span class="line">In [18]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(rows,cols))</span><br><span class="line">342 548</span><br><span class="line">In [19]: nrows &#x3D; cv.getOptimalDFTSize(rows)</span><br><span class="line">In [20]: ncols &#x3D; cv.getOptimalDFTSize(cols)</span><br><span class="line">In [21]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(nrows,ncols))</span><br><span class="line">360 576</span><br></pre></td></tr></table></figure>
<p>参见，将大小(342,548)修改为(360，576)。现在让我们用零填充（对于OpenCV），并找到其DFT计算性能。您可以通过创建一个新的零数组并将数据复制到其中来完成此操作，或者使用<strong>cv.copyMakeBorder</strong>()。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nimg &#x3D; np.zeros((nrows,ncols))</span><br><span class="line">nimg[:rows,:cols] &#x3D; img</span><br></pre></td></tr></table></figure>
<p>或者:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">right &#x3D; ncols - cols</span><br><span class="line">bottom &#x3D; nrows - rows</span><br><span class="line">bordertype &#x3D; cv.BORDER_CONSTANT ＃只是为了避免PDF文件中的行中断</span><br><span class="line">nimg &#x3D; cv.copyMakeBorder(img,0,bottom,0,right,bordertype, value &#x3D; 0)</span><br></pre></td></tr></table></figure>
<p>现在，我们计算Numpy函数的DFT性能比较：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [22]: %timeit fft1 &#x3D; np.fft.fft2(img)</span><br><span class="line">10 loops, best of 3: 40.9 ms per loop</span><br><span class="line">In [23]: %timeit fft2 &#x3D; np.fft.fft2(img,[nrows,ncols])</span><br><span class="line">100 loops, best of 3: 10.4 ms per loop</span><br></pre></td></tr></table></figure>
<p>它显示了4倍的加速。现在，我们将尝试使用OpenCV函数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [24]: %timeit dft1&#x3D; cv.dft(np.float32(img),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 13.5 ms per loop</span><br><span class="line">In [27]: %timeit dft2&#x3D; cv.dft(np.float32(nimg),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 3.11 ms per loop</span><br></pre></td></tr></table></figure>
<p>它还显示了4倍的加速。您还可以看到OpenCV函数比Numpy函数快3倍左右。也可以对逆FFT进行测试，这留给您练习。</p>
<h2 id="为什么拉普拉斯算子是高通滤波器？"><a href="#为什么拉普拉斯算子是高通滤波器？" class="headerlink" title="为什么拉普拉斯算子是高通滤波器？"></a>为什么拉普拉斯算子是高通滤波器？</h2><p>在一个论坛上也有人提出了类似的问题。问题是，为什么拉普拉斯变换是高通滤波器?</p>
<p>为什么Sobel是HPF?等。第一个答案是关于傅里叶变换的。对于更大的FFT只需要拉普拉斯变换。分析下面的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"># 没有缩放参数的简单均值滤波器</span><br><span class="line">mean_filter &#x3D; np.ones((3,3))</span><br><span class="line"># 创建高斯滤波器</span><br><span class="line">x &#x3D; cv.getGaussianKernel(5,10)</span><br><span class="line">gaussian &#x3D; x*x.T</span><br><span class="line"># 不同的边缘检测滤波器</span><br><span class="line"># x方向上的scharr</span><br><span class="line">scharr &#x3D; np.array([[-3, 0, 3],</span><br><span class="line">                   [-10,0,10],</span><br><span class="line">                   [-3, 0, 3]])</span><br><span class="line"># x方向上的sobel</span><br><span class="line">sobel_x&#x3D; np.array([[-1, 0, 1],</span><br><span class="line">                   [-2, 0, 2],</span><br><span class="line">                   [-1, 0, 1]])</span><br><span class="line"># y方向上的sobel</span><br><span class="line">sobel_y&#x3D; np.array([[-1,-2,-1],</span><br><span class="line">                   [0, 0, 0],</span><br><span class="line">                   [1, 2, 1]])</span><br><span class="line"># 拉普拉斯变换</span><br><span class="line">laplacian&#x3D;np.array([[0, 1, 0],</span><br><span class="line">                    [1,-4, 1],</span><br><span class="line">                    [0, 1, 0]])</span><br><span class="line">filters &#x3D; [mean_filter, gaussian, laplacian, sobel_x, sobel_y, scharr]</span><br><span class="line">filter_name &#x3D; [&#39;mean_filter&#39;, &#39;gaussian&#39;,&#39;laplacian&#39;, &#39;sobel_x&#39;, \</span><br><span class="line">                &#39;sobel_y&#39;, &#39;scharr_x&#39;]</span><br><span class="line">fft_filters &#x3D; [np.fft.fft2(x) for x in filters]</span><br><span class="line">fft_shift &#x3D; [np.fft.fftshift(y) for y in fft_filters]</span><br><span class="line">mag_spectrum &#x3D; [np.log(np.abs(z)+1) for z in fft_shift]</span><br><span class="line">for i in range(6):</span><br><span class="line">    plt.subplot(2,3,i+1),plt.imshow(mag_spectrum[i],cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(filter_name[i]), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>看看结果：<br><img src="http://qiniu.aihubs.net/fft5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="模板匹配"><a href="#模板匹配" class="headerlink" title="模板匹配"></a><span id="header2">模板匹配</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本章中，您将学习 - 使用模板匹配在图像中查找对象 - 你将看到以下功能：cv.matchTemplate()，cv.minMaxLoc()</p>
<h2 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h2><p>模板匹配是一种用于在较大图像中搜索和查找模板图像位置的方法。</p>
<p>为此，OpenCV带有一个函数<strong>cv.matchTemplate</strong>()。 </p>
<p>它只是将模板图​​像滑动到输入图像上（就像在2D卷积中一样），然后在模板图像下比较模板和输入图像的拼图。 </p>
<p>OpenCV中实现了几种比较方法。（您可以检查文档以了解更多详细信息）。它返回一个灰度图像，其中每个像素表示该像素的邻域与模板匹配的程度。</p>
<p>如果输入图像的大小为(WxH)，而模板图像的大小为(wxh)，则输出图像的大小将为(W-w + 1，H-h + 1)。得到结果后，可以使用<strong>cv.minMaxLoc</strong>()函数查找最大/最小值在哪。将其作为矩形的左上角，并以(w，h)作为矩形的宽度和高度。该矩形是您模板的区域。</p>
<p>注意 如果使用<strong>cv.TM_SQDIFF</strong>作为比较方法，则最小值提供最佳匹配。</p>
<h2 id="OpenCV中的模板匹配"><a href="#OpenCV中的模板匹配" class="headerlink" title="OpenCV中的模板匹配"></a>OpenCV中的模板匹配</h2><p>作为示例，我们将在梅西的照片中搜索他的脸。所以我创建了一个模板，如下所示： <img src="http://qiniu.aihubs.net/messi_face.jpg" alt> 我们将尝试所有比较方法，以便我们可以看到它们的结果如何：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">img2 &#x3D; img.copy()</span><br><span class="line">template &#x3D; cv.imread(&#39;template.jpg&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line"># 列表中所有的6种比较方法</span><br><span class="line">methods &#x3D; [&#39;cv.TM_CCOEFF&#39;, &#39;cv.TM_CCOEFF_NORMED&#39;, &#39;cv.TM_CCORR&#39;,</span><br><span class="line">            &#39;cv.TM_CCORR_NORMED&#39;, &#39;cv.TM_SQDIFF&#39;, &#39;cv.TM_SQDIFF_NORMED&#39;]</span><br><span class="line">for meth in methods:</span><br><span class="line">    img &#x3D; img2.copy()</span><br><span class="line">    method &#x3D; eval(meth)</span><br><span class="line">    # 应用模板匹配</span><br><span class="line">    res &#x3D; cv.matchTemplate(img,template,method)</span><br><span class="line">    min_val, max_val, min_loc, max_loc &#x3D; cv.minMaxLoc(res)</span><br><span class="line">    # 如果方法是TM_SQDIFF或TM_SQDIFF_NORMED，则取最小值</span><br><span class="line">    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:</span><br><span class="line">        top_left &#x3D; min_loc</span><br><span class="line">    else:</span><br><span class="line">        top_left &#x3D; max_loc</span><br><span class="line">    bottom_right &#x3D; (top_left[0] + w, top_left[1] + h)</span><br><span class="line">    cv.rectangle(img,top_left, bottom_right, 255, 2)</span><br><span class="line">    plt.subplot(121),plt.imshow(res,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Matching Result&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(122),plt.imshow(img,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Detected Point&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.suptitle(meth)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>查看以下结果：</p>
<ul>
<li>cv.TM_CCOEFF<img src="http://qiniu.aihubs.net/template_ccoeff_1.jpg" alt></li>
<li>cv.TM_CCOEFF_NORMED<img src="http://qiniu.aihubs.net/template_ccoeffn_2.jpg" alt></li>
<li>cv.TM_CCORR<img src="http://qiniu.aihubs.net/template_ccorr_3.jpg" alt></li>
<li>cv.TM_CCORR_NORMED<img src="http://qiniu.aihubs.net/template_ccorrn_4.jpg" alt></li>
<li>cv.TM_SQDIFF<img src="http://qiniu.aihubs.net/template_sqdiff_5.jpg" alt></li>
<li>cv.TM_SQDIFF_NORMED<img src="http://qiniu.aihubs.net/template_sqdiffn_6.jpg" alt><br>使用<strong>cv.TM_CCORR</strong>的结果并不理想。</li>
</ul>
<h2 id="多对象的模板匹配"><a href="#多对象的模板匹配" class="headerlink" title="多对象的模板匹配"></a>多对象的模板匹配</h2><p>在上一节中，我们在图像中搜索了梅西的脸，该脸在图像中仅出现一次。</p>
<p>假设您正在搜索具有多次出现的对象，则<strong>cv.minMaxLoc</strong>()不会为您提供所有位置。</p>
<p>在这种情况下，我们将使用阈值化。因此，在此示例中，我们将使用著名游戏<strong>Mario</strong>的屏幕截图，并在其中找到硬币。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img_rgb &#x3D; cv.imread(&#39;mario.png&#39;)</span><br><span class="line">img_gray &#x3D; cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)</span><br><span class="line">template &#x3D; cv.imread(&#39;mario_coin.png&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line">res &#x3D; cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)</span><br><span class="line">threshold &#x3D; 0.8</span><br><span class="line">loc &#x3D; np.where( res &gt;&#x3D; threshold)</span><br><span class="line">for pt in zip(*loc[::-1]):</span><br><span class="line">    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)</span><br><span class="line">cv.imwrite(&#39;res.png&#39;,img_rgb)</span><br></pre></td></tr></table></figure>
<p>结果:<br><img src="http://qiniu.aihubs.net/res_mario.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="霍夫线变换"><a href="#霍夫线变换" class="headerlink" title="霍夫线变换"></a><span id="header3">霍夫线变换</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在这一章当中， </p>
<ul>
<li>我们将了解霍夫变换的概念。 </li>
<li>我们将看到如何使用它来检测图像中的线条。 </li>
<li>我们将看到以下函数：cv.HoughLines()，cv.HoughLinesP()</li>
</ul>
<h2 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h2><p>如果可以用数学形式表示形状，则霍夫变换是一种检测任何形状的流行技术。</p>
<p>即使形状有些破损或变形，也可以检测出形状。我们将看到它如何作用于一条线。</p>
<p>一条线可以表示为y=mx+c或以参数形式表示为ρ=xcosθ+ysinθ，</p>
<p>其中ρ是从原点到该线的垂直距离，</p>
<p>而θ是由该垂直线和水平轴形成的角度以逆时针方向测量（该方向随您如何表示坐标系而变化。此表示形式在OpenCV中使用）。</p>
<p>查看下面的图片：<br><img src="http://qiniu.aihubs.net/1.png" alt></p>
<p>因此，如果线在原点下方通过，则它将具有正的ρ且角度小于180。如果线在原点上方，则将角度取为小于180，而不是大于180的角度。</p>
<p>ρ取负值。任何垂直线将具有0度，水平线将具有90度。</p>
<p>现在，让我们看一下霍夫变换如何处理线条。任何一条线都可以用(ρ，θ)这两个术语表示。</p>
<p>因此，首先创建2D数组或累加器（以保存两个参数的值），并将其初始设置为0。</p>
<p>让行表示ρ，列表示θ。阵列的大小取决于所需的精度。假设您希望角度的精度为1度，则需要180列。</p>
<p>对于ρ，最大距离可能是图像的对角线长度。因此，以一个像素精度为准，行数可以是图像的对角线长度。</p>
<p>考虑一个100x100的图像，中间有一条水平线。</p>
<p>取直线的第一点。您知道它的(x，y)值。</p>
<p>现在在线性方程式中，将值θ= 0,1,2，….. 180放进去，然后检查得到ρ。</p>
<p>对于每对(ρ，θ)，在累加器中对应的(ρ，θ)单元格将值增加1。所以现在在累加器中，单元格(50,90)= 1以及其他一些单元格。</p>
<p>现在，对行的第二个点。执行与上述相同的操作。递增(ρ，θ)对应的单元格中的值。</p>
<p>这次，单元格(50,90)=2。实际上，您正在对(ρ，θ)值进行投票。</p>
<p>您对线路上的每个点都继续执行此过程。</p>
<p>在每个点上，单元格(50,90)都会增加或投票，而其他单元格可能会或可能不会投票。</p>
<p>这样一来，最后，单元格(50,90)的投票数将最高。</p>
<p>因此，如果您在累加器中搜索最大票数，则将获得(50,90)值，该值表示该图像中的一条线与原点的距离为50，角度为90度。</p>
<p>在下面的动画中很好地显示了该图片(图片提供：Amos Storkey)<br><img src="http://qiniu.aihubs.net/houghlinesdemo.gif" alt></p>
<p>这就是霍夫变换对线条的工作方式。它很简单，也许您可​​以自己使用Numpy来实现它。</p>
<p>下图显示了累加器。某些位置的亮点表示它们是图像中可能的线条的参数。</p>
<p><img src="http://qiniu.aihubs.net/houghlines2.jpg" alt></p>
<h2 id="OpenCV中的霍夫曼变换"><a href="#OpenCV中的霍夫曼变换" class="headerlink" title="OpenCV中的霍夫曼变换"></a>OpenCV中的霍夫曼变换</h2><p>上面说明的所有内容都封装在OpenCV函数<strong>cv.HoughLines</strong>()中。</p>
<p>它只是返回一个：math:(rho，theta)值的数组。ρ以像素为单位，θ以弧度为单位。</p>
<p>第一个参数，输入图像应该是二进制图像，因此在应用霍夫变换之前，请应用阈值或使用Canny边缘检测。</p>
<p>第二和第三参数分别是ρ和θ精度。</p>
<p>第四个参数是阈值，这意味着应该将其视为行的最低投票。</p>
<p>请记住，票数取决于线上的点数。因此，它表示应检测到的最小线长。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLines(edges,1,np.pi&#x2F;180,200)</span><br><span class="line">for line in lines:</span><br><span class="line">    rho,theta &#x3D; line[0]</span><br><span class="line">    a &#x3D; np.cos(theta)</span><br><span class="line">    b &#x3D; np.sin(theta)</span><br><span class="line">    x0 &#x3D; a*rho</span><br><span class="line">    y0 &#x3D; b*rho</span><br><span class="line">    x1 &#x3D; int(x0 + 1000*(-b))</span><br><span class="line">    y1 &#x3D; int(y0 + 1000*(a))</span><br><span class="line">    x2 &#x3D; int(x0 - 1000*(-b))</span><br><span class="line">    y2 &#x3D; int(y0 - 1000*(a))</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,0,255),2)</span><br><span class="line">cv.imwrite(&#39;houghlines3.jpg&#39;,img)</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.aihubs.net/houghlines3.jpg" alt></p>
<h2 id="概率霍夫变换"><a href="#概率霍夫变换" class="headerlink" title="概率霍夫变换"></a>概率霍夫变换</h2><p>在霍夫变换中，您可以看到，即使对于带有两个参数的行，也需要大量计算。</p>
<p>概率霍夫变换是我们看到的霍夫变换的优化。它没有考虑所有要点。</p>
<p>取而代之的是，它仅采用随机的点子集，足以进行线检测。</p>
<p>只是我们必须降低阈值。参见下图，比较了霍夫空间中的霍夫变换和概率霍夫变换。<br><img src="http://qiniu.aihubs.net/houghlines4.png" alt></p>
<p>OpenCV的实现基于Matas,J.和Galambos,C.和Kittler, J.V.使用渐进概率霍夫变换对行进行的稳健检测[145]。</p>
<p>使用的函数是<strong>cv.HoughLinesP</strong>()。它有两个新的论点。 </p>
<ul>
<li>minLineLength - 最小行长。小于此长度的线段将被拒绝。 </li>
<li>maxLineGap - 线段之间允许将它们视为一条线的最大间隙。</li>
</ul>
<p>最好的是，它直接返回行的两个端点。在以前的情况下，您仅获得线的参数，并且必须找到所有点。在这里，一切都是直接而简单的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLinesP(edges,1,np.pi&#x2F;180,100,minLineLength&#x3D;100,maxLineGap&#x3D;10)</span><br><span class="line">for line in lines:</span><br><span class="line">    x1,y1,x2,y2 &#x3D; line[0]</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)</span><br><span class="line">cv.imwrite(&#39;houghlines5.jpg&#39;,img)</span><br></pre></td></tr></table></figure>
<p>看到如下结果：<br><img src="http://qiniu.aihubs.net/houghlines5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="霍夫圈变换"><a href="#霍夫圈变换" class="headerlink" title="霍夫圈变换"></a><span id="header4">霍夫圈变换</span></h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><p>在本章中， </p>
<ul>
<li>我们将学习使用霍夫变换来查找图像中的圆。 </li>
<li>我们将看到以下函数：cv.HoughCircles()</li>
</ul>
<p>理论<br>圆在数学上表示为$(x−x_{center})^2+(y−y_{center})^2=r^2$，其中$(x_{center},y_{center})$是圆的中心，r是圆的半径。从等式中，我们可以看到我们有3个参数，因此我们需要3D累加器进行霍夫变换，这将非常低效。因此，OpenCV使用更加技巧性的方法，即使用边缘的梯度信息的<strong>Hough梯度方法</strong>。</p>
<p>我们在这里使用的函数是<strong>cv.HoughCircles</strong>()。它有很多参数，这些参数在文档中有很好的解释。因此，我们直接转到代码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;opencv-logo-white.png&#39;,0)</span><br><span class="line">img &#x3D; cv.medianBlur(img,5)</span><br><span class="line">cimg &#x3D; cv.cvtColor(img,cv.COLOR_GRAY2BGR)</span><br><span class="line">circles &#x3D; cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20,</span><br><span class="line">                            param1&#x3D;50,param2&#x3D;30,minRadius&#x3D;0,maxRadius&#x3D;0)</span><br><span class="line">circles &#x3D; np.uint16(np.around(circles))</span><br><span class="line">for i in circles[0,:]:</span><br><span class="line">    # 绘制外圆</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2)</span><br><span class="line">    # 绘制圆心</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),2,(0,0,255),3)</span><br><span class="line">cv.imshow(&#39;detected circles&#39;,cimg)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/houghcircles2.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="图像分割与Watershed算法"><a href="#图像分割与Watershed算法" class="headerlink" title="图像分割与Watershed算法"></a><span id="header5">图像分割与Watershed算法</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>在本章中， - 我们将学习使用分水岭算法实现基于标记的图像分割 - 我们将看到：cv.watershed()</p>
<h2 id="理论-3"><a href="#理论-3" class="headerlink" title="理论"></a>理论</h2><p>任何灰度图像都可以看作是一个地形表面，其中高强度表示山峰，低强度表示山谷。</p>
<p>你开始用不同颜色的水(标签)填充每个孤立的山谷(局部最小值)。</p>
<p>随着水位的上升，根据附近的山峰(坡度)，来自不同山谷的水明显会开始合并，颜色也不同。</p>
<p>为了避免这种情况，你要在水融合的地方建造屏障。你继续填满水，建造障碍，直到所有的山峰都在水下。</p>
<p>然后你创建的屏障将返回你的分割结果。这就是Watershed背后的“思想”。</p>
<p>你可以访问Watershed的CMM网页，了解它与一些动画的帮助。</p>
<p>但是这种方法会由于图像中的噪声或其他不规则性而产生过度分割的结果。</p>
<p>因此OpenCV实现了一个基于标记的分水岭算法，你可以指定哪些是要合并的山谷点，哪些不是。</p>
<p>这是一个交互式的图像分割。我们所做的是给我们知道的对象赋予不同的标签。</p>
<p>用一种颜色(或强度)标记我们确定为前景或对象的区域，用另一种颜色标记我们确定为背景或非对象的区域，最后用0标记我们不确定的区域。</p>
<p>这是我们的标记。然后应用分水岭算法。然后我们的标记将使用我们给出的标签进行更新，对象的边界值将为-1。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>下面我们将看到一个有关如何使用距离变换和分水岭来分割相互接触的对象的示例。</p>
<p>考虑下面的硬币图像，硬币彼此接触。即使你设置阈值，它也会彼此接触。<br><img src="http://qiniu.aihubs.net/water_coins.jpg" alt></p>
<p>我们先从寻找硬币的近似估计开始。因此，我们可以使用Otsu的二值化。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;coins.png&#39;)</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh &#x3D; cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/water_thresh.jpg" alt></p>
<p>现在我们需要去除图像中的任何白点噪声。为此，我们可以使用形态学扩张。</p>
<p>要去除对象中的任何小孔，我们可以使用形态学侵蚀。因此，现在我们可以确定，靠近对象中心的区域是前景，而离对象中心很远的区域是背景。</p>
<p>我们不确定的唯一区域是硬币的边界区域。</p>
<p>因此，我们需要提取我们可确定为硬币的区域。侵蚀会去除边界像素。</p>
<p>因此，无论剩余多少，我们都可以肯定它是硬币。如果物体彼此不接触，那将起作用。</p>
<p>但是，由于它们彼此接触，因此另一个好选择是找到距离变换并应用适当的阈值。</p>
<p>接下来，我们需要找到我们确定它们不是硬币的区域。</p>
<p>为此，我们扩张了结果。膨胀将对象边界增加到背景。</p>
<p>这样，由于边界区域已删除，因此我们可以确保结果中背景中的任何区域实际上都是背景。参见下图。<br><img src="http://qiniu.aihubs.net/water_fgbg.jpg" alt></p>
<p>剩下的区域是我们不知道的区域，无论是硬币还是背景。分水岭算法应该找到它。</p>
<p>这些区域通常位于前景和背景相遇（甚至两个不同的硬币相遇）的硬币边界附近。我们称之为边界。可以通过从sure_bg区域中减去sure_fg区域来获得。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 噪声去除</span><br><span class="line">kernel &#x3D; np.ones((3,3),np.uint8)</span><br><span class="line">opening &#x3D; cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations &#x3D; 2)</span><br><span class="line"># 确定背景区域</span><br><span class="line">sure_bg &#x3D; cv.dilate(opening,kernel,iterations&#x3D;3)</span><br><span class="line"># 寻找前景区域</span><br><span class="line">dist_transform &#x3D; cv.distanceTransform(opening,cv.DIST_L2,5)</span><br><span class="line">ret, sure_fg &#x3D; cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)</span><br><span class="line"># 找到未知区域</span><br><span class="line">sure_fg &#x3D; np.uint8(sure_fg)</span><br><span class="line">unknown &#x3D; cv.subtract(sure_bg,sure_fg)</span><br></pre></td></tr></table></figure>
<p>查看结果。在阈值图像中，我们得到了一些硬币区域，我们确定它们是硬币，并且现在已分离它们。（在某些情况下，你可能只对前景分割感兴趣，而不对分离相互接触的对象感兴趣。在那种情况下，你无需使用距离变换，只需侵蚀就足够了。侵蚀只是提取确定前景区域的另一种方法。）</p>
<p><img src="http://qiniu.aihubs.net/water_dt.jpg" alt></p>
<p>现在我们可以确定哪些是硬币的区域，哪些是背景。</p>
<p>因此，我们创建了标记（它的大小与原始图像的大小相同，但具有int32数据类型），并标记其中的区域。</p>
<p>我们肯定知道的区域（无论是前景还是背景）都标有任何正整数，但是带有不同的整数，而我们不确定的区域则保留为零。</p>
<p>为此，我们使用<strong>cv.connectedComponents</strong>()。它用0标记图像的背景，然后其他对象用从1开始的整数标记。</p>
<p>但是我们知道，如果背景标记为0，则分水岭会将其视为未知区域。所以我们想用不同的整数来标记它。相反，我们将未知定义的未知区域标记为0。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 类别标记</span><br><span class="line">ret, markers &#x3D; cv.connectedComponents(sure_fg)</span><br><span class="line"># 为所有的标记加1，保证背景是0而不是1</span><br><span class="line">markers &#x3D; markers+1</span><br><span class="line"># 现在让所有的未知区域为0</span><br><span class="line">markers[unknown&#x3D;&#x3D;255] &#x3D; 0</span><br></pre></td></tr></table></figure>
<p>参见JET colormap中显示的结果。深蓝色区域显示未知区域。当然,硬币的颜色不同。剩下,肯定为背景的区域显示在较浅的蓝色，跟未知区域相比。<br><img src="http://qiniu.aihubs.net/water_marker.jpg" alt></p>
<p>现在我们的标记已准备就绪。现在是最后一步的时候了，使用分水岭算法。然后标记图像将被修改。边界区域将标记为-1。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">markers &#x3D; cv.watershed(img,markers) </span><br><span class="line">img[markers &#x3D;&#x3D; -1] &#x3D; [255,0,0]</span><br></pre></td></tr></table></figure>
<p>请参阅下面的结果。对某些硬币，它们接触的区域被正确地分割，而对于某些硬币，却不是。</p>
<p><img src="http://qiniu.aihubs.net/water_result.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="交互式前景提取使用GrabCut算法"><a href="#交互式前景提取使用GrabCut算法" class="headerlink" title="交互式前景提取使用GrabCut算法"></a><span id="header6">交互式前景提取使用GrabCut算法</span></h1><h2 id="目标-4"><a href="#目标-4" class="headerlink" title="目标"></a>目标</h2><p>在本章中， - 我们将看到GrabCut算法来提取图像中的前景 - 我们将为此创建一个交互式应用程序。</p>
<h2 id="理论-4"><a href="#理论-4" class="headerlink" title="理论"></a>理论</h2><p>GrabCut算法由英国微软研究院的Carsten Rother，Vladimir Kolmogorov和Andrew Blake设计。</p>
<p>在他们的论文“GrabCut”中：使用迭代图割的交互式前景提取。需要用最少的用户交互进行前景提取的算法，结果是GrabCut。</p>
<p>从用户角度来看，它是如何工作的？</p>
<p>最初，用户在前景区域周围绘制一个矩形（前景区域应完全位于矩形内部）。</p>
<p>然后，算法会对其进行迭代分割，以获得最佳结果。</p>
<p>做完了但在某些情况下，分割可能不会很好，例如，可能已将某些前景区域标记为背景，反之亦然。在这种情况下，需要用户进行精修。</p>
<p>只需在图像错误分割区域上画些笔画。笔画基本上说 “嘿，该区域应该是前景，你将其标记为背景，在下一次迭代中对其进行校正”或与背景相反。</p>
<p>然后在下一次迭代中，你将获得更好的结果。</p>
<p>参见下图。</p>
<p>第一名球员和橄榄球被封闭在一个蓝色矩形中。然后用白色笔划（表示前景）和黑色笔划（表示背景）进行最后的修饰。而且我们得到了不错的结果。</p>
<p><img src="http://qiniu.aihubs.net/grabcut_output1.jpg" alt></p>
<p>那么背景发生了什么呢？ </p>
<ul>
<li>用户输入矩形。此矩形外部的所有内容都将作为背景（这是在矩形应包含所有对象之前提到的原因）。矩形内的所有内容都是未知的。同样，任何指定前景和背景的用户输入都被视为硬标签，这意味着它们在此过程中不会更改。 </li>
<li>计算机根据我们提供的数据进行初始标记。它标记前景和背景像素（或对其进行硬标记），现在使用高斯混合模型(GMM)对前景和背景进行建模。 </li>
<li>根据我们提供的数据，GMM可以学习并创建新的像素分布。也就是说，未知像素根据颜色统计上与其他硬标记像素的关系而被标记为可能的前景或可能的背景（就像聚类一样）。 </li>
<li>根据此像素分布构建图形。图中的节点为像素。添加了另外两个节点，即“源”节点和“接收器”节点。每个前景像素都连接到源节点，每个背景像素都连接到接收器节点。 </li>
<li>通过像素是前景/背景的概率来定义将像素连接到源节点/末端节点的边缘的权重。像素之间的权重由边缘信息或像素相似度定义。如果像素颜色差异很大，则它们之间的边缘将变低。 </li>
<li>然后使用mincut算法对图进行分割。它将图切成具有最小成本函数的两个分离的源节点和宿节点。成本函数是被切割边缘的所有权重的总和。剪切后，连接到“源”节点的所有像素都变为前景，而连接到“接收器”节点的像素都变为背景。 </li>
<li>继续该过程，直到分类收敛为止。</li>
</ul>
<p>如下图所示（图片提供：<a href="http://www.cs.ru.ac.za/research/g02m1682/）" target="_blank" rel="noopener">http://www.cs.ru.ac.za/research/g02m1682/）</a><br><img src="http://qiniu.aihubs.net/grabcut_scheme.jpg" alt></p>
<p>示例<br>现在我们使用OpenCV进行抓取算法。OpenCV为此具有功能<strong>cv.grabCut</strong>()，我们将首先看到其参数： </p>
<ul>
<li>img - 输入图像 </li>
<li>mask - 这是一个掩码图像，在其中我们指定哪些区域是背景，前景或可能的背景/前景等。这是通过以下标志完成的：cv.GC_BGD,cv.GC_FGD, cv.GC_PR_BGD,cv.GC_PR_FGD，或直接将0,1,2,3传递给图像。 </li>
<li>rect - 它是矩形的坐标，其中包括前景对象，格式为(x,y,w,h) - bdgModel, fgdModel - 这些是算法内部使用的数组。你只需创建两个大小为(1,65)的np.float64类型零数组。 </li>
<li>iterCount - 算法应运行的迭代次数。 </li>
<li>model - 应该是<strong>cv.GC_INIT_WITH_RECT</strong>或<strong>cv.GC_INIT_WITH_MASK</strong>或两者结合，决定我们要绘制矩形还是最终的修饰笔触。</li>
</ul>
<p>首先让我们看看矩形模式。我们加载图像，创建类似的mask图像。 </p>
<p>我们创建<em>fgdModel</em>和<em>bgdModel</em>。我们给出矩形参数。一切都是直截了当的。</p>
<p>让算法运行5次迭代。模式应为<strong>cv.GC_INIT_WITH_RECT</strong>, 因为我们使用的是矩形。 </p>
<p>然后运行grabcut。修改mask图像。在新的mask图像中，像素将被标记有四个标记，分别表示上面指定的背景/前景。</p>
<p>因此，我们修改mask，使所有0像素和2像素都置为0（即背景），而所有1像素和3像素均置为1（即前景像素）。</p>
<p>现在，我们的最终mask已经准备就绪。只需将其与输入图像相乘即可得到分割的图像。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;)</span><br><span class="line">mask &#x3D; np.zeros(img.shape[:2],np.uint8)</span><br><span class="line">bgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">fgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">rect &#x3D; (50,50,450,290)</span><br><span class="line">cv.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv.GC_INIT_WITH_RECT)</span><br><span class="line">mask2 &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask2[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/grabcut_rect.jpg" alt></p>
<p>糟糕，梅西的头发不见了。谁会喜欢没有头发的梅西？我们需要把它找回来。</p>
<p>因此，我们将使用1像素（确保前景）进行精细修饰。</p>
<p>同时，一些不需要的地面也出现在图片里。我们需要删除它们。</p>
<p>在那里，我们给出了一些0像素的修饰（确保背景）。</p>
<p>因此，如现在所说，我们在以前的情况下修改生成的mask。</p>
<p>我实际上所做的是，我在paint应用程序中打开了输入图像，并在图像中添加了另一层。</p>
<p>使用画笔中的画笔工具，我在新图层上用白色标记了错过的前景（头发，鞋子，球等），而用白色标记了不需要的背景（例如logo，地面等）。</p>
<p>然后用灰色填充剩余的背景。</p>
<p>然后将该mask图像加载到OpenCV中，编辑我们在新添加的mask图像中具有相应值的原始mask图像。</p>
<p>检查以下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">＃newmask是我手动标记过的mask图像</span><br><span class="line">newmask &#x3D; cv.imread(&#39;newmask.png&#39;,0)</span><br><span class="line"># 标记为白色（确保前景）的地方，更改mask &#x3D; 1</span><br><span class="line"># 标记为黑色（确保背景）的地方，更改mask &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 0] &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 255] &#x3D; 1</span><br><span class="line">mask, bgdModel, fgdModel &#x3D; cv.grabCut(img,mask,None,bgdModel,fgdModel,5,cv.GC_INIT_WITH_MASK)</span><br><span class="line">mask &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure>
<p>就是这样了。在这里，你无需直接在rect模式下初始化，而可以直接进入mask模式。</p>
<p>只需用2像素或3像素（可能的背景/前景）标记mask图像中的矩形区域。</p>
<p>然后像在第二个示例中一样，将我们的sure_foreground标记为1像素。然后直接在mask模式下应用grabCut功能。<br><img src="http://qiniu.aihubs.net/grabcut_mask.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战-泰坦尼克号获救预测</title>
    <url>/2020/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B/</url>
    <content><![CDATA[<p>数据集:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/titanic_train.csv" target="_blank" rel="noopener">titanic_train.csv</a></p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">titianic = pandas.read_csv(<span class="string">'../data/titanic_train.csv'</span>)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[<span class="string">'Age'</span>]=titianic[<span class="string">'Age'</span>].fillna(titianic[<span class="string">'Age'</span>].median())</span><br><span class="line">print(titianic.describe())</span><br></pre></td></tr></table></figure>

<pre><code>       PassengerId    Survived      Pclass         Age       SibSp  \
count   891.000000  891.000000  891.000000  891.000000  891.000000   
mean    446.000000    0.383838    2.308642   29.361582    0.523008   
std     257.353842    0.486592    0.836071   13.019697    1.102743   
min       1.000000    0.000000    1.000000    0.420000    0.000000   
25%     223.500000    0.000000    2.000000   22.000000    0.000000   
50%     446.000000    0.000000    3.000000   28.000000    0.000000   
75%     668.500000    1.000000    3.000000   35.000000    1.000000   
max     891.000000    1.000000    3.000000   80.000000    8.000000   

            Parch        Fare  
count  891.000000  891.000000  
mean     0.381594   32.204208  
std      0.806057   49.693429  
min      0.000000    0.000000  
25%      0.000000    7.910400  
50%      0.000000   14.454200  
75%      0.000000   31.000000  
max      6.000000  512.329200  </code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(titianic[<span class="string">'Sex'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Sex'</span>] = titianic[<span class="string">'Sex'</span>].map(&#123;<span class="string">'male'</span>:<span class="number">0</span>,<span class="string">'female'</span>:<span class="number">1</span>&#125;)</span><br><span class="line"><span class="comment"># 把male变成0，把female变成1</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "male", "Sex"] = 0</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "female", "Sex"] = 1</span></span><br></pre></td></tr></table></figure>

<pre><code>[&apos;male&apos; &apos;female&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(titianic[<span class="string">'Embarked'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>)</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].map(&#123;<span class="string">'S'</span>:<span class="number">0</span>,<span class="string">'C'</span>:<span class="number">1</span>,<span class="string">'Q'</span>:<span class="number">2</span>&#125;)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;S&apos; &apos;C&apos; &apos;Q&apos; nan]</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">predictors = [<span class="string">'Pclass'</span>,<span class="string">'Sex'</span>,<span class="string">'Age'</span>,<span class="string">'SibSp'</span>,<span class="string">'Parch'</span>,<span class="string">'Fare'</span>,<span class="string">'Embarked'</span>]</span><br><span class="line">x_data = titianic[predictors]</span><br><span class="line">y_data = titianic[<span class="string">'Survived'</span>]</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">x_data = scaler.fit_transform(x_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">LR = LogisticRegression()</span><br><span class="line">scores = model_selection.cross_val_score(LR,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.7901234567901234</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">20</span>,<span class="number">10</span>),max_iter=<span class="number">1000</span>)</span><br><span class="line">scores = model_selection.cross_val_score(mlp,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.8002244668911335</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line">knn = neighbors.KNeighborsClassifier(<span class="number">21</span>)</span><br><span class="line">scores=model_selection.cross_val_score(knn,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8125701459034792</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">dtree = tree.DecisionTreeClassifier(max_depth=<span class="number">5</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(dtree,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8080808080808081</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">RF1 = RandomForestClassifier(random_state=<span class="number">1</span>,n_estimators=<span class="number">10</span>,min_samples_split=<span class="number">2</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF1,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.7991021324354657</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RF2 = RandomForestClassifier(n_estimators=<span class="number">100</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF2,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8125701459034792</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line">bagging_clf = BaggingClassifier(RF2,n_estimators=<span class="number">20</span>)</span><br><span class="line">scores=model_selection.cross_val_score(bagging_clf,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.819304152637486</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">adaboost = AdaBoostClassifier(bagging_clf,n_estimators=<span class="number">10</span>)</span><br><span class="line">scores=model_selection.cross_val_score(adaboost,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8181818181818182</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line">sclf = StackingClassifier(classifiers=[bagging_clf,mlp,LR],</span><br><span class="line">                         meta_classifier=LogisticRegression())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.819304152637486</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sclf2 = VotingClassifier([(<span class="string">'adaboost'</span>,adaboost),(<span class="string">'mlp'</span>,mlp),(<span class="string">'LR'</span>,LR),(<span class="string">'knn'</span>,knn),(<span class="string">'dtree'</span>,dtree)])</span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf2,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.8159371492704826</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-cifar10tutorial-visualizing</title>
    <url>/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/</url>
    <content><![CDATA[<p>Pytorch-Learning-cifar10tutorial-visualizing</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="Training-a-Classifier"><a href="#Training-a-Classifier" class="headerlink" title="Training a Classifier"></a>Training a Classifier</h1><p>This is it. You have seen how to define neural networks, compute loss and make<br>updates to the weights of the network.</p>
<p>Now you might be thinking,</p>
<h2 id="What-about-data"><a href="#What-about-data" class="headerlink" title="What about data?"></a>What about data?</h2><p>Generally, when you have to deal with image, text, audio or video data,<br>you can use standard python packages that load data into a numpy array.<br>Then you can convert this array into a <code>torch.*Tensor</code>.</p>
<ul>
<li>For images, packages such as Pillow, OpenCV are useful</li>
<li>For audio, packages such as scipy and librosa</li>
<li>For text, either raw Python or Cython based loading, or NLTK and<br>SpaCy are useful</li>
</ul>
<p>Specifically for vision, we have created a package called<br><code>torchvision</code>, that has data loaders for common datasets such as<br>Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,<br><code>torchvision.datasets</code> and <code>torch.utils.data.DataLoader</code>.</p>
<p>This provides a huge convenience and avoids writing boilerplate code.</p>
<p>For this tutorial, we will use the CIFAR10 dataset.<br>It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,<br>‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of<br>size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.</p>
<p><img src="https://pytorch.org/tutorials/_images/cifar10.png" alt></p>
<p>   cifar10</p>
<h2 id="Training-an-image-classifier"><a href="#Training-an-image-classifier" class="headerlink" title="Training an image classifier"></a>Training an image classifier</h2><p>We will do the following steps in order:</p>
<ol>
<li><p>Load and normalizing the CIFAR10 training and test datasets using<br><code>torchvision</code></p>
</li>
<li><p>Define a Convolutional Neural Network</p>
</li>
<li><p>Define a loss function</p>
</li>
<li><p>Train the network on the training data</p>
</li>
<li><p>Test the network on the test data</p>
</li>
<li><p>Loading and normalizing CIFAR10</p>
</li>
</ol>
<hr>
<p>Using <code>torchvision</code>, it’s extremely easy to load CIFAR10.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>

<p>The output of torchvision datasets are PILImage images of range [0, 1].<br>We transform them to Tensors of normalized range [-1, 1].</p>
<div class="alert alert-info"><h4>Note</h4><p>If running on Windows and you get a BrokenPipeError, try setting
    the num_worker of torch.utils.data.DataLoader() to 0.</p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Files already downloaded and verified
Files already downloaded and verified</code></pre><p>Let us show some of the training images, for fun.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_6_0.png" alt="png"></p>
<pre><code>horse   cat  deer   cat</code></pre><ol start="2">
<li>Define a Convolutional Neural Network</li>
</ol>
<hr>
<p>Copy the neural network from the Neural Networks section before and modify it to<br>take 3-channel images (instead of 1-channel images as it was defined).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Define a Loss function and optimizer</li>
</ol>
<hr>
<p>Let’s use a Classification Cross-Entropy loss and SGD with momentum.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>Train the network</li>
</ol>
<hr>
<p>This is when things start to get interesting.<br>We simply have to loop over our data iterator, and feed the inputs to the<br>network and optimize.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[1,  2000] loss: 2.173
[1,  4000] loss: 1.818
[1,  6000] loss: 1.647
[1,  8000] loss: 1.545
[1, 10000] loss: 1.490
[1, 12000] loss: 1.436
[2,  2000] loss: 1.384
[2,  4000] loss: 1.348
[2,  6000] loss: 1.341
[2,  8000] loss: 1.306
[2, 10000] loss: 1.292
[2, 12000] loss: 1.283
Finished Training</code></pre><p>Let’s quickly save our trained model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">'./cifar_net.pth'</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<p>See <code>here &lt;https://pytorch.org/docs/stable/notes/serialization.html&gt;</code>_<br>for more details on saving PyTorch models.</p>
<ol start="5">
<li>Test the network on the test data</li>
</ol>
<hr>
<p>We have trained the network for 2 passes over the training dataset.<br>But we need to check if the network has learnt anything at all.</p>
<p>We will check this by predicting the class label that the neural network<br>outputs, and checking it against the ground-truth. If the prediction is<br>correct, we add the sample to the list of correct predictions.</p>
<p>Okay, first step. Let us display an image from the test set to get familiar.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_16_0.png" alt="png"></p>
<pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p>Next, let’s load back in our saved model (note: saving and re-loading the model<br>wasn’t necessary here, we only did it to illustrate how to do so):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>




<pre><code>IncompatibleKeys(missing_keys=[], unexpected_keys=[])</code></pre><p>Okay, now let us see what the neural network thinks these examples above are:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure>

<p>The outputs are energies for the 10 classes.<br>The higher the energy for a class, the more the network<br>thinks that the image is of the particular class.<br>So, let’s get the index of the highest energy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>Predicted:    cat plane plane plane</code></pre><p>The results seem pretty good.</p>
<p>Let us look at how the network performs on the whole dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of the network on the 10000 test images: 53 %</code></pre><p>That looks way better than chance, which is 10% accuracy (randomly picking<br>a class out of 10 classes).<br>Seems like the network learnt something.</p>
<p>Hmmm, what are the classes that performed well, and the classes that did<br>not perform well:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of plane : 71 %
Accuracy of   car : 57 %
Accuracy of  bird : 26 %
Accuracy of   cat : 32 %
Accuracy of  deer : 52 %
Accuracy of   dog : 40 %
Accuracy of  frog : 72 %
Accuracy of horse : 74 %
Accuracy of  ship : 57 %
Accuracy of truck : 53 %</code></pre><p>Okay, so what next?</p>
<p>How do we run these neural networks on the GPU?</p>
<h2 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h2><p>Just like how you transfer a Tensor onto the GPU, you transfer the neural<br>net onto the GPU.</p>
<p>Let’s first define our device as the first visible cuda device if we have<br>CUDA available:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<pre><code>cpu</code></pre><p>The rest of this section assumes that <code>device</code> is a CUDA device.</p>
<p>Then these methods will recursively go over all modules and convert their<br>parameters and buffers to CUDA tensors:</p>
<p>.. code:: python</p>
<pre><code>net.to(device)</code></pre><p>Remember that you will have to send the inputs and targets at every step<br>to the GPU too:</p>
<p>.. code:: python</p>
<pre><code>inputs, labels = data[0].to(device), data[1].to(device)</code></pre><p>Why dont I notice MASSIVE speedup compared to CPU? Because your network<br>is really small.</p>
<p><strong>Exercise:</strong> Try increasing the width of your network (argument 2 of<br>the first <code>nn.Conv2d</code>, and argument 1 of the second <code>nn.Conv2d</code> –<br>they need to be the same number), see what kind of speedup you get.</p>
<p><strong>Goals achieved</strong>:</p>
<ul>
<li>Understanding PyTorch’s Tensor library and neural networks at a high level.</li>
<li>Train a small neural network to classify images</li>
</ul>
<h2 id="Training-on-multiple-GPUs"><a href="#Training-on-multiple-GPUs" class="headerlink" title="Training on multiple GPUs"></a>Training on multiple GPUs</h2><p>If you want to see even more MASSIVE speedup using all of your GPUs,<br>please check out :doc:<code>data_parallel_tutorial</code>.</p>
<h2 id="Where-do-I-go-next"><a href="#Where-do-I-go-next" class="headerlink" title="Where do I go next?"></a>Where do I go next?</h2><ul>
<li>:doc:<code>Train neural nets to play video games &lt;/intermediate/reinforcement_q_learning&gt;</code></li>
<li><code>Train a state-of-the-art ResNet network on imagenet</code>_</li>
<li><code>Train a face generator using Generative Adversarial Networks</code>_</li>
<li><code>Train a word-level language model using Recurrent LSTM networks</code>_</li>
<li><code>More examples</code>_</li>
<li><code>More tutorials</code>_</li>
<li><code>Discuss PyTorch on the Forums</code>_</li>
<li><code>Chat with other users on Slack</code>_</li>
</ul>
<h2 id="VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD"><a href="#VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD" class="headerlink" title="VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD"></a>VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD</h2><p>In the 60 Minute Blitz, we show you how to load in data, feed it through a model we define as a subclass of nn.Module, train this model on training data, and test it on test data. To see what’s happening, we print out some statistics as the model is training to get a sense for whether training is progressing. However, we can do much better than that: PyTorch integrates with TensorBoard, a tool designed for visualizing the results of neural network training runs. This tutorial illustrates some of its functionality, using the Fashion-MNIST dataset which can be read into PyTorch using torchvision.datasets.</p>
<p>In this tutorial, we’ll learn how to:</p>
<ul>
<li>Read in data and with appropriate transforms (nearly identical to the prior tutorial).</li>
<li>Set up TensorBoard.</li>
<li>Write to TensorBoard.</li>
<li>Inspect a model architecture using TensorBoard.</li>
</ul>
<p>Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code<br>Specifically, on point #5, we’ll see:</p>
<ul>
<li>A couple of ways to inspect our training data</li>
<li>How to track our model’s performance as it trains</li>
<li>How to assess our model’s performance once it is trained.</li>
<li>We’ll begin with similar boilerplate code as in the CIFAR-10 tutorial:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># transforms</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># datasets</span></span><br><span class="line">trainset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line">testset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataloaders</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># constant for classes</span></span><br><span class="line">classes = (<span class="string">'T-shirt/top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,</span><br><span class="line">        <span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle Boot'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function to show an image</span></span><br><span class="line"><span class="comment"># (used in the `plot_classes_preds` function below)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matplotlib_imshow</span><span class="params">(img, one_channel=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        img = img.mean(dim=<span class="number">0</span>)</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        plt.imshow(npimg, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\train-images-idx3-ubyte.gz


100.0%

Extracting ./data\FashionMNIST\raw\train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gz


111.0%

Extracting ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz


100.0%

Extracting ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz


159.1%

Extracting ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz
Processing...
Done!</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. TensorBoard setup</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">'runs/fashion_mnist_experiment_1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Writing to TensorBoard</span></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">matplotlib_imshow(img_grid, one_channel=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># write to tensorboard</span></span><br><span class="line">writer.add_image(<span class="string">'four_fashion_mnist_images'</span>, img_grid)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Now running: tensorboard --logdir=runs</span></span><br><span class="line"><span class="comment"># from the command line and then navigating to https://localhost:6006</span></span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_32_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3. Inspect the model using TensorBoard</span></span><br><span class="line">writer.add_graph(net, images)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4. Adding a “Projector” to TensorBoard</span></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_n_random</span><span class="params">(data, labels, n=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Selects n random datapoints and their corresponding labels from a dataset</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> len(data) == len(labels)</span><br><span class="line"></span><br><span class="line">    perm = torch.randperm(len(data))</span><br><span class="line">    <span class="keyword">return</span> data[perm][:n], labels[perm][:n]</span><br><span class="line"></span><br><span class="line"><span class="comment"># select random images and their target indices</span></span><br><span class="line">images, labels = select_n_random(trainset.data, trainset.targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the class labels for each image</span></span><br><span class="line">class_labels = [classes[lab] <span class="keyword">for</span> lab <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="comment"># log embeddings</span></span><br><span class="line">features = images.view(<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>)</span><br><span class="line">writer.add_embedding(features,</span><br><span class="line">                    metadata=class_labels,</span><br><span class="line">                    label_img=images.unsqueeze(<span class="number">1</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5. Tracking model training with TensorBoard</span></span><br><span class="line"><span class="comment"># helper functions</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">images_to_probs</span><span class="params">(net, images)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates predictions and corresponding probabilities from a trained</span></span><br><span class="line"><span class="string">    network and a list of images</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    output = net(images)</span><br><span class="line">    <span class="comment"># convert output probabilities to predicted class</span></span><br><span class="line">    _, preds_tensor = torch.max(output, <span class="number">1</span>)</span><br><span class="line">    preds = np.squeeze(preds_tensor.numpy())</span><br><span class="line">    <span class="keyword">return</span> preds, [F.softmax(el, dim=<span class="number">0</span>)[i].item() <span class="keyword">for</span> i, el <span class="keyword">in</span> zip(preds, output)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_classes_preds</span><span class="params">(net, images, labels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates matplotlib Figure using a trained network, along with images</span></span><br><span class="line"><span class="string">    and labels from a batch, that shows the network's top prediction along</span></span><br><span class="line"><span class="string">    with its probability, alongside the actual label, coloring this</span></span><br><span class="line"><span class="string">    information based on whether the prediction was correct or not.</span></span><br><span class="line"><span class="string">    Uses the "images_to_probs" function.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    preds, probs = images_to_probs(net, images)</span><br><span class="line">    <span class="comment"># plot the images in the batch, along with predicted and true labels</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">48</span>))</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> np.arange(<span class="number">4</span>):</span><br><span class="line">        ax = fig.add_subplot(<span class="number">1</span>, <span class="number">4</span>, idx+<span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        matplotlib_imshow(images[idx], one_channel=<span class="literal">True</span>)</span><br><span class="line">        ax.set_title(<span class="string">"&#123;0&#125;, &#123;1:.1f&#125;%\n(label: &#123;2&#125;)"</span>.format(</span><br><span class="line">            classes[preds[idx]],</span><br><span class="line">            probs[idx] * <span class="number">100.0</span>,</span><br><span class="line">            classes[labels[idx]]),</span><br><span class="line">                    color=(<span class="string">"green"</span> <span class="keyword">if</span> preds[idx]==labels[idx].item() <span class="keyword">else</span> <span class="string">"red"</span>))</span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line">running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">999</span>:    <span class="comment"># every 1000 mini-batches...</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log the running loss</span></span><br><span class="line">            writer.add_scalar(<span class="string">'training loss'</span>,</span><br><span class="line">                            running_loss / <span class="number">1000</span>,</span><br><span class="line">                            epoch * len(trainloader) + i)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log a Matplotlib Figure showing the model's predictions on a</span></span><br><span class="line">            <span class="comment"># random mini-batch</span></span><br><span class="line">            writer.add_figure(<span class="string">'predictions vs. actuals'</span>,</span><br><span class="line">                            plot_classes_preds(net, inputs, labels),</span><br><span class="line">                            global_step=epoch * len(trainloader) + i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Finished Training</code></pre><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_1.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_2.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_3.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_4.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_5.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_6.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_7.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_8.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_9.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_10.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_11.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_12.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_13.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_14.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_15.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 6. Assessing trained models with TensorBoard</span></span><br><span class="line"><span class="comment"># 1. gets the probability predictions in a test_size x num_classes Tensor</span></span><br><span class="line"><span class="comment"># 2. gets the preds in a test_size Tensor</span></span><br><span class="line"><span class="comment"># takes ~10 seconds to run</span></span><br><span class="line">class_probs = []</span><br><span class="line">class_preds = []</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        output = net(images)</span><br><span class="line">        class_probs_batch = [F.softmax(el, dim=<span class="number">0</span>) <span class="keyword">for</span> el <span class="keyword">in</span> output]</span><br><span class="line">        _, class_preds_batch = torch.max(output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        class_probs.append(class_probs_batch)</span><br><span class="line">        class_preds.append(class_preds_batch)</span><br><span class="line"></span><br><span class="line">test_probs = torch.cat([torch.stack(batch) <span class="keyword">for</span> batch <span class="keyword">in</span> class_probs])</span><br><span class="line">test_preds = torch.cat(class_preds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_pr_curve_tensorboard</span><span class="params">(class_index, test_probs, test_preds, global_step=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Takes in a "class_index" from 0 to 9 and plots the corresponding</span></span><br><span class="line"><span class="string">    precision-recall curve</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tensorboard_preds = test_preds == class_index</span><br><span class="line">    tensorboard_probs = test_probs[:, class_index]</span><br><span class="line"></span><br><span class="line">    writer.add_pr_curve(classes[class_index],</span><br><span class="line">                        tensorboard_preds,</span><br><span class="line">                        tensorboard_probs,</span><br><span class="line">                        global_step=global_step)</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot all the pr curves</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">    add_pr_curve_tensorboard(i, test_probs, test_preds)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-examples</title>
    <url>/2020/07/23/Pytorch-Learning-examples/</url>
    <content><![CDATA[<p>Pytorch-Learning-examples</p>
<a id="more"></a>

<ul>
<li>1.<a href="#header1">warm-up:numpy</a></li>
<li>2.<a href="#header2">pytorch:tensors</a></li>
<li>3.<a href="#header3">pytorch:tensors and autograd</a></li>
<li>4.<a href="#header4">pytorch:defining new autograd functions</a></li>
<li>5.<a href="#header5">tensorflow1.x:static graphs</a></li>
<li>6.<a href="#header6">pytorch:nn</a></li>
<li>7.<a href="#header7">pytorch:optim</a></li>
<li>8.<a href="#header8">pytorch:custom nn modules</a></li>
<li>9.<a href="#header9">pytorch:control flow+weight sharing</a></li>
</ul>
<h1 id="warm-up-numpy"><a href="#warm-up-numpy" class="headerlink" title="warm-up:numpy"></a><span id="header1">warm-up:numpy</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="Warm-up-numpy"><a href="#Warm-up-numpy" class="headerlink" title="Warm-up: numpy"></a>Warm-up: numpy</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x using Euclidean error.</p>
<p>This implementation uses numpy to manually compute the forward pass, loss, and<br>backward pass.</p>
<p>A numpy array is a generic n-dimensional array; it does not know anything about<br>deep learning or gradients or computational graphs, and is just a way to perform<br>generic numeric computations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<pre><code>0 38502658.12178467
1 39021030.505150035
2 43783005.35946928
3 43513345.95754772
4 33130647.574302156
5 18494184.582708906
6 8309147.312092974
7 3709728.9004302337
8 1973222.0064019447
9 1292160.3020421679
10 969207.6696523366
11 777313.2166244711
12 642885.4458057204
13 539862.0964943856
14 457706.36910393136
15 390776.99980368273
16 335541.3660443882
17 289534.9698406697
18 250999.0891175047
19 218440.8922160021
20 190801.03505385082
21 167229.41111619392
22 147030.14995170798
23 129644.55669752343
24 114634.85573944401
25 101654.50910880938
26 90379.66910158034
27 80547.50420605141
28 71942.30234259031
29 64389.53897462465
30 57755.011608667795
31 51903.61478445082
32 46747.37821866218
33 42180.36670548356
34 38120.10143244453
35 34504.8367862085
36 31283.625825762603
37 28409.554826229883
38 25832.84410659271
39 23520.944901953713
40 21443.92872486158
41 19573.49619579054
42 17887.52961185431
43 16364.901917158142
44 14986.999074876061
45 13739.21782497934
46 12607.392910105698
47 11579.322653387639
48 10644.061828317645
49 9792.956168204091
50 9016.956906200578
51 8309.619387713636
52 7664.013985165039
53 7073.449800791166
54 6532.833920817042
55 6037.538464055722
56 5583.274822316345
57 5166.3150372957025
58 4783.438966887356
59 4431.265118218456
60 4107.306250671735
61 3809.173244284424
62 3534.3065526324035
63 3281.382487196609
64 3048.2720327779725
65 2832.9613638905826
66 2634.0552486543734
67 2450.1208680788013
68 2280.0038612106828
69 2122.5020348125718
70 1976.6140763769765
71 1841.480282193922
72 1716.1436144355846
73 1599.8498210515127
74 1491.925103606006
75 1391.74788163408
76 1298.700821661702
77 1212.212450782299
78 1131.7942696970626
79 1057.0430043302988
80 987.4925199287406
81 922.7460298895005
82 862.4637455820518
83 806.335290707932
84 754.0347890268129
85 705.3032590197868
86 659.8640583682109
87 617.4715043492135
88 577.9369320777199
89 541.0382711590596
90 506.62139688224494
91 474.46146460235906
92 444.4289850228761
93 416.3831714509241
94 390.1781262414818
95 365.68416156867966
96 342.7828243058609
97 321.37451033250693
98 301.34344345528007
99 282.6049616675032
100 265.0769405906856
101 248.66706624922423
102 233.31182982470165
103 218.93579583019107
104 205.47399586723316
105 192.8673639685237
106 181.05784775853311
107 169.9914161272534
108 159.62205201703807
109 149.90414548125696
110 140.7986893389537
111 132.2586907147042
112 124.25279696970381
113 116.74222194734885
114 109.70210950592357
115 103.09497001058955
116 96.89525543441061
117 91.07884065063246
118 85.6198765958778
119 80.49732520749234
120 75.6882961733577
121 71.1722154368565
122 66.93235677110607
123 62.95241596773887
124 59.21332801738676
125 55.70220136122839
126 52.40276978766286
127 49.30324491666052
128 46.39219465065453
129 43.6561631420701
130 41.08445097890545
131 38.66705716574056
132 36.39569090451589
133 34.2596896820596
134 32.25129366141559
135 30.36277332849072
136 28.58840749936592
137 26.91886404849481
138 25.348295684128104
139 23.871887568407733
140 22.482392230639924
141 21.1752612429013
142 19.94540814626695
143 18.78848359075504
144 17.699455611505552
145 16.674867891142245
146 15.710568962031482
147 14.802825886952098
148 13.94842074029503
149 13.144227723348116
150 12.387131550628439
151 11.674026481428772
152 11.002689146006661
153 10.370609949309603
154 9.775509369377593
155 9.215072226656517
156 8.687075426828947
157 8.189767782525552
158 7.7214576865624025
159 7.280233710481657
160 6.864642688917494
161 6.473008103216368
162 6.104105173833666
163 5.7564916113754085
164 5.428879681554832
165 5.120147993533515
166 4.829259142691539
167 4.555145740046442
168 4.29672213329475
169 4.053143156190888
170 3.8235214202376655
171 3.607138244094545
172 3.4031196927940224
173 3.210757574413539
174 3.0293765387034064
175 2.858454621478234
176 2.697234384156207
177 2.545177266862872
178 2.4017939117722404
179 2.2665997813870664
180 2.1391195167593566
181 2.0188693271562754
182 1.9054496369340477
183 1.7984763417415828
184 1.6975705115269373
185 1.6023729047397333
186 1.512573512380922
187 1.4278650471502181
188 1.347969970837449
189 1.272567002922919
190 1.2014204465432794
191 1.1342855863955332
192 1.0709568915888343
193 1.0111820179227746
194 0.9547746744541861
195 0.9015532749111042
196 0.8513302278674784
197 0.803922014568537
198 0.7591740066091374
199 0.7169375303267338
200 0.6770877114086349
201 0.6394607979618921
202 0.6039480138637082
203 0.5704178896870162
204 0.5387711941311117
205 0.5088920505641997
206 0.4806800135074349
207 0.45404935956164216
208 0.4289117734184146
209 0.40517486327826213
210 0.38275697069924214
211 0.36158944936278636
212 0.3416070794747825
213 0.3227344916114124
214 0.3049141167911339
215 0.28808300182326085
216 0.27219060149023244
217 0.2571773488343734
218 0.24299758077475503
219 0.22960605968706874
220 0.21696181922245522
221 0.20501866875786806
222 0.19373590402638502
223 0.1830764093023291
224 0.17300985254584955
225 0.1635004063397864
226 0.15451596252804456
227 0.14602859536330753
228 0.13801270444499963
229 0.1304395543791902
230 0.12328307118542486
231 0.1165215280650118
232 0.11013498048221826
233 0.10409915585381448
234 0.09839683048227854
235 0.09300849009893092
236 0.08791893479291592
237 0.08310796594355277
238 0.07856156604359649
239 0.07426508307950577
240 0.07020565731703925
241 0.06636946700613897
242 0.0627437660794708
243 0.059317781060088316
244 0.05608034093448906
245 0.05301955241121613
246 0.050126674069563906
247 0.04739256213401111
248 0.04480928906259292
249 0.04236691748155608
250 0.04005879599601969
251 0.03787661761938381
252 0.03581475257492655
253 0.033864928740949526
254 0.032021804201541854
255 0.030279858100986354
256 0.02863334928312954
257 0.02707658789186648
258 0.025604569332906155
259 0.024213249395320283
260 0.022897970661672578
261 0.02165425187746097
262 0.020478737432682297
263 0.01936719332160696
264 0.018316430865928372
265 0.017322619307502316
266 0.016383050617920083
267 0.015494734481326376
268 0.014655032299327592
269 0.013860819190983916
270 0.013109809002567857
271 0.0123997755042141
272 0.011728358290906498
273 0.011093355906296416
274 0.01049298268330884
275 0.009925290535664889
276 0.009388360895355642
277 0.008880549450207519
278 0.008400420664047465
279 0.007946325800609208
280 0.00751686776061394
281 0.007110688095051999
282 0.006726592064373132
283 0.00636331544306615
284 0.006019761179348237
285 0.005694800059913335
286 0.005387424438387055
287 0.005096730276146309
288 0.004821747870498604
289 0.004561741644547909
290 0.004315728687198646
291 0.004083088241466739
292 0.0038630342216639736
293 0.0036548280743543514
294 0.003457908894412565
295 0.0032716544897889873
296 0.003095480430783593
297 0.0029287903501173454
298 0.0027711283850350016
299 0.002622002726801867
300 0.0024808885834647024
301 0.0023473968087844807
302 0.00222113284310873
303 0.00210170542574699
304 0.0019887074497667136
305 0.0018817802500325769
306 0.00178065116010868
307 0.0016849552143364992
308 0.0015944091801416442
309 0.001508753706017779
310 0.0014277397746070208
311 0.0013510695821452332
312 0.0012785229777043136
313 0.0012098928983483854
314 0.0011449625641886667
315 0.0010835156717760649
316 0.001025377588941091
317 0.0009703758384736077
318 0.0009183361752611588
319 0.0008690850720838991
320 0.0008224940700348837
321 0.0007784029943592372
322 0.0007366791771033156
323 0.0006971989990605096
324 0.0006598426281655082
325 0.000624495741732001
326 0.0005910439292796761
327 0.0005593973773407745
328 0.0005294403512899198
329 0.0005010976939705757
330 0.00047427317778801455
331 0.0004488877470686361
332 0.0004248669939759654
333 0.0004021385047171281
334 0.0003806279081745162
335 0.0003602664168058468
336 0.00034099755680728995
337 0.0003227653070463328
338 0.00030550707403562225
339 0.0002891754204576711
340 0.000273721587865668
341 0.00025909406293294767
342 0.0002452471530517434
343 0.0002321429824444712
344 0.00021974540966147302
345 0.00020800723976222412
346 0.00019689755859322988
347 0.00018638458674544666
348 0.00017643522452308918
349 0.00016701530053816744
350 0.0001581003279303452
351 0.00014966407110653316
352 0.00014167707307891997
353 0.00013411790417482185
354 0.00012696356477874673
355 0.00012019230146127977
356 0.00011378157083006711
357 0.00010771346551538498
358 0.0001019705019676451
359 9.653404961343065e-05
360 9.138760257508527e-05
361 8.651709721760377e-05
362 8.19070094203869e-05
363 7.754226881202036e-05
364 7.341041687150536e-05
365 6.94995078743375e-05
366 6.579726835351665e-05
367 6.229309750532338e-05
368 5.897611236256165e-05
369 5.583565153701364e-05
370 5.286297313654044e-05
371 5.004834708849745e-05
372 4.738444170826265e-05
373 4.486241750382573e-05
374 4.2474866618345255e-05
375 4.0215019660048885e-05
376 3.807518640145299e-05
377 3.604963907599714e-05
378 3.4131911558009186e-05
379 3.231655671510322e-05
380 3.059770246224207e-05
381 2.8970757618958097e-05
382 2.743057310133526e-05
383 2.5972006538400987e-05
384 2.459140169431031e-05
385 2.3284245013094282e-05
386 2.2046767101248254e-05
387 2.0874974705148337e-05
388 1.976572632623195e-05
389 1.8715764940418173e-05
390 1.7721415862570632e-05
391 1.6779971953419194e-05
392 1.588871637152896e-05
393 1.5044836588187851e-05
394 1.4245828226986497e-05
395 1.3489410778399332e-05
396 1.2773364399028685e-05
397 1.2095139789484804e-05
398 1.145299443496586e-05
399 1.0845077903682161e-05
400 1.0269508212283454e-05
401 9.724527789368986e-06
402 9.208527492374383e-06
403 8.720045436070135e-06
404 8.2573575243078e-06
405 7.819257808270196e-06
406 7.404463436549593e-06
407 7.011821523303816e-06
408 6.639943046336263e-06
409 6.2878326555936834e-06
410 5.954457206733875e-06
411 5.638725815406013e-06
412 5.339791165439266e-06
413 5.056690012109548e-06
414 4.7886913275527965e-06
415 4.5348871524205895e-06
416 4.294550962428522e-06
417 4.0669989344986414e-06
418 3.851507524029607e-06
419 3.6474147842461815e-06
420 3.4541567135768535e-06
421 3.2712066098541254e-06
422 3.0979501309951946e-06
423 2.9338623570734417e-06
424 2.778486859288849e-06
425 2.6313579995304816e-06
426 2.492005877958706e-06
427 2.3600400344687442e-06
428 2.235117848833567e-06
429 2.1168180481419086e-06
430 2.0047497870126444e-06
431 1.8986381787503396e-06
432 1.7981531121369732e-06
433 1.7029874912083225e-06
434 1.6128687459205246e-06
435 1.5275476848984372e-06
436 1.4467329944515942e-06
437 1.3701865914534269e-06
438 1.2977020898043846e-06
439 1.2290629088171652e-06
440 1.164051733130194e-06
441 1.1024831847468408e-06
442 1.0441903271914875e-06
443 9.889751182032595e-07
444 9.366772125412096e-07
445 8.871553290035977e-07
446 8.402520019677647e-07
447 7.958308104042071e-07
448 7.53763518098307e-07
449 7.139244346971557e-07
450 6.761960655207478e-07
451 6.404595540956439e-07
452 6.066124906303541e-07
453 5.74556854302957e-07
454 5.442030621013854e-07
455 5.154497792919255e-07
456 4.882203577154002e-07
457 4.6243074105866146e-07
458 4.380049551012981e-07
459 4.1486836737937236e-07
460 3.929552794591328e-07
461 3.722082406282778e-07
462 3.525512434892848e-07
463 3.339350935587304e-07
464 3.163035075274776e-07
465 2.9960560558970377e-07
466 2.8378833766961356e-07
467 2.688083814724989e-07
468 2.5461985965457267e-07
469 2.4117912639785815e-07
470 2.2845076123088963e-07
471 2.1639404311547854e-07
472 2.049768154586423e-07
473 1.9416035947365062e-07
474 1.8391578287894605e-07
475 1.742122640542393e-07
476 1.6502075323866398e-07
477 1.5631602307949188e-07
478 1.4807025488750153e-07
479 1.4026124873333745e-07
480 1.3286339655833968e-07
481 1.2585529347481789e-07
482 1.1921849306789818e-07
483 1.1293151084301514e-07
484 1.069766166825837e-07
485 1.0133619692710931e-07
486 9.599472852904284e-08
487 9.093341825332011e-08
488 8.613939196599216e-08
489 8.159862515209676e-08
490 7.729751975546753e-08
491 7.322345528572764e-08
492 6.936479915203619e-08
493 6.57099781685289e-08
494 6.224692745752194e-08
495 5.896657665795129e-08
496 5.585950062511546e-08
497 5.291656163586972e-08
498 5.012926872369238e-08
499 4.748859133659835e-08</code></pre><h1 id="pytorch-tensors"><a href="#pytorch-tensors" class="headerlink" title="pytorch:tensors"></a><span id="header2">pytorch:tensors</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p>
<p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p>
<p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<pre><code>99 784.6785888671875
199 5.850834846496582
299 0.07988587021827698
399 0.0017072007758542895
499 0.00015852594515308738</code></pre><h1 id="pytorch-tensors-and-autograd"><a href="#pytorch-tensors-and-autograd" class="headerlink" title="pytorch:tensors and autograd"></a><span id="header3">pytorch:tensors and autograd</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Tensors-and-autograd"><a href="#PyTorch-Tensors-and-autograd" class="headerlink" title="PyTorch: Tensors and autograd"></a>PyTorch: Tensors and autograd</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation computes the forward pass using operations on PyTorch<br>Tensors, and uses PyTorch autograd to compute gradients.</p>
<p>A PyTorch Tensor represents a node in a computational graph. If <code>x</code> is a<br>Tensor that has <code>x.requires_grad=True</code> then <code>x.grad</code> is another Tensor<br>holding the gradient of <code>x</code> with respect to some scalar value.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></span><br><span class="line"><span class="comment"># with respect to these Tensors during the backward pass.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Tensors during the backward pass.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Tensors; these</span></span><br><span class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></span><br><span class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></span><br><span class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss using operations on Tensors.</span></span><br><span class="line">    <span class="comment"># Now loss is a Tensor of shape (1,)</span></span><br><span class="line">    <span class="comment"># loss.item() gets the scalar value held in the loss.</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></span><br><span class="line">    <span class="comment"># gradient of loss with respect to all Tensors with requires_grad=True.</span></span><br><span class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Tensors holding the gradient</span></span><br><span class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually update weights using gradient descent. Wrap in torch.no_grad()</span></span><br><span class="line">    <span class="comment"># because weights have requires_grad=True, but we don't need to track this</span></span><br><span class="line">    <span class="comment"># in autograd.</span></span><br><span class="line">    <span class="comment"># An alternative way is to operate on weight.data and weight.grad.data.</span></span><br><span class="line">    <span class="comment"># Recall that tensor.data gives a tensor that shares the storage with</span></span><br><span class="line">    <span class="comment"># tensor, but doesn't track history.</span></span><br><span class="line">    <span class="comment"># You can also use torch.optim.SGD to achieve this.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure>

<pre><code>99 850.6499633789062
199 5.497010231018066
299 0.0542689710855484
399 0.0009686618577688932
499 0.000102342150057666</code></pre><h1 id="pytorch-defining-new-autograd-functions"><a href="#pytorch-defining-new-autograd-functions" class="headerlink" title="pytorch:defining new autograd functions"></a><span id="header4">pytorch:defining new autograd functions</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Defining-New-autograd-Functions"><a href="#PyTorch-Defining-New-autograd-Functions" class="headerlink" title="PyTorch: Defining New autograd Functions"></a>PyTorch: Defining New autograd Functions</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation computes the forward pass using operations on PyTorch<br>Variables, and uses PyTorch autograd to compute gradients.</p>
<p>In this implementation we implement our own custom autograd function to perform<br>the ReLU function.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># To apply our Function, we use Function.apply method. We alias this as 'relu'.</span></span><br><span class="line">    relu = MyReLU.apply</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure>

<pre><code>99 173.57586669921875
199 0.16617316007614136
299 0.0004797253059223294
399 3.693650069180876e-05
499 1.2812281056540087e-05</code></pre><h1 id="pytorch-static-graphs"><a href="#pytorch-static-graphs" class="headerlink" title="pytorch:static graphs"></a><span id="header5">pytorch:static graphs</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Tensors-1"><a href="#PyTorch-Tensors-1" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p>
<p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p>
<p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<pre><code>99 784.6785888671875
199 5.850834846496582
299 0.07988587021827698
399 0.0017072007758542895
499 0.00015852594515308738</code></pre><h1 id="pytorch-nn"><a href="#pytorch-nn" class="headerlink" title="pytorch:nn"></a><span id="header6">pytorch:nn</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation uses the nn package from PyTorch to build the network.<br>PyTorch autograd makes it easy to define computational graphs and take gradients,<br>but raw autograd can be a bit too low-level for defining complex neural networks;<br>this is where the nn package can help. The nn package defines a set of Modules,<br>which you can think of as a neural network layer that has produces output from<br>input and may have some trainable weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></span><br><span class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></span><br><span class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></span><br><span class="line"><span class="comment"># linear function, and holds internal Tensors for its weight and bias.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></span><br><span class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></span><br><span class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></span><br><span class="line">    <span class="comment"># doing so you pass a Tensor of input data to the Module and it produces</span></span><br><span class="line">    <span class="comment"># a Tensor of output data.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss. We pass Tensors containing the predicted and true</span></span><br><span class="line">    <span class="comment"># values of y, and the loss function returns a Tensor containing the</span></span><br><span class="line">    <span class="comment"># loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></span><br><span class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></span><br><span class="line">    <span class="comment"># in Tensors with requires_grad=True, so this call will compute gradients for</span></span><br><span class="line">    <span class="comment"># all learnable parameters in the model.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Tensor, so</span></span><br><span class="line">    <span class="comment"># we can access its gradients like we did before.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param -= learning_rate * param.grad</span><br></pre></td></tr></table></figure>

<pre><code>99 2.456005573272705
199 0.04037925601005554
299 0.001298694172874093
399 5.4667791118845344e-05
499 2.6507393613428576e-06</code></pre><h1 id="pytorch-optim"><a href="#pytorch-optim" class="headerlink" title="pytorch:optim"></a><span id="header7">pytorch:optim</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation uses the nn package from PyTorch to build the network.</p>
<p>Rather than manually updating the weights of the model as we have been doing,<br>we use the optim package to define an Optimizer that will update the weights<br>for us. The optim package defines many optimization algorithms that are commonly<br>used for deep learning, including SGD+momentum, RMSProp, Adam, etc.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></span><br><span class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></span><br><span class="line"><span class="comment"># optimization algorithms. The first argument to the Adam constructor tells the</span></span><br><span class="line"><span class="comment"># optimizer which Tensors it should update.</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></span><br><span class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable</span></span><br><span class="line">    <span class="comment"># weights of the model). This is because by default, gradients are</span></span><br><span class="line">    <span class="comment"># accumulated in buffers( i.e, not overwritten) whenever .backward()</span></span><br><span class="line">    <span class="comment"># is called. Checkout docs of torch.autograd.backward for more details.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<pre><code>99 40.16399383544922
199 0.3977137506008148
299 0.001604456570930779
399 1.6438591046608053e-05
499 8.815557350771996e-08</code></pre><h1 id="pytorch-custom-nn-modules"><a href="#pytorch-custom-nn-modules" class="headerlink" title="pytorch:custom nn modules"></a><span id="header8">pytorch:custom nn modules</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch: Custom nn Modules"></a>PyTorch: Custom nn Modules</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation defines the model as a custom Module subclass. Whenever you<br>want a model more complex than a simple sequence of existing Modules you will<br>need to define your model this way.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as</span></span><br><span class="line"><span class="string">        member variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward function we accept a Tensor of input data and we must return</span></span><br><span class="line"><span class="string">        a Tensor of output data. We can use Modules defined in the constructor as</span></span><br><span class="line"><span class="string">        well as arbitrary operators on Tensors.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.linear2(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></span><br><span class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></span><br><span class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>




<h1 id="pytorch-control-flow-weight-sharing"><a href="#pytorch-control-flow-weight-sharing" class="headerlink" title="pytorch:control flow+weight sharing"></a><span id="header9">pytorch:control flow+weight sharing</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch: Control Flow + Weight Sharing"></a>PyTorch: Control Flow + Weight Sharing</h2><p>To showcase the power of PyTorch dynamic graphs, we will implement a very strange<br>model: a fully-connected ReLU network that on each forward pass randomly chooses<br>a number between 1 and 4 and has that many hidden layers, reusing the same<br>weights multiple times to compute the innermost hidden layers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we construct three nn.Linear instances that we will use</span></span><br><span class="line"><span class="string">        in the forward pass.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(DynamicNet, self).__init__()</span><br><span class="line">        self.input_linear = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.middle_linear = torch.nn.Linear(H, H)</span><br><span class="line">        self.output_linear = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</span></span><br><span class="line"><span class="string">        and reuse the middle_linear Module that many times to compute hidden layer</span></span><br><span class="line"><span class="string">        representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Since each forward pass builds a dynamic computation graph, we can use normal</span></span><br><span class="line"><span class="string">        Python control-flow operators like loops or conditional statements when</span></span><br><span class="line"><span class="string">        defining the forward pass of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Here we also see that it is perfectly safe to reuse the same Module many</span></span><br><span class="line"><span class="string">        times when defining a computational graph. This is a big improvement from Lua</span></span><br><span class="line"><span class="string">        Torch, where each Module could be used only once.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</span><br><span class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.output_linear(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = DynamicNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></span><br><span class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<pre><code>99 18.901199340820312
199 7.5054731369018555
299 1.1003623008728027
399 0.8731748461723328
499 2.003668785095215</code></pre>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-neural_newworks</title>
    <url>/2020/07/23/Pytorch-Learning-neural-newworks/</url>
    <content><![CDATA[<p>Pytorch-Learning-autograd:</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><p>Neural networks can be constructed using the <code>torch.nn</code> package.</p>
<p>Now that you had a glimpse of <code>autograd</code>, <code>nn</code> depends on<br><code>autograd</code> to define models and differentiate them.<br>An <code>nn.Module</code> contains layers, and a method <code>forward(input)</code> that<br>returns the <code>output</code>.</p>
<p>For example, look at this network that classifies digit images:</p>
<p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt></p>
<p>   convnet</p>
<p>It is a simple feed-forward network. It takes the input, feeds it<br>through several layers one after the other, and then finally gives the<br>output.</p>
<p>A typical training procedure for a neural network is as follows:</p>
<ul>
<li>Define the neural network that has some learnable parameters (or<br>weights)</li>
<li>Iterate over a dataset of inputs</li>
<li>Process input through the network</li>
<li>Compute the loss (how far is the output from being correct)</li>
<li>Propagate gradients back into the network’s parameters</li>
<li>Update the weights of the network, typically using a simple update rule:<br><code>weight = weight - learning_rate * gradient</code></li>
</ul>
<h2 id="Define-the-network"><a href="#Define-the-network" class="headerlink" title="Define the network"></a>Define the network</h2><p>Let’s define this network:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)  <span class="comment"># 6*6 from image dimension </span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>

<pre><code>Net(
  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=576, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)</code></pre><p>You just have to define the <code>forward</code> function, and the <code>backward</code><br>function (where gradients are computed) is automatically defined for you<br>using <code>autograd</code>.<br>You can use any of the Tensor operations in the <code>forward</code> function.</p>
<p>The learnable parameters of a model are returned by <code>net.parameters()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's .weight</span></span><br></pre></td></tr></table></figure>

<pre><code>10
torch.Size([6, 1, 3, 3])</code></pre><p>Let’s try a random 32x32 input.<br>Note: expected input size of this net (LeNet) is 32x32. To use this net on<br>the MNIST dataset, please resize the images from the dataset to 32x32.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.0416,  0.0926, -0.0761, -0.0135, -0.0745, -0.0158,  0.0696, -0.0040,
         -0.0099, -0.1799]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>Zero the gradient buffers of all parameters and backprops with random<br>gradients:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<div class="alert alert-info">
    <h4>Note</h4>

<pre><code>torch.nn only supports mini-batches. The entire torch.nn
package only supports inputs that are a mini-batch of samples, and not
a single sample.

For example, nn.Conv2d will take in a 4D Tensor of
nSamples x nChannels x Height x Width.

If you have a single sample, just use input.unsqueeze(0) to add
a fake batch dimension.</code></pre></div>

<p>Before proceeding further, let’s recap all the classes you’ve seen so far.</p>
<p><strong>Recap:</strong></p>
<ul>
<li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd<br>operations like <code>backward()</code>. Also <em>holds the gradient</em> w.r.t. the<br>tensor.</li>
<li><code>nn.Module</code> - Neural network module. <em>Convenient way of<br>encapsulating parameters</em>, with helpers for moving them to GPU,<br>exporting, loading, etc.</li>
<li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically<br>registered as a parameter when assigned as an attribute to a</em><br><code>Module</code>.</li>
<li><code>autograd.Function</code> - Implements <em>forward and backward definitions<br>of an autograd operation</em>. Every <code>Tensor</code> operation creates at<br>least a single <code>Function</code> node that connects to functions that<br>created a <code>Tensor</code> and <em>encodes its history</em>.</li>
</ul>
<p><strong>At this point, we covered:</strong></p>
<ul>
<li>Defining a neural network</li>
<li>Processing inputs and calling backward</li>
</ul>
<p><strong>Still Left:</strong></p>
<ul>
<li>Computing the loss</li>
<li>Updating the weights of the network</li>
</ul>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>A loss function takes the (output, target) pair of inputs, and computes a<br>value that estimates how far away the output is from the target.</p>
<p>There are several different<br><code>loss functions &lt;https://pytorch.org/docs/nn.html#loss-functions&gt;</code>_ under the<br>nn package .<br>A simple loss is: <code>nn.MSELoss</code> which computes the mean-squared error<br>between the input and the target.</p>
<p>For example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.9128, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>Now, if you follow <code>loss</code> in the backward direction, using its<br><code>.grad_fn</code> attribute, you will see a graph of computations that looks<br>like this:</p>
<p>::</p>
<pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
      -&gt; MSELoss
      -&gt; loss</code></pre><p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated<br>w.r.t. the loss, and all Tensors in the graph that has <code>requires_grad=True</code><br>will have their <code>.grad</code> Tensor accumulated with the gradient.</p>
<p>For illustration, let us follow a few steps backward:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure>

<pre><code>&lt;MseLossBackward object at 0x000001F109346C88&gt;
&lt;AddmmBackward object at 0x000001F109346EB8&gt;
&lt;AccumulateGrad object at 0x000001F109346C88&gt;</code></pre><h2 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a>Backprop</h2><p>To backpropagate the error all we have to do is to <code>loss.backward()</code>.<br>You need to clear the existing gradients though, else gradients will be<br>accumulated to existing gradients.</p>
<p>Now we shall call <code>loss.backward()</code>, and have a look at conv1’s bias<br>gradients before and after the backward.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<pre><code>conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([-0.0032, -0.0131,  0.0148,  0.0334, -0.0327, -0.0073])</code></pre><p>Now, we have seen how to use loss functions.</p>
<p><strong>Read Later:</strong></p>
<p>  The neural network package contains various modules and loss functions<br>  that form the building blocks of deep neural networks. A full list with<br>  documentation is <code>here &lt;https://pytorch.org/docs/nn&gt;</code>_.</p>
<p><strong>The only thing left to learn is:</strong></p>
<ul>
<li>Updating the weights of the network</li>
</ul>
<h2 id="Update-the-weights"><a href="#Update-the-weights" class="headerlink" title="Update the weights"></a>Update the weights</h2><p>The simplest update rule used in practice is the Stochastic Gradient<br>Descent (SGD):</p>
<pre><code>``weight = weight - learning_rate * gradient``</code></pre><p>We can implement this using simple Python code:</p>
<p>.. code:: python</p>
<pre><code>learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)</code></pre><p>However, as you use neural networks, you want to use various different<br>update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.<br>To enable this, we built a small package: <code>torch.optim</code> that<br>implements all these methods. Using it is very simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>

<p>.. Note::</p>
<pre><code>Observe how gradient buffers had to be manually set to zero using
``optimizer.zero_grad()``. This is because gradients are accumulated
as explained in the `Backprop`_ section.</code></pre><h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">feed-forward:前向</span><br><span class="line">glimpse:一瞥</span><br><span class="line">proce:进行</span><br><span class="line">recap:回顾</span><br><span class="line">encapsulat:封装</span><br><span class="line">assign:分配</span><br><span class="line">For illustration:为了说明</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-tensor</title>
    <url>/2020/07/23/Pytorch-Learning-tensor/</url>
    <content><![CDATA[<p>Pytorch-Learning-tensor:</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="What-is-PyTorch"><a href="#What-is-PyTorch" class="headerlink" title="What is PyTorch?"></a>What is PyTorch?</h1><p>It’s a Python-based scientific computing package targeted at two sets of<br>audiences:</p>
<ul>
<li>A replacement for NumPy to use the power of GPUs</li>
<li>a deep learning research platform that provides maximum flexibility<br>and speed</li>
</ul>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>Tensors</p>
<p>Tensors are similar to NumPy’s ndarrays, with the addition being that<br>Tensors can also be used on a GPU to accelerate computing.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<div class="alert alert-info"><h4>Note</h4><p>An uninitialized matrix is declared,
    but does not contain definite known
    values before it is used. When an
    uninitialized matrix is created,
    whatever values were in the allocated
    memory at the time will appear as the initial values.</p></div>



<p>Construct a 5x3 matrix, uninitialized:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-3.4374e-14,  7.1046e-43, -3.4374e-14],
        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],
        [-3.4374e-14,  7.1046e-43, -3.4374e-14],
        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],
        [-3.4374e-14,  7.1046e-43, -3.4374e-14]])</code></pre><p>Construct a randomly initialized matrix:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0.6385, 0.8264, 0.0737],
        [0.1567, 0.5029, 0.7141],
        [0.8297, 0.3453, 0.2860],
        [0.0158, 0.3826, 0.7823],
        [0.3434, 0.0977, 0.1530]])</code></pre><p>Construct a matrix filled zeros and of dtype long:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])</code></pre><p>Construct a tensor directly from data:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.5000, 3.0000])</code></pre><p>or create a tensor based on an existing tensor. These methods<br>will reuse properties of the input tensor, e.g. dtype, unless<br>new values are provided by user</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)      <span class="comment"># new_* methods take in sizes</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)    <span class="comment"># override dtype!</span></span><br><span class="line">print(x)                                      <span class="comment"># result has the same size</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 2.0072,  0.0294,  0.1776],
        [-0.3961, -1.7436, -0.1741],
        [ 0.7820,  0.5535, -0.0059],
        [-1.9826, -0.7387, -0.3942],
        [ 0.3501,  0.5796, -1.3633]])</code></pre><p>Get its size:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 3])</code></pre><div class="alert alert-info"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div>

<p>Operations</p>
<p>There are multiple syntaxes for operations. In the following<br>example, we will take a look at the addition operation.</p>
<p>Addition: syntax 1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: syntax 2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: providing an output tensor as argument</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: in-place</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><div class="alert alert-info"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.
    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div>

<p>You can use standard NumPy-like indexing with all bells and whistles!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([ 0.0294, -1.7436,  0.5535, -0.7387,  0.5796])</code></pre><p>Resizing: If you want to resize/reshape tensor, you can use <code>torch.view</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>If you have a one element tensor, use <code>.item()</code> to get the value as a<br>Python number</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1.0191])
1.0191349983215332</code></pre><p><strong>Read later:</strong></p>
<p>  100+ Tensor operations, including transposing, indexing, slicing,<br>  mathematical operations, linear algebra, random numbers, etc.,<br>  are described<br>  <code>here &lt;https://pytorch.org/docs/torch&gt;</code>_.</p>
<h2 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h2><p>Converting a Torch Tensor to a NumPy array and vice versa is a breeze.</p>
<p>The Torch Tensor and NumPy array will share their underlying memory<br>locations (if the Torch Tensor is on CPU), and changing one will change<br>the other.</p>
<p>Converting a Torch Tensor to a NumPy Array</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1., 1., 1., 1., 1.])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>[1. 1. 1. 1. 1.]</code></pre><p>See how the numpy array changed in value.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]</code></pre><p>Converting NumPy Array to Torch Tensor</p>
<p>See how changing the np array changed the Torch Tensor automatically</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><p>All the Tensors on the CPU except a CharTensor support converting to<br>NumPy and back.</p>
<h2 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h2><p>Tensors can be moved onto any device using the <code>.to</code> method.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="comment"># We will use ``torch.device`` objects to move tensors in and out of GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># a CUDA device object</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` can also change dtype together!</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">allocate:分配</span><br><span class="line">mutate:变异</span><br><span class="line">Bridge:桥</span><br><span class="line">vice versa:反之亦然</span><br><span class="line">underlying:底层的</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-torch.nn</title>
    <url>/2020/07/23/Pytorch-Learning-torch-nn/</url>
    <content><![CDATA[<p>Pytorch-Learning-torch.nn</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="What-is-torch-nn-really"><a href="#What-is-torch-nn-really" class="headerlink" title="What is torch.nn really?"></a>What is <code>torch.nn</code> <em>really</em>?</h1><p>by Jeremy Howard, <code>fast.ai &lt;https://www.fast.ai&gt;</code>_. Thanks to Rachel Thomas and Francisco Ingham.</p>
<p>We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,<br>click the link at the top of the page.</p>
<p>PyTorch provides the elegantly designed modules and classes <code>torch.nn &lt;https://pytorch.org/docs/stable/nn.html&gt;</code>_ ,<br><code>torch.optim &lt;https://pytorch.org/docs/stable/optim.html&gt;</code>_ ,<br><code>Dataset &lt;https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset&gt;</code>_ ,<br>and <code>DataLoader &lt;https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader&gt;</code>_<br>to help you create and train neural networks.<br>In order to fully utilize their power and customize<br>them for your problem, you need to really understand exactly what they’re<br>doing. To develop this understanding, we will first train basic neural net<br>on the MNIST data set without using any features from these models; we will<br>initially only use the most basic PyTorch tensor functionality. Then, we will<br>incrementally add one feature from <code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, or<br><code>DataLoader</code> at a time, showing exactly what each piece does, and how it<br>works to make the code either more concise, or more flexible.</p>
<p><strong>This tutorial assumes you already have PyTorch installed, and are familiar<br>with the basics of tensor operations.</strong> (If you’re familiar with Numpy array<br>operations, you’ll find the PyTorch tensor operations used here nearly identical).</p>
<h2 id="MNIST-data-setup"><a href="#MNIST-data-setup" class="headerlink" title="MNIST data setup"></a>MNIST data setup</h2><p>We will use the classic <code>MNIST &lt;http://deeplearning.net/data/mnist/&gt;</code>_ dataset,<br>which consists of black-and-white images of hand-drawn digits (between 0 and 9).</p>
<p>We will use <code>pathlib &lt;https://docs.python.org/3/library/pathlib.html&gt;</code>_<br>for dealing with paths (part of the Python 3 standard library), and will<br>download the dataset using<br><code>requests &lt;http://docs.python-requests.org/en/master/&gt;</code>_. We will only<br>import modules when we use them, so you can see exactly what’s being<br>used at each point.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">"data"</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">"mnist"</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">"http://deeplearning.net/data/mnist/"</span></span><br><span class="line">FILENAME = <span class="string">"mnist.pkl.gz"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).open(<span class="string">"wb"</span>).write(content)</span><br></pre></td></tr></table></figure>

<p>This dataset is in numpy array format, and has been stored using pickle,<br>a python-specific format for serializing data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.open((PATH / FILENAME).as_posix(), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">"latin-1"</span>)</span><br></pre></td></tr></table></figure>

<p>Each image is 28 x 28, and is being stored as a flattened row of length<br>784 (=28x28). Let’s take a look at one; we need to reshape it to 2d<br>first.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">"gray"</span>)</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(50000, 784)</code></pre><p><img src="/2020/07/23/Pytorch-Learning-torch-nn/output_7_1.png" alt="png"></p>
<p>PyTorch uses <code>torch.tensor</code>, rather than numpy arrays, so we need to<br>convert our data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = map(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line">x_train, x_train.shape, y_train.min(), y_train.max()</span><br><span class="line">print(x_train, y_train)</span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.min(), y_train.max())</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])
torch.Size([50000, 784])
tensor(0) tensor(9)</code></pre><h2 id="Neural-net-from-scratch-no-torch-nn"><a href="#Neural-net-from-scratch-no-torch-nn" class="headerlink" title="Neural net from scratch (no torch.nn)"></a>Neural net from scratch (no torch.nn)</h2><p>Let’s first create a model using nothing but PyTorch tensor operations. We’re assuming<br>you’re already familiar with the basics of neural networks. (If you’re not, you can<br>learn them at <code>course.fast.ai &lt;https://course.fast.ai&gt;</code>_).</p>
<p>PyTorch provides methods to create random or zero-filled tensors, which we will<br>use to create our weights and bias for a simple linear model. These are just regular<br>tensors, with one very special addition: we tell PyTorch that they require a<br>gradient. This causes PyTorch to record all of the operations done on the tensor,<br>so that it can calculate the gradient during back-propagation <em>automatically</em>!</p>
<p>For the weights, we set <code>requires_grad</code> <strong>after</strong> the initialization, since we<br>don’t want that step included in the gradient. (Note that a trailling <code>_</code> in<br>PyTorch signifies that the operation is performed in-place.)</p>
<div class="alert alert-info"><h4>Note</h4><p>We are initializing the weights here with
   `Xavier initialisation <http: proceedings.mlr.press v9 glorot10a glorot10a.pdf>`_
   (by multiplying with 1/sqrt(n)).</http:></p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>Thanks to PyTorch’s ability to calculate gradients automatically, we can<br>use any standard Python function (or callable object) as a model! So<br>let’s just write a plain matrix multiplication and broadcasted addition<br>to create a simple linear model. We also need an activation function, so<br>we’ll write <code>log_softmax</code> and use it. Remember: although PyTorch<br>provides lots of pre-written loss functions, activation functions, and<br>so forth, you can easily write your own using plain python. PyTorch will<br>even create fast GPU or vectorized CPU code for your function<br>automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x - x.exp().sum(<span class="number">-1</span>).log().unsqueeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>

<p>In the above, the <code>@</code> stands for the dot product operation. We will call<br>our function on one batch of data (in this case, 64 images).  This is<br>one <em>forward pass</em>.  Note that our predictions won’t be any better than<br>random at this stage, since we start with random weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bs = <span class="number">64</span>  <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs]  <span class="comment"># a mini-batch from x</span></span><br><span class="line">preds = model(xb)  <span class="comment"># predictions</span></span><br><span class="line">preds[<span class="number">0</span>], preds.shape</span><br><span class="line">print(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-2.2669, -2.6024, -2.8454, -1.5665, -2.7687, -2.2455, -2.6885, -2.4918,
        -2.1065, -2.1682], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])</code></pre><p>As you see, the <code>preds</code> tensor contains not only the tensor values, but also a<br>gradient function. We’ll use this later to do backprop.</p>
<p>Let’s implement negative log-likelihood to use as the loss function<br>(again, we can just use standard Python):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nll</span><span class="params">(input, target)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -input[range(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>

<p>Let’s check our loss with our random model, so we can see if we improve<br>after a backprop pass later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line">print(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.2549, grad_fn=&lt;NegBackward&gt;)</code></pre><p>Let’s also implement a function to calculate the accuracy of our model.<br>For each prediction, if the index with the largest value matches the<br>target value, then the prediction was correct.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(out, yb)</span>:</span></span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).float().mean()</span><br></pre></td></tr></table></figure>

<p>Let’s check the accuracy of our random model, so we can see if our<br>accuracy improves as our loss improves.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.1562)</code></pre><p>We can now run a training loop.  For each iteration, we will:</p>
<ul>
<li>select a mini-batch of data (of size <code>bs</code>)</li>
<li>use the model to make predictions</li>
<li>calculate the loss</li>
<li><code>loss.backward()</code> updates the gradients of the model, in this case, <code>weights</code><br>and <code>bias</code>.</li>
</ul>
<p>We now use these gradients to update the weights and bias.  We do this<br>within the <code>torch.no_grad()</code> context manager, because we do not want these<br>actions to be recorded for our next calculation of the gradient.  You can read<br>more about how PyTorch’s Autograd records operations<br><code>here &lt;https://pytorch.org/docs/stable/notes/autograd.html&gt;</code>_.</p>
<p>We then set the<br>gradients to zero, so that we are ready for the next loop.<br>Otherwise, our gradients would record a running tally of all the operations<br>that had happened (i.e. <code>loss.backward()</code> <em>adds</em> the gradients to whatever is<br>already stored, rather than replacing them).</p>
<p>.. tip:: You can use the standard python debugger to step through PyTorch<br>   code, allowing you to check the various variable values at each step.<br>   Uncomment <code>set_trace()</code> below to try it out.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># how many epochs to train for</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line"><span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>That’s it: we’ve created and trained a minimal neural network (in this case, a<br>logistic regression, since we have no hidden layers) entirely from scratch!</p>
<p>Let’s check the loss and accuracy and compare those to what we got<br>earlier. We expect that the loss will have decreased and accuracy to<br>have increased, and they have.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0781, grad_fn=&lt;NegBackward&gt;) tensor(1.)</code></pre><h2 id="Using-torch-nn-functional"><a href="#Using-torch-nn-functional" class="headerlink" title="Using torch.nn.functional"></a>Using torch.nn.functional</h2><p>We will now refactor our code, so that it does the same thing as before, only<br>we’ll start taking advantage of PyTorch’s <code>nn</code> classes to make it more concise<br>and flexible. At each step from here, we should be making our code one or more<br>of: shorter, more understandable, and/or more flexible.</p>
<p>The first and easiest step is to make our code shorter by replacing our<br>hand-written activation and loss functions with those from <code>torch.nn.functional</code><br>(which is generally imported into the namespace <code>F</code> by convention). This module<br>contains all the functions in the <code>torch.nn</code> library (whereas other parts of the<br>library contain classes). As well as a wide range of loss and activation<br>functions, you’ll also find here some convenient functions for creating neural<br>nets, such as pooling functions. (There are also functions for doing convolutions,<br>linear layers, etc, but as we’ll see, these are usually better handled using<br>other parts of the library.)</p>
<p>If you’re using negative log likelihood loss and log softmax activation,<br>then Pytorch provides a single function <code>F.cross_entropy</code> that combines<br>the two. So we can even remove the activation function from our model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure>

<p>Note that we no longer call <code>log_softmax</code> in the <code>model</code> function. Let’s<br>confirm that our loss and accuracy are the same as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0781, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)</code></pre><h2 id="Refactor-using-nn-Module"><a href="#Refactor-using-nn-Module" class="headerlink" title="Refactor using nn.Module"></a>Refactor using nn.Module</h2><p>Next up, we’ll use <code>nn.Module</code> and <code>nn.Parameter</code>, for a clearer and more<br>concise training loop. We subclass <code>nn.Module</code> (which itself is a class and<br>able to keep track of state).  In this case, we want to create a class that<br>holds our weights, bias, and method for the forward step.  <code>nn.Module</code> has a<br>number of attributes and methods (such as <code>.parameters()</code> and <code>.zero_grad()</code>)<br>which we will be using.</p>
<div class="alert alert-info"><h4>Note</h4><p>``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a
   class we'll be using a lot. ``nn.Module`` is not to be confused with the Python
   concept of a (lowercase ``m``) `module <https: 3 docs.python.org tutorial modules.html>`_,
   which is a file of Python code that can be imported.</https:></p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>

<p>Since we’re now using an object instead of just using a function, we<br>first have to instantiate our model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure>

<p>Now we can calculate the loss in the same way as before. Note that<br><code>nn.Module</code> objects are used as if they are functions (i.e they are<br><em>callable</em>), but behind the scenes Pytorch will call our <code>forward</code><br>method automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.4768, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>Previously for our training loop we had to update the values for each parameter<br>by name, and manually zero out the grads for each parameter separately, like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    weights -&#x3D; weights.grad * lr</span><br><span class="line">    bias -&#x3D; bias.grad * lr</span><br><span class="line">    weights.grad.zero_()</span><br><span class="line">    bias.grad.zero_()</span><br></pre></td></tr></table></figure>


<p>Now we can take advantage of model.parameters() and model.zero_grad() (which<br>are both defined by PyTorch for <code>nn.Module</code>) to make those steps more concise<br>and less prone to the error of forgetting some of our parameters, particularly<br>if we had a more complicated model:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure>


<p>We’ll wrap our little training loop in a <code>fit</code> function so we can run it<br>again later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>

<p>Let’s double-check that our loss has gone down:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0860, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-nn-Linear"><a href="#Refactor-using-nn-Linear" class="headerlink" title="Refactor using nn.Linear"></a>Refactor using nn.Linear</h2><p>We continue to refactor our code.  Instead of manually defining and<br>initializing <code>self.weights</code> and <code>self.bias</code>, and calculating <code>xb  @
self.weights + self.bias</code>, we will instead use the Pytorch class<br><code>nn.Linear &lt;https://pytorch.org/docs/stable/nn.html#linear-layers&gt;</code>_ for a<br>linear layer, which does all that for us. Pytorch has many types of<br>predefined layers that can greatly simplify our code, and often makes it<br>faster too.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure>

<p>We instantiate our model and calculate the loss in the same way as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.3752, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>We are still able to use our same <code>fit</code> method as before.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0814, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-optim"><a href="#Refactor-using-optim" class="headerlink" title="Refactor using optim"></a>Refactor using optim</h2><p>Pytorch also has a package with various optimization algorithms, <code>torch.optim</code>.<br>We can use the <code>step</code> method from our optimizer to take a forward step, instead<br>of manually updating each parameter.</p>
<p>This will let us replace our previous manually coded optimization step:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure>
<p>and instead use just:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">opt.step()</span><br><span class="line">opt.zero_grad()</span><br></pre></td></tr></table></figure>

<p>(<code>optim.zero_grad()</code> resets the gradient to 0 and we need to call it before<br>computing the gradient for the next minibatch.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>

<p>We’ll define a little function to create our model and optimizer so we<br>can reuse it in the future.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.2501, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.0822, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-Dataset"><a href="#Refactor-using-Dataset" class="headerlink" title="Refactor using Dataset"></a>Refactor using Dataset</h2><p>PyTorch has an abstract Dataset class.  A Dataset can be anything that has<br>a <code>__len__</code> function (called by Python’s standard <code>len</code> function) and<br>a <code>__getitem__</code> function as a way of indexing into it.<br><code>This tutorial &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code>_<br>walks through a nice example of creating a custom <code>FacialLandmarkDataset</code> class<br>as a subclass of <code>Dataset</code>.</p>
<p>PyTorch’s <code>TensorDataset &lt;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset&gt;</code>_<br>is a Dataset wrapping tensors. By defining a length and way of indexing,<br>this also gives us a way to iterate, index, and slice along the first<br>dimension of a tensor. This will make it easier to access both the<br>independent and dependent variables in the same line as we train.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br></pre></td></tr></table></figure>

<p>Both <code>x_train</code> and <code>y_train</code> can be combined in a single <code>TensorDataset</code>,<br>which will be easier to iterate over and slice.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>

<p>Previously, we had to iterate through minibatches of x and y values separately:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xb &#x3D; x_train[start_i:end_i]</span><br><span class="line">yb &#x3D; y_train[start_i:end_i]</span><br></pre></td></tr></table></figure>

<p>Now, we can do these two steps together:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xb,yb &#x3D; train_ds[i*bs : i*bs+bs]</span><br></pre></td></tr></table></figure>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0801, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-DataLoader"><a href="#Refactor-using-DataLoader" class="headerlink" title="Refactor using DataLoader"></a>Refactor using DataLoader</h2><p>Pytorch’s <code>DataLoader</code> is responsible for managing batches. You can<br>create a <code>DataLoader</code> from any <code>Dataset</code>. <code>DataLoader</code> makes it easier<br>to iterate over batches. Rather than having to use <code>train_ds[i*bs : i*bs+bs]</code>,<br>the DataLoader gives us each minibatch automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure>

<p>Previously, our loop iterated over batches (xb, yb) like this:<br>::<br>      for i in range((n-1)//bs + 1):<br>          xb,yb = train_ds[i<em>bs : i</em>bs+bs]<br>          pred = model(xb)</p>
<p>Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:<br>::<br>      for xb,yb in train_dl:<br>          pred = model(xb)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0824, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>Thanks to Pytorch’s <code>nn.Module</code>, <code>nn.Parameter</code>, <code>Dataset</code>, and <code>DataLoader</code>,<br>our training loop is now dramatically smaller and easier to understand. Let’s<br>now try to add the basic features necessary to create effecive models in practice.</p>
<h2 id="Add-validation"><a href="#Add-validation" class="headerlink" title="Add validation"></a>Add validation</h2><p>In section 1, we were just trying to get a reasonable training loop set up for<br>use on our training data.  In reality, you <strong>always</strong> should also have<br>a <code>validation set &lt;https://www.fast.ai/2017/11/13/validation-sets/&gt;</code>_, in order<br>to identify if you are overfitting.</p>
<p>Shuffling the training data is<br><code>important &lt;https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks&gt;</code>_<br>to prevent correlation between batches and overfitting. On the other hand, the<br>validation loss will be identical whether we shuffle the validation set or not.<br>Since shuffling takes extra time, it makes no sense to shuffle the validation data.</p>
<p>We’ll use a batch size for the validation set that is twice as large as<br>that for the training set. This is because the validation set does not<br>need backpropagation and thus takes less memory (it doesn’t need to<br>store the gradients). We take advantage of this to use a larger batch<br>size and compute the loss more quickly.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>We will calculate and print the validation loss at the end of each epoch.</p>
<p>(Note that we always call <code>model.train()</code> before training, and <code>model.eval()</code><br>before inference, because these are used by layers such as <code>nn.BatchNorm2d</code><br>and <code>nn.Dropout</code> to ensure appropriate behaviour for these different phases.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = sum(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line"></span><br><span class="line">    print(epoch, valid_loss / len(valid_dl))</span><br></pre></td></tr></table></figure>

<pre><code>0 tensor(0.3169)
1 tensor(0.4910)</code></pre><h2 id="Create-fit-and-get-data"><a href="#Create-fit-and-get-data" class="headerlink" title="Create fit() and get_data()"></a>Create fit() and get_data()</h2><p>We’ll now do a little refactoring of our own. Since we go through a similar<br>process twice of calculating the loss for both the training set and the<br>validation set, let’s make that into its own function, <code>loss_batch</code>, which<br>computes the loss for one batch.</p>
<p>We pass an optimizer in for the training set, and use it to perform<br>backprop.  For the validation set, we don’t pass an optimizer, so the<br>method doesn’t perform backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_batch</span><span class="params">(model, loss_func, xb, yb, opt=None)</span>:</span></span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), len(xb)</span><br></pre></td></tr></table></figure>

<p><code>fit</code> runs the necessary operations to train our model and compute the<br>training and validation losses for each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(epochs, model, loss_func, opt, train_dl, valid_dl)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = zip(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)</span><br><span class="line"></span><br><span class="line">        print(epoch, val_loss)</span><br></pre></td></tr></table></figure>

<p><code>get_data</code> returns dataloaders for the training and validation sets.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(train_ds, valid_ds, bs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>Now, our whole process of obtaining the data loaders and fitting the<br>model can be run in 3 lines of code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.30705235414505005
1 0.31455935287475584</code></pre><p>You can use these basic 3 lines of code to train a wide variety of models.<br>Let’s see if we can use them to train a convolutional neural network (CNN)!</p>
<h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>We are now going to build our neural network with three convolutional layers.<br>Because none of the functions in the previous section assume anything about<br>the model form, we’ll be able to use them to train a CNN without any modification.</p>
<p>We will use Pytorch’s predefined<br><code>Conv2d &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code>_ class<br>as our convolutional layer. We define a CNN with 3 convolutional layers.<br>Each convolution is followed by a ReLU.  At the end, we perform an<br>average pooling.  (Note that <code>view</code> is PyTorch’s version of numpy’s<br><code>reshape</code>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        xb = xb.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> xb.view(<span class="number">-1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>

<p><code>Momentum &lt;https://cs231n.github.io/neural-networks-3/#sgd&gt;</code>_ is a variation on<br>stochastic gradient descent that takes previous updates into account as well<br>and generally leads to faster training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.8898362560272217
1 0.7796085683822632</code></pre><h2 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h2><p><code>torch.nn</code> has another handy class we can use to simply our code:<br><code>Sequential &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential&gt;</code>_ .<br>A <code>Sequential</code> object runs each of the modules contained within it, in a<br>sequential manner. This is a simpler way of writing our neural network.</p>
<p>To take advantage of this, we need to be able to easily define a<br><strong>custom layer</strong> from a given function.  For instance, PyTorch doesn’t<br>have a <code>view</code> layer, and we need to create one for our network. <code>Lambda</code><br>will create a layer that we can then use when defining a network with<br><code>Sequential</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure>

<p>The model created with <code>Sequential</code> is simply:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.4746176950454712
1 0.3015344765663147</code></pre><h2 id="Wrapping-DataLoader"><a href="#Wrapping-DataLoader" class="headerlink" title="Wrapping DataLoader"></a>Wrapping DataLoader</h2><p>Our CNN is fairly concise, but it only works with MNIST, because:</p>
<ul>
<li>It assumes the input is a 28*28 long vector</li>
<li>It assumes that the final CNN grid size is 4*4 (since that’s the average<br>pooling kernel size we used)</li>
</ul>
<p>Let’s get rid of these two assumptions, so our model works with any 2d<br>single channel image. First, we can remove the initial Lambda layer but<br>moving the data preprocessing into a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WrappedDataLoader</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, func)</span>:</span></span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        batches = iter(self.dl)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> batches:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>

<p>Next, we can replace <code>nn.AvgPool2d</code> with <code>nn.AdaptiveAvgPool2d</code>, which<br>allows us to define the size of the <em>output</em> tensor we want, rather than<br>the <em>input</em> tensor we have. As a result, our model will work with any<br>size input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s try it out:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.43946951394081113
1 0.2360862446308136</code></pre><h2 id="Using-your-GPU"><a href="#Using-your-GPU" class="headerlink" title="Using your GPU"></a>Using your GPU</h2><p>If you’re lucky enough to have access to a CUDA-capable GPU (you can<br>rent one for about $0.50/hour from most cloud providers) you can<br>use it to speed up your code. First check that your GPU is working in<br>Pytorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>

<pre><code>False</code></pre><p>And then create a device object for it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dev = torch.device(</span><br><span class="line">    <span class="string">"cuda"</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s update <code>preprocess</code> to move batches to the GPU:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(dev), y.to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>

<p>Finally, we can move our model to the GPU.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>You should find it runs faster now:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.20087482118606567
1 0.21629996614456176</code></pre><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>We now have a general data pipeline and training loop which you can use for<br>training many types of models using Pytorch. To see how simple training a model<br>can now be, take a look at the <code>mnist_sample</code> sample notebook.</p>
<p>Of course, there are many things you’ll want to add, such as data augmentation,<br>hyperparameter tuning, monitoring training, transfer learning, and so forth.<br>These features are available in the fastai library, which has been developed<br>using the same design approach shown in this tutorial, providing a natural<br>next step for practitioners looking to take their models further.</p>
<p>We promised at the start of this tutorial we’d explain through example each of<br><code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, and <code>DataLoader</code>. So let’s summarize<br>what we’ve seen:</p>
<ul>
<li><p><strong>torch.nn</strong></p>
<ul>
<li><code>Module</code>: creates a callable which behaves like a function, but can also<br>contain state(such as neural net layer weights). It knows what <code>Parameter</code> (s) it<br>contains and can zero all their gradients, loop through them for weight updates, etc.</li>
<li><code>Parameter</code>: a wrapper for a tensor that tells a <code>Module</code> that it has weights<br>that need updating during backprop. Only tensors with the <code>requires_grad</code> attribute set are updated</li>
<li><code>functional</code>: a module(usually imported into the <code>F</code> namespace by convention)<br>which contains activation functions, loss functions, etc, as well as non-stateful<br>versions of layers such as convolutional and linear layers.</li>
</ul>
</li>
<li><p><code>torch.optim</code>: Contains optimizers such as <code>SGD</code>, which update the weights<br>of <code>Parameter</code> during the backward step</p>
</li>
<li><p><code>Dataset</code>: An abstract interface of objects with a <code>__len__</code> and a <code>__getitem__</code>,<br>including classes provided with Pytorch such as <code>TensorDataset</code></p>
</li>
<li><p><code>DataLoader</code>: Takes any <code>Dataset</code> and creates an iterator which returns batches of data.</p>
</li>
</ul>
<h2 id="我不会的单词"><a href="#我不会的单词" class="headerlink" title="我不会的单词"></a>我不会的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">utilize:利用</span><br><span class="line">customize:定制</span><br><span class="line">refactor:重构</span><br><span class="line">identical:相同的</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-autograd</title>
    <url>/2020/07/23/Pytorch-Learning-autograd/</url>
    <content><![CDATA[<p>Pytorch-Learning-autograd:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px">Pytorch1.5.1官网教程目录</div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1 id="Autograd-Automatic-Differentiation"><a href="#Autograd-Automatic-Differentiation" class="headerlink" title="Autograd: Automatic Differentiation"></a>Autograd: Automatic Differentiation</h1><p>Central to all neural networks in PyTorch is the <code>autograd</code> package.<br>Let’s first briefly visit this, and we will then go to training our<br>first neural network.</p>
<p>The <code>autograd</code> package provides automatic differentiation for all operations<br>on Tensors. It is a define-by-run framework, which means that your backprop is<br>defined by how your code is run, and that every single iteration can be<br>different.</p>
<p>Let us see this in more simple terms with some examples.</p>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><code>torch.Tensor</code> is the central class of the package. If you set its attribute<br><code>.requires_grad</code> as <code>True</code>, it starts to track all operations on it. When<br>you finish your computation you can call <code>.backward()</code> and have all the<br>gradients computed automatically. The gradient for this tensor will be<br>accumulated into <code>.grad</code> attribute.</p>
<p>To stop a tensor from tracking history, you can call <code>.detach()</code> to detach<br>it from the computation history, and to prevent future computation from being<br>tracked.</p>
<p>To prevent tracking history (and using memory), you can also wrap the code block<br>in <code>with torch.no_grad():</code>. This can be particularly helpful when evaluating a<br>model because the model may have trainable parameters with<br><code>requires_grad=True</code>, but for which we don’t need the gradients.</p>
<p>There’s one more class which is very important for autograd<br>implementation - a <code>Function</code>.</p>
<p><code>Tensor</code> and <code>Function</code> are interconnected and build up an acyclic<br>graph, that encodes a complete history of computation. Each tensor has<br>a <code>.grad_fn</code> attribute that references a <code>Function</code> that has created<br>the <code>Tensor</code> (except for Tensors created by the user - their<br><code>grad_fn is None</code>).</p>
<p>If you want to compute the derivatives, you can call <code>.backward()</code> on<br>a <code>Tensor</code>. If <code>Tensor</code> is a scalar (i.e. it holds a one element<br>data), you don’t need to specify any arguments to <code>backward()</code>,<br>however if it has more elements, you need to specify a <code>gradient</code><br>argument that is a tensor of matching shape.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>Create a tensor and set <code>requires_grad=True</code> to track computation with it</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
</code></pre><p>Do a tensor operation:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
</code></pre><p><code>y</code> was created as a result of an operation, so it has a <code>grad_fn</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;AddBackward0 object at 0x0000015370CAB438&gt;
</code></pre><p>Do more operations on <code>y</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)
</code></pre><p><code>.requires_grad_( ... )</code> changes an existing Tensor’s <code>requires_grad</code><br>flag in-place. The input flag defaults to <code>False</code> if not given.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>False
True
&lt;SumBackward0 object at 0x000001536D5A24E0&gt;
</code></pre><h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>Let’s backprop now.<br>Because <code>out</code> contains a single scalar, <code>out.backward()</code> is<br>equivalent to <code>out.backward(torch.tensor(1.))</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>
<p>Print gradients d(out)/dx</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
</code></pre><p>You should have got a matrix of <code>4.5</code>. Let’s call the <code>out</code><br><em>Tensor</em> “$o$”.<br>We have that $o = \frac{1}{4}\sum_i z_i$,<br>$z_i = 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$.<br>Therefore,<br>$\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)$, hence<br>$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$.</p>
<p>Mathematically, if you have a vector valued function $\vec{y}=f(\vec{x})$,<br>then the gradient of $\vec{y}$ with respect to $\vec{x}$<br>is a Jacobian matrix:</p>
<p>\begin{align}J=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\<br>   \vdots &amp; \ddots &amp; \vdots\\<br>   \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p>
<p>Generally speaking, <code>torch.autograd</code> is an engine for computing<br>vector-Jacobian product. That is, given any vector<br>$v=\left(\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m}\end{array}\right)^{T}$,<br>compute the product $v^{T}\cdot J$. If $v$ happens to be<br>the gradient of a scalar function $l=g\left(\vec{y}\right)$,<br>that is,<br>$v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$,<br>then by the chain rule, the vector-Jacobian product would be the<br>gradient of $l$ with respect to $\vec{x}$:</p>
<p>\begin{align}J^{T}\cdot v=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}\\<br>   \vdots &amp; \ddots &amp; \vdots\\<br>   \frac{\partial y_{1}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\left(\begin{array}{c}<br>   \frac{\partial l}{\partial y_{1}}\\<br>   \vdots\\<br>   \frac{\partial l}{\partial y_{m}}<br>   \end{array}\right)=\left(\begin{array}{c}<br>   \frac{\partial l}{\partial x_{1}}\\<br>   \vdots\\<br>   \frac{\partial l}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p>
<p>(Note that $v^{T}\cdot J$ gives a row vector which can be<br>treated as a column vector by taking $J^{T}\cdot v$.)</p>
<p>This characteristic of vector-Jacobian product makes it very<br>convenient to feed external gradients into a model that has<br>non-scalar output.</p>
<p>Now let’s take a look at an example of vector-Jacobian product:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([849.9269, 245.4694, 633.8903], grad_fn=&lt;MulBackward0&gt;)
</code></pre><p>Now in this case <code>y</code> is no longer a scalar. <code>torch.autograd</code><br>could not compute the full Jacobian directly, but if we just<br>want the vector-Jacobian product, simply pass the vector to<br><code>backward</code> as argument:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])
</code></pre><p>You can also stop autograd from tracking history on Tensors<br>with <code>.requires_grad=True</code> either by wrapping the code block in<br><code>with torch.no_grad():</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">	print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>
<pre><code>True
True
False
</code></pre><p>Or by using <code>.detach()</code> to get a new Tensor with the same<br>content but that does not require gradients:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">y = x.detach()</span><br><span class="line">print(y.requires_grad)</span><br><span class="line">print(x.eq(y).all())</span><br></pre></td></tr></table></figure>
<pre><code>True
False
tensor(1, dtype=torch.uint8)
</code></pre><p><strong>Read Later:</strong></p>
<p>Document about <code>autograd.Function</code> is at<br><a href="https://pytorch.org/docs/stable/autograd.html#function" target="_blank" rel="noopener">https://pytorch.org/docs/stable/autograd.html#function</a></p>
<h2 id="不认识的单词"><a href="#不认识的单词" class="headerlink" title="不认识的单词"></a>不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Automatic:自动的</span><br><span class="line">briefly:短暂的</span><br><span class="line">track:跟踪</span><br><span class="line">specify:指明</span><br><span class="line">accumulate:累加</span><br><span class="line">particularly:尤其,格外的</span><br><span class="line">interconnect:互相连接</span><br><span class="line">acyclic:非循环的,无环的</span><br><span class="line">Mathematically:数学上</span><br><span class="line">Jacobian:雅可比</span><br><span class="line">scalar:标量</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-DCGAN教程</title>
    <url>/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>Pytorch-Image-DCGAN教程:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="DCGAN-Tutorial"><a href="#DCGAN-Tutorial" class="headerlink" title="DCGAN Tutorial"></a>DCGAN Tutorial</h1><p><strong>Author</strong>: <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This tutorial will give an introduction to DCGANs through an example. We<br>will train a generative adversarial network (GAN) to generate new<br>celebrities after showing it pictures of many real celebrities. Most of<br>the code here is from the dcgan implementation in<br><code>pytorch/examples &lt;https://github.com/pytorch/examples&gt;</code>__, and this<br>document will give a thorough explanation of the implementation and shed<br>light on how and why this model works. But don’t worry, no prior<br>knowledge of GANs is required, but it may require a first-timer to spend<br>some time reasoning about what is actually happening under the hood.<br>Also, for the sake of time it will help to have a GPU, or two. Lets<br>start from the beginning.</p>
<h2 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h2><p>What is a GAN?</p>
<p>GANs are a framework for teaching a DL model to capture the training<br>data’s distribution so we can generate new data from that same<br>distribution. GANs were invented by Ian Goodfellow in 2014 and first<br>described in the paper <code>Generative Adversarial
Nets &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__.<br>They are made of two distinct models, a <em>generator</em> and a<br><em>discriminator</em>. The job of the generator is to spawn ‘fake’ images that<br>look like the training images. The job of the discriminator is to look<br>at an image and output whether or not it is a real training image or a<br>fake image from the generator. During training, the generator is<br>constantly trying to outsmart the discriminator by generating better and<br>better fakes, while the discriminator is working to become a better<br>detective and correctly classify the real and fake images. The<br>equilibrium of this game is when the generator is generating perfect<br>fakes that look as if they came directly from the training data, and the<br>discriminator is left to always guess at 50% confidence that the<br>generator output is real or fake.</p>
<p>Now, lets define some notation to be used throughout tutorial starting<br>with the discriminator. Let $x$ be data representing an image.<br>$D(x)$ is the discriminator network which outputs the (scalar)<br>probability that $x$ came from training data rather than the<br>generator. Here, since we are dealing with images the input to<br>$D(x)$ is an image of CHW size 3x64x64. Intuitively, $D(x)$<br>should be HIGH when $x$ comes from training data and LOW when<br>$x$ comes from the generator. $D(x)$ can also be thought of<br>as a traditional binary classifier.</p>
<p>For the generator’s notation, let $z$ be a latent space vector<br>sampled from a standard normal distribution. $G(z)$ represents the<br>generator function which maps the latent vector $z$ to data-space.<br>The goal of $G$ is to estimate the distribution that the training<br>data comes from ($p_{data}$) so it can generate fake samples from<br>that estimated distribution ($p_g$).</p>
<p>So, $D(G(z))$ is the probability (scalar) that the output of the<br>generator $G$ is a real image. As described in <code>Goodfellow’s
paper &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__,<br>$D$ and $G$ play a minimax game in which $D$ tries to<br>maximize the probability it correctly classifies reals and fakes<br>($logD(x)$), and $G$ tries to minimize the probability that<br>$D$ will predict its outputs are fake ($log(1-D(G(x)))$).<br>From the paper, the GAN loss function is</p>
<p>$\begin{align}\underset{G}{\text{min}} \underset{D}{\text{max}}V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}\big[logD(x)\big] + \mathbb{E}_{z\sim p_{z}(z)}\big[log(1-D(G(z)))\big]\end{align}$</p>
<p>In theory, the solution to this minimax game is where<br>$p_g = p_{data}$, and the discriminator guesses randomly if the<br>inputs are real or fake. However, the convergence theory of GANs is<br>still being actively researched and in reality models do not always<br>train to this point.</p>
<p>What is a DCGAN?</p>
<p>A DCGAN is a direct extension of the GAN described above, except that it<br>explicitly uses convolutional and convolutional-transpose layers in the<br>discriminator and generator, respectively. It was first described by<br>Radford et. al. in the paper <code>Unsupervised Representation Learning With
Deep Convolutional Generative Adversarial
Networks &lt;https://arxiv.org/pdf/1511.06434.pdf&gt;</code>. The discriminator<br>is made up of strided<br><code>convolution &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code><br>layers, <code>batch
norm &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d&gt;</code><br>layers, and<br><code>LeakyReLU &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU&gt;</code><br>activations. The input is a 3x64x64 input image and the output is a<br>scalar probability that the input is from the real data distribution.<br>The generator is comprised of<br><code>convolutional-transpose &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d&gt;</code><br>layers, batch norm layers, and<br><code>ReLU &lt;https://pytorch.org/docs/stable/nn.html#relu&gt;</code>__ activations. The<br>input is a latent vector, $z$, that is drawn from a standard<br>normal distribution and the output is a 3x64x64 RGB image. The strided<br>conv-transpose layers allow the latent vector to be transformed into a<br>volume with the same shape as an image. In the paper, the authors also<br>give some tips about how to setup the optimizers, how to calculate the<br>loss functions, and how to initialize the model weights, all of which<br>will be explained in the coming sections.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="comment">#%matplotlib inline</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed for reproducibility</span></span><br><span class="line">manualSeed = <span class="number">999</span></span><br><span class="line"><span class="comment">#manualSeed = random.randint(1, 10000) # use if you want new results</span></span><br><span class="line">print(<span class="string">"Random Seed: "</span>, manualSeed)</span><br><span class="line">random.seed(manualSeed)</span><br><span class="line">torch.manual_seed(manualSeed)</span><br></pre></td></tr></table></figure>
<pre><code>Random Seed:  999





&lt;torch._C.Generator at 0x268c24f1e50&gt;
</code></pre><h2 id="Inputs"><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h2><p>Let’s define some inputs for the run:</p>
<ul>
<li><strong>dataroot</strong> - the path to the root of the dataset folder. We will<br>talk more about the dataset in the next section</li>
<li><strong>workers</strong> - the number of worker threads for loading the data with<br>the DataLoader</li>
<li><strong>batch_size</strong> - the batch size used in training. The DCGAN paper<br>uses a batch size of 128</li>
<li><strong>image_size</strong> - the spatial size of the images used for training.<br>This implementation defaults to 64x64. If another size is desired,<br>the structures of D and G must be changed. See<br><code>here &lt;https://github.com/pytorch/examples/issues/70&gt;</code>__ for more<br>details</li>
<li><strong>nc</strong> - number of color channels in the input images. For color<br>images this is 3</li>
<li><strong>nz</strong> - length of latent vector</li>
<li><strong>ngf</strong> - relates to the depth of feature maps carried through the<br>generator</li>
<li><strong>ndf</strong> - sets the depth of feature maps propagated through the<br>discriminator</li>
<li><strong>num_epochs</strong> - number of training epochs to run. Training for<br>longer will probably lead to better results but will also take much<br>longer</li>
<li><strong>lr</strong> - learning rate for training. As described in the DCGAN paper,<br>this number should be 0.0002</li>
<li><strong>beta1</strong> - beta1 hyperparameter for Adam optimizers. As described in<br>paper, this number should be 0.5</li>
<li><strong>ngpu</strong> - number of GPUs available. If this is 0, code will run in<br>CPU mode. If this number is greater than 0 it will run on that number<br>of GPUs</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Root directory for dataset</span></span><br><span class="line">dataroot = <span class="string">"data/celeba"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of workers for dataloader</span></span><br><span class="line">workers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch size during training</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spatial size of training images. All images will be resized to this</span></span><br><span class="line"><span class="comment">#   size using a transformer.</span></span><br><span class="line">image_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of channels in the training images. For color images this is 3</span></span><br><span class="line">nc = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of z latent vector (i.e. size of generator input)</span></span><br><span class="line">nz = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in generator</span></span><br><span class="line">ngf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in discriminator</span></span><br><span class="line">ndf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of training epochs</span></span><br><span class="line"><span class="comment"># num_epochs = 5</span></span><br><span class="line">num_epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate for optimizers</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Beta1 hyperparam for Adam optimizers</span></span><br><span class="line">beta1 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of GPUs available. Use 0 for CPU mode.</span></span><br><span class="line">ngpu = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>In this tutorial we will use the <code>Celeb-A Faces
dataset &lt;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&gt;</code><strong> which can<br>be downloaded at the linked site, or in <code>Google
Drive &lt;https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg&gt;</code></strong>.<br>The dataset will download as a file named <em>img_align_celeba.zip</em>. Once<br>downloaded, create a directory named <em>celeba</em> and extract the zip file<br>into that directory. Then, set the <em>dataroot</em> input for this notebook to<br>the <em>celeba</em> directory you just created. The resulting directory<br>structure should be:</p>
<p>::</p>
<p>   /path/to/celeba<br>       -&gt; img_align_celeba<br>           -&gt; 188242.jpg<br>           -&gt; 173822.jpg<br>           -&gt; 284702.jpg<br>           -&gt; 537394.jpg<br>              …</p>
<p>This is an important step because we will be using the ImageFolder<br>dataset class, which requires there to be subdirectories in the<br>dataset’s root folder. Now, we can create the dataset, create the<br>dataloader, set the device to run on, and finally visualize some of the<br>training data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We can use an image folder dataset the way we have it setup.</span></span><br><span class="line"><span class="comment"># Create the dataset</span></span><br><span class="line">dataset = dset.ImageFolder(root=dataroot,</span><br><span class="line">                           transform=transforms.Compose([</span><br><span class="line">                               transforms.Resize(image_size),</span><br><span class="line">                               transforms.CenterCrop(image_size),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">                           ]))</span><br><span class="line"><span class="comment"># Create the dataloader</span></span><br><span class="line">dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>, num_workers=workers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot some training images</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Training Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">2</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x268c49247f0&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/output_7_1.png" alt="png"></p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>With our input parameters set and the dataset prepared, we can now get<br>into the implementation. We will start with the weigth initialization<br>strategy, then talk about the generator, discriminator, loss functions,<br>and training loop in detail.</p>
<p>Weight Initialization</p>
<p>From the DCGAN paper, the authors specify that all model weights shall<br>be randomly initialized from a Normal distribution with mean=0,<br>stdev=0.02. The <code>weights_init</code> function takes an initialized model as<br>input and reinitializes all convolutional, convolutional-transpose, and<br>batch normalization layers to meet this criteria. This function is<br>applied to the models immediately after initialization.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># custom weights initialization called on netG and netD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        nn.init.constant_(m.bias.data, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>Generator</p>
<p>The generator, $G$, is designed to map the latent space vector<br>($z$) to data-space. Since our data are images, converting<br>$z$ to data-space means ultimately creating a RGB image with the<br>same size as the training images (i.e. 3x64x64). In practice, this is<br>accomplished through a series of strided two dimensional convolutional<br>transpose layers, each paired with a 2d batch norm layer and a relu<br>activation. The output of the generator is fed through a tanh function<br>to return it to the input data range of $[-1,1]$. It is worth<br>noting the existence of the batch norm functions after the<br>conv-transpose layers, as this is a critical contribution of the DCGAN<br>paper. These layers help with the flow of gradients during training. An<br>image of the generator from the DCGAN paper is shown below.</p>
<p>.. figure:: /_static/img/dcgan_generator.png<br>   :alt: dcgan_generator</p>
<p>Notice, the how the inputs we set in the input section (<em>nz</em>, <em>ngf</em>, and<br><em>nc</em>) influence the generator architecture in code. <em>nz</em> is the length<br>of the z input vector, <em>ngf</em> relates to the size of the feature maps<br>that are propagated through the generator, and <em>nc</em> is the number of<br>channels in the output image (set to 3 for RGB images). Below is the<br>code for the generator.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generator Code</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is Z, going into a convolution</span></span><br><span class="line">            nn.ConvTranspose2d( nz, ngf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">8</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*8) x 4 x 4</span></span><br><span class="line">            nn.ConvTranspose2d(ngf * <span class="number">8</span>, ngf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">4</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*4) x 8 x 8</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">4</span>, ngf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*2) x 16 x 16</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">2</span>, ngf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf) x 32 x 32</span></span><br><span class="line">            nn.ConvTranspose2d( ngf, nc, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">            <span class="comment"># state size. (nc) x 64 x 64</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure>
<p>Now, we can instantiate the generator and apply the <code>weights_init</code><br>function. Check out the printed model to see how the generator object is<br>structured.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the generator</span></span><br><span class="line">netG = Generator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netG = nn.DataParallel(netG, list(range(ngpu)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netG.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netG)</span><br></pre></td></tr></table></figure>
<pre><code>Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
</code></pre><p>Discriminator</p>
<p>As mentioned, the discriminator, $D$, is a binary classification<br>network that takes an image as input and outputs a scalar probability<br>that the input image is real (as opposed to fake). Here, $D$ takes<br>a 3x64x64 input image, processes it through a series of Conv2d,<br>BatchNorm2d, and LeakyReLU layers, and outputs the final probability<br>through a Sigmoid activation function. This architecture can be extended<br>with more layers if necessary for the problem, but there is significance<br>to the use of the strided convolution, BatchNorm, and LeakyReLUs. The<br>DCGAN paper mentions it is a good practice to use strided convolution<br>rather than pooling to downsample because it lets the network learn its<br>own pooling function. Also batch norm and leaky relu functions promote<br>healthy gradient flow which is critical for the learning process of both<br>$G$ and $D$.</p>
<p>Discriminator Code</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is (nc) x 64 x 64</span></span><br><span class="line">            nn.Conv2d(nc, ndf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf) x 32 x 32</span></span><br><span class="line">            nn.Conv2d(ndf, ndf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">2</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*2) x 16 x 16</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">2</span>, ndf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">4</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*4) x 8 x 8</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">4</span>, ndf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">8</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*8) x 4 x 4</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">8</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure>
<p>Now, as with the generator, we can create the discriminator, apply the<br><code>weights_init</code> function, and print the model’s structure.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the Discriminator</span></span><br><span class="line">netD = Discriminator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netD = nn.DataParallel(netD, list(range(ngpu)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netD.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netD)</span><br></pre></td></tr></table></figure>
<pre><code>Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (12): Sigmoid()
  )
)
</code></pre><p>Loss Functions and Optimizers</p>
<p>With $D$ and $G$ setup, we can specify how they learn<br>through the loss functions and optimizers. We will use the Binary Cross<br>Entropy loss<br>(<code>BCELoss &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss&gt;</code>__)<br>function which is defined in PyTorch as:</p>
<p>\begin{align}\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]\end{align}</p>
<p>Notice how this function provides the calculation of both log components<br>in the objective function (i.e. $log(D(x))$ and<br>$log(1-D(G(z)))$). We can specify what part of the BCE equation to<br>use with the $y$ input. This is accomplished in the training loop<br>which is coming up soon, but it is important to understand how we can<br>choose which component we wish to calculate just by changing $y$<br>(i.e. GT labels).</p>
<p>Next, we define our real label as 1 and the fake label as 0. These<br>labels will be used when calculating the losses of $D$ and<br>$G$, and this is also the convention used in the original GAN<br>paper. Finally, we set up two separate optimizers, one for $D$ and<br>one for $G$. As specified in the DCGAN paper, both are Adam<br>optimizers with learning rate 0.0002 and Beta1 = 0.5. For keeping track<br>of the generator’s learning progression, we will generate a fixed batch<br>of latent vectors that are drawn from a Gaussian distribution<br>(i.e. fixed_noise) . In the training loop, we will periodically input<br>this fixed_noise into $G$, and over the iterations we will see<br>images form out of the noise.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize BCELoss function</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create batch of latent vectors that we will use to visualize</span></span><br><span class="line"><span class="comment">#  the progression of the generator</span></span><br><span class="line">fixed_noise = torch.randn(<span class="number">64</span>, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Establish convention for real and fake labels during training</span></span><br><span class="line">real_label = <span class="number">1</span></span><br><span class="line">fake_label = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup Adam optimizers for both G and D</span></span><br><span class="line">optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure>
<p>Training</p>
<p>Finally, now that we have all of the parts of the GAN framework defined,<br>we can train it. Be mindful that training GANs is somewhat of an art<br>form, as incorrect hyperparameter settings lead to mode collapse with<br>little explanation of what went wrong. Here, we will closely follow<br>Algorithm 1 from Goodfellow’s paper, while abiding by some of the best<br>practices shown in <code>ganhacks &lt;https://github.com/soumith/ganhacks&gt;</code>__.<br>Namely, we will “construct different mini-batches for real and fake”<br>images, and also adjust G’s objective function to maximize<br>$logD(G(z))$. Training is split up into two main parts. Part 1<br>updates the Discriminator and Part 2 updates the Generator.</p>
<p><strong>Part 1 - Train the Discriminator</strong></p>
<p>Recall, the goal of training the discriminator is to maximize the<br>probability of correctly classifying a given input as real or fake. In<br>terms of Goodfellow, we wish to “update the discriminator by ascending<br>its stochastic gradient”. Practically, we want to maximize<br>$log(D(x)) + log(1-D(G(z)))$. Due to the separate mini-batch<br>suggestion from ganhacks, we will calculate this in two steps. First, we<br>will construct a batch of real samples from the training set, forward<br>pass through $D$, calculate the loss ($log(D(x))$), then<br>calculate the gradients in a backward pass. Secondly, we will construct<br>a batch of fake samples with the current generator, forward pass this<br>batch through $D$, calculate the loss ($log(1-D(G(z)))$),<br>and <em>accumulate</em> the gradients with a backward pass. Now, with the<br>gradients accumulated from both the all-real and all-fake batches, we<br>call a step of the Discriminator’s optimizer.</p>
<p><strong>Part 2 - Train the Generator</strong></p>
<p>As stated in the original paper, we want to train the Generator by<br>minimizing $log(1-D(G(z)))$ in an effort to generate better fakes.<br>As mentioned, this was shown by Goodfellow to not provide sufficient<br>gradients, especially early in the learning process. As a fix, we<br>instead wish to maximize $log(D(G(z)))$. In the code we accomplish<br>this by: classifying the Generator output from Part 1 with the<br>Discriminator, computing G’s loss <em>using real labels as GT</em>, computing<br>G’s gradients in a backward pass, and finally updating G’s parameters<br>with an optimizer step. It may seem counter-intuitive to use the real<br>labels as GT labels for the loss function, but this allows us to use the<br>$log(x)$ part of the BCELoss (rather than the $log(1-x)$<br>part) which is exactly what we want.</p>
<p>Finally, we will do some statistic reporting and at the end of each<br>epoch we will push our fixed_noise batch through the generator to<br>visually track the progress of G’s training. The training statistics<br>reported are:</p>
<ul>
<li><strong>Loss_D</strong> - discriminator loss calculated as the sum of losses for<br>the all real and all fake batches ($log(D(x)) + log(D(G(z)))$).</li>
<li><strong>Loss_G</strong> - generator loss calculated as $log(D(G(z)))$</li>
<li><strong>D(x)</strong> - the average output (across the batch) of the discriminator<br>for the all real batch. This should start close to 1 then<br>theoretically converge to 0.5 when G gets better. Think about why<br>this is.</li>
<li><strong>D(G(z))</strong> - average discriminator outputs for the all fake batch.<br>The first number is before D is updated and the second number is<br>after D is updated. These numbers should start near 0 and converge to<br>0.5 as G gets better. Think about why this is.</li>
</ul>
<p><strong>Note:</strong> This step might take a while, depending on how many epochs you<br>run and if you removed some data from the dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Training Loop</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Lists to keep track of progress</span></span><br><span class="line">img_list = []</span><br><span class="line">G_losses = []</span><br><span class="line">D_losses = []</span><br><span class="line">iters = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Starting Training Loop..."</span>)</span><br><span class="line"><span class="comment"># For each epoch</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># For each batch in the dataloader</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(dataloader, <span class="number">0</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        <span class="comment">## Train with all-real batch</span></span><br><span class="line">        netD.zero_grad()</span><br><span class="line">        <span class="comment"># Format batch</span></span><br><span class="line">        real_cpu = data[<span class="number">0</span>].to(device)</span><br><span class="line">        b_size = real_cpu.size(<span class="number">0</span>)</span><br><span class="line">        label = torch.full((b_size,), real_label, device=device)</span><br><span class="line">        <span class="comment"># Forward pass real batch through D</span></span><br><span class="line">        output = netD(real_cpu).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate loss on all-real batch</span></span><br><span class="line">        errD_real = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for D in backward pass</span></span><br><span class="line">        errD_real.backward()</span><br><span class="line">        D_x = output.mean().item()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Train with all-fake batch</span></span><br><span class="line">        <span class="comment"># Generate batch of latent vectors</span></span><br><span class="line">        noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        <span class="comment"># Generate fake image batch with G</span></span><br><span class="line">        fake = netG(noise)</span><br><span class="line">        label.fill_(fake_label)</span><br><span class="line">        <span class="comment"># Classify all fake batch with D</span></span><br><span class="line">        output = netD(fake.detach()).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate D's loss on the all-fake batch</span></span><br><span class="line">        errD_fake = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate the gradients for this batch</span></span><br><span class="line">        errD_fake.backward()</span><br><span class="line">        D_G_z1 = output.mean().item()</span><br><span class="line">        <span class="comment"># Add the gradients from the all-real and all-fake batches</span></span><br><span class="line">        errD = errD_real + errD_fake</span><br><span class="line">        <span class="comment"># Update D</span></span><br><span class="line">        optimizerD.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (2) Update G network: maximize log(D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        label.fill_(real_label)  <span class="comment"># fake labels are real for generator cost</span></span><br><span class="line">        <span class="comment"># Since we just updated D, perform another forward pass of all-fake batch through D</span></span><br><span class="line">        output = netD(fake).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate G's loss based on this output</span></span><br><span class="line">        errG = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for G</span></span><br><span class="line">        errG.backward()</span><br><span class="line">        D_G_z2 = output.mean().item()</span><br><span class="line">        <span class="comment"># Update G</span></span><br><span class="line">        optimizerG.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Output training stats</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'</span></span><br><span class="line">                  % (epoch, num_epochs, i, len(dataloader),</span><br><span class="line">                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Save Losses for plotting later</span></span><br><span class="line">        G_losses.append(errG.item())</span><br><span class="line">        D_losses.append(errD.item())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check how the generator is doing by saving G's output on fixed_noise</span></span><br><span class="line">        <span class="keyword">if</span> (iters % <span class="number">500</span> == <span class="number">0</span>) <span class="keyword">or</span> ((epoch == num_epochs<span class="number">-1</span>) <span class="keyword">and</span> (i == len(dataloader)<span class="number">-1</span>)):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = netG(fixed_noise).detach().cpu()</span><br><span class="line">            img_list.append(vutils.make_grid(fake, padding=<span class="number">2</span>, normalize=<span class="literal">True</span>))</span><br><span class="line">            </span><br><span class="line">        iters += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>Starting Training Loop...


..\aten\src\ATen\native\TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.


[0/2][0/1583]    Loss_D: 1.8664    Loss_G: 4.9949    D(x): 0.5050    D(G(z)): 0.5928 / 0.0106
[0/2][50/1583]    Loss_D: 0.1046    Loss_G: 7.1177    D(x): 0.9758    D(G(z)): 0.0124 / 0.0086
[0/2][100/1583]    Loss_D: 1.0915    Loss_G: 12.3498    D(x): 0.9553    D(G(z)): 0.4722 / 0.0000
[0/2][150/1583]    Loss_D: 1.7593    Loss_G: 5.9544    D(x): 0.3933    D(G(z)): 0.0053 / 0.0070
[0/2][200/1583]    Loss_D: 0.8020    Loss_G: 5.9020    D(x): 0.6117    D(G(z)): 0.0295 / 0.0069
[0/2][250/1583]    Loss_D: 0.6124    Loss_G: 4.1050    D(x): 0.7546    D(G(z)): 0.1926 / 0.0268
[0/2][300/1583]    Loss_D: 0.9607    Loss_G: 1.8866    D(x): 0.5101    D(G(z)): 0.0519 / 0.2133
[0/2][350/1583]    Loss_D: 0.5478    Loss_G: 3.3292    D(x): 0.7691    D(G(z)): 0.1973 / 0.0492
[0/2][400/1583]    Loss_D: 1.2509    Loss_G: 1.3775    D(x): 0.4456    D(G(z)): 0.1254 / 0.2993
[0/2][450/1583]    Loss_D: 0.4384    Loss_G: 5.5121    D(x): 0.7491    D(G(z)): 0.0298 / 0.0068
[0/2][500/1583]    Loss_D: 0.4170    Loss_G: 3.6389    D(x): 0.7676    D(G(z)): 0.0610 / 0.0396
[0/2][550/1583]    Loss_D: 0.5595    Loss_G: 4.2165    D(x): 0.8009    D(G(z)): 0.1784 / 0.0296
[0/2][600/1583]    Loss_D: 1.2295    Loss_G: 3.3066    D(x): 0.5244    D(G(z)): 0.0907 / 0.0752
[0/2][650/1583]    Loss_D: 0.5091    Loss_G: 4.7135    D(x): 0.7118    D(G(z)): 0.0292 / 0.0203
[0/2][700/1583]    Loss_D: 0.3912    Loss_G: 2.7194    D(x): 0.8198    D(G(z)): 0.1221 / 0.1098
[0/2][750/1583]    Loss_D: 0.7578    Loss_G: 7.3482    D(x): 0.9471    D(G(z)): 0.4270 / 0.0015
[0/2][800/1583]    Loss_D: 0.7080    Loss_G: 5.8282    D(x): 0.9395    D(G(z)): 0.4118 / 0.0053
[0/2][850/1583]    Loss_D: 0.5755    Loss_G: 3.2657    D(x): 0.6967    D(G(z)): 0.0784 / 0.0637
[0/2][900/1583]    Loss_D: 1.0720    Loss_G: 6.0690    D(x): 0.8734    D(G(z)): 0.5405 / 0.0047
[0/2][950/1583]    Loss_D: 0.6901    Loss_G: 5.7612    D(x): 0.9313    D(G(z)): 0.3993 / 0.0064
[0/2][1000/1583]    Loss_D: 0.2473    Loss_G: 5.5474    D(x): 0.8535    D(G(z)): 0.0574 / 0.0123
[0/2][1050/1583]    Loss_D: 0.6581    Loss_G: 5.6885    D(x): 0.9537    D(G(z)): 0.4081 / 0.0069
[0/2][1100/1583]    Loss_D: 0.5447    Loss_G: 2.8289    D(x): 0.7523    D(G(z)): 0.1252 / 0.0966
[0/2][1150/1583]    Loss_D: 0.3936    Loss_G: 3.4201    D(x): 0.8383    D(G(z)): 0.1639 / 0.0528
[0/2][1200/1583]    Loss_D: 0.7676    Loss_G: 5.3321    D(x): 0.8581    D(G(z)): 0.3643 / 0.0112
[0/2][1250/1583]    Loss_D: 0.4362    Loss_G: 3.9937    D(x): 0.8223    D(G(z)): 0.1671 / 0.0321
[0/2][1300/1583]    Loss_D: 0.6602    Loss_G: 5.5204    D(x): 0.9106    D(G(z)): 0.3806 / 0.0072
[0/2][1350/1583]    Loss_D: 0.6352    Loss_G: 3.2255    D(x): 0.7404    D(G(z)): 0.1788 / 0.0708
[0/2][1400/1583]    Loss_D: 0.7936    Loss_G: 3.5125    D(x): 0.7248    D(G(z)): 0.2855 / 0.0474
[0/2][1450/1583]    Loss_D: 1.0550    Loss_G: 1.2678    D(x): 0.7036    D(G(z)): 0.3793 / 0.3957
[0/2][1500/1583]    Loss_D: 0.4235    Loss_G: 2.8506    D(x): 0.7756    D(G(z)): 0.1001 / 0.0801
[0/2][1550/1583]    Loss_D: 0.4839    Loss_G: 4.1835    D(x): 0.8839    D(G(z)): 0.2465 / 0.0254
[1/2][0/1583]    Loss_D: 0.7391    Loss_G: 3.9735    D(x): 0.8016    D(G(z)): 0.3263 / 0.0284
[1/2][50/1583]    Loss_D: 0.5051    Loss_G: 3.8654    D(x): 0.7839    D(G(z)): 0.1756 / 0.0337
[1/2][100/1583]    Loss_D: 0.4857    Loss_G: 4.5489    D(x): 0.8677    D(G(z)): 0.2420 / 0.0209
[1/2][150/1583]    Loss_D: 0.6025    Loss_G: 4.4404    D(x): 0.8519    D(G(z)): 0.2920 / 0.0212
[1/2][200/1583]    Loss_D: 0.4301    Loss_G: 4.4767    D(x): 0.8909    D(G(z)): 0.2399 / 0.0190
[1/2][250/1583]    Loss_D: 1.2600    Loss_G: 7.6782    D(x): 0.9744    D(G(z)): 0.6415 / 0.0013
[1/2][300/1583]    Loss_D: 0.5044    Loss_G: 3.7002    D(x): 0.8408    D(G(z)): 0.2446 / 0.0375
[1/2][350/1583]    Loss_D: 0.4184    Loss_G: 3.2221    D(x): 0.7736    D(G(z)): 0.0924 / 0.0649
[1/2][400/1583]    Loss_D: 0.5320    Loss_G: 4.6695    D(x): 0.9051    D(G(z)): 0.3072 / 0.0150
[1/2][450/1583]    Loss_D: 0.3804    Loss_G: 3.3363    D(x): 0.7888    D(G(z)): 0.0978 / 0.0636
[1/2][500/1583]    Loss_D: 0.4293    Loss_G: 4.2911    D(x): 0.9014    D(G(z)): 0.2399 / 0.0226
[1/2][550/1583]    Loss_D: 0.3940    Loss_G: 2.7648    D(x): 0.7634    D(G(z)): 0.0777 / 0.0929
[1/2][600/1583]    Loss_D: 0.4044    Loss_G: 3.3666    D(x): 0.8438    D(G(z)): 0.1664 / 0.0598
[1/2][650/1583]    Loss_D: 0.3879    Loss_G: 3.4838    D(x): 0.8517    D(G(z)): 0.1754 / 0.0455
[1/2][700/1583]    Loss_D: 0.4487    Loss_G: 3.6364    D(x): 0.8773    D(G(z)): 0.2434 / 0.0370
[1/2][750/1583]    Loss_D: 0.7588    Loss_G: 2.0882    D(x): 0.6144    D(G(z)): 0.1150 / 0.1773
[1/2][800/1583]    Loss_D: 0.6134    Loss_G: 4.0046    D(x): 0.9100    D(G(z)): 0.3546 / 0.0278
[1/2][850/1583]    Loss_D: 0.5061    Loss_G: 2.2267    D(x): 0.7046    D(G(z)): 0.0860 / 0.1488
[1/2][900/1583]    Loss_D: 0.6032    Loss_G: 1.8834    D(x): 0.6518    D(G(z)): 0.0847 / 0.2023
[1/2][950/1583]    Loss_D: 1.1199    Loss_G: 2.2135    D(x): 0.4166    D(G(z)): 0.0332 / 0.1791
[1/2][1000/1583]    Loss_D: 0.8061    Loss_G: 2.2557    D(x): 0.5479    D(G(z)): 0.0442 / 0.1506
[1/2][1050/1583]    Loss_D: 0.7723    Loss_G: 2.7941    D(x): 0.5652    D(G(z)): 0.0532 / 0.0968
[1/2][1100/1583]    Loss_D: 0.6160    Loss_G: 1.4266    D(x): 0.6152    D(G(z)): 0.0460 / 0.2874
[1/2][1150/1583]    Loss_D: 1.1706    Loss_G: 5.4761    D(x): 0.9509    D(G(z)): 0.6143 / 0.0088
[1/2][1200/1583]    Loss_D: 0.5637    Loss_G: 2.2863    D(x): 0.7523    D(G(z)): 0.1901 / 0.1335
[1/2][1250/1583]    Loss_D: 0.4913    Loss_G: 2.1290    D(x): 0.7336    D(G(z)): 0.1155 / 0.1592
[1/2][1300/1583]    Loss_D: 0.4753    Loss_G: 2.9672    D(x): 0.8157    D(G(z)): 0.1986 / 0.0763
[1/2][1350/1583]    Loss_D: 0.6133    Loss_G: 2.9954    D(x): 0.8253    D(G(z)): 0.2826 / 0.0687
[1/2][1400/1583]    Loss_D: 0.4921    Loss_G: 3.1019    D(x): 0.8035    D(G(z)): 0.1985 / 0.0676
</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Finally, lets check out how we did. Here, we will look at three<br>different results. First, we will see how D and G’s losses changed<br>during training. Second, we will visualize G’s output on the fixed_noise<br>batch for every epoch. And third, we will look at a batch of real data<br>next to a batch of fake data from G.</p>
<p><strong>Loss versus training iteration</strong></p>
<p>Below is a plot of D &amp; G’s losses versus training iterations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">"Generator and Discriminator Loss During Training"</span>)</span><br><span class="line">plt.plot(G_losses,label=<span class="string">"G"</span>)</span><br><span class="line">plt.plot(D_losses,label=<span class="string">"D"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"iterations"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>Visualization of G’s progression</strong></p>
<p>Remember how we saved the generator’s output on the fixed_noise batch<br>after every epoch of training. Now, we can visualize the training<br>progression of G with an animation. Press the play button to start the<br>animation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#%%capture</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">ims = [[plt.imshow(np.transpose(i,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)), animated=<span class="literal">True</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> img_list]</span><br><span class="line">ani = animation.ArtistAnimation(fig, ims, interval=<span class="number">1000</span>, repeat_delay=<span class="number">1000</span>, blit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">HTML(ani.to_jshtml())</span><br></pre></td></tr></table></figure>
<p><strong>Real Images vs. Fake Images</strong></p>
<p>Finally, lets take a look at some real images and fake images side by<br>side.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Grab a batch of real images from the dataloader</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the real images</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Real Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">5</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the fake images from the last epoch</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Fake Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(img_list[<span class="number">-1</span>],(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Where-to-Go-Next"><a href="#Where-to-Go-Next" class="headerlink" title="Where to Go Next"></a>Where to Go Next</h2><p>We have reached the end of our journey, but there are several places you<br>could go from here. You could:</p>
<ul>
<li>Train for longer to see how good the results get</li>
<li>Modify this model to take a different dataset and possibly change the<br>size of the images and the model architecture</li>
<li>Check out some other cool GAN projects<br><code>here &lt;https://github.com/nashory/gans-awesome-applications&gt;</code>__</li>
<li>Create GANs that generate<br><code>music &lt;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&gt;</code>__</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 我不认识的单词</span></span><br></pre></td></tr></table></figure>
<p>Intuitively:直观地<br><code>
</code></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-计算机视觉迁移学习</title>
    <url>/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>Pytorch-Image-计算机视觉迁移学习:</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="Transfer-Learning-for-Computer-Vision-Tutorial"><a href="#Transfer-Learning-for-Computer-Vision-Tutorial" class="headerlink" title="Transfer Learning for Computer Vision Tutorial"></a>Transfer Learning for Computer Vision Tutorial</h1><p><strong>Author</strong>: <code>Sasank Chilamkurthy &lt;https://chsasank.github.io&gt;</code>_</p>
<p>In this tutorial, you will learn how to train a convolutional neural network for<br>image classification using transfer learning. You can read more about the transfer<br>learning at <code>cs231n notes &lt;https://cs231n.github.io/transfer-learning/&gt;</code>__</p>
<p>Quoting these notes,</p>
<pre><code>In practice, very few people train an entire Convolutional Network
from scratch (with random initialization), because it is relatively
rare to have a dataset of sufficient size. Instead, it is common to
pretrain a ConvNet on a very large dataset (e.g. ImageNet, which
contains 1.2 million images with 1000 categories), and then use the
ConvNet either as an initialization or a fixed feature extractor for
the task of interest.</code></pre><p>These two major transfer learning scenarios look as follows:</p>
<ul>
<li><strong>Finetuning the convnet</strong>: Instead of random initializaion, we<br>initialize the network with a pretrained network, like the one that is<br>trained on imagenet 1000 dataset. Rest of the training looks as<br>usual.</li>
<li><strong>ConvNet as fixed feature extractor</strong>: Here, we will freeze the weights<br>for all of the network except that of the final fully connected<br>layer. This last fully connected layer is replaced with a new one<br>with random weights and only this layer is trained.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># License: BSD</span></span><br><span class="line"><span class="comment"># Author: Sasank Chilamkurthy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure>

<h2 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h2><p>We will use torchvision and torch.utils.data packages for loading the<br>data.</p>
<p>The problem we’re going to solve today is to train a model to classify<br><strong>ants</strong> and <strong>bees</strong>. We have about 120 training images each for ants and bees.<br>There are 75 validation images for each class. Usually, this is a very<br>small dataset to generalize upon, if trained from scratch. Since we<br>are using transfer learning, we should be able to generalize reasonably<br>well.</p>
<p>This dataset is a very small subset of imagenet.</p>
<p>.. Note ::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/hymenoptera_data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Data augmentation and normalization for training</span></span><br><span class="line"><span class="comment"># Just normalization for validation</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">'val'</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">'data/hymenoptera_data'</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">                                          data_transforms[x])</span><br><span class="line">                  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataset_sizes = &#123;x: len(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">class_names = image_datasets[<span class="string">'train'</span>].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>

<p>Visualize a few images</p>
<p>Let’s visualize a few training images so as to understand the data<br>augmentations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch of training data</span></span><br><span class="line">inputs, classes = next(iter(dataloaders[<span class="string">'train'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a grid from batch</span></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=[class_names[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_6_0.png" alt="png"></p>
<h2 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h2><p>Now, let’s write a general function to train a model. Here, we will<br>illustrate:</p>
<ul>
<li>Scheduling the learning rate</li>
<li>Saving the best model</li>
</ul>
<p>In the following, parameter <code>scheduler</code> is an LR scheduler object from<br><code>torch.optim.lr_scheduler</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter gradients</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># statistics</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>Visualizing the model predictions</p>
<p>Generic function to display predictions for a few images</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span><span class="params">(model, num_images=<span class="number">6</span>)</span>:</span></span><br><span class="line">    was_training = model.training</span><br><span class="line">    model.eval()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(dataloaders[<span class="string">'val'</span>]):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(inputs.size()[<span class="number">0</span>]):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">'off'</span>)</span><br><span class="line">                ax.set_title(<span class="string">'predicted: &#123;&#125;'</span>.format(class_names[preds[j]]))</span><br><span class="line">                imshow(inputs.cpu().data[j])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train(mode=was_training)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train(mode=was_training)</span><br></pre></td></tr></table></figure>

<h2 id="Finetuning-the-convnet"><a href="#Finetuning-the-convnet" class="headerlink" title="Finetuning the convnet"></a>Finetuning the convnet</h2><p>Load a pretrained model and reset final fully connected layer.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"><span class="comment"># Here the size of each output sample is set to 2.</span></span><br><span class="line"><span class="comment"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 0/24
----------
train Loss: 0.5563 Acc: 0.7131
val Loss: 0.3475 Acc: 0.8693

Epoch 1/24
----------
train Loss: 0.4089 Acc: 0.8156
val Loss: 0.2445 Acc: 0.9085

Epoch 2/24
----------
train Loss: 0.5546 Acc: 0.7910
val Loss: 0.2798 Acc: 0.8758

Epoch 3/24
----------
train Loss: 0.4420 Acc: 0.8238
val Loss: 0.2220 Acc: 0.9085

Epoch 4/24
----------
train Loss: 0.5138 Acc: 0.8279
val Loss: 0.4449 Acc: 0.8497

Epoch 5/24
----------
train Loss: 0.4944 Acc: 0.8320
val Loss: 0.2964 Acc: 0.9281

Epoch 6/24
----------
train Loss: 0.6059 Acc: 0.7541
val Loss: 0.2792 Acc: 0.8889

Epoch 7/24
----------
train Loss: 0.4492 Acc: 0.8279
val Loss: 0.2148 Acc: 0.9085

Epoch 8/24
----------
train Loss: 0.3162 Acc: 0.8730
val Loss: 0.2214 Acc: 0.9281

Epoch 9/24
----------
train Loss: 0.2760 Acc: 0.8730
val Loss: 0.2317 Acc: 0.9281

Epoch 10/24
----------
train Loss: 0.2800 Acc: 0.8811
val Loss: 0.2063 Acc: 0.9216

Epoch 11/24
----------
train Loss: 0.2789 Acc: 0.8975
val Loss: 0.2132 Acc: 0.9281

Epoch 12/24
----------
train Loss: 0.2112 Acc: 0.9180
val Loss: 0.2114 Acc: 0.9346

Epoch 13/24
----------
train Loss: 0.3116 Acc: 0.8811
val Loss: 0.2009 Acc: 0.9346

Epoch 14/24
----------
train Loss: 0.2907 Acc: 0.8975
val Loss: 0.1990 Acc: 0.9346

Epoch 15/24
----------
train Loss: 0.2431 Acc: 0.9098
val Loss: 0.2149 Acc: 0.9346

Epoch 16/24
----------
train Loss: 0.2203 Acc: 0.9180
val Loss: 0.2014 Acc: 0.9346

Epoch 17/24
----------
train Loss: 0.2727 Acc: 0.8689
val Loss: 0.1924 Acc: 0.9346

Epoch 18/24
----------
train Loss: 0.2276 Acc: 0.9139
val Loss: 0.1987 Acc: 0.9281

Epoch 19/24
----------
train Loss: 0.1850 Acc: 0.9180
val Loss: 0.2287 Acc: 0.9346

Epoch 20/24
----------
train Loss: 0.2624 Acc: 0.8893
val Loss: 0.2368 Acc: 0.9281

Epoch 21/24
----------
train Loss: 0.2524 Acc: 0.8975
val Loss: 0.2231 Acc: 0.9346

Epoch 22/24
----------
train Loss: 0.2732 Acc: 0.8730
val Loss: 0.1966 Acc: 0.9346

Epoch 23/24
----------
train Loss: 0.3067 Acc: 0.8811
val Loss: 0.1995 Acc: 0.9346

Epoch 24/24
----------
train Loss: 0.2301 Acc: 0.8934
val Loss: 0.1958 Acc: 0.9281

Training complete in 80m 47s
Best val Acc: 0.934641</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_0.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_1.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_2.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_3.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_4.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_5.png" alt="png"></p>
<h2 id="ConvNet-as-fixed-feature-extractor"><a href="#ConvNet-as-fixed-feature-extractor" class="headerlink" title="ConvNet as fixed feature extractor"></a>ConvNet as fixed feature extractor</h2><p>Here, we need to freeze all the network except the final layer. We need<br>to set <code>requires_grad == False</code> to freeze the parameters so that the<br>gradients are not computed in <code>backward()</code>.</p>
<p>You can read more about this in the documentation<br><code>here &lt;https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></span><br><span class="line"><span class="comment"># opposed to before.</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p>Train and evaluate</p>
<p>On CPU this will take about half the time compared to previous scenario.<br>This is expected as gradients don’t need to be computed for most of the<br>network. However, forward does need to be computed.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 0/24
----------
train Loss: 0.6604 Acc: 0.6516
val Loss: 0.4662 Acc: 0.7582

Epoch 1/24
----------
train Loss: 0.4774 Acc: 0.7910
val Loss: 0.2465 Acc: 0.9020

Epoch 2/24
----------
train Loss: 0.7743 Acc: 0.7049
val Loss: 0.3536 Acc: 0.8431

Epoch 3/24
----------
train Loss: 0.4459 Acc: 0.8033
val Loss: 0.1802 Acc: 0.9346

Epoch 4/24
----------
train Loss: 0.8349 Acc: 0.6803
val Loss: 0.3540 Acc: 0.8627

Epoch 5/24
----------
train Loss: 0.4686 Acc: 0.8033
val Loss: 0.1727 Acc: 0.9608

Epoch 6/24
----------
train Loss: 0.5511 Acc: 0.7623
val Loss: 0.3009 Acc: 0.8889

Epoch 7/24
----------
train Loss: 0.3496 Acc: 0.8525
val Loss: 0.1738 Acc: 0.9542

Epoch 8/24
----------
train Loss: 0.3696 Acc: 0.8484
val Loss: 0.1663 Acc: 0.9542

Epoch 9/24
----------
train Loss: 0.2564 Acc: 0.8770
val Loss: 0.1647 Acc: 0.9608

Epoch 10/24
----------
train Loss: 0.3623 Acc: 0.8402
val Loss: 0.1873 Acc: 0.9346

Epoch 11/24
----------
train Loss: 0.3846 Acc: 0.8320
val Loss: 0.1770 Acc: 0.9477

Epoch 12/24
----------
train Loss: 0.3871 Acc: 0.8238
val Loss: 0.1760 Acc: 0.9477

Epoch 13/24
----------
train Loss: 0.3481 Acc: 0.8525
val Loss: 0.1711 Acc: 0.9542

Epoch 14/24
----------
train Loss: 0.3504 Acc: 0.8402
val Loss: 0.1635 Acc: 0.9477

Epoch 15/24
----------
train Loss: 0.4247 Acc: 0.8279
val Loss: 0.1630 Acc: 0.9608

Epoch 16/24
----------
train Loss: 0.3036 Acc: 0.8607
val Loss: 0.1695 Acc: 0.9608

Epoch 17/24
----------
train Loss: 0.2761 Acc: 0.8934
val Loss: 0.1709 Acc: 0.9608

Epoch 18/24
----------
train Loss: 0.4223 Acc: 0.8238
val Loss: 0.1854 Acc: 0.9412

Epoch 19/24
----------
train Loss: 0.3503 Acc: 0.8402
val Loss: 0.1845 Acc: 0.9216

Epoch 20/24
----------
train Loss: 0.2934 Acc: 0.8811
val Loss: 0.1648 Acc: 0.9412

Epoch 21/24
----------
train Loss: 0.3156 Acc: 0.8402
val Loss: 0.1775 Acc: 0.9346

Epoch 22/24
----------
train Loss: 0.4119 Acc: 0.8115
val Loss: 0.1744 Acc: 0.9477

Epoch 23/24
----------
train Loss: 0.2424 Acc: 0.8893
val Loss: 0.1738 Acc: 0.9412

Epoch 24/24
----------
train Loss: 0.3547 Acc: 0.8361
val Loss: 0.1687 Acc: 0.9412

Training complete in 46m 23s
Best val Acc: 0.960784</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_0.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_1.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_2.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_3.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_4.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_5.png" alt="png"></p>
<h2 id="Further-Learning"><a href="#Further-Learning" class="headerlink" title="Further Learning"></a>Further Learning</h2><p>If you would like to learn more about the applications of transfer learning,<br>checkout our <code>Quantized Transfer Learning for Computer Vision Tutorial &lt;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&gt;</code>_.</p>
<h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sufficient:足够的</span><br><span class="line">Finetuning:微调</span><br><span class="line">extractor:提取器</span><br><span class="line">generalize:概括</span><br><span class="line">flip:翻转</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-对抗样本生成</title>
    <url>/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<p>Pytorch-Image-对抗样本生成:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Adversarial-Example-Generation"><a href="#Adversarial-Example-Generation" class="headerlink" title="Adversarial Example Generation"></a>Adversarial Example Generation</h1><p><strong>Author:</strong> <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p>
<p>If you are reading this, hopefully you can appreciate how effective some<br>machine learning models are. Research is constantly pushing ML models to<br>be faster, more accurate, and more efficient. However, an often<br>overlooked aspect of designing and training models is security and<br>robustness, especially in the face of an adversary who wishes to fool<br>the model.</p>
<p>This tutorial will raise your awareness to the security vulnerabilities<br>of ML models, and will give insight into the hot topic of adversarial<br>machine learning. You may be surprised to find that adding imperceptible<br>perturbations to an image <em>can</em> cause drastically different model<br>performance. Given that this is a tutorial, we will explore the topic<br>via example on an image classifier. Specifically we will use one of the<br>first and most popular attack methods, the Fast Gradient Sign Attack<br>(FGSM), to fool an MNIST classifier.</p>
<h2 id="Threat-Model"><a href="#Threat-Model" class="headerlink" title="Threat Model"></a>Threat Model</h2><p>For context, there are many categories of adversarial attacks, each with<br>a different goal and assumption of the attacker’s knowledge. However, in<br>general the overarching goal is to add the least amount of perturbation<br>to the input data to cause the desired misclassification. There are<br>several kinds of assumptions of the attacker’s knowledge, two of which<br>are: <strong>white-box</strong> and <strong>black-box</strong>. A <em>white-box</em> attack assumes the<br>attacker has full knowledge and access to the model, including<br>architecture, inputs, outputs, and weights. A <em>black-box</em> attack assumes<br>the attacker only has access to the inputs and outputs of the model, and<br>knows nothing about the underlying architecture or weights. There are<br>also several types of goals, including <strong>misclassification</strong> and<br><strong>source/target misclassification</strong>. A goal of <em>misclassification</em> means<br>the adversary only wants the output classification to be wrong but does<br>not care what the new classification is. A <em>source/target<br>misclassification</em> means the adversary wants to alter an image that is<br>originally of a specific source class so that it is classified as a<br>specific target class.</p>
<p>In this case, the FGSM attack is a <em>white-box</em> attack with the goal of<br><em>misclassification</em>. With this background information, we can now<br>discuss the attack in detail.</p>
<p>Fast Gradient Sign Attack</p>
<p>One of the first and most popular adversarial attacks to date is<br>referred to as the <em>Fast Gradient Sign Attack (FGSM)</em> and is described<br>by Goodfellow et. al. in <code>Explaining and Harnessing Adversarial
Examples &lt;https://arxiv.org/abs/1412.6572&gt;</code>__. The attack is remarkably<br>powerful, and yet intuitive. It is designed to attack neural networks by<br>leveraging the way they learn, <em>gradients</em>. The idea is simple, rather<br>than working to minimize the loss by adjusting the weights based on the<br>backpropagated gradients, the attack <em>adjusts the input data to maximize<br>the loss</em> based on the same backpropagated gradients. In other words,<br>the attack uses the gradient of the loss w.r.t the input data, then<br>adjusts the input data to maximize the loss.</p>
<p>Before we jump into the code, let’s look at the famous<br><code>FGSM &lt;https://arxiv.org/abs/1412.6572&gt;</code>__ panda example and extract<br>some notation.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_images/fgsm_panda_image.png" alt></p>
<p>From the figure, $\mathbf{x}$ is the original input image<br>correctly classified as a “panda”, $y$ is the ground truth label<br>for $\mathbf{x}$, $\mathbf{\theta}$ represents the model<br>parameters, and $J(\mathbf{\theta}, \mathbf{x}, y)$ is the loss<br>that is used to train the network. The attack backpropagates the<br>gradient back to the input data to calculate<br>$\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$. Then, it adjusts<br>the input data by a small step ($\epsilon$ or $0.007$ in the<br>picture) in the direction (i.e.<br>$sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))$) that will<br>maximize the loss. The resulting perturbed image, $x’$, is then<br><em>misclassified</em> by the target network as a “gibbon” when it is still<br>clearly a “panda”.</p>
<p>Hopefully now the motivation for this tutorial is clear, so lets jump<br>into the implementation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>In this section, we will discuss the input parameters for the tutorial,<br>define the model under attack, then code the attack and run some tests.</p>
<p>Inputs</p>
<p>There are only three inputs for this tutorial, and are defined as<br>follows:</p>
<ul>
<li><p><strong>epsilons</strong> - List of epsilon values to use for the run. It is<br>important to keep 0 in the list because it represents the model<br>performance on the original test set. Also, intuitively we would<br>expect the larger the epsilon, the more noticeable the perturbations<br>but the more effective the attack in terms of degrading model<br>accuracy. Since the data range here is $[0,1]$, no epsilon<br>value should exceed 1.</p>
</li>
<li><p><strong>pretrained_model</strong> - path to the pretrained MNIST model which was<br>trained with<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code><strong>.<br>For simplicity, download the pretrained model <code>here &lt;https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing&gt;</code></strong>.</p>
</li>
<li><p><strong>use_cuda</strong> - boolean flag to use CUDA if desired and available.<br>Note, a GPU with CUDA is not critical for this tutorial as a CPU will<br>not take much time.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilons = [<span class="number">0</span>, <span class="number">.05</span>, <span class="number">.1</span>, <span class="number">.15</span>, <span class="number">.2</span>, <span class="number">.25</span>, <span class="number">.3</span>]</span><br><span class="line">pretrained_model = <span class="string">"data/lenet_mnist_model.pth"</span></span><br><span class="line">use_cuda=<span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>Model Under Attack</p>
<p>As mentioned, the model under attack is the same MNIST model from<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code>__.<br>You may train and save your own MNIST model or you can download and use<br>the provided model. The <em>Net</em> definition and test dataloader here have<br>been copied from the MNIST example. The purpose of this section is to<br>define the model and dataloader, then initialize the model and load the<br>pretrained weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># LeNet Model definition</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MNIST Test dataset and dataloader declaration</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            ])), </span><br><span class="line">        batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define what device we are using</span></span><br><span class="line">print(<span class="string">"CUDA Available: "</span>,torch.cuda.is_available())</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> (use_cuda <span class="keyword">and</span> torch.cuda.is_available()) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the network</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pretrained model</span></span><br><span class="line">model.load_state_dict(torch.load(pretrained_model, map_location=<span class="string">'cpu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the model in evaluation mode. In this case this is for the Dropout layers</span></span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>
<pre><code>Using downloaded and verified file: data\MNIST\raw\train-images-idx3-ubyte.gz
Extracting data\MNIST\raw\train-images-idx3-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\train-labels-idx1-ubyte.gz
Extracting data\MNIST\raw\train-labels-idx1-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\t10k-images-idx3-ubyte.gz
Extracting data\MNIST\raw\t10k-images-idx3-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\t10k-labels-idx1-ubyte.gz
Extracting data\MNIST\raw\t10k-labels-idx1-ubyte.gz
Processing...
Done!
CUDA Available:  False





Net(
  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))
  (conv2_drop): Dropout2d(p=0.5)
  (fc1): Linear(in_features=320, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=10, bias=True)
)
</code></pre><p>FGSM Attack</p>
<p>Now, we can define the function that creates the adversarial examples by<br>perturbing the original inputs. The <code>fgsm_attack</code> function takes three<br>inputs, <em>image</em> is the original clean image ($x$), <em>epsilon</em> is<br>the pixel-wise perturbation amount ($\epsilon$), and <em>data_grad</em><br>is gradient of the loss w.r.t the input image<br>($\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$). The function<br>then creates perturbed image as</p>
<p>\begin{align}perturbed_image = image + epsilon<em>sign(data_grad) = x + \epsilon </em> sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))\end{align}</p>
<p>Finally, in order to maintain the original range of the data, the<br>perturbed image is clipped to range $[0,1]$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># FGSM attack code</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fgsm_attack</span><span class="params">(image, epsilon, data_grad)</span>:</span></span><br><span class="line">    <span class="comment"># Collect the element-wise sign of the data gradient</span></span><br><span class="line">    sign_data_grad = data_grad.sign()</span><br><span class="line">    <span class="comment"># Create the perturbed image by adjusting each pixel of the input image</span></span><br><span class="line">    perturbed_image = image + epsilon*sign_data_grad</span><br><span class="line">    <span class="comment"># Adding clipping to maintain [0,1] range</span></span><br><span class="line">    perturbed_image = torch.clamp(perturbed_image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Return the perturbed image</span></span><br><span class="line">    <span class="keyword">return</span> perturbed_image</span><br></pre></td></tr></table></figure>
<p>Testing Function</p>
<p>Finally, the central result of this tutorial comes from the <code>test</code><br>function. Each call to this test function performs a full test step on<br>the MNIST test set and reports a final accuracy. However, notice that<br>this function also takes an <em>epsilon</em> input. This is because the<br><code>test</code> function reports the accuracy of a model that is under attack<br>from an adversary with strength $\epsilon$. More specifically, for<br>each sample in the test set, the function computes the gradient of the<br>loss w.r.t the input data ($data_grad$), creates a perturbed<br>image with <code>fgsm_attack</code> ($perturbed_data$), then checks to see<br>if the perturbed example is adversarial. In addition to testing the<br>accuracy of the model, the function also saves and returns some<br>successful adversarial examples to be visualized later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">( model, device, test_loader, epsilon )</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Accuracy counter</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    adv_examples = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop over all examples in test set</span></span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Send the data and label to the device</span></span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set requires_grad attribute of tensor. Important for Attack</span></span><br><span class="line">        data.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass the data through the model</span></span><br><span class="line">        output = model(data)</span><br><span class="line">        init_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the initial prediction is wrong, dont bother attacking, just move on</span></span><br><span class="line">        <span class="keyword">if</span> init_pred.item() != target.item():</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the loss</span></span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero all existing gradients</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate gradients of model in backward pass</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Collect datagrad</span></span><br><span class="line">        data_grad = data.grad.data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Call FGSM Attack</span></span><br><span class="line">        perturbed_data = fgsm_attack(data, epsilon, data_grad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Re-classify the perturbed image</span></span><br><span class="line">        output = model(perturbed_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for success</span></span><br><span class="line">        final_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">        <span class="keyword">if</span> final_pred.item() == target.item():</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Special case for saving 0 epsilon examples</span></span><br><span class="line">            <span class="keyword">if</span> (epsilon == <span class="number">0</span>) <span class="keyword">and</span> (len(adv_examples) &lt; <span class="number">5</span>):</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Save some adv examples for visualization later</span></span><br><span class="line">            <span class="keyword">if</span> len(adv_examples) &lt; <span class="number">5</span>:</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate final accuracy for this epsilon</span></span><br><span class="line">    final_acc = correct/float(len(test_loader))</span><br><span class="line">    print(<span class="string">"Epsilon: &#123;&#125;\tTest Accuracy = &#123;&#125; / &#123;&#125; = &#123;&#125;"</span>.format(epsilon, correct, len(test_loader), final_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the accuracy and an adversarial example</span></span><br><span class="line">    <span class="keyword">return</span> final_acc, adv_examples</span><br></pre></td></tr></table></figure>
<p>Run Attack</p>
<p>The last part of the implementation is to actually run the attack. Here,<br>we run a full test step for each epsilon value in the <em>epsilons</em> input.<br>For each epsilon we also save the final accuracy and some successful<br>adversarial examples to be plotted in the coming sections. Notice how<br>the printed accuracies decrease as the epsilon value increases. Also,<br>note the $\epsilon=0$ case represents the original test accuracy,<br>with no attack.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracies = []</span><br><span class="line">examples = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run test for each epsilon</span></span><br><span class="line"><span class="keyword">for</span> eps <span class="keyword">in</span> epsilons:</span><br><span class="line">    acc, ex = test(model, device, test_loader, eps)</span><br><span class="line">    accuracies.append(acc)</span><br><span class="line">    examples.append(ex)</span><br></pre></td></tr></table></figure>
<pre><code>Epsilon: 0    Test Accuracy = 9810 / 10000 = 0.981
Epsilon: 0.05    Test Accuracy = 9426 / 10000 = 0.9426
Epsilon: 0.1    Test Accuracy = 8510 / 10000 = 0.851
Epsilon: 0.15    Test Accuracy = 6826 / 10000 = 0.6826
Epsilon: 0.2    Test Accuracy = 4301 / 10000 = 0.4301
Epsilon: 0.25    Test Accuracy = 2082 / 10000 = 0.2082
Epsilon: 0.3    Test Accuracy = 869 / 10000 = 0.0869
</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Accuracy vs Epsilon</p>
<p>The first result is the accuracy versus epsilon plot. As alluded to<br>earlier, as epsilon increases we expect the test accuracy to decrease.<br>This is because larger epsilons mean we take a larger step in the<br>direction that will maximize the loss. Notice the trend in the curve is<br>not linear even though the epsilon values are linearly spaced. For<br>example, the accuracy at $\epsilon=0.05$ is only about 4% lower<br>than $\epsilon=0$, but the accuracy at $\epsilon=0.2$ is 25%<br>lower than $\epsilon=0.15$. Also, notice the accuracy of the model<br>hits random accuracy for a 10-class classifier between<br>$\epsilon=0.25$ and $\epsilon=0.3$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(epsilons, accuracies, <span class="string">"*-"</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">1.1</span>, step=<span class="number">0.1</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, <span class="number">.35</span>, step=<span class="number">0.05</span>))</span><br><span class="line">plt.title(<span class="string">"Accuracy vs Epsilon"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epsilon"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_15_0.png" alt="png"></p>
<p>Sample Adversarial Examples</p>
<p>Remember the idea of no free lunch? In this case, as epsilon increases<br>the test accuracy decreases <strong>BUT</strong> the perturbations become more easily<br>perceptible. In reality, there is a tradeoff between accuracy<br>degredation and perceptibility that an attacker must consider. Here, we<br>show some examples of successful adversarial examples at each epsilon<br>value. Each row of the plot shows a different epsilon value. The first<br>row is the $\epsilon=0$ examples which represent the original<br>“clean” images with no perturbation. The title of each image shows the<br>“original classification -&gt; adversarial classification.” Notice, the<br>perturbations start to become evident at $\epsilon=0.15$ and are<br>quite evident at $\epsilon=0.3$. However, in all cases humans are<br>still capable of identifying the correct class despite the added noise.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Plot several examples of adversarial samples at each epsilon</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(epsilons)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(examples[i])):</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.subplot(len(epsilons),len(examples[<span class="number">0</span>]),cnt)</span><br><span class="line">        plt.xticks([], [])</span><br><span class="line">        plt.yticks([], [])</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            plt.ylabel(<span class="string">"Eps: &#123;&#125;"</span>.format(epsilons[i]), fontsize=<span class="number">14</span>)</span><br><span class="line">        orig,adv,ex = examples[i][j]</span><br><span class="line">        plt.title(<span class="string">"&#123;&#125; -&gt; &#123;&#125;"</span>.format(orig, adv))</span><br><span class="line">        plt.imshow(ex, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_17_0.png" alt="png"></p>
<h2 id="Where-to-go-next"><a href="#Where-to-go-next" class="headerlink" title="Where to go next?"></a>Where to go next?</h2><p>Hopefully this tutorial gives some insight into the topic of adversarial<br>machine learning. There are many potential directions to go from here.<br>This attack represents the very beginning of adversarial attack research<br>and since there have been many subsequent ideas for how to attack and<br>defend ML models from an adversary. In fact, at NIPS 2017 there was an<br>adversarial attack and defense competition and many of the methods used<br>in the competition are described in this paper: <code>Adversarial Attacks and
Defences Competition &lt;https://arxiv.org/pdf/1804.00097.pdf&gt;</code>__. The work<br>on defense also leads into the idea of making machine learning models<br>more <em>robust</em> in general, to both naturally perturbed and adversarially<br>crafted inputs.</p>
<p>Another direction to go is adversarial attacks and defense in different<br>domains. Adversarial research is not limited to the image domain, check<br>out <code>this &lt;https://arxiv.org/pdf/1801.01944.pdf&gt;</code>__ attack on<br>speech-to-text models. But perhaps the best way to learn more about<br>adversarial machine learning is to get your hands dirty. Try to<br>implement a different attack from the NIPS 2017 competition, and see how<br>it differs from FGSM. Then, try to defend the model from your own<br>attacks.</p>
<h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">overlooked:被忽视</span><br><span class="line">robustness:健壮性&#x2F;鲁棒性</span><br><span class="line">vulnerabilities:漏洞</span><br><span class="line">imperceptible:不可察觉的</span><br><span class="line">drastically:剧烈地</span><br><span class="line">via:通过</span><br><span class="line">remarkably:显着地</span><br><span class="line">intuitive:直觉的</span><br><span class="line">tradeoff:交易</span><br><span class="line">perturbations:摄动,扰动</span><br><span class="line">adversarial:对抗的</span><br><span class="line">However, in general the overarching goal is to add the least amount of perturbation to the input data to cause the desired misclassification.:一般来说，总体目标是向输入数据添加最少的扰动量，从而导致所需的错误分类</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-微调TorchVision对象检测</title>
    <url>/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>Pytorch-Image-微调TorchVision对象检测:<br><a id="more"></a></p>
<h1 id="TorchVision-0-3-Object-Detection-finetuning-tutorial"><a href="#TorchVision-0-3-Object-Detection-finetuning-tutorial" class="headerlink" title="TorchVision 0.3 Object Detection finetuning tutorial"></a>TorchVision 0.3 Object Detection finetuning tutorial</h1><p>For this tutorial, we will be finetuning a pre-trained <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a> model in the <a href="https://www.cis.upenn.edu/jshi/ped_html/" target="_blank" rel="noopener"><em>Penn-Fudan Database for Pedestrian Detection and Segmentation</em></a>. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.</p>
<p>First, we need to install <code>pycocotools</code>. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line">pip install cython</span><br><span class="line"><span class="comment"># Install pycocotools, the version by default in Colab</span></span><br><span class="line"><span class="comment"># has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354</span></span><br><span class="line">pip install -U <span class="string">'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</span></span><br></pre></td></tr></table></figure>
<pre><code>Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)
Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI
  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-h3isg2r5
  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-h3isg2r5
Requirement already satisfied, skipping upgrade: setuptools&gt;=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.1.0)
Requirement already satisfied, skipping upgrade: cython&gt;=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)
Requirement already satisfied, skipping upgrade: matplotlib&gt;=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)
Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.4.7)
Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.8.1)
Requirement already satisfied, skipping upgrade: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.18.5)
Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (0.10.0)
Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.2.0)
Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.15.0)
Building wheels for collected packages: pycocotools
  Building wheel for pycocotools (setup.py) ... [?25l[?25hdone
  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266460 sha256=3292fbae19c3df30ceb54183f71e1e7288d447743b1dcb8f88257833cf2f23e1
  Stored in directory: /tmp/pip-ephem-wheel-cache-y2jf5d3p/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a
Successfully built pycocotools
Installing collected packages: pycocotools
  Found existing installation: pycocotools 2.0.1
    Uninstalling pycocotools-2.0.1:
      Successfully uninstalled pycocotools-2.0.1
Successfully installed pycocotools-2.0
</code></pre><h2 id="Defining-the-Dataset"><a href="#Defining-the-Dataset" class="headerlink" title="Defining the Dataset"></a>Defining the Dataset</h2><p>The <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision reference scripts for training object detection, instance segmentation and person keypoint detection</a> allows for easily supporting adding new custom datasets.<br>The dataset should inherit from the standard <code>torch.utils.data.Dataset</code> class, and implement <code>__len__</code> and <code>__getitem__</code>.</p>
<p>The only specificity that we require is that the dataset <code>__getitem__</code> should return:</p>
<ul>
<li>image: a PIL Image of size (H, W)</li>
<li>target: a dict containing the following fields<ul>
<li><code>boxes</code> (<code>FloatTensor[N, 4]</code>): the coordinates of the <code>N</code> bounding boxes in <code>[x0, y0, x1, y1]</code> format, ranging from <code>0</code> to <code>W</code> and <code>0</code> to <code>H</code></li>
<li><code>labels</code> (<code>Int64Tensor[N]</code>): the label for each bounding box</li>
<li><code>image_id</code> (<code>Int64Tensor[1]</code>): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation</li>
<li><code>area</code> (<code>Tensor[N]</code>): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.</li>
<li><code>iscrowd</code> (<code>UInt8Tensor[N]</code>): instances with <code>iscrowd=True</code> will be ignored during evaluation.</li>
<li>(optionally) <code>masks</code> (<code>UInt8Tensor[N, H, W]</code>): The segmentation masks for each one of the objects</li>
<li>(optionally) <code>keypoints</code> (<code>FloatTensor[N, K, 3]</code>): For each one of the <code>N</code> objects, it contains the <code>K</code> keypoints in <code>[x, y, visibility]</code> format, defining the object. <code>visibility=0</code> means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt <code>references/detection/transforms.py</code> for your new keypoint representation</li>
</ul>
</li>
</ul>
<p>If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.</p>
<p>Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a <code>get_height_and_width</code> method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via <code>__getitem__</code> , which loads the image in memory and is slower than if a custom method is provided.</p>
<h3 id="Writing-a-custom-dataset-for-Penn-Fudan"><a href="#Writing-a-custom-dataset-for-Penn-Fudan" class="headerlink" title="Writing a custom dataset for Penn-Fudan"></a>Writing a custom dataset for Penn-Fudan</h3><p>Let’s write a dataset for the Penn-Fudan dataset.</p>
<p>First, let’s download and extract the data, present in a zip file at <a href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip" target="_blank" rel="noopener">https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the Penn-Fudan dataset</span></span><br><span class="line">wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .</span><br><span class="line"><span class="comment"># extract it in the current folder</span></span><br><span class="line">unzip PennFudanPed.zip</span><br></pre></td></tr></table></figure>
<pre><code>--2020-07-23 13:46:08--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip
Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d
Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 53723336 (51M) [application/zip]
Saving to: ‘PennFudanPed.zip’

PennFudanPed.zip    100%[===================&gt;]  51.23M  1009KB/s    in 48s     

2020-07-23 13:46:58 (1.06 MB/s) - ‘PennFudanPed.zip’ saved [53723336/53723336]

--2020-07-23 13:46:58--  http://./
Resolving . (.)... failed: No address associated with hostname.
wget: unable to resolve host address ‘.’
FINISHED --2020-07-23 13:46:58--
Total wall clock time: 50s
Downloaded: 1 files, 51M in 48s (1.06 MB/s)
Archive:  PennFudanPed.zip
   creating: PennFudanPed/
  inflating: PennFudanPed/added-object-list.txt  
   creating: PennFudanPed/Annotation/
  inflating: PennFudanPed/Annotation/FudanPed00001.txt  
  inflating: PennFudanPed/Annotation/FudanPed00002.txt  
  inflating: PennFudanPed/Annotation/FudanPed00003.txt  
  inflating: PennFudanPed/Annotation/FudanPed00004.txt  
  inflating: PennFudanPed/Annotation/FudanPed00005.txt  
  inflating: PennFudanPed/Annotation/FudanPed00006.txt  
  inflating: PennFudanPed/Annotation/FudanPed00007.txt  
  inflating: PennFudanPed/Annotation/FudanPed00008.txt  
  inflating: PennFudanPed/Annotation/FudanPed00009.txt  
  inflating: PennFudanPed/Annotation/FudanPed00010.txt  
  inflating: PennFudanPed/Annotation/FudanPed00011.txt  
  inflating: PennFudanPed/Annotation/FudanPed00012.txt  
  inflating: PennFudanPed/Annotation/FudanPed00013.txt  
  inflating: PennFudanPed/Annotation/FudanPed00014.txt  
  inflating: PennFudanPed/Annotation/FudanPed00015.txt  
  inflating: PennFudanPed/Annotation/FudanPed00016.txt  
  inflating: PennFudanPed/Annotation/FudanPed00017.txt  
  inflating: PennFudanPed/Annotation/FudanPed00018.txt  
  inflating: PennFudanPed/Annotation/FudanPed00019.txt  
  inflating: PennFudanPed/Annotation/FudanPed00020.txt  
  inflating: PennFudanPed/Annotation/FudanPed00021.txt  
  inflating: PennFudanPed/Annotation/FudanPed00022.txt  
  inflating: PennFudanPed/Annotation/FudanPed00023.txt  
  inflating: PennFudanPed/Annotation/FudanPed00024.txt  
  inflating: PennFudanPed/Annotation/FudanPed00025.txt  
  inflating: PennFudanPed/Annotation/FudanPed00026.txt  
  inflating: PennFudanPed/Annotation/FudanPed00027.txt  
  inflating: PennFudanPed/Annotation/FudanPed00028.txt  
  inflating: PennFudanPed/Annotation/FudanPed00029.txt  
  inflating: PennFudanPed/Annotation/FudanPed00030.txt  
  inflating: PennFudanPed/Annotation/FudanPed00031.txt  
  inflating: PennFudanPed/Annotation/FudanPed00032.txt  
  inflating: PennFudanPed/Annotation/FudanPed00033.txt  
  inflating: PennFudanPed/Annotation/FudanPed00034.txt  
  inflating: PennFudanPed/Annotation/FudanPed00035.txt  
  inflating: PennFudanPed/Annotation/FudanPed00036.txt  
  inflating: PennFudanPed/Annotation/FudanPed00037.txt  
  inflating: PennFudanPed/Annotation/FudanPed00038.txt  
  inflating: PennFudanPed/Annotation/FudanPed00039.txt  
  inflating: PennFudanPed/Annotation/FudanPed00040.txt  
  inflating: PennFudanPed/Annotation/FudanPed00041.txt  
  inflating: PennFudanPed/Annotation/FudanPed00042.txt  
  inflating: PennFudanPed/Annotation/FudanPed00043.txt  
  inflating: PennFudanPed/Annotation/FudanPed00044.txt  
  inflating: PennFudanPed/Annotation/FudanPed00045.txt  
  inflating: PennFudanPed/Annotation/FudanPed00046.txt  
  inflating: PennFudanPed/Annotation/FudanPed00047.txt  
  inflating: PennFudanPed/Annotation/FudanPed00048.txt  
  inflating: PennFudanPed/Annotation/FudanPed00049.txt  
  inflating: PennFudanPed/Annotation/FudanPed00050.txt  
  inflating: PennFudanPed/Annotation/FudanPed00051.txt  
  inflating: PennFudanPed/Annotation/FudanPed00052.txt  
  inflating: PennFudanPed/Annotation/FudanPed00053.txt  
  inflating: PennFudanPed/Annotation/FudanPed00054.txt  
  inflating: PennFudanPed/Annotation/FudanPed00055.txt  
  inflating: PennFudanPed/Annotation/FudanPed00056.txt  
  inflating: PennFudanPed/Annotation/FudanPed00057.txt  
  inflating: PennFudanPed/Annotation/FudanPed00058.txt  
  inflating: PennFudanPed/Annotation/FudanPed00059.txt  
  inflating: PennFudanPed/Annotation/FudanPed00060.txt  
  inflating: PennFudanPed/Annotation/FudanPed00061.txt  
  inflating: PennFudanPed/Annotation/FudanPed00062.txt  
  inflating: PennFudanPed/Annotation/FudanPed00063.txt  
  inflating: PennFudanPed/Annotation/FudanPed00064.txt  
  inflating: PennFudanPed/Annotation/FudanPed00065.txt  
  inflating: PennFudanPed/Annotation/FudanPed00066.txt  
  inflating: PennFudanPed/Annotation/FudanPed00067.txt  
  inflating: PennFudanPed/Annotation/FudanPed00068.txt  
  inflating: PennFudanPed/Annotation/FudanPed00069.txt  
  inflating: PennFudanPed/Annotation/FudanPed00070.txt  
  inflating: PennFudanPed/Annotation/FudanPed00071.txt  
  inflating: PennFudanPed/Annotation/FudanPed00072.txt  
  inflating: PennFudanPed/Annotation/FudanPed00073.txt  
  inflating: PennFudanPed/Annotation/FudanPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00001.txt  
  inflating: PennFudanPed/Annotation/PennPed00002.txt  
  inflating: PennFudanPed/Annotation/PennPed00003.txt  
  inflating: PennFudanPed/Annotation/PennPed00004.txt  
  inflating: PennFudanPed/Annotation/PennPed00005.txt  
  inflating: PennFudanPed/Annotation/PennPed00006.txt  
  inflating: PennFudanPed/Annotation/PennPed00007.txt  
  inflating: PennFudanPed/Annotation/PennPed00008.txt  
  inflating: PennFudanPed/Annotation/PennPed00009.txt  
  inflating: PennFudanPed/Annotation/PennPed00010.txt  
  inflating: PennFudanPed/Annotation/PennPed00011.txt  
  inflating: PennFudanPed/Annotation/PennPed00012.txt  
  inflating: PennFudanPed/Annotation/PennPed00013.txt  
  inflating: PennFudanPed/Annotation/PennPed00014.txt  
  inflating: PennFudanPed/Annotation/PennPed00015.txt  
  inflating: PennFudanPed/Annotation/PennPed00016.txt  
  inflating: PennFudanPed/Annotation/PennPed00017.txt  
  inflating: PennFudanPed/Annotation/PennPed00018.txt  
  inflating: PennFudanPed/Annotation/PennPed00019.txt  
  inflating: PennFudanPed/Annotation/PennPed00020.txt  
  inflating: PennFudanPed/Annotation/PennPed00021.txt  
  inflating: PennFudanPed/Annotation/PennPed00022.txt  
  inflating: PennFudanPed/Annotation/PennPed00023.txt  
  inflating: PennFudanPed/Annotation/PennPed00024.txt  
  inflating: PennFudanPed/Annotation/PennPed00025.txt  
  inflating: PennFudanPed/Annotation/PennPed00026.txt  
  inflating: PennFudanPed/Annotation/PennPed00027.txt  
  inflating: PennFudanPed/Annotation/PennPed00028.txt  
  inflating: PennFudanPed/Annotation/PennPed00029.txt  
  inflating: PennFudanPed/Annotation/PennPed00030.txt  
  inflating: PennFudanPed/Annotation/PennPed00031.txt  
  inflating: PennFudanPed/Annotation/PennPed00032.txt  
  inflating: PennFudanPed/Annotation/PennPed00033.txt  
  inflating: PennFudanPed/Annotation/PennPed00034.txt  
  inflating: PennFudanPed/Annotation/PennPed00035.txt  
  inflating: PennFudanPed/Annotation/PennPed00036.txt  
  inflating: PennFudanPed/Annotation/PennPed00037.txt  
  inflating: PennFudanPed/Annotation/PennPed00038.txt  
  inflating: PennFudanPed/Annotation/PennPed00039.txt  
  inflating: PennFudanPed/Annotation/PennPed00040.txt  
  inflating: PennFudanPed/Annotation/PennPed00041.txt  
  inflating: PennFudanPed/Annotation/PennPed00042.txt  
  inflating: PennFudanPed/Annotation/PennPed00043.txt  
  inflating: PennFudanPed/Annotation/PennPed00044.txt  
  inflating: PennFudanPed/Annotation/PennPed00045.txt  
  inflating: PennFudanPed/Annotation/PennPed00046.txt  
  inflating: PennFudanPed/Annotation/PennPed00047.txt  
  inflating: PennFudanPed/Annotation/PennPed00048.txt  
  inflating: PennFudanPed/Annotation/PennPed00049.txt  
  inflating: PennFudanPed/Annotation/PennPed00050.txt  
  inflating: PennFudanPed/Annotation/PennPed00051.txt  
  inflating: PennFudanPed/Annotation/PennPed00052.txt  
  inflating: PennFudanPed/Annotation/PennPed00053.txt  
  inflating: PennFudanPed/Annotation/PennPed00054.txt  
  inflating: PennFudanPed/Annotation/PennPed00055.txt  
  inflating: PennFudanPed/Annotation/PennPed00056.txt  
  inflating: PennFudanPed/Annotation/PennPed00057.txt  
  inflating: PennFudanPed/Annotation/PennPed00058.txt  
  inflating: PennFudanPed/Annotation/PennPed00059.txt  
  inflating: PennFudanPed/Annotation/PennPed00060.txt  
  inflating: PennFudanPed/Annotation/PennPed00061.txt  
  inflating: PennFudanPed/Annotation/PennPed00062.txt  
  inflating: PennFudanPed/Annotation/PennPed00063.txt  
  inflating: PennFudanPed/Annotation/PennPed00064.txt  
  inflating: PennFudanPed/Annotation/PennPed00065.txt  
  inflating: PennFudanPed/Annotation/PennPed00066.txt  
  inflating: PennFudanPed/Annotation/PennPed00067.txt  
  inflating: PennFudanPed/Annotation/PennPed00068.txt  
  inflating: PennFudanPed/Annotation/PennPed00069.txt  
  inflating: PennFudanPed/Annotation/PennPed00070.txt  
  inflating: PennFudanPed/Annotation/PennPed00071.txt  
  inflating: PennFudanPed/Annotation/PennPed00072.txt  
  inflating: PennFudanPed/Annotation/PennPed00073.txt  
  inflating: PennFudanPed/Annotation/PennPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00075.txt  
  inflating: PennFudanPed/Annotation/PennPed00076.txt  
  inflating: PennFudanPed/Annotation/PennPed00077.txt  
  inflating: PennFudanPed/Annotation/PennPed00078.txt  
  inflating: PennFudanPed/Annotation/PennPed00079.txt  
  inflating: PennFudanPed/Annotation/PennPed00080.txt  
  inflating: PennFudanPed/Annotation/PennPed00081.txt  
  inflating: PennFudanPed/Annotation/PennPed00082.txt  
  inflating: PennFudanPed/Annotation/PennPed00083.txt  
  inflating: PennFudanPed/Annotation/PennPed00084.txt  
  inflating: PennFudanPed/Annotation/PennPed00085.txt  
  inflating: PennFudanPed/Annotation/PennPed00086.txt  
  inflating: PennFudanPed/Annotation/PennPed00087.txt  
  inflating: PennFudanPed/Annotation/PennPed00088.txt  
  inflating: PennFudanPed/Annotation/PennPed00089.txt  
  inflating: PennFudanPed/Annotation/PennPed00090.txt  
  inflating: PennFudanPed/Annotation/PennPed00091.txt  
  inflating: PennFudanPed/Annotation/PennPed00092.txt  
  inflating: PennFudanPed/Annotation/PennPed00093.txt  
  inflating: PennFudanPed/Annotation/PennPed00094.txt  
  inflating: PennFudanPed/Annotation/PennPed00095.txt  
  inflating: PennFudanPed/Annotation/PennPed00096.txt  
   creating: PennFudanPed/PedMasks/
  inflating: PennFudanPed/PedMasks/FudanPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00014_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00017_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00027_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00052_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00061_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00065_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00068_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00069_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00071_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00073_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00014_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00017_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00027_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00052_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00061_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00065_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00068_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00069_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00071_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00073_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00075_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00076_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00077_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00078_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00079_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00080_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00081_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00082_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00083_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00084_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00085_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00086_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00087_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00088_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00089_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00090_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00091_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00092_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00093_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00094_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00095_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00096_mask.png  
   creating: PennFudanPed/PNGImages/
  inflating: PennFudanPed/PNGImages/FudanPed00001.png  
  inflating: PennFudanPed/PNGImages/FudanPed00002.png  
  inflating: PennFudanPed/PNGImages/FudanPed00003.png  
  inflating: PennFudanPed/PNGImages/FudanPed00004.png  
  inflating: PennFudanPed/PNGImages/FudanPed00005.png  
  inflating: PennFudanPed/PNGImages/FudanPed00006.png  
  inflating: PennFudanPed/PNGImages/FudanPed00007.png  
  inflating: PennFudanPed/PNGImages/FudanPed00008.png  
  inflating: PennFudanPed/PNGImages/FudanPed00009.png  
  inflating: PennFudanPed/PNGImages/FudanPed00010.png  
  inflating: PennFudanPed/PNGImages/FudanPed00011.png  
  inflating: PennFudanPed/PNGImages/FudanPed00012.png  
  inflating: PennFudanPed/PNGImages/FudanPed00013.png  
  inflating: PennFudanPed/PNGImages/FudanPed00014.png  
  inflating: PennFudanPed/PNGImages/FudanPed00015.png  
  inflating: PennFudanPed/PNGImages/FudanPed00016.png  
  inflating: PennFudanPed/PNGImages/FudanPed00017.png  
  inflating: PennFudanPed/PNGImages/FudanPed00018.png  
  inflating: PennFudanPed/PNGImages/FudanPed00019.png  
  inflating: PennFudanPed/PNGImages/FudanPed00020.png  
  inflating: PennFudanPed/PNGImages/FudanPed00021.png  
  inflating: PennFudanPed/PNGImages/FudanPed00022.png  
  inflating: PennFudanPed/PNGImages/FudanPed00023.png  
  inflating: PennFudanPed/PNGImages/FudanPed00024.png  
  inflating: PennFudanPed/PNGImages/FudanPed00025.png  
  inflating: PennFudanPed/PNGImages/FudanPed00026.png  
  inflating: PennFudanPed/PNGImages/FudanPed00027.png  
  inflating: PennFudanPed/PNGImages/FudanPed00028.png  
  inflating: PennFudanPed/PNGImages/FudanPed00029.png  
  inflating: PennFudanPed/PNGImages/FudanPed00030.png  
  inflating: PennFudanPed/PNGImages/FudanPed00031.png  
  inflating: PennFudanPed/PNGImages/FudanPed00032.png  
  inflating: PennFudanPed/PNGImages/FudanPed00033.png  
  inflating: PennFudanPed/PNGImages/FudanPed00034.png  
  inflating: PennFudanPed/PNGImages/FudanPed00035.png  
  inflating: PennFudanPed/PNGImages/FudanPed00036.png  
  inflating: PennFudanPed/PNGImages/FudanPed00037.png  
  inflating: PennFudanPed/PNGImages/FudanPed00038.png  
  inflating: PennFudanPed/PNGImages/FudanPed00039.png  
  inflating: PennFudanPed/PNGImages/FudanPed00040.png  
  inflating: PennFudanPed/PNGImages/FudanPed00041.png  
  inflating: PennFudanPed/PNGImages/FudanPed00042.png  
  inflating: PennFudanPed/PNGImages/FudanPed00043.png  
  inflating: PennFudanPed/PNGImages/FudanPed00044.png  
  inflating: PennFudanPed/PNGImages/FudanPed00045.png  
  inflating: PennFudanPed/PNGImages/FudanPed00046.png  
  inflating: PennFudanPed/PNGImages/FudanPed00047.png  
  inflating: PennFudanPed/PNGImages/FudanPed00048.png  
  inflating: PennFudanPed/PNGImages/FudanPed00049.png  
  inflating: PennFudanPed/PNGImages/FudanPed00050.png  
  inflating: PennFudanPed/PNGImages/FudanPed00051.png  
  inflating: PennFudanPed/PNGImages/FudanPed00052.png  
  inflating: PennFudanPed/PNGImages/FudanPed00053.png  
  inflating: PennFudanPed/PNGImages/FudanPed00054.png  
  inflating: PennFudanPed/PNGImages/FudanPed00055.png  
  inflating: PennFudanPed/PNGImages/FudanPed00056.png  
  inflating: PennFudanPed/PNGImages/FudanPed00057.png  
  inflating: PennFudanPed/PNGImages/FudanPed00058.png  
  inflating: PennFudanPed/PNGImages/FudanPed00059.png  
  inflating: PennFudanPed/PNGImages/FudanPed00060.png  
  inflating: PennFudanPed/PNGImages/FudanPed00061.png  
  inflating: PennFudanPed/PNGImages/FudanPed00062.png  
  inflating: PennFudanPed/PNGImages/FudanPed00063.png  
  inflating: PennFudanPed/PNGImages/FudanPed00064.png  
  inflating: PennFudanPed/PNGImages/FudanPed00065.png  
  inflating: PennFudanPed/PNGImages/FudanPed00066.png  
  inflating: PennFudanPed/PNGImages/FudanPed00067.png  
  inflating: PennFudanPed/PNGImages/FudanPed00068.png  
  inflating: PennFudanPed/PNGImages/FudanPed00069.png  
  inflating: PennFudanPed/PNGImages/FudanPed00070.png  
  inflating: PennFudanPed/PNGImages/FudanPed00071.png  
  inflating: PennFudanPed/PNGImages/FudanPed00072.png  
  inflating: PennFudanPed/PNGImages/FudanPed00073.png  
  inflating: PennFudanPed/PNGImages/FudanPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00001.png  
  inflating: PennFudanPed/PNGImages/PennPed00002.png  
  inflating: PennFudanPed/PNGImages/PennPed00003.png  
  inflating: PennFudanPed/PNGImages/PennPed00004.png  
  inflating: PennFudanPed/PNGImages/PennPed00005.png  
  inflating: PennFudanPed/PNGImages/PennPed00006.png  
  inflating: PennFudanPed/PNGImages/PennPed00007.png  
  inflating: PennFudanPed/PNGImages/PennPed00008.png  
  inflating: PennFudanPed/PNGImages/PennPed00009.png  
  inflating: PennFudanPed/PNGImages/PennPed00010.png  
  inflating: PennFudanPed/PNGImages/PennPed00011.png  
  inflating: PennFudanPed/PNGImages/PennPed00012.png  
  inflating: PennFudanPed/PNGImages/PennPed00013.png  
  inflating: PennFudanPed/PNGImages/PennPed00014.png  
  inflating: PennFudanPed/PNGImages/PennPed00015.png  
  inflating: PennFudanPed/PNGImages/PennPed00016.png  
  inflating: PennFudanPed/PNGImages/PennPed00017.png  
  inflating: PennFudanPed/PNGImages/PennPed00018.png  
  inflating: PennFudanPed/PNGImages/PennPed00019.png  
  inflating: PennFudanPed/PNGImages/PennPed00020.png  
  inflating: PennFudanPed/PNGImages/PennPed00021.png  
  inflating: PennFudanPed/PNGImages/PennPed00022.png  
  inflating: PennFudanPed/PNGImages/PennPed00023.png  
  inflating: PennFudanPed/PNGImages/PennPed00024.png  
  inflating: PennFudanPed/PNGImages/PennPed00025.png  
  inflating: PennFudanPed/PNGImages/PennPed00026.png  
  inflating: PennFudanPed/PNGImages/PennPed00027.png  
  inflating: PennFudanPed/PNGImages/PennPed00028.png  
  inflating: PennFudanPed/PNGImages/PennPed00029.png  
  inflating: PennFudanPed/PNGImages/PennPed00030.png  
  inflating: PennFudanPed/PNGImages/PennPed00031.png  
  inflating: PennFudanPed/PNGImages/PennPed00032.png  
  inflating: PennFudanPed/PNGImages/PennPed00033.png  
  inflating: PennFudanPed/PNGImages/PennPed00034.png  
  inflating: PennFudanPed/PNGImages/PennPed00035.png  
  inflating: PennFudanPed/PNGImages/PennPed00036.png  
  inflating: PennFudanPed/PNGImages/PennPed00037.png  
  inflating: PennFudanPed/PNGImages/PennPed00038.png  
  inflating: PennFudanPed/PNGImages/PennPed00039.png  
  inflating: PennFudanPed/PNGImages/PennPed00040.png  
  inflating: PennFudanPed/PNGImages/PennPed00041.png  
  inflating: PennFudanPed/PNGImages/PennPed00042.png  
  inflating: PennFudanPed/PNGImages/PennPed00043.png  
  inflating: PennFudanPed/PNGImages/PennPed00044.png  
  inflating: PennFudanPed/PNGImages/PennPed00045.png  
  inflating: PennFudanPed/PNGImages/PennPed00046.png  
  inflating: PennFudanPed/PNGImages/PennPed00047.png  
  inflating: PennFudanPed/PNGImages/PennPed00048.png  
  inflating: PennFudanPed/PNGImages/PennPed00049.png  
  inflating: PennFudanPed/PNGImages/PennPed00050.png  
  inflating: PennFudanPed/PNGImages/PennPed00051.png  
  inflating: PennFudanPed/PNGImages/PennPed00052.png  
  inflating: PennFudanPed/PNGImages/PennPed00053.png  
  inflating: PennFudanPed/PNGImages/PennPed00054.png  
  inflating: PennFudanPed/PNGImages/PennPed00055.png  
  inflating: PennFudanPed/PNGImages/PennPed00056.png  
  inflating: PennFudanPed/PNGImages/PennPed00057.png  
  inflating: PennFudanPed/PNGImages/PennPed00058.png  
  inflating: PennFudanPed/PNGImages/PennPed00059.png  
  inflating: PennFudanPed/PNGImages/PennPed00060.png  
  inflating: PennFudanPed/PNGImages/PennPed00061.png  
  inflating: PennFudanPed/PNGImages/PennPed00062.png  
  inflating: PennFudanPed/PNGImages/PennPed00063.png  
  inflating: PennFudanPed/PNGImages/PennPed00064.png  
  inflating: PennFudanPed/PNGImages/PennPed00065.png  
  inflating: PennFudanPed/PNGImages/PennPed00066.png  
  inflating: PennFudanPed/PNGImages/PennPed00067.png  
  inflating: PennFudanPed/PNGImages/PennPed00068.png  
  inflating: PennFudanPed/PNGImages/PennPed00069.png  
  inflating: PennFudanPed/PNGImages/PennPed00070.png  
  inflating: PennFudanPed/PNGImages/PennPed00071.png  
  inflating: PennFudanPed/PNGImages/PennPed00072.png  
  inflating: PennFudanPed/PNGImages/PennPed00073.png  
  inflating: PennFudanPed/PNGImages/PennPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00075.png  
  inflating: PennFudanPed/PNGImages/PennPed00076.png  
  inflating: PennFudanPed/PNGImages/PennPed00077.png  
  inflating: PennFudanPed/PNGImages/PennPed00078.png  
  inflating: PennFudanPed/PNGImages/PennPed00079.png  
  inflating: PennFudanPed/PNGImages/PennPed00080.png  
  inflating: PennFudanPed/PNGImages/PennPed00081.png  
  inflating: PennFudanPed/PNGImages/PennPed00082.png  
  inflating: PennFudanPed/PNGImages/PennPed00083.png  
  inflating: PennFudanPed/PNGImages/PennPed00084.png  
  inflating: PennFudanPed/PNGImages/PennPed00085.png  
  inflating: PennFudanPed/PNGImages/PennPed00086.png  
  inflating: PennFudanPed/PNGImages/PennPed00087.png  
  inflating: PennFudanPed/PNGImages/PennPed00088.png  
  inflating: PennFudanPed/PNGImages/PennPed00089.png  
  inflating: PennFudanPed/PNGImages/PennPed00090.png  
  inflating: PennFudanPed/PNGImages/PennPed00091.png  
  inflating: PennFudanPed/PNGImages/PennPed00092.png  
  inflating: PennFudanPed/PNGImages/PennPed00093.png  
  inflating: PennFudanPed/PNGImages/PennPed00094.png  
  inflating: PennFudanPed/PNGImages/PennPed00095.png  
  inflating: PennFudanPed/PNGImages/PennPed00096.png  
  inflating: PennFudanPed/readme.txt  
</code></pre><p>Let’s have a look at the dataset and how it is layed down.</p>
<p>The data is structured as follows<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PennFudanPed&#x2F;</span><br><span class="line">  PedMasks&#x2F;</span><br><span class="line">    FudanPed00001_mask.png</span><br><span class="line">    FudanPed00002_mask.png</span><br><span class="line">    FudanPed00003_mask.png</span><br><span class="line">    FudanPed00004_mask.png</span><br><span class="line">    ...</span><br><span class="line">  PNGImages&#x2F;</span><br><span class="line">    FudanPed00001.png</span><br><span class="line">    FudanPed00002.png</span><br><span class="line">    FudanPed00003.png</span><br><span class="line">    FudanPed00004.png</span><br></pre></td></tr></table></figure></p>
<p>Here is one example of an image in the dataset, with its corresponding instance segmentation mask</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">Image.open(<span class="string">'PennFudanPed/PNGImages/FudanPed00001.png'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask = Image.open(<span class="string">'PennFudanPed/PedMasks/FudanPed00001_mask.png'</span>)</span><br><span class="line"><span class="comment"># each mask instance has a different color, from zero to N, where</span></span><br><span class="line"><span class="comment"># N is the number of instances. In order to make visualization easier,</span></span><br><span class="line"><span class="comment"># let's adda color palette to the mask.</span></span><br><span class="line">mask.putpalette([</span><br><span class="line">    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># black background</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># index 1 is red</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>, <span class="comment"># index 2 is yellow</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">153</span>, <span class="number">0</span>, <span class="comment"># index 3 is orange</span></span><br><span class="line">])</span><br><span class="line">mask</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_7_0.png" alt="png"></p>
<p>So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a <code>torch.utils.data.Dataset</code> class for this dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PennFudanDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms=None)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># load all image files, sorting them to</span></span><br><span class="line">        <span class="comment"># ensure that they are aligned</span></span><br><span class="line">        self.imgs = list(sorted(os.listdir(os.path.join(root, <span class="string">"PNGImages"</span>))))</span><br><span class="line">        self.masks = list(sorted(os.listdir(os.path.join(root, <span class="string">"PedMasks"</span>))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="comment"># load images ad masks</span></span><br><span class="line">        img_path = os.path.join(self.root, <span class="string">"PNGImages"</span>, self.imgs[idx])</span><br><span class="line">        mask_path = os.path.join(self.root, <span class="string">"PedMasks"</span>, self.masks[idx])</span><br><span class="line">        img = Image.open(img_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        <span class="comment"># note that we haven't converted the mask to RGB,</span></span><br><span class="line">        <span class="comment"># because each color corresponds to a different instance</span></span><br><span class="line">        <span class="comment"># with 0 being background</span></span><br><span class="line">        mask = Image.open(mask_path)</span><br><span class="line"></span><br><span class="line">        mask = np.array(mask)</span><br><span class="line">        <span class="comment"># instances are encoded as different colors</span></span><br><span class="line">        obj_ids = np.unique(mask)</span><br><span class="line">        <span class="comment"># first id is the background, so remove it</span></span><br><span class="line">        obj_ids = obj_ids[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split the color-encoded mask into a set</span></span><br><span class="line">        <span class="comment"># of binary masks</span></span><br><span class="line">        masks = mask == obj_ids[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get bounding box coordinates for each mask</span></span><br><span class="line">        num_objs = len(obj_ids)</span><br><span class="line">        boxes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_objs):</span><br><span class="line">            pos = np.where(masks[i])</span><br><span class="line">            xmin = np.min(pos[<span class="number">1</span>])</span><br><span class="line">            xmax = np.max(pos[<span class="number">1</span>])</span><br><span class="line">            ymin = np.min(pos[<span class="number">0</span>])</span><br><span class="line">            ymax = np.max(pos[<span class="number">0</span>])</span><br><span class="line">            boxes.append([xmin, ymin, xmax, ymax])</span><br><span class="line"></span><br><span class="line">        boxes = torch.as_tensor(boxes, dtype=torch.float32)</span><br><span class="line">        <span class="comment"># there is only one class</span></span><br><span class="line">        labels = torch.ones((num_objs,), dtype=torch.int64)</span><br><span class="line">        masks = torch.as_tensor(masks, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        image_id = torch.tensor([idx])</span><br><span class="line">        area = (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]) * (boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>])</span><br><span class="line">        <span class="comment"># suppose all instances are not crowd</span></span><br><span class="line">        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">        target = &#123;&#125;</span><br><span class="line">        target[<span class="string">"boxes"</span>] = boxes</span><br><span class="line">        target[<span class="string">"labels"</span>] = labels</span><br><span class="line">        target[<span class="string">"masks"</span>] = masks</span><br><span class="line">        target[<span class="string">"image_id"</span>] = image_id</span><br><span class="line">        target[<span class="string">"area"</span>] = area</span><br><span class="line">        target[<span class="string">"iscrowd"</span>] = iscrowd</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img, target = self.transforms(img, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure>
<p>That’s all for the dataset. Let’s see how the outputs are structured for this dataset</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed/'</span>)</span><br><span class="line">dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(&lt;PIL.Image.Image image mode=RGB size=559x536 at 0x7FEBFE8767F0&gt;,
 {&#39;area&#39;: tensor([35358., 36225.]), &#39;boxes&#39;: tensor([[159., 181., 301., 430.],
          [419., 170., 534., 485.]]), &#39;image_id&#39;: tensor([0]), &#39;iscrowd&#39;: tensor([0, 0]), &#39;labels&#39;: tensor([1, 1]), &#39;masks&#39;: tensor([[[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]],

          [[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})
</code></pre><p>So we can see that by default, the dataset returns a <code>PIL.Image</code> and a dictionary<br>containing several fields, including <code>boxes</code>, <code>labels</code> and <code>masks</code>.</p>
<h2 id="Defining-your-model"><a href="#Defining-your-model" class="headerlink" title="Defining your model"></a>Defining your model</h2><p>In this tutorial, we will be using <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>, which is based on top of <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image03.png" alt="Faster R-CNN"></p>
<p>Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image04.png" alt="Mask R-CNN"></p>
<p>There are two common situations where one might want to modify one of the available models in torchvision modelzoo.<br>The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).</p>
<p>Let’s go see how we would do one or another in the following sections.</p>
<h3 id="1-Finetuning-from-a-pretrained-model"><a href="#1-Finetuning-from-a-pretrained-model" class="headerlink" title="1 - Finetuning from a pretrained model"></a>1 - Finetuning from a pretrained model</h3><p>Let’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection.faster_rcnn import FastRCNNPredictor</span><br><span class="line"></span><br><span class="line"># load a model pre-trained pre-trained on COCO</span><br><span class="line">model &#x3D; torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained&#x3D;True)</span><br><span class="line"></span><br><span class="line"># replace the classifier with a new one, that has</span><br><span class="line"># num_classes which is user-defined</span><br><span class="line">num_classes &#x3D; 2  # 1 class (person) + background</span><br><span class="line"># get number of input features for the classifier</span><br><span class="line">in_features &#x3D; model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line"># replace the pre-trained head with a new one</span><br><span class="line">model.roi_heads.box_predictor &#x3D; FastRCNNPredictor(in_features, num_classes)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-Modifying-the-model-to-add-a-different-backbone"><a href="#2-Modifying-the-model-to-add-a-different-backbone" class="headerlink" title="2 - Modifying the model to add a different backbone"></a>2 - Modifying the model to add a different backbone</h3><p>Another common situation arises when the user wants to replace the backbone of a detection<br>model with a different one. For example, the current default backbone (ResNet-50) might be too big for some applications, and smaller models might be necessary.</p>
<p>Here is how we would go into leveraging the functions provided by torchvision to modify a backbone.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection import FasterRCNN</span><br><span class="line">from torchvision.models.detection.rpn import AnchorGenerator</span><br><span class="line"></span><br><span class="line"># load a pre-trained model for classification and return</span><br><span class="line"># only the features</span><br><span class="line">backbone &#x3D; torchvision.models.mobilenet_v2(pretrained&#x3D;True).features</span><br><span class="line"># FasterRCNN needs to know the number of</span><br><span class="line"># output channels in a backbone. For mobilenet_v2, it&#39;s 1280</span><br><span class="line"># so we need to add it here</span><br><span class="line">backbone.out_channels &#x3D; 1280</span><br><span class="line"></span><br><span class="line"># let&#39;s make the RPN generate 5 x 3 anchors per spatial</span><br><span class="line"># location, with 5 different sizes and 3 different aspect</span><br><span class="line"># ratios. We have a Tuple[Tuple[int]] because each feature</span><br><span class="line"># map could potentially have different sizes and</span><br><span class="line"># aspect ratios </span><br><span class="line">anchor_generator &#x3D; AnchorGenerator(sizes&#x3D;((32, 64, 128, 256, 512),),</span><br><span class="line">                                   aspect_ratios&#x3D;((0.5, 1.0, 2.0),))</span><br><span class="line"></span><br><span class="line"># let&#39;s define what are the feature maps that we will</span><br><span class="line"># use to perform the region of interest cropping, as well as</span><br><span class="line"># the size of the crop after rescaling.</span><br><span class="line"># if your backbone returns a Tensor, featmap_names is expected to</span><br><span class="line"># be [0]. More generally, the backbone should return an</span><br><span class="line"># OrderedDict[Tensor], and in featmap_names you can choose which</span><br><span class="line"># feature maps to use.</span><br><span class="line">roi_pooler &#x3D; torchvision.ops.MultiScaleRoIAlign(featmap_names&#x3D;[0],</span><br><span class="line">                                                output_size&#x3D;7,</span><br><span class="line">                                                sampling_ratio&#x3D;2)</span><br><span class="line"></span><br><span class="line"># put the pieces together inside a FasterRCNN model</span><br><span class="line">model &#x3D; FasterRCNN(backbone,</span><br><span class="line">                   num_classes&#x3D;2,</span><br><span class="line">                   rpn_anchor_generator&#x3D;anchor_generator,</span><br><span class="line">                   box_roi_pool&#x3D;roi_pooler)</span><br></pre></td></tr></table></figure>
<h3 id="An-Instance-segmentation-model-for-PennFudan-Dataset"><a href="#An-Instance-segmentation-model-for-PennFudan-Dataset" class="headerlink" title="An Instance segmentation model for PennFudan Dataset"></a>An Instance segmentation model for PennFudan Dataset</h3><p>In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.</p>
<p>Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> FastRCNNPredictor</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.mask_rcnn <span class="keyword">import</span> MaskRCNNPredictor</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_instance_segmentation_model</span><span class="params">(num_classes)</span>:</span></span><br><span class="line">    <span class="comment"># load an instance segmentation model pre-trained on COCO</span></span><br><span class="line">    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the number of input features for the classifier</span></span><br><span class="line">    in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    <span class="comment"># replace the pre-trained head with a new one</span></span><br><span class="line">    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now get the number of input features for the mask classifier</span></span><br><span class="line">    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels</span><br><span class="line">    hidden_layer = <span class="number">256</span></span><br><span class="line">    <span class="comment"># and replace the mask predictor with a new one</span></span><br><span class="line">    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,</span><br><span class="line">                                                       hidden_layer,</span><br><span class="line">                                                       num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>That’s it, this will make model be ready to be trained and evaluated on our custom dataset.</p>
<h2 id="Training-and-evaluation-functions"><a href="#Training-and-evaluation-functions" class="headerlink" title="Training and evaluation functions"></a>Training and evaluation functions</h2><p>In <code>references/detection/,</code> we have a number of helper functions to simplify training and evaluating detection models.<br>Here, we will use <code>references/detection/engine.py</code>, <code>references/detection/utils.py</code> and <code>references/detection/transforms.py</code>.</p>
<p>Let’s copy those files (and their dependencies) in here so that they are available in the notebook</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download TorchVision repo to use some files from</span></span><br><span class="line"><span class="comment"># references/detection</span></span><br><span class="line">git clone https://github.com/pytorch/vision.git</span><br><span class="line">cd vision</span><br><span class="line">git checkout v0<span class="number">.3</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">cp references/detection/utils.py ../</span><br><span class="line">cp references/detection/transforms.py ../</span><br><span class="line">cp references/detection/coco_eval.py ../</span><br><span class="line">cp references/detection/engine.py ../</span><br><span class="line">cp references/detection/coco_utils.py ../</span><br></pre></td></tr></table></figure>
<pre><code>Cloning into &#39;vision&#39;...
remote: Enumerating objects: 20, done.[K
remote: Counting objects: 100% (20/20), done.[K
remote: Compressing objects: 100% (20/20), done.[K
remote: Total 9278 (delta 7), reused 3 (delta 0), pack-reused 9258[K
Receiving objects: 100% (9278/9278), 11.24 MiB | 9.51 MiB/s, done.
Resolving deltas: 100% (6426/6426), done.
Note: checking out &#39;v0.3.0&#39;.

You are in &#39;detached HEAD&#39; state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b &lt;new-branch-name&gt;

HEAD is now at be37608 version check against PyTorch&#39;s CUDA version
</code></pre><p>Let’s write some helper functions for data augmentation / transformation, which leverages the functions in <code>refereces/detection</code> that we have just copied:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> engine <span class="keyword">import</span> train_one_epoch, evaluate</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(train)</span>:</span></span><br><span class="line">    transforms = []</span><br><span class="line">    <span class="comment"># converts the image, a PIL image, into a PyTorch Tensor</span></span><br><span class="line">    transforms.append(T.ToTensor())</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        <span class="comment"># during training, randomly flip the training images</span></span><br><span class="line">        <span class="comment"># and ground-truth for data augmentation</span></span><br><span class="line">        transforms.append(T.RandomHorizontalFlip(<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> T.Compose(transforms)</span><br></pre></td></tr></table></figure>
<h4 id="Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model"><a href="#Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model" class="headerlink" title="Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."></a>Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.</h4><h3 id="Putting-everything-together"><a href="#Putting-everything-together" class="headerlink" title="Putting everything together"></a>Putting everything together</h3><p>We now have the dataset class, the models and the data transforms. Let’s instantiate them</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use our dataset and defined transformations</span></span><br><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">True</span>))</span><br><span class="line">dataset_test = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split the dataset in train and test set</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">indices = torch.randperm(len(dataset)).tolist()</span><br><span class="line">dataset = torch.utils.data.Subset(dataset, indices[:<span class="number">-50</span>])</span><br><span class="line">dataset_test = torch.utils.data.Subset(dataset_test, indices[<span class="number">-50</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># define training and validation data loaders</span></span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br><span class="line"></span><br><span class="line">data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_test, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br></pre></td></tr></table></figure>
<p>Now let’s instantiate the model and the optimizer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># our dataset has two classes only - background and person</span></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get the model using our helper function</span></span><br><span class="line">model = get_instance_segmentation_model(num_classes)</span><br><span class="line"><span class="comment"># move model to the right device</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct an optimizer</span></span><br><span class="line">params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=<span class="number">0.005</span>,</span><br><span class="line">                            momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and a learning rate scheduler which decreases the learning rate by</span></span><br><span class="line"><span class="comment"># 10x every 3 epochs</span></span><br><span class="line">lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,</span><br><span class="line">                                               step_size=<span class="number">3</span>,</span><br><span class="line">                                               gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading: &quot;https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth&quot; to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth



HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value=&#39;&#39;)))
</code></pre><p>And now let’s train the model for 10 epochs, evaluating at the end of every epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># let's train it for 10 epochs</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># train for one epoch, printing every 10 iterations</span></span><br><span class="line">    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># update the learning rate</span></span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    <span class="comment"># evaluate on the test dataset</span></span><br><span class="line">    evaluate(model, data_loader_test, device=device)</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:
    nonzero(Tensor input, *, Tensor out)
Consider using one of the following signatures instead:
    nonzero(Tensor input, *, bool as_tuple)


Epoch: [0]  [ 0/60]  eta: 0:02:18  lr: 0.000090  loss: 3.5827 (3.5827)  loss_classifier: 0.7385 (0.7385)  loss_box_reg: 0.1523 (0.1523)  loss_mask: 2.6620 (2.6620)  loss_objectness: 0.0224 (0.0224)  loss_rpn_box_reg: 0.0076 (0.0076)  time: 2.3152  data: 0.2933  max mem: 2303
Epoch: [0]  [10/60]  eta: 0:01:14  lr: 0.000936  loss: 1.5605 (2.1212)  loss_classifier: 0.4479 (0.4976)  loss_box_reg: 0.1826 (0.1906)  loss_mask: 0.9259 (1.4017)  loss_objectness: 0.0224 (0.0208)  loss_rpn_box_reg: 0.0090 (0.0105)  time: 1.4865  data: 0.0356  max mem: 2860
Epoch: [0]  [20/60]  eta: 0:00:57  lr: 0.001783  loss: 0.8700 (1.4312)  loss_classifier: 0.2338 (0.3409)  loss_box_reg: 0.1579 (0.1731)  loss_mask: 0.4010 (0.8836)  loss_objectness: 0.0191 (0.0216)  loss_rpn_box_reg: 0.0099 (0.0120)  time: 1.3888  data: 0.0096  max mem: 2861
Epoch: [0]  [30/60]  eta: 0:00:43  lr: 0.002629  loss: 0.5382 (1.1211)  loss_classifier: 0.0968 (0.2569)  loss_box_reg: 0.1155 (0.1598)  loss_mask: 0.2489 (0.6751)  loss_objectness: 0.0105 (0.0176)  loss_rpn_box_reg: 0.0099 (0.0117)  time: 1.4144  data: 0.0095  max mem: 3596
Epoch: [0]  [40/60]  eta: 0:00:28  lr: 0.003476  loss: 0.4041 (0.9495)  loss_classifier: 0.0690 (0.2099)  loss_box_reg: 0.1090 (0.1521)  loss_mask: 0.2121 (0.5609)  loss_objectness: 0.0038 (0.0142)  loss_rpn_box_reg: 0.0118 (0.0124)  time: 1.4593  data: 0.0098  max mem: 3596
Epoch: [0]  [50/60]  eta: 0:00:14  lr: 0.004323  loss: 0.3387 (0.8263)  loss_classifier: 0.0496 (0.1785)  loss_box_reg: 0.0833 (0.1393)  loss_mask: 0.1797 (0.4837)  loss_objectness: 0.0035 (0.0122)  loss_rpn_box_reg: 0.0118 (0.0128)  time: 1.4368  data: 0.0101  max mem: 3596
Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.2567 (0.7367)  loss_classifier: 0.0392 (0.1566)  loss_box_reg: 0.0545 (0.1240)  loss_mask: 0.1464 (0.4332)  loss_objectness: 0.0020 (0.0106)  loss_rpn_box_reg: 0.0109 (0.0122)  time: 1.4374  data: 0.0101  max mem: 3596
Epoch: [0] Total time: 0:01:26 (1.4425 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3444 (0.3444)  evaluator_time: 0.0059 (0.0059)  time: 0.4881  data: 0.1360  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)  time: 0.3262  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3309 s / it)
Averaged stats: model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.698
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.901
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.759
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.700
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.886
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.716
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.741
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.745
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.751
Epoch: [1]  [ 0/60]  eta: 0:01:39  lr: 0.005000  loss: 0.1716 (0.1716)  loss_classifier: 0.0231 (0.0231)  loss_box_reg: 0.0309 (0.0309)  loss_mask: 0.1041 (0.1041)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0124 (0.0124)  time: 1.6632  data: 0.3040  max mem: 3596
Epoch: [1]  [10/60]  eta: 0:01:14  lr: 0.005000  loss: 0.2137 (0.2460)  loss_classifier: 0.0314 (0.0385)  loss_box_reg: 0.0309 (0.0406)  loss_mask: 0.1438 (0.1540)  loss_objectness: 0.0011 (0.0017)  loss_rpn_box_reg: 0.0113 (0.0111)  time: 1.4996  data: 0.0356  max mem: 3596
Epoch: [1]  [20/60]  eta: 0:00:59  lr: 0.005000  loss: 0.2565 (0.2636)  loss_classifier: 0.0484 (0.0464)  loss_box_reg: 0.0338 (0.0442)  loss_mask: 0.1639 (0.1582)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0123 (0.0131)  time: 1.4682  data: 0.0102  max mem: 3596
Epoch: [1]  [30/60]  eta: 0:00:43  lr: 0.005000  loss: 0.2174 (0.2409)  loss_classifier: 0.0349 (0.0410)  loss_box_reg: 0.0266 (0.0365)  loss_mask: 0.1426 (0.1502)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0080 (0.0115)  time: 1.4462  data: 0.0105  max mem: 3596
Epoch: [1]  [40/60]  eta: 0:00:29  lr: 0.005000  loss: 0.1930 (0.2327)  loss_classifier: 0.0274 (0.0406)  loss_box_reg: 0.0189 (0.0334)  loss_mask: 0.1380 (0.1463)  loss_objectness: 0.0007 (0.0015)  loss_rpn_box_reg: 0.0075 (0.0109)  time: 1.4625  data: 0.0096  max mem: 3596
Epoch: [1]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.2011 (0.2291)  loss_classifier: 0.0344 (0.0409)  loss_box_reg: 0.0253 (0.0325)  loss_mask: 0.1287 (0.1427)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0079 (0.0115)  time: 1.5020  data: 0.0099  max mem: 3596
Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1680 (0.2220)  loss_classifier: 0.0294 (0.0398)  loss_box_reg: 0.0148 (0.0302)  loss_mask: 0.1265 (0.1394)  loss_objectness: 0.0011 (0.0016)  loss_rpn_box_reg: 0.0075 (0.0110)  time: 1.4470  data: 0.0098  max mem: 3596
Epoch: [1] Total time: 0:01:27 (1.4662 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3606 (0.3606)  evaluator_time: 0.0046 (0.0046)  time: 0.5041  data: 0.1374  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)  time: 0.3234  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3253 s / it)
Averaged stats: model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.772
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.747
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.891
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794
Epoch: [2]  [ 0/60]  eta: 0:01:38  lr: 0.005000  loss: 0.1239 (0.1239)  loss_classifier: 0.0123 (0.0123)  loss_box_reg: 0.0068 (0.0068)  loss_mask: 0.0971 (0.0971)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.6336  data: 0.2465  max mem: 3596
Epoch: [2]  [10/60]  eta: 0:01:12  lr: 0.005000  loss: 0.1866 (0.1751)  loss_classifier: 0.0283 (0.0308)  loss_box_reg: 0.0135 (0.0162)  loss_mask: 0.1129 (0.1189)  loss_objectness: 0.0007 (0.0011)  loss_rpn_box_reg: 0.0072 (0.0082)  time: 1.4478  data: 0.0310  max mem: 3596
Epoch: [2]  [20/60]  eta: 0:00:55  lr: 0.005000  loss: 0.1433 (0.1623)  loss_classifier: 0.0203 (0.0253)  loss_box_reg: 0.0094 (0.0129)  loss_mask: 0.1074 (0.1162)  loss_objectness: 0.0003 (0.0008)  loss_rpn_box_reg: 0.0046 (0.0071)  time: 1.3800  data: 0.0095  max mem: 3596
Epoch: [2]  [30/60]  eta: 0:00:42  lr: 0.005000  loss: 0.1621 (0.1821)  loss_classifier: 0.0218 (0.0294)  loss_box_reg: 0.0101 (0.0170)  loss_mask: 0.1160 (0.1257)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0077 (0.0088)  time: 1.4109  data: 0.0095  max mem: 3596
Epoch: [2]  [40/60]  eta: 0:00:28  lr: 0.005000  loss: 0.1841 (0.1834)  loss_classifier: 0.0286 (0.0291)  loss_box_reg: 0.0157 (0.0164)  loss_mask: 0.1288 (0.1278)  loss_objectness: 0.0005 (0.0012)  loss_rpn_box_reg: 0.0081 (0.0088)  time: 1.4780  data: 0.0099  max mem: 3596
Epoch: [2]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.1970 (0.1878)  loss_classifier: 0.0279 (0.0295)  loss_box_reg: 0.0173 (0.0175)  loss_mask: 0.1317 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0083 (0.0092)  time: 1.4749  data: 0.0099  max mem: 3596
Epoch: [2]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1872 (0.1894)  loss_classifier: 0.0279 (0.0307)  loss_box_reg: 0.0173 (0.0177)  loss_mask: 0.1296 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0094 (0.0095)  time: 1.5513  data: 0.0099  max mem: 3596
Epoch: [2] Total time: 0:01:28 (1.4738 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:21  model_time: 0.3020 (0.3020)  evaluator_time: 0.0047 (0.0047)  time: 0.4358  data: 0.1272  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)  time: 0.3183  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3203 s / it)
Averaged stats: model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.921
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798
Epoch: [3]  [ 0/60]  eta: 0:01:55  lr: 0.000500  loss: 0.1690 (0.1690)  loss_classifier: 0.0193 (0.0193)  loss_box_reg: 0.0098 (0.0098)  loss_mask: 0.1339 (0.1339)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0058 (0.0058)  time: 1.9331  data: 0.4201  max mem: 3596
Epoch: [3]  [10/60]  eta: 0:01:19  lr: 0.000500  loss: 0.1668 (0.1790)  loss_classifier: 0.0294 (0.0273)  loss_box_reg: 0.0102 (0.0148)  loss_mask: 0.1203 (0.1283)  loss_objectness: 0.0005 (0.0010)  loss_rpn_box_reg: 0.0058 (0.0076)  time: 1.5992  data: 0.0464  max mem: 3596
Epoch: [3]  [20/60]  eta: 0:01:01  lr: 0.000500  loss: 0.1635 (0.1723)  loss_classifier: 0.0225 (0.0257)  loss_box_reg: 0.0088 (0.0133)  loss_mask: 0.1203 (0.1243)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0061 (0.0079)  time: 1.5232  data: 0.0096  max mem: 3596
Epoch: [3]  [30/60]  eta: 0:00:44  lr: 0.000500  loss: 0.1603 (0.1683)  loss_classifier: 0.0212 (0.0251)  loss_box_reg: 0.0083 (0.0121)  loss_mask: 0.1198 (0.1228)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0060 (0.0073)  time: 1.4131  data: 0.0097  max mem: 3596
Epoch: [3]  [40/60]  eta: 0:00:29  lr: 0.000500  loss: 0.1603 (0.1725)  loss_classifier: 0.0266 (0.0268)  loss_box_reg: 0.0093 (0.0127)  loss_mask: 0.1150 (0.1239)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0069 (0.0082)  time: 1.4049  data: 0.0097  max mem: 3596
Epoch: [3]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1715 (0.1754)  loss_classifier: 0.0266 (0.0267)  loss_box_reg: 0.0109 (0.0134)  loss_mask: 0.1232 (0.1261)  loss_objectness: 0.0005 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0083)  time: 1.4872  data: 0.0099  max mem: 3596
Epoch: [3]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1509 (0.1709)  loss_classifier: 0.0256 (0.0263)  loss_box_reg: 0.0093 (0.0126)  loss_mask: 0.1055 (0.1231)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0081)  time: 1.4687  data: 0.0096  max mem: 3596
Epoch: [3] Total time: 0:01:28 (1.4791 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3690 (0.3690)  evaluator_time: 0.0046 (0.0046)  time: 0.5078  data: 0.1324  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)  time: 0.3199  data: 0.0051  max mem: 3596
Test: Total time: 0:00:16 (0.3224 s / it)
Averaged stats: model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.509
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.377
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.869
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.755
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.917
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.805
Epoch: [4]  [ 0/60]  eta: 0:01:27  lr: 0.000500  loss: 0.1045 (0.1045)  loss_classifier: 0.0070 (0.0070)  loss_box_reg: 0.0029 (0.0029)  loss_mask: 0.0902 (0.0902)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0043 (0.0043)  time: 1.4538  data: 0.2039  max mem: 3596
Epoch: [4]  [10/60]  eta: 0:01:12  lr: 0.000500  loss: 0.1510 (0.1583)  loss_classifier: 0.0209 (0.0197)  loss_box_reg: 0.0101 (0.0126)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0007)  loss_rpn_box_reg: 0.0071 (0.0075)  time: 1.4482  data: 0.0278  max mem: 3596
Epoch: [4]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1510 (0.1582)  loss_classifier: 0.0209 (0.0215)  loss_box_reg: 0.0073 (0.0110)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0060 (0.0066)  time: 1.3827  data: 0.0100  max mem: 3596
Epoch: [4]  [30/60]  eta: 0:00:41  lr: 0.000500  loss: 0.1427 (0.1624)  loss_classifier: 0.0227 (0.0243)  loss_box_reg: 0.0077 (0.0117)  loss_mask: 0.1005 (0.1176)  loss_objectness: 0.0005 (0.0014)  loss_rpn_box_reg: 0.0058 (0.0074)  time: 1.3627  data: 0.0097  max mem: 3596
Epoch: [4]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1472 (0.1611)  loss_classifier: 0.0255 (0.0253)  loss_box_reg: 0.0085 (0.0114)  loss_mask: 0.1079 (0.1161)  loss_objectness: 0.0004 (0.0013)  loss_rpn_box_reg: 0.0063 (0.0071)  time: 1.4537  data: 0.0094  max mem: 3596
Epoch: [4]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1548 (0.1612)  loss_classifier: 0.0250 (0.0249)  loss_box_reg: 0.0079 (0.0113)  loss_mask: 0.1106 (0.1161)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0068 (0.0077)  time: 1.4913  data: 0.0094  max mem: 3596
Epoch: [4]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1548 (0.1647)  loss_classifier: 0.0250 (0.0256)  loss_box_reg: 0.0079 (0.0121)  loss_mask: 0.1106 (0.1179)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0079 (0.0080)  time: 1.4724  data: 0.0097  max mem: 3596
Epoch: [4] Total time: 0:01:26 (1.4357 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3692 (0.3692)  evaluator_time: 0.0045 (0.0045)  time: 0.5054  data: 0.1301  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)  time: 0.3202  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3220 s / it)
Averaged stats: model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.813
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.944
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.520
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.825
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.868
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.920
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815
Epoch: [5]  [ 0/60]  eta: 0:02:13  lr: 0.000500  loss: 0.1545 (0.1545)  loss_classifier: 0.0223 (0.0223)  loss_box_reg: 0.0059 (0.0059)  loss_mask: 0.1200 (0.1200)  loss_objectness: 0.0004 (0.0004)  loss_rpn_box_reg: 0.0059 (0.0059)  time: 2.2323  data: 0.5519  max mem: 3596
Epoch: [5]  [10/60]  eta: 0:01:10  lr: 0.000500  loss: 0.1409 (0.1489)  loss_classifier: 0.0178 (0.0211)  loss_box_reg: 0.0076 (0.0094)  loss_mask: 0.1140 (0.1118)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0057 (0.0061)  time: 1.4098  data: 0.0540  max mem: 3596
Epoch: [5]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1379 (0.1454)  loss_classifier: 0.0189 (0.0208)  loss_box_reg: 0.0076 (0.0088)  loss_mask: 0.1032 (0.1091)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0054 (0.0061)  time: 1.3437  data: 0.0070  max mem: 3596
Epoch: [5]  [30/60]  eta: 0:00:42  lr: 0.000500  loss: 0.1430 (0.1597)  loss_classifier: 0.0240 (0.0246)  loss_box_reg: 0.0092 (0.0114)  loss_mask: 0.1032 (0.1154)  loss_objectness: 0.0005 (0.0008)  loss_rpn_box_reg: 0.0069 (0.0075)  time: 1.4147  data: 0.0100  max mem: 3596
Epoch: [5]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1503 (0.1609)  loss_classifier: 0.0242 (0.0243)  loss_box_reg: 0.0102 (0.0117)  loss_mask: 0.1148 (0.1163)  loss_objectness: 0.0004 (0.0008)  loss_rpn_box_reg: 0.0083 (0.0078)  time: 1.4826  data: 0.0101  max mem: 3596
Epoch: [5]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1397 (0.1571)  loss_classifier: 0.0198 (0.0237)  loss_box_reg: 0.0075 (0.0107)  loss_mask: 0.1017 (0.1144)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0066 (0.0075)  time: 1.4890  data: 0.0096  max mem: 3596
Epoch: [5]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1422 (0.1581)  loss_classifier: 0.0197 (0.0241)  loss_box_reg: 0.0066 (0.0107)  loss_mask: 0.1042 (0.1149)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0064 (0.0076)  time: 1.5030  data: 0.0094  max mem: 3596
Epoch: [5] Total time: 0:01:27 (1.4584 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3761 (0.3761)  evaluator_time: 0.0041 (0.0041)  time: 0.5475  data: 0.1655  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)  time: 0.3195  data: 0.0049  max mem: 3596
Test: Total time: 0:00:16 (0.3235 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [6]  [ 0/60]  eta: 0:01:53  lr: 0.000050  loss: 0.1645 (0.1645)  loss_classifier: 0.0255 (0.0255)  loss_box_reg: 0.0121 (0.0121)  loss_mask: 0.1195 (0.1195)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.8885  data: 0.2840  max mem: 3596
Epoch: [6]  [10/60]  eta: 0:01:17  lr: 0.000050  loss: 0.1414 (0.1454)  loss_classifier: 0.0206 (0.0221)  loss_box_reg: 0.0056 (0.0076)  loss_mask: 0.1039 (0.1099)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0058 (0.0053)  time: 1.5460  data: 0.0336  max mem: 3596
Epoch: [6]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1414 (0.1516)  loss_classifier: 0.0206 (0.0241)  loss_box_reg: 0.0065 (0.0101)  loss_mask: 0.1030 (0.1104)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0059 (0.0066)  time: 1.5057  data: 0.0092  max mem: 3596
Epoch: [6]  [30/60]  eta: 0:00:45  lr: 0.000050  loss: 0.1479 (0.1531)  loss_classifier: 0.0255 (0.0261)  loss_box_reg: 0.0087 (0.0099)  loss_mask: 0.1030 (0.1098)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0072 (0.0068)  time: 1.4797  data: 0.0104  max mem: 3596
Epoch: [6]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1493 (0.1593)  loss_classifier: 0.0255 (0.0267)  loss_box_reg: 0.0087 (0.0111)  loss_mask: 0.1043 (0.1137)  loss_objectness: 0.0005 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0072)  time: 1.4498  data: 0.0104  max mem: 3596
Epoch: [6]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1440 (0.1584)  loss_classifier: 0.0234 (0.0261)  loss_box_reg: 0.0088 (0.0110)  loss_mask: 0.1129 (0.1135)  loss_objectness: 0.0004 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0073)  time: 1.4308  data: 0.0097  max mem: 3596
Epoch: [6]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1440 (0.1588)  loss_classifier: 0.0216 (0.0260)  loss_box_reg: 0.0080 (0.0110)  loss_mask: 0.1118 (0.1140)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0070 (0.0073)  time: 1.4508  data: 0.0095  max mem: 3596
Epoch: [6] Total time: 0:01:28 (1.4739 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3615 (0.3615)  evaluator_time: 0.0038 (0.0038)  time: 0.5174  data: 0.1505  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)  time: 0.3199  data: 0.0057  max mem: 3596
Test: Total time: 0:00:16 (0.3225 s / it)
Averaged stats: model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.351
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [7]  [ 0/60]  eta: 0:01:47  lr: 0.000050  loss: 0.1122 (0.1122)  loss_classifier: 0.0151 (0.0151)  loss_box_reg: 0.0039 (0.0039)  loss_mask: 0.0920 (0.0920)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.7859  data: 0.4771  max mem: 3596
Epoch: [7]  [10/60]  eta: 0:01:11  lr: 0.000050  loss: 0.1316 (0.1467)  loss_classifier: 0.0157 (0.0221)  loss_box_reg: 0.0052 (0.0086)  loss_mask: 0.1004 (0.1097)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0037 (0.0053)  time: 1.4340  data: 0.0504  max mem: 3596
Epoch: [7]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1570 (0.1557)  loss_classifier: 0.0288 (0.0274)  loss_box_reg: 0.0085 (0.0100)  loss_mask: 0.1075 (0.1104)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0066 (0.0068)  time: 1.4943  data: 0.0092  max mem: 3596
Epoch: [7]  [30/60]  eta: 0:00:44  lr: 0.000050  loss: 0.1447 (0.1519)  loss_classifier: 0.0257 (0.0255)  loss_box_reg: 0.0076 (0.0093)  loss_mask: 0.1062 (0.1092)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0071 (0.0068)  time: 1.4985  data: 0.0104  max mem: 3596
Epoch: [7]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1418 (0.1546)  loss_classifier: 0.0222 (0.0251)  loss_box_reg: 0.0068 (0.0098)  loss_mask: 0.1095 (0.1120)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0054 (0.0069)  time: 1.4120  data: 0.0099  max mem: 3596
Epoch: [7]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1616 (0.1590)  loss_classifier: 0.0232 (0.0253)  loss_box_reg: 0.0082 (0.0107)  loss_mask: 0.1132 (0.1150)  loss_objectness: 0.0002 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0072)  time: 1.4197  data: 0.0096  max mem: 3596
Epoch: [7]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1474 (0.1592)  loss_classifier: 0.0230 (0.0256)  loss_box_reg: 0.0060 (0.0106)  loss_mask: 0.1101 (0.1150)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0057 (0.0072)  time: 1.4280  data: 0.0095  max mem: 3596
Epoch: [7] Total time: 0:01:27 (1.4505 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3761 (0.3761)  evaluator_time: 0.0044 (0.0044)  time: 0.5139  data: 0.1315  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)  time: 0.3181  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3223 s / it)
Averaged stats: model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.946
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.923
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.382
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.352
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [8]  [ 0/60]  eta: 0:01:59  lr: 0.000050  loss: 0.1533 (0.1533)  loss_classifier: 0.0187 (0.0187)  loss_box_reg: 0.0076 (0.0076)  loss_mask: 0.1242 (0.1242)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0027 (0.0027)  time: 1.9847  data: 0.7161  max mem: 3596
Epoch: [8]  [10/60]  eta: 0:01:14  lr: 0.000050  loss: 0.1533 (0.1537)  loss_classifier: 0.0227 (0.0245)  loss_box_reg: 0.0076 (0.0104)  loss_mask: 0.1094 (0.1121)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0047 (0.0061)  time: 1.4974  data: 0.0700  max mem: 3596
Epoch: [8]  [20/60]  eta: 0:00:58  lr: 0.000050  loss: 0.1512 (0.1592)  loss_classifier: 0.0217 (0.0230)  loss_box_reg: 0.0084 (0.0112)  loss_mask: 0.1071 (0.1168)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0070 (0.0072)  time: 1.4405  data: 0.0081  max mem: 3596
Epoch: [8]  [30/60]  eta: 0:00:43  lr: 0.000050  loss: 0.1390 (0.1557)  loss_classifier: 0.0217 (0.0237)  loss_box_reg: 0.0084 (0.0111)  loss_mask: 0.1021 (0.1130)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0071)  time: 1.4221  data: 0.0102  max mem: 3596
Epoch: [8]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1438 (0.1632)  loss_classifier: 0.0257 (0.0257)  loss_box_reg: 0.0086 (0.0117)  loss_mask: 0.1112 (0.1172)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0076 (0.0075)  time: 1.4616  data: 0.0096  max mem: 3596
Epoch: [8]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1613 (0.1628)  loss_classifier: 0.0265 (0.0265)  loss_box_reg: 0.0104 (0.0116)  loss_mask: 0.1130 (0.1157)  loss_objectness: 0.0005 (0.0011)  loss_rpn_box_reg: 0.0076 (0.0080)  time: 1.5084  data: 0.0096  max mem: 3596
Epoch: [8]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1426 (0.1593)  loss_classifier: 0.0209 (0.0254)  loss_box_reg: 0.0060 (0.0107)  loss_mask: 0.1046 (0.1148)  loss_objectness: 0.0003 (0.0011)  loss_rpn_box_reg: 0.0056 (0.0073)  time: 1.4303  data: 0.0097  max mem: 3596
Epoch: [8] Total time: 0:01:27 (1.4531 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3871 (0.3871)  evaluator_time: 0.0041 (0.0041)  time: 0.5413  data: 0.1481  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)  time: 0.3191  data: 0.0052  max mem: 3596
Test: Total time: 0:00:16 (0.3230 s / it)
Averaged stats: model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [9]  [ 0/60]  eta: 0:01:35  lr: 0.000005  loss: 0.1384 (0.1384)  loss_classifier: 0.0122 (0.0122)  loss_box_reg: 0.0035 (0.0035)  loss_mask: 0.1193 (0.1193)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0031 (0.0031)  time: 1.5976  data: 0.1940  max mem: 3596
Epoch: [9]  [10/60]  eta: 0:01:12  lr: 0.000005  loss: 0.1391 (0.1678)  loss_classifier: 0.0229 (0.0239)  loss_box_reg: 0.0087 (0.0123)  loss_mask: 0.1188 (0.1247)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0055 (0.0064)  time: 1.4416  data: 0.0261  max mem: 3596
Epoch: [9]  [20/60]  eta: 0:00:57  lr: 0.000005  loss: 0.1595 (0.1658)  loss_classifier: 0.0253 (0.0262)  loss_box_reg: 0.0106 (0.0121)  loss_mask: 0.1154 (0.1203)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0056 (0.0066)  time: 1.4367  data: 0.0094  max mem: 3596
Epoch: [9]  [30/60]  eta: 0:00:43  lr: 0.000005  loss: 0.1595 (0.1680)  loss_classifier: 0.0256 (0.0275)  loss_box_reg: 0.0088 (0.0125)  loss_mask: 0.1150 (0.1199)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0060 (0.0074)  time: 1.4554  data: 0.0095  max mem: 3596
Epoch: [9]  [40/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1449 (0.1605)  loss_classifier: 0.0212 (0.0258)  loss_box_reg: 0.0070 (0.0111)  loss_mask: 0.1049 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0064 (0.0069)  time: 1.4446  data: 0.0094  max mem: 3596
Epoch: [9]  [50/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1504 (0.1591)  loss_classifier: 0.0195 (0.0256)  loss_box_reg: 0.0083 (0.0109)  loss_mask: 0.1037 (0.1149)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0071)  time: 1.4887  data: 0.0095  max mem: 3596
Epoch: [9]  [59/60]  eta: 0:00:01  lr: 0.000005  loss: 0.1527 (0.1602)  loss_classifier: 0.0224 (0.0256)  loss_box_reg: 0.0083 (0.0108)  loss_mask: 0.1102 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0072)  time: 1.4984  data: 0.0097  max mem: 3596
Epoch: [9] Total time: 0:01:27 (1.4592 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3668 (0.3668)  evaluator_time: 0.0042 (0.0042)  time: 0.5024  data: 0.1296  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)  time: 0.3215  data: 0.0061  max mem: 3596
Test: Total time: 0:00:16 (0.3233 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.822
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.832
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.874
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.774
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
</code></pre><p>Now that training has finished, let’s have a look at what it actually predicts in a test image</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pick one image from the test set</span></span><br><span class="line">img, _ = dataset_test[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># put the model in evaluation mode</span></span><br><span class="line">model.eval()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    prediction = model([img.to(device)])</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
</code></pre><p>Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.<br>The dictionary contains the predictions for the image we passed. In this case, we can see that it contains <code>boxes</code>, <code>labels</code>, <code>masks</code> and <code>scores</code> as fields.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prediction</span><br></pre></td></tr></table></figure>
<pre><code>[{&#39;boxes&#39;: tensor([[ 59.6432,  41.9334, 195.6993, 327.8640],
          [276.4631,  22.6867, 290.8581,  73.6079]], device=&#39;cuda:0&#39;),
  &#39;labels&#39;: tensor([1, 1], device=&#39;cuda:0&#39;),
  &#39;masks&#39;: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]],


          [[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]]], device=&#39;cuda:0&#39;),
  &#39;scores&#39;: tensor([0.9991, 0.8170], device=&#39;cuda:0&#39;)}]
</code></pre><p>Let’s inspect the image and the predicted segmentation masks.</p>
<p>For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in <code>[C, H, W]</code> format.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Image.fromarray(img.mul(<span class="number">255</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).byte().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_31_0.png" alt="png"></p>
<p>And let’s now visualize the top predicted segmentation mask. The masks are predicted as <code>[N, 1, H, W]</code>, where <code>N</code> is the number of predictions, and are probability maps between 0-1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Image.fromarray(prediction[<span class="number">0</span>][<span class="string">'masks'</span>][<span class="number">0</span>, <span class="number">0</span>].mul(<span class="number">255</span>).byte().cpu().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_33_0.png" alt="png"></p>
<p>Looks pretty good!</p>
<h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset.<br>For that, you wrote a <code>torch.utils.data.Dataset</code> class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.</p>
<p>For a more complete example, which includes multi-machine / multi-gpu training, check <code>references/detection/train.py</code>, which is present in the <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision GitHub repo</a>. </p>
<p>#<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">coordinates:坐标, 座标,协调, 配合, 接应</span><br><span class="line">segmentation:分割</span><br><span class="line">backbone:主干</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Audio-torchaudio</title>
    <url>/2020/07/24/Pytorch-Audio-torchaudio/</url>
    <content><![CDATA[<p>Pytorch-Audio-torchaudio:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line">pip install torchaudio</span><br></pre></td></tr></table></figure>
<pre><code>Collecting torchaudio
[?25l  Downloading https://files.pythonhosted.org/packages/e9/0a/40e53c686c2af65b2a4e818d11d9b76fa79178440caf99f3ceb2a32c3b04/torchaudio-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)
[K     |████████████████████████████████| 3.2MB 2.8MB/s 
[?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)
Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (0.16.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (1.18.5)
Installing collected packages: torchaudio
Successfully installed torchaudio-0.5.1
</code></pre><h1 id="torchaudio-Tutorial"><a href="#torchaudio-Tutorial" class="headerlink" title="torchaudio Tutorial"></a>torchaudio Tutorial</h1><p>PyTorch is an open source deep learning platform that provides a<br>seamless path from research prototyping to production deployment with<br>GPU support.</p>
<p>Significant effort in solving machine learning problems goes into data<br>preparation. <code>torchaudio</code> leverages PyTorch’s GPU support, and provides<br>many tools to make data loading easy and more readable. In this<br>tutorial, we will see how to load and preprocess data from a simple<br>dataset.</p>
<p>For this tutorial, please make sure the <code>matplotlib</code> package is<br>installed for easier visualization.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchaudio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="Opening-a-file"><a href="#Opening-a-file" class="headerlink" title="Opening a file"></a>Opening a file</h2><p><code>torchaudio</code> also supports loading sound files in the wav and mp3 format. We<br>call waveform the resulting raw audio signal.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename = <span class="string">"test.mp3"</span></span><br><span class="line">waveform, sample_rate = torchaudio.load(filename)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of waveform: &#123;&#125;"</span>.format(waveform.size()))</span><br><span class="line">print(<span class="string">"Sample rate of waveform: &#123;&#125;"</span>.format(sample_rate))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of waveform: torch.Size([2, 10857600])
Sample rate of waveform: 44100





[&lt;matplotlib.lines.Line2D at 0x7f91dd44a4a8&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dd44a5c0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_5_2.png" alt="png"></p>
<p>When you load a file in <code>torchaudio</code>, you can optionally specify the backend to use either<br><code>SoX &lt;https://pypi.org/project/sox/&gt;</code>_ or <code>SoundFile &lt;https://pypi.org/project/SoundFile/&gt;</code>_<br>via <code>torchaudio.set_audio_backend</code>. These backends are loaded lazily when needed.</p>
<p><code>torchaudio</code> also makes JIT compilation optional for functions, and uses <code>nn.Module</code> where possible.</p>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p><code>torchaudio</code> supports a growing list of<br><code>transformations &lt;https://pytorch.org/audio/transforms.html&gt;</code>_.</p>
<ul>
<li><strong>Resample</strong>: Resample waveform to a different sample rate.</li>
<li><strong>Spectrogram</strong>: Create a spectrogram from a waveform.</li>
<li><strong>GriffinLim</strong>: Compute waveform from a linear scale magnitude spectrogram using<br>the Griffin-Lim transformation.</li>
<li><strong>ComputeDeltas</strong>: Compute delta coefficients of a tensor, usually a spectrogram.</li>
<li><strong>ComplexNorm</strong>: Compute the norm of a complex tensor.</li>
<li><strong>MelScale</strong>: This turns a normal STFT into a Mel-frequency STFT,<br>using a conversion matrix.</li>
<li><strong>AmplitudeToDB</strong>: This turns a spectrogram from the<br>power/amplitude scale to the decibel scale.</li>
<li><strong>MFCC</strong>: Create the Mel-frequency cepstrum coefficients from a<br>waveform.</li>
<li><strong>MelSpectrogram</strong>: Create MEL Spectrograms from a waveform using the<br>STFT function in PyTorch.</li>
<li><strong>MuLawEncoding</strong>: Encode waveform based on mu-law companding.</li>
<li><strong>MuLawDecoding</strong>: Decode mu-law encoded waveform.</li>
<li><strong>TimeStretch</strong>: Stretch a spectrogram in time without modifying pitch for a given rate.</li>
<li><strong>FrequencyMasking</strong>: Apply masking to a spectrogram in the frequency domain.</li>
<li><strong>TimeMasking</strong>: Apply masking to a spectrogram in the time domain.</li>
</ul>
<p>Each transform supports batching: you can perform a transform on a single raw<br>audio signal or spectrogram, or many of the same shape.</p>
<p>Since all transforms are <code>nn.Modules</code> or <code>jit.ScriptModules</code>, they can be<br>used as part of a neural network at any point.</p>
<p>To start, we can look at the log of the spectrogram on a log scale.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">specgram = torchaudio.transforms.Spectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([2, 201, 54289])





&lt;matplotlib.image.AxesImage at 0x7f91dcf1ec50&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_9_2.png" alt="png"></p>
<p>Or we can look at the Mel Spectrogram on a log scale.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">specgram = torchaudio.transforms.MelSpectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">p = plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([2, 128, 54289])
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_11_1.png" alt="png"></p>
<p>We can resample the waveform, one channel at a time.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_sample_rate = sample_rate/<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Since Resample applies to a single channel, we resample first channel here</span></span><br><span class="line">channel = <span class="number">0</span></span><br><span class="line">transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([1, 1085760])





[&lt;matplotlib.lines.Line2D at 0x7f91dce23240&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_13_2.png" alt="png"></p>
<p>As another example of transformations, we can encode the signal based on<br>Mu-Law enconding. But to do so, we need the signal to be between -1 and</p>
<ol>
<li>Since the tensor is just a regular PyTorch tensor, we can apply<br>standard operators on it.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Let's check if the tensor is in the interval [-1,1]</span></span><br><span class="line">print(<span class="string">"Min of waveform: &#123;&#125;\nMax of waveform: &#123;&#125;\nMean of waveform: &#123;&#125;"</span>.format(waveform.min(), waveform.max(), waveform.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Min of waveform: -1.0
Max of waveform: 1.0
Mean of waveform: -4.018312756670639e-05
</code></pre><p>Since the waveform is already between -1 and 1, we do not need to<br>normalize it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    <span class="comment"># Subtract the mean, and scale to the interval [-1,1]</span></span><br><span class="line">    tensor_minusmean = tensor - tensor.mean()</span><br><span class="line">    <span class="keyword">return</span> tensor_minusmean/tensor_minusmean.abs().max()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's normalize to the full interval [-1,1]</span></span><br><span class="line"><span class="comment"># waveform = normalize(waveform)</span></span><br></pre></td></tr></table></figure>
<p>Let’s apply encode the waveform.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transformed = torchaudio.transforms.MuLawEncoding()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dce02b38&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_19_2.png" alt="png"></p>
<p>And now decode.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reconstructed = torchaudio.transforms.MuLawDecoding()(transformed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of recovered waveform: &#123;&#125;"</span>.format(reconstructed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(reconstructed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of recovered waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dcd62ef0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_21_2.png" alt="png"></p>
<p>We can finally compare the original waveform with its reconstructed<br>version.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Compute median relative difference</span></span><br><span class="line">err = ((waveform-reconstructed).abs() / waveform.abs()).median()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Median relative difference between original and MuLaw reconstucted signals: &#123;:.2%&#125;"</span>.format(err))</span><br></pre></td></tr></table></figure>
<pre><code>Median relative difference between original and MuLaw reconstucted signals: 1.20%
</code></pre><h2 id="Functional"><a href="#Functional" class="headerlink" title="Functional"></a>Functional</h2><p>The transformations seen above rely on lower level stateless functions for their computations.<br>These functions are available under <code>torchaudio.functional</code>. The complete list is available<br><code>here &lt;https://pytorch.org/audio/functional.html&gt;</code>_ and includes:</p>
<ul>
<li><strong>istft</strong>: Inverse short time Fourier Transform.</li>
<li><strong>gain</strong>: Applies amplification or attenuation to the whole waveform.</li>
<li><strong>dither</strong>: Increases the perceived dynamic range of audio stored at a<br>particular bit-depth.</li>
<li><strong>compute_deltas</strong>: Compute delta coefficients of a tensor.</li>
<li><strong>equalizer_biquad</strong>: Design biquad peaking equalizer filter and perform filtering.</li>
<li><strong>lowpass_biquad</strong>: Design biquad lowpass filter and perform filtering.</li>
<li><strong>highpass_biquad</strong>:Design biquad highpass filter and perform filtering.</li>
</ul>
<p>For example, let’s try the <code>mu_law_encoding</code> functional:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mu_law_encoding_waveform = torchaudio.functional.mu_law_encoding(waveform, quantization_channels=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(mu_law_encoding_waveform.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(mu_law_encoding_waveform[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dcd545f8&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_25_2.png" alt="png"></p>
<p>You can see how the output fron <code>torchaudio.functional.mu_law_encoding</code> is the same as<br>the output from <code>torchaudio.transforms.MuLawEncoding</code>.</p>
<p>Now let’s experiment with a few of the other functionals and visualize their output. Taking our<br>spectogram, we can compute it’s deltas:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">computed = torchaudio.functional.compute_deltas(specgram.contiguous(), win_length=<span class="number">3</span>)</span><br><span class="line">print(<span class="string">"Shape of computed deltas: &#123;&#125;"</span>.format(computed.shape))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(computed.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of computed deltas: torch.Size([2, 128, 54289])





&lt;matplotlib.image.AxesImage at 0x7f91dccb0b70&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_27_2.png" alt="png"></p>
<p>We can take the original waveform and apply different effects to it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gain_waveform = torchaudio.functional.gain(waveform, gain_db=<span class="number">5.0</span>)</span><br><span class="line">print(<span class="string">"Min of gain_waveform: &#123;&#125;\nMax of gain_waveform: &#123;&#125;\nMean of gain_waveform: &#123;&#125;"</span>.format(gain_waveform.min(), gain_waveform.max(), gain_waveform.mean()))</span><br><span class="line"></span><br><span class="line">dither_waveform = torchaudio.functional.dither(waveform)</span><br><span class="line">print(<span class="string">"Min of dither_waveform: &#123;&#125;\nMax of dither_waveform: &#123;&#125;\nMean of dither_waveform: &#123;&#125;"</span>.format(dither_waveform.min(), dither_waveform.max(), dither_waveform.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Min of gain_waveform: -1.778279423713684
Max of gain_waveform: 1.778279423713684
Mean of gain_waveform: -7.145693234633654e-05
Min of dither_waveform: -0.99993896484375
Max of dither_waveform: 0.999969482421875
Mean of dither_waveform: -2.492486237315461e-05
</code></pre><p>Another example of the capabilities in <code>torchaudio.functional</code> are applying filters to our<br>waveform. Applying the lowpass biquad filter to our waveform will output a new waveform with<br>the signal of the frequency modified.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowpass_waveform = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of lowpass_waveform: &#123;&#125;\nMax of lowpass_waveform: &#123;&#125;\nMean of lowpass_waveform: &#123;&#125;"</span>.format(lowpass_waveform.min(), lowpass_waveform.max(), lowpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(lowpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Min of lowpass_waveform: -1.0
Max of lowpass_waveform: 1.0
Mean of lowpass_waveform: -4.02079094783403e-05





[&lt;matplotlib.lines.Line2D at 0x7f91dcb8e278&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dcb8e390&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_31_2.png" alt="png"></p>
<p>We can also visualize a waveform with the highpass biquad filter.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">highpass_waveform = torchaudio.functional.highpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of highpass_waveform: &#123;&#125;\nMax of highpass_waveform: &#123;&#125;\nMean of highpass_waveform: &#123;&#125;"</span>.format(highpass_waveform.min(), highpass_waveform.max(), highpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(highpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Min of highpass_waveform: -0.8367071151733398
Max of highpass_waveform: 0.7935813069343567
Mean of highpass_waveform: -9.841002679422672e-09





[&lt;matplotlib.lines.Line2D at 0x7f91dcaf3e48&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dcaf3f60&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_33_2.png" alt="png"></p>
<h2 id="Migrating-to-torchaudio-from-Kaldi"><a href="#Migrating-to-torchaudio-from-Kaldi" class="headerlink" title="Migrating to torchaudio from Kaldi"></a>Migrating to torchaudio from Kaldi</h2><p>Users may be familiar with<br><code>Kaldi &lt;http://github.com/kaldi-asr/kaldi&gt;</code>_, a toolkit for speech<br>recognition. <code>torchaudio</code> offers compatibility with it in<br><code>torchaudio.kaldi_io</code>. It can indeed read from kaldi scp, or ark file<br>or streams with:</p>
<ul>
<li>read_vec_int_ark</li>
<li>read_vec_flt_scp</li>
<li>read_vec_flt_arkfile/stream</li>
<li>read_mat_scp</li>
<li>read_mat_ark</li>
</ul>
<p><code>torchaudio</code> provides Kaldi-compatible transforms for <code>spectrogram</code>,<br><code>fbank</code>, <code>mfcc</code>, and <code>`resample_waveform with the benefit of GPU support, see</code>here <compliance.kaldi.html>`__ for more information.</compliance.kaldi.html></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_fft = <span class="number">400.0</span></span><br><span class="line">frame_length = n_fft / sample_rate * <span class="number">1000.0</span></span><br><span class="line">frame_shift = frame_length / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">"channel"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"dither"</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">"window_type"</span>: <span class="string">"hanning"</span>,</span><br><span class="line">    <span class="string">"frame_length"</span>: frame_length,</span><br><span class="line">    <span class="string">"frame_shift"</span>: frame_shift,</span><br><span class="line">    <span class="string">"remove_dc_offset"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"round_to_power_of_two"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"sample_frequency"</span>: sample_rate,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([54287, 201])





&lt;matplotlib.image.AxesImage at 0x7f91dca6d240&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_35_2.png" alt="png"></p>
<p>We also support computing the filterbank features from waveforms,<br>matching Kaldi’s implementation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fbank = torchaudio.compliance.kaldi.fbank(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of fbank: &#123;&#125;"</span>.format(fbank.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(fbank.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of fbank: torch.Size([54287, 23])





&lt;matplotlib.image.AxesImage at 0x7f91dca440f0&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_37_2.png" alt="png"></p>
<p>You can create mel frequency cepstral coefficients from a raw audio signal<br>This matches the input/output of Kaldi’s compute-mfcc-feats.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mfcc = torchaudio.compliance.kaldi.mfcc(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of mfcc: &#123;&#125;"</span>.format(mfcc.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(mfcc.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of mfcc: torch.Size([54287, 13])





&lt;matplotlib.image.AxesImage at 0x7f91dca16828&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_39_2.png" alt="png"></p>
<h2 id="Available-Datasets"><a href="#Available-Datasets" class="headerlink" title="Available Datasets"></a>Available Datasets</h2><p>If you do not want to create your own dataset to train your model, <code>torchaudio</code> offers a<br>unified dataset interface. This interface supports lazy-loading of files to memory, download<br>and extract functions, and datasets to build models.</p>
<p>The datasets <code>torchaudio</code> currently supports are:</p>
<ul>
<li><strong>VCTK</strong>: Speech data uttered by 109 native speakers of English with various accents<br>(<code>Read more here &lt;https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html&gt;</code>_).</li>
<li><strong>Yesno</strong>: Sixty recordings of one individual saying yes or no in Hebrew; each<br>recording is eight words long (<code>Read more here &lt;https://www.openslr.org/1/&gt;</code>_).</li>
<li><strong>Common Voice</strong>: An open source, multi-language dataset of voices that anyone can use<br>to train speech-enabled applications (<code>Read more here &lt;https://voice.mozilla.org/en/datasets&gt;</code>_).</li>
<li><strong>LibriSpeech</strong>: Large-scale (1000 hours) corpus of read English speech (<code>Read more here &lt;http://www.openslr.org/12&gt;</code>_).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yesno_data = torchaudio.datasets.YESNO(<span class="string">'./'</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A data point in Yesno is a tuple (waveform, sample_rate, labels) where labels is a list of integers with 1 for yes and 0 for no.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pick data point number 3 to see an example of the the yesno_data:</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">waveform, sample_rate, labels = yesno_data[n]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Waveform: &#123;&#125;\nSample rate: &#123;&#125;\nLabels: &#123;&#125;"</span>.format(waveform, sample_rate, labels))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=4703754.0), HTML(value=&#39;&#39;)))



Waveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -8.5449e-04,
         -1.0986e-03, -8.8501e-04]])
Sample rate: 8000
Labels: [1, 0, 0, 0, 1, 0, 0, 1]





[&lt;matplotlib.lines.Line2D at 0x7f91dbcfb1d0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_41_3.png" alt="png"></p>
<p>Now, whenever you ask for a sound file from the dataset, it is loaded in memory only when you ask for it.<br>Meaning, the dataset only loads and keeps in memory the items that you want and use, saving on memory.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We used an example raw audio signal, or waveform, to illustrate how to<br>open an audio file using <code>torchaudio</code>, and how to pre-process,<br>transform, and apply functions to such waveform. We also demonstrated how<br>to use familiar Kaldi functions, as well as utilize built-in datasets to<br>construct our models. Given that <code>torchaudio</code> is built on PyTorch,<br>these techniques can be used as building blocks for more advanced audio<br>applications, such as speech recognition, while leveraging GPUs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Audio</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Reinforcement-Learning</title>
    <url>/2020/07/25/Pytorch-Reinforcement-Learning/</url>
    <content><![CDATA[<p>Pytorch-Reinforcement-Learning:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Reinforcement-Learning-DQN-Tutorial"><a href="#Reinforcement-Learning-DQN-Tutorial" class="headerlink" title="Reinforcement Learning (DQN) Tutorial"></a>Reinforcement Learning (DQN) Tutorial</h1><p><strong>Author</strong>: <code>Adam Paszke &lt;https://github.com/apaszke&gt;</code>_</p>
<p>This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent<br>on the CartPole-v0 task from the <code>OpenAI Gym &lt;https://gym.openai.com/&gt;</code>__.</p>
<p><strong>Task</strong></p>
<p>The agent has to decide between two actions - moving the cart left or<br>right - so that the pole attached to it stays upright. You can find an<br>official leaderboard with various algorithms and visualizations at the<br><code>Gym website &lt;https://gym.openai.com/envs/CartPole-v0&gt;</code>__.</p>
<p><img src="https://pytorch.org/tutorials/_images/cartpole.gif" alt></p>
<p>   cartpole</p>
<p>As the agent observes the current state of the environment and chooses<br>an action, the environment <em>transitions</em> to a new state, and also<br>returns a reward that indicates the consequences of the action. In this<br>task, rewards are +1 for every incremental timestep and the environment<br>terminates if the pole falls over too far or the cart moves more then 2.4<br>units away from center. This means better performing scenarios will run<br>for longer duration, accumulating larger return.</p>
<p>The CartPole task is designed so that the inputs to the agent are 4 real<br>values representing the environment state (position, velocity, etc.).<br>However, neural networks can solve the task purely by looking at the<br>scene, so we’ll use a patch of the screen centered on the cart as an<br>input. Because of this, our results aren’t directly comparable to the<br>ones from the official leaderboard - our task is much harder.<br>Unfortunately this does slow down the training, because we have to<br>render all the frames.</p>
<p>Strictly speaking, we will present the state as the difference between<br>the current screen patch and the previous one. This will allow the agent<br>to take the velocity of the pole into account from one image.</p>
<p><strong>Packages</strong></p>
<p>First, let’s import needed packages. Firstly, we need<br><code>gym &lt;https://gym.openai.com/docs&gt;</code>__ for the environment<br>(Install using <code>pip install gym</code>).<br>We’ll also use the following from PyTorch:</p>
<ul>
<li>neural networks (<code>torch.nn</code>)</li>
<li>optimization (<code>torch.optim</code>)</li>
<li>automatic differentiation (<code>torch.autograd</code>)</li>
<li>utilities for vision tasks (<code>torchvision</code> - <code>a separate
package &lt;https://github.com/pytorch/vision&gt;</code>__).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line"><span class="comment"># set up matplotlib</span></span><br><span class="line">is_ipython = <span class="string">'inline'</span> <span class="keyword">in</span> matplotlib.get_backend()</span><br><span class="line"><span class="keyword">if</span> is_ipython:</span><br><span class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="comment"># if gpu is to be used</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><p>We’ll be using experience replay memory for training our DQN. It stores<br>the transitions that the agent observes, allowing us to reuse this data<br>later. By sampling from it randomly, the transitions that build up a<br>batch are decorrelated. It has been shown that this greatly stabilizes<br>and improves the DQN training procedure.</p>
<p>For this, we’re going to need two classses:</p>
<ul>
<li><code>Transition</code> - a named tuple representing a single transition in<br>our environment. It essentially maps (state, action) pairs<br>to their (next_state, reward) result, with the state being the<br>screen difference image as described later on.</li>
<li><code>ReplayMemory</code> - a cyclic buffer of bounded size that holds the<br>transitions observed recently. It also implements a <code>.sample()</code><br>method for selecting a random batch of transitions for training.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>,</span><br><span class="line">                        (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.memory = []</span><br><span class="line">        self.position = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Saves a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.memory) &lt; self.capacity:</span><br><span class="line">            self.memory.append(<span class="literal">None</span>)</span><br><span class="line">        self.memory[self.position] = Transition(*args)</span><br><span class="line">        self.position = (self.position + <span class="number">1</span>) % self.capacity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.memory, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.memory)</span><br></pre></td></tr></table></figure>
<p>Now, let’s define our model. But first, let quickly recap what a DQN is.</p>
<h2 id="DQN-algorithm"><a href="#DQN-algorithm" class="headerlink" title="DQN algorithm"></a>DQN algorithm</h2><p>Our environment is deterministic, so all equations presented here are<br>also formulated deterministically for the sake of simplicity. In the<br>reinforcement learning literature, they would also contain expectations<br>over stochastic transitions in the environment.</p>
<p>Our aim will be to train a policy that tries to maximize the discounted,<br>cumulative reward<br>$R_{t_0} = \sum_{t=t_0}^{\infty} \gamma^{t - t_0} r_t$, where<br>$R_{t_0}$ is also known as the <em>return</em>. The discount,<br>$\gamma$, should be a constant between $0$ and $1$<br>that ensures the sum converges. It makes rewards from the uncertain far<br>future less important for our agent than the ones in the near future<br>that it can be fairly confident about.</p>
<p>The main idea behind Q-learning is that if we had a function<br>$Q^*: State \times Action \rightarrow \mathbb{R}$, that could tell<br>us what our return would be, if we were to take an action in a given<br>state, then we could easily construct a policy that maximizes our<br>rewards:</p>
<p>\begin{align}\pi^<em>(s) = \arg!\max_a \ Q^</em>(s, a)\end{align}</p>
<p>However, we don’t know everything about the world, so we don’t have<br>access to $Q^<em>$. But, since neural networks are universal function<br>approximators, we can simply create one and train it to resemble<br>$Q^</em>$.</p>
<p>For our training update rule, we’ll use a fact that every $Q$<br>function for some policy obeys the Bellman equation:</p>
<p>\begin{align}Q^{\pi}(s, a) = r + \gamma Q^{\pi}(s’, \pi(s’))\end{align}</p>
<p>The difference between the two sides of the equality is known as the<br>temporal difference error, $\delta$:</p>
<p>\begin{align}\delta = Q(s, a) - (r + \gamma \max_a Q(s’, a))\end{align}</p>
<p>To minimise this error, we will use the <code>Huber
loss &lt;https://en.wikipedia.org/wiki/Huber_loss&gt;</code>__. The Huber loss acts<br>like the mean squared error when the error is small, but like the mean<br>absolute error when the error is large - this makes it more robust to<br>outliers when the estimates of $Q$ are very noisy. We calculate<br>this over a batch of transitions, $B$, sampled from the replay<br>memory:</p>
<p>\begin{align}\mathcal{L} = \frac{1}{|B|}\sum_{(s, a, s’, r) \ \in \ B} \mathcal{L}(\delta)\end{align}</p>
<p>\begin{align}\text{where} \quad \mathcal{L}(\delta) = \begin{cases}<br>     \frac{1}{2}{\delta^2}  &amp; \text{for } |\delta| \le 1, \\<br>     |\delta| - \frac{1}{2} &amp; \text{otherwise.}<br>   \end{cases}\end{align}</p>
<p>Q-network</p>
<p>Our model will be a convolutional neural network that takes in the<br>difference between the current and previous screen patches. It has two<br>outputs, representing $Q(s, \mathrm{left})$ and<br>$Q(s, \mathrm{right})$ (where $s$ is the input to the<br>network). In effect, the network is trying to predict the <em>expected return</em> of<br>taking each action given the current input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, w, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Number of Linear input connections depends on output of conv2d layers</span></span><br><span class="line">        <span class="comment"># and therefore the input image size, so compute it.</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv2d_size_out</span><span class="params">(size, kernel_size = <span class="number">5</span>, stride = <span class="number">2</span>)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> (size - (kernel_size - <span class="number">1</span>) - <span class="number">1</span>) // stride  + <span class="number">1</span></span><br><span class="line">        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))</span><br><span class="line">        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))</span><br><span class="line">        linear_input_size = convw * convh * <span class="number">32</span></span><br><span class="line">        self.head = nn.Linear(linear_input_size, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Called with either one element to determine next action, or a batch</span></span><br><span class="line">    <span class="comment"># during optimization. Returns tensor([[left0exp,right0exp]...]).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        <span class="keyword">return</span> self.head(x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<p>Input extraction</p>
<p>The code below are utilities for extracting and processing rendered<br>images from the environment. It uses the <code>torchvision</code> package, which<br>makes it easy to compose image transforms. Once you run the cell it will<br>display an example patch that it extracted.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">resize = T.Compose([T.ToPILImage(),</span><br><span class="line">                    T.Resize(<span class="number">40</span>, interpolation=Image.CUBIC),</span><br><span class="line">                    T.ToTensor()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cart_location</span><span class="params">(screen_width)</span>:</span></span><br><span class="line">    world_width = env.x_threshold * <span class="number">2</span></span><br><span class="line">    scale = screen_width / world_width</span><br><span class="line">    <span class="keyword">return</span> int(env.state[<span class="number">0</span>] * scale + screen_width / <span class="number">2.0</span>)  <span class="comment"># MIDDLE OF CART</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_screen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Returned screen requested by gym is 400x600x3, but is sometimes larger</span></span><br><span class="line">    <span class="comment"># such as 800x1200x3. Transpose it into torch order (CHW).</span></span><br><span class="line">    screen = env.render(mode=<span class="string">'rgb_array'</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Cart is in the lower half, so strip off the top and bottom of the screen</span></span><br><span class="line">    _, screen_height, screen_width = screen.shape</span><br><span class="line">    screen = screen[:, int(screen_height*<span class="number">0.4</span>):int(screen_height * <span class="number">0.8</span>)]</span><br><span class="line">    view_width = int(screen_width * <span class="number">0.6</span>)</span><br><span class="line">    cart_location = get_cart_location(screen_width)</span><br><span class="line">    <span class="keyword">if</span> cart_location &lt; view_width // <span class="number">2</span>:</span><br><span class="line">        slice_range = slice(view_width)</span><br><span class="line">    <span class="keyword">elif</span> cart_location &gt; (screen_width - view_width // <span class="number">2</span>):</span><br><span class="line">        slice_range = slice(-view_width, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        slice_range = slice(cart_location - view_width // <span class="number">2</span>,</span><br><span class="line">                            cart_location + view_width // <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Strip off the edges, so that we have a square image centered on a cart</span></span><br><span class="line">    screen = screen[:, :, slice_range]</span><br><span class="line">    <span class="comment"># Convert to float, rescale, convert to torch tensor</span></span><br><span class="line">    <span class="comment"># (this doesn't require a copy)</span></span><br><span class="line">    screen = np.ascontiguousarray(screen, dtype=np.float32) / <span class="number">255</span></span><br><span class="line">    screen = torch.from_numpy(screen)</span><br><span class="line">    <span class="comment"># Resize, and add a batch dimension (BCHW)</span></span><br><span class="line">    <span class="keyword">return</span> resize(screen).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env.reset()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(get_screen().cpu().squeeze(<span class="number">0</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy(),</span><br><span class="line">           interpolation=<span class="string">'none'</span>)</span><br><span class="line">plt.title(<span class="string">'Example extracted screen'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/25/Pytorch-Reinforcement-Learning/output_8_0.png" alt="png"></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Hyperparameters and utilities</p>
<p>This cell instantiates our model and its optimizer, and defines some<br>utilities:</p>
<ul>
<li><code>select_action</code> - will select an action accordingly to an epsilon<br>greedy policy. Simply put, we’ll sometimes use our model for choosing<br>the action, and sometimes we’ll just sample one uniformly. The<br>probability of choosing a random action will start at <code>EPS_START</code><br>and will decay exponentially towards <code>EPS_END</code>. <code>EPS_DECAY</code><br>controls the rate of the decay.</li>
<li><code>plot_durations</code> - a helper for plotting the durations of episodes,<br>along with an average over the last 100 episodes (the measure used in<br>the official evaluations). The plot will be underneath the cell<br>containing the main training loop, and will update after every<br>episode.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.999</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get screen size so that we can initialize layers correctly based on shape</span></span><br><span class="line"><span class="comment"># returned from AI gym. Typical dimensions at this point are close to 3x40x90</span></span><br><span class="line"><span class="comment"># which is the result of a clamped and down-scaled render buffer in get_screen()</span></span><br><span class="line">init_screen = get_screen()</span><br><span class="line">_, _, screen_height, screen_width = init_screen.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get number of actions from gym action space</span></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line"></span><br><span class="line">policy_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.RMSprop(policy_net.parameters())</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * \</span><br><span class="line">        math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(state).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">episode_durations = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_durations</span><span class="params">()</span>:</span></span><br><span class="line">    plt.figure(<span class="number">2</span>)</span><br><span class="line">    plt.clf()</span><br><span class="line">    durations_t = torch.tensor(episode_durations, dtype=torch.float)</span><br><span class="line">    plt.title(<span class="string">'Training...'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Episode'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Duration'</span>)</span><br><span class="line">    plt.plot(durations_t.numpy())</span><br><span class="line">    <span class="comment"># Take 100 episode averages and plot them too</span></span><br><span class="line">    <span class="keyword">if</span> len(durations_t) &gt;= <span class="number">100</span>:</span><br><span class="line">        means = durations_t.unfold(<span class="number">0</span>, <span class="number">100</span>, <span class="number">1</span>).mean(<span class="number">1</span>).view(<span class="number">-1</span>)</span><br><span class="line">        means = torch.cat((torch.zeros(<span class="number">99</span>), means))</span><br><span class="line">        plt.plot(means.numpy())</span><br><span class="line"></span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line">    <span class="keyword">if</span> is_ipython:</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">        display.display(plt.gcf())</span><br></pre></td></tr></table></figure>
<p>Training loop</p>
<p>Finally, the code for training our model.</p>
<p>Here, you can find an <code>optimize_model</code> function that performs a<br>single step of the optimization. It first samples a batch, concatenates<br>all the tensors into a single one, computes $Q(s_t, a_t)$ and<br>$V(s_{t+1}) = \max_a Q(s_{t+1}, a)$, and combines them into our<br>loss. By defition we set $V(s) = 0$ if $s$ is a terminal<br>state. We also use a target network to compute $V(s_{t+1})$ for<br>added stability. The target network has its weights kept frozen most of<br>the time, but is updated with the policy network’s weights every so often.<br>This is usually a set number of steps but we shall use episodes for<br>simplicity.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    <span class="comment"># Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for</span></span><br><span class="line">    <span class="comment"># detailed explanation). This converts batch-array of Transitions</span></span><br><span class="line">    <span class="comment"># to Transition of batch-arrays.</span></span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute a mask of non-final states and concatenate the batch elements</span></span><br><span class="line">    <span class="comment"># (a final state would've been the one after which simulation ended)</span></span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span> s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">                                          batch.next_state)), device=device, dtype=torch.bool)</span><br><span class="line">    non_final_next_states = torch.cat([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state</span><br><span class="line">                                                <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>])</span><br><span class="line">    state_batch = torch.cat(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.cat(batch.reward)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q(s_t, a) - the model computes Q(s_t), then we select the</span></span><br><span class="line">    <span class="comment"># columns of actions taken. These are the actions which would've been taken</span></span><br><span class="line">    <span class="comment"># for each batch state according to policy_net</span></span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute V(s_&#123;t+1&#125;) for all next states.</span></span><br><span class="line">    <span class="comment"># Expected values of actions for non_final_next_states are computed based</span></span><br><span class="line">    <span class="comment"># on the "older" target_net; selecting their best reward with max(1)[0].</span></span><br><span class="line">    <span class="comment"># This is merged based on the mask, such that we'll have either the expected</span></span><br><span class="line">    <span class="comment"># state value or 0 in case the state was final.</span></span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE, device=device)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    <span class="comment"># Compute the expected Q values</span></span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Huber loss</span></span><br><span class="line">    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimize the model</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>Below, you can find the main training loop. At the beginning we reset<br>the environment and initialize the <code>state</code> Tensor. Then, we sample<br>an action, execute it, observe the next screen and the reward (always<br>1), and optimize our model once. When the episode ends (our model<br>fails), we restart the loop.</p>
<p>Below, <code>num_episodes</code> is set small. You should download<br>the notebook and run lot more epsiodes, such as 300+ for meaningful<br>duration improvements.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_episodes = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    <span class="comment"># Initialize the environment and state</span></span><br><span class="line">    env.reset()</span><br><span class="line">    last_screen = get_screen()</span><br><span class="line">    current_screen = get_screen()</span><br><span class="line">    state = current_screen - last_screen</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        <span class="comment"># Select and perform an action</span></span><br><span class="line">        action = select_action(state)</span><br><span class="line">        _, reward, done, _ = env.step(action.item())</span><br><span class="line">        reward = torch.tensor([reward], device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Observe new state</span></span><br><span class="line">        last_screen = current_screen</span><br><span class="line">        current_screen = get_screen()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> done:</span><br><span class="line">            next_state = current_screen - last_screen</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Store the transition in memory</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Move to the next state</span></span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform one step of the optimization (on the target network)</span></span><br><span class="line">        optimize_model()</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            episode_durations.append(t + <span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># Update the target network, copying all weights and biases in DQN</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Complete'</span>)</span><br><span class="line">env.render()</span><br><span class="line">env.close()</span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;


Complete



&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre><p>Here is the diagram that illustrates the overall resulting data flow.</p>
<p><img src="https://pytorch.org/tutorials/_images/reinforcement_learning_diagram.jpg" alt></p>
<p>Actions are chosen either randomly or based on a policy, getting the next<br>step sample from the gym environment. We record the results in the<br>replay memory and also run optimization step on every iteration.<br>Optimization picks a random batch from the replay memory to do training of the<br>new policy. “Older” target_net is also used in optimization to compute the<br>expected Q values; it is updated occasionally to keep it current.</p>
<h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample:采样</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-ReinforcementLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</title>
    <url>/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<p>Pytorch-Text-使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>This is the third and final tutorial on doing “NLP From Scratch”, where we<br>write our own classes and functions to preprocess the data to do our NLP<br>modeling tasks. We hope after you complete this tutorial that you’ll proceed to<br>learn how <code>torchtext</code> can handle much of this preprocessing for you in the<br>three tutorials immediately following this one.</p>
<p>In this project we will be teaching a neural network to translate from<br>French to English.</p>
<p>::</p>
<pre><code>[KEY: &gt; input, = target, &lt; output]

&gt; il est en train de peindre un tableau .
= he is painting a picture .
&lt; he is painting a picture .

&gt; pourquoi ne pas essayer ce vin delicieux ?
= why not try that delicious wine ?
&lt; why not try that delicious wine ?

&gt; elle n est pas poete mais romanciere .
= she is not a poet but a novelist .
&lt; she not not a poet but a novelist .

&gt; vous etes trop maigre .
= you re too skinny .
&lt; you re all alone .
</code></pre><p>… to varying degrees of success.</p>
<p>This is made possible by the simple but powerful idea of the <code>sequence
to sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code>__, in which two<br>recurrent neural networks work together to transform one sequence to<br>another. An encoder network condenses an input sequence into a vector,<br>and a decoder network unfolds that vector into a new sequence.</p>
<p>.. figure:: /_static/img/seq-seq-images/seq2seq.png<br>   :alt:</p>
<p>To improve upon this model we’ll use an <code>attention
mechanism &lt;https://arxiv.org/abs/1409.0473&gt;</code>__, which lets the decoder<br>learn to focus over a specific range of the input sequence.</p>
<p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about Sequence to Sequence networks and<br>how they work:</p>
<ul>
<li><code>Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li>
<li><code>Sequence to Sequence Learning with Neural
Networks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li>
<li><code>Neural Machine Translation by Jointly Learning to Align and
Translate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li>
<li><code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li>
</ul>
<p>You will also find the previous tutorials on<br>:doc:<code>/intermediate/char_rnn_classification_tutorial</code><br>and :doc:<code>/intermediate/char_rnn_generation_tutorial</code><br>helpful as those concepts are very similar to the Encoder and Decoder<br>models, respectively.</p>
<p>And for more, read the papers that introduced these topics:</p>
<ul>
<li><code>Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li>
<li><code>Sequence to Sequence Learning with Neural
Networks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li>
<li><code>Neural Machine Translation by Jointly Learning to Align and
Translate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li>
<li><code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li>
</ul>
<p><strong>Requirements</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Loading-data-files"><a href="#Loading-data-files" class="headerlink" title="Loading data files"></a>Loading data files</h1><p>The data for this project is a set of many thousands of English to<br>French translation pairs.</p>
<p><code>This question on Open Data Stack
Exchange &lt;https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages&gt;</code>__<br>pointed me to the open translation site <a href="https://tatoeba.org/" target="_blank" rel="noopener">https://tatoeba.org/</a> which has<br>downloads available at <a href="https://tatoeba.org/eng/downloads" target="_blank" rel="noopener">https://tatoeba.org/eng/downloads</a> - and better<br>yet, someone did the extra work of splitting language pairs into<br>individual text files here: <a href="https://www.manythings.org/anki/" target="_blank" rel="noopener">https://www.manythings.org/anki/</a></p>
<p>The English to French pairs are too big to include in the repo, so<br>download to <code>data/eng-fra.txt</code> before continuing. The file is a tab<br>separated list of translation pairs:</p>
<p>::</p>
<pre><code>I am cold.    J&#39;ai froid.
</code></pre><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>Similar to the character encoding used in the character-level RNN<br>tutorials, we will be representing each word in a language as a one-hot<br>vector, or giant vector of zeros except for a single one (at the index<br>of the word). Compared to the dozens of characters that might exist in a<br>language, there are many many more words, so the encoding vector is much<br>larger. We will however cheat a bit and trim the data to only use a few<br>thousand words per language.</p>
<p>.. figure:: /_static/img/seq-seq-images/word-encoding.png<br>   :alt:</p>
<p>We’ll need a unique index per word to use as the inputs and targets of<br>the networks later. To keep track of all this we will use a helper class<br>called <code>Lang</code> which has word → index (<code>word2index</code>) and index → word<br>(<code>index2word</code>) dictionaries, as well as a count of each word<br><code>word2count</code> to use to later replace rare words.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SOS_token = <span class="number">0</span></span><br><span class="line">EOS_token = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lang</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        self.word2count = &#123;&#125;</span><br><span class="line">        self.index2word = &#123;<span class="number">0</span>: <span class="string">"SOS"</span>, <span class="number">1</span>: <span class="string">"EOS"</span>&#125;</span><br><span class="line">        self.n_words = <span class="number">2</span>  <span class="comment"># Count SOS and EOS</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addSentence</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</span><br><span class="line">            self.addWord(word)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2index:</span><br><span class="line">            self.word2index[word] = self.n_words</span><br><span class="line">            self.word2count[word] = <span class="number">1</span></span><br><span class="line">            self.index2word[self.n_words] = word</span><br><span class="line">            self.n_words += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.word2count[word] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>The files are all in Unicode, to simplify we will turn Unicode<br>characters to ASCII, make everything lowercase, and trim most<br>punctuation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to</span></span><br><span class="line"><span class="comment"># https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># 在Unicode中，某些字符能够用多个合法的编码表示，在需要比较字符串的程序中使用字符的多种表示会产生问题。 </span></span><br><span class="line"><span class="comment"># 为了修正这个问题，你可以使用unicodedata模块先将文本标准化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Lowercase, trim, and remove non-letter characters</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeString</span><span class="params">(s)</span>:</span></span><br><span class="line">    s = unicodeToAscii(s.lower().strip())</span><br><span class="line">    s = re.sub(<span class="string">r"([.!?])"</span>, <span class="string">r" \1"</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">r"[^a-zA-Z.!?]+"</span>, <span class="string">r" "</span>, s)</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p>To read the data file we will split the file into lines, and then split<br>lines into pairs. The files are all English → Other Language, so if we<br>want to translate from Other Language → English I added the <code>reverse</code><br>flag to reverse the pairs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLangs</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    print(<span class="string">"Reading lines..."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Read the file and split into lines</span></span><br><span class="line">    lines = open(<span class="string">'data/%s-%s.txt'</span> % (lang1, lang2), encoding=<span class="string">'utf-8'</span>).\</span><br><span class="line">        read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split every line into pairs and normalize</span></span><br><span class="line">    pairs = [[normalizeString(s) <span class="keyword">for</span> s <span class="keyword">in</span> l.split(<span class="string">'\t'</span>)] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reverse pairs, make Lang instances</span></span><br><span class="line">    <span class="keyword">if</span> reverse:</span><br><span class="line">        pairs = [list(reversed(p)) <span class="keyword">for</span> p <span class="keyword">in</span> pairs]</span><br><span class="line">        input_lang = Lang(lang2)</span><br><span class="line">        output_lang = Lang(lang1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_lang = Lang(lang1)</span><br><span class="line">        output_lang = Lang(lang2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br></pre></td></tr></table></figure>
<p>Since there are a <em>lot</em> of example sentences and we want to train<br>something quickly, we’ll trim the data set to only relatively short and<br>simple sentences. Here the maximum length is 10 words (that includes<br>ending punctuation) and we’re filtering to sentences that translate to<br>the form “I am” or “He is” etc. (accounting for apostrophes replaced<br>earlier).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_LENGTH = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">eng_prefixes = (</span><br><span class="line">    <span class="string">"i am "</span>, <span class="string">"i m "</span>,</span><br><span class="line">    <span class="string">"he is"</span>, <span class="string">"he s "</span>,</span><br><span class="line">    <span class="string">"she is"</span>, <span class="string">"she s "</span>,</span><br><span class="line">    <span class="string">"you are"</span>, <span class="string">"you re "</span>,</span><br><span class="line">    <span class="string">"we are"</span>, <span class="string">"we re "</span>,</span><br><span class="line">    <span class="string">"they are"</span>, <span class="string">"they re "</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPair</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(p[<span class="number">0</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        len(p[<span class="number">1</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        p[<span class="number">1</span>].startswith(eng_prefixes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPairs</span><span class="params">(pairs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [pair <span class="keyword">for</span> pair <span class="keyword">in</span> pairs <span class="keyword">if</span> filterPair(pair)]</span><br></pre></td></tr></table></figure>
<p>The full process for preparing the data is:</p>
<ul>
<li>Read text file and split into lines, split lines into pairs</li>
<li>Normalize text, filter by length and content</li>
<li>Make word lists from sentences in pairs</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareData</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)</span><br><span class="line">    print(<span class="string">"Read %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    pairs = filterPairs(pairs)</span><br><span class="line">    print(<span class="string">"Trimmed to %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    print(<span class="string">"Counting words..."</span>)</span><br><span class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</span><br><span class="line">        input_lang.addSentence(pair[<span class="number">0</span>])</span><br><span class="line">        output_lang.addSentence(pair[<span class="number">1</span>])</span><br><span class="line">    print(<span class="string">"Counted words:"</span>)</span><br><span class="line">    print(input_lang.name, input_lang.n_words)</span><br><span class="line">    print(output_lang.name, output_lang.n_words)</span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_lang, output_lang, pairs = prepareData(<span class="string">'eng'</span>, <span class="string">'fra'</span>, <span class="literal">True</span>)</span><br><span class="line">print(random.choice(pairs))</span><br></pre></td></tr></table></figure>
<pre><code>Reading lines...
Read 135842 sentence pairs
Trimmed to 10599 sentence pairs
Counting words...
Counted words:
fra 4345
eng 2803
[&#39;nous sommes tous sur le meme bateau .&#39;, &#39;we re all in the same boat .&#39;]
</code></pre><h1 id="The-Seq2Seq-Model"><a href="#The-Seq2Seq-Model" class="headerlink" title="The Seq2Seq Model"></a>The Seq2Seq Model</h1><p>A Recurrent Neural Network, or RNN, is a network that operates on a<br>sequence and uses its own output as input for subsequent steps.</p>
<p>A <code>Sequence to Sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code><strong>, or<br>seq2seq network, or <code>Encoder Decoder
network &lt;https://arxiv.org/pdf/1406.1078v3.pdf&gt;</code></strong>, is a model<br>consisting of two RNNs called the encoder and decoder. The encoder reads<br>an input sequence and outputs a single vector, and the decoder reads<br>that vector to produce an output sequence.</p>
<p><img src="https://pytorch.org/tutorials/_images/seq2seq.png" alt></p>
<p>Unlike sequence prediction with a single RNN, where every input<br>corresponds to an output, the seq2seq model frees us from sequence<br>length and order, which makes it ideal for translation between two<br>languages.</p>
<p>Consider the sentence “Je ne suis pas le chat noir” → “I am not the<br>black cat”. Most of the words in the input sentence have a direct<br>translation in the output sentence, but are in slightly different<br>orders, e.g. “chat noir” and “black cat”. Because of the “ne/pas”<br>construction there is also one more word in the input sentence. It would<br>be difficult to produce a correct translation directly from the sequence<br>of input words.</p>
<p>With a seq2seq model the encoder creates a single vector which, in the<br>ideal case, encodes the “meaning” of the input sequence into a single<br>vector — a single point in some N dimensional space of sentences.</p>
<h2 id="The-Encoder"><a href="#The-Encoder" class="headerlink" title="The Encoder"></a>The Encoder</h2><p>The encoder of a seq2seq network is a RNN that outputs some value for<br>every word from the input sentence. For every input word the encoder<br>outputs a vector and a hidden state, and uses the hidden state for the<br>next input word.</p>
<p><img src="https://pytorch.org/tutorials/_images/decoder-network.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size)</span>:</span></span><br><span class="line">        super(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = embedded</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>
<h2 id="The-Decoder"><a href="#The-Decoder" class="headerlink" title="The Decoder"></a>The Decoder</h2><p>The decoder is another RNN that takes the encoder output vector(s) and<br>outputs a sequence of words to create the translation.</p>
<p>Simple Decoder</p>
<p>In the simplest seq2seq decoder we use only last output of the encoder.<br>This last output is sometimes called the <em>context vector</em> as it encodes<br>context from the entire sequence. This context vector is used as the<br>initial hidden state of the decoder.</p>
<p>At every step of decoding, the decoder is given an input token and<br>hidden state. The initial input token is the start-of-string <code>&lt;SOS&gt;</code><br>token, and the first hidden state is the context vector (the encoder’s<br>last hidden state).</p>
<p><img src="https://pytorch.org/tutorials/_images/attention-decoder-network.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(DecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line">        self.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        output = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        output = self.softmax(self.out(output[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>
<p>I encourage you to train and observe the results of this model, but to<br>save space we’ll be going straight for the gold and introducing the<br>Attention Mechanism.</p>
<p>Attention Decoder</p>
<p>If only the context vector is passed betweeen the encoder and decoder,<br>that single vector carries the burden of encoding the entire sentence.</p>
<p>Attention allows the decoder network to “focus” on a different part of<br>the encoder’s outputs for every step of the decoder’s own outputs. First<br>we calculate a set of <em>attention weights</em>. These will be multiplied by<br>the encoder output vectors to create a weighted combination. The result<br>(called <code>attn_applied</code> in the code) should contain information about<br>that specific part of the input sequence, and thus help the decoder<br>choose the right output words.</p>
<p>.. figure:: <a href="https://i.imgur.com/1152PYf.png" target="_blank" rel="noopener">https://i.imgur.com/1152PYf.png</a><br>   :alt:</p>
<p>Calculating the attention weights is done with another feed-forward<br>layer <code>attn</code>, using the decoder’s input and hidden state as inputs.<br>Because there are sentences of all sizes in the training data, to<br>actually create and train this layer we have to choose a maximum<br>sentence length (input length, for encoder outputs) that it can apply<br>to. Sentences of the maximum length will use all the attention weights,<br>while shorter sentences will only use the first few.</p>
<p>.. figure:: /_static/img/seq-seq-images/attention-decoder-network.png<br>   :alt:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnDecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">        super(AttnDecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.dropout_p = dropout_p</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</span><br><span class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(self.dropout_p)</span><br><span class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</span><br><span class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden, encoder_outputs)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line"></span><br><span class="line">        attn_weights = F.softmax(</span><br><span class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)), dim=<span class="number">1</span>)</span><br><span class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>)</span><br><span class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>
<div class="alert alert-info"><h4>Note</h4><p>There are other forms of attention that work around the length
  limitation by using a relative position approach. Read about "local
  attention" in `Effective Approaches to Attention-based Neural Machine
  Translation <https: arxiv.org abs 1508.04025>`__.</https:></p></div>

<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-Training-Data"><a href="#Preparing-Training-Data" class="headerlink" title="Preparing Training Data"></a>Preparing Training Data</h2><p>To train, for each pair we will need an input tensor (indexes of the<br>words in the input sentence) and target tensor (indexes of the words in<br>the target sentence). While creating these vectors we will append the<br>EOS token to both sequences.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">indexesFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [lang.word2index[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    indexes = indexesFromSentence(lang, sentence)</span><br><span class="line">    indexes.append(EOS_token)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(indexes, dtype=torch.long, device=device).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorsFromPair</span><span class="params">(pair)</span>:</span></span><br><span class="line">    input_tensor = tensorFromSentence(input_lang, pair[<span class="number">0</span>])</span><br><span class="line">    target_tensor = tensorFromSentence(output_lang, pair[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> (input_tensor, target_tensor)</span><br></pre></td></tr></table></figure>
<h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>To train we run the input sentence through the encoder, and keep track<br>of every output and the latest hidden state. Then the decoder is given<br>the <code>&lt;SOS&gt;</code> token as its first input, and the last hidden state of the<br>encoder as its first hidden state.</p>
<p>“Teacher forcing” is the concept of using the real target outputs as<br>each next input, instead of using the decoder’s guess as the next input.<br>Using teacher forcing causes it to converge faster but <code>when the trained
network is exploited, it may exhibit
instability &lt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&amp;rep=rep1&amp;type=pdf&gt;</code>__.</p>
<p>You can observe outputs of teacher-forced networks that read with<br>coherent grammar but wander far from the correct translation -<br>intuitively it has learned to represent the output grammar and can “pick<br>up” the meaning once the teacher tells it the first few words, but it<br>has not properly learned how to create the sentence from the translation<br>in the first place.</p>
<p>Because of the freedom PyTorch’s autograd gives us, we can randomly<br>choose to use teacher forcing or not with a simple if statement. Turn<br><code>teacher_forcing_ratio</code> up to use more of it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">teacher_forcing_ratio = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.zero_grad()</span><br><span class="line">    decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    input_length = input_tensor.size(<span class="number">0</span>)</span><br><span class="line">    target_length = target_tensor.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">        encoder_output, encoder_hidden = encoder(</span><br><span class="line">            input_tensor[ei], encoder_hidden)</span><br><span class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoder_input = torch.tensor([[SOS_token]], device=device)</span><br><span class="line"></span><br><span class="line">    decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">    use_teacher_forcing = <span class="literal">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_teacher_forcing:</span><br><span class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            decoder_input = target_tensor[di]  <span class="comment"># Teacher forcing</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            topv, topi = decoder_output.topk(<span class="number">1</span>)</span><br><span class="line">            decoder_input = topi.squeeze().detach()  <span class="comment"># detach from history as input</span></span><br><span class="line"></span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            <span class="keyword">if</span> decoder_input.item() == EOS_token:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.step()</span><br><span class="line">    decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item() / target_length</span><br></pre></td></tr></table></figure>
<p>This is a helper function to print time elapsed and estimated time<br>remaining given the current time and progress %.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asMinutes</span><span class="params">(s)</span>:</span></span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since, percent)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    es = s / (percent)</span><br><span class="line">    rs = es - s</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%s (- %s)'</span> % (asMinutes(s), asMinutes(rs))</span><br></pre></td></tr></table></figure>
<p>The whole training process looks like this:</p>
<ul>
<li>Start a timer</li>
<li>Initialize optimizers and criterion</li>
<li>Create set of training pairs</li>
<li>Start empty losses array for plotting</li>
</ul>
<p>Then we call <code>train</code> many times and occasionally print the progress (%<br>of examples, time so far, estimated time) and average loss.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainIters</span><span class="params">(encoder, decoder, n_iters, print_every=<span class="number">1000</span>, plot_every=<span class="number">100</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    plot_losses = []</span><br><span class="line">    print_loss_total = <span class="number">0</span>  <span class="comment"># Reset every print_every</span></span><br><span class="line">    plot_loss_total = <span class="number">0</span>  <span class="comment"># Reset every plot_every</span></span><br><span class="line"></span><br><span class="line">    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)</span><br><span class="line">    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)</span><br><span class="line">    training_pairs = [tensorsFromPair(random.choice(pairs))</span><br><span class="line">                      <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iters)]</span><br><span class="line">    criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">        training_pair = training_pairs[iter - <span class="number">1</span>]</span><br><span class="line">        input_tensor = training_pair[<span class="number">0</span>]</span><br><span class="line">        target_tensor = training_pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        loss = train(input_tensor, target_tensor, encoder,</span><br><span class="line">                     decoder, encoder_optimizer, decoder_optimizer, criterion)</span><br><span class="line">        print_loss_total += loss</span><br><span class="line">        plot_loss_total += loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">            print_loss_avg = print_loss_total / print_every</span><br><span class="line">            print_loss_total = <span class="number">0</span></span><br><span class="line">            print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start, iter / n_iters),</span><br><span class="line">                                         iter, iter / n_iters * <span class="number">100</span>, print_loss_avg))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">            plot_loss_avg = plot_loss_total / plot_every</span><br><span class="line">            plot_losses.append(plot_loss_avg)</span><br><span class="line">            plot_loss_total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    showPlot(plot_losses)</span><br></pre></td></tr></table></figure>
<h2 id="Plotting-results"><a href="#Plotting-results" class="headerlink" title="Plotting results"></a>Plotting results</h2><p>Plotting is done with matplotlib, using the array of loss values<br><code>plot_losses</code> saved while training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.switch_backend(<span class="string">'agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPlot</span><span class="params">(points)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># this locator puts ticks at regular intervals</span></span><br><span class="line">    loc = ticker.MultipleLocator(base=<span class="number">0.2</span>)</span><br><span class="line">    ax.yaxis.set_major_locator(loc)</span><br><span class="line">    plt.plot(points)</span><br></pre></td></tr></table></figure>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Evaluation is mostly the same as training, but there are no targets so<br>we simply feed the decoder’s predictions back to itself for each step.<br>Every time it predicts a word we add it to the output string, and if it<br>predicts the EOS token we stop there. We also store the decoder’s<br>attention outputs for display later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(encoder, decoder, sentence, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_tensor = tensorFromSentence(input_lang, sentence)</span><br><span class="line">        input_length = input_tensor.size()[<span class="number">0</span>]</span><br><span class="line">        encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">            encoder_output, encoder_hidden = encoder(input_tensor[ei],</span><br><span class="line">                                                     encoder_hidden)</span><br><span class="line">            encoder_outputs[ei] += encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        decoder_input = torch.tensor([[SOS_token]], device=device)  <span class="comment"># SOS</span></span><br><span class="line"></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">        decoded_words = []</span><br><span class="line">        decoder_attentions = torch.zeros(max_length, max_length)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(max_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            decoder_attentions[di] = decoder_attention.data</span><br><span class="line">            topv, topi = decoder_output.data.topk(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> topi.item() == EOS_token:</span><br><span class="line">                decoded_words.append(<span class="string">'&lt;EOS&gt;'</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                decoded_words.append(output_lang.index2word[topi.item()])</span><br><span class="line"></span><br><span class="line">            decoder_input = topi.squeeze().detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoded_words, decoder_attentions[:di + <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>We can evaluate random sentences from the training set and print out the<br>input, target, and output to make some subjective quality judgements:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateRandomly</span><span class="params">(encoder, decoder, n=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        pair = random.choice(pairs)</span><br><span class="line">        print(<span class="string">'&gt;'</span>, pair[<span class="number">0</span>])</span><br><span class="line">        print(<span class="string">'='</span>, pair[<span class="number">1</span>])</span><br><span class="line">        output_words, attentions = evaluate(encoder, decoder, pair[<span class="number">0</span>])</span><br><span class="line">        output_sentence = <span class="string">' '</span>.join(output_words)</span><br><span class="line">        print(<span class="string">'&lt;'</span>, output_sentence)</span><br><span class="line">        print(<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Training-and-Evaluating"><a href="#Training-and-Evaluating" class="headerlink" title="Training and Evaluating"></a>Training and Evaluating</h1><p>With all these helper functions in place (it looks like extra work, but<br>it makes it easier to run multiple experiments) we can actually<br>initialize a network and start training.</p>
<p>Remember that the input sentences were heavily filtered. For this small<br>dataset we can use relatively small networks of 256 hidden nodes and a<br>single GRU layer. After about 40 minutes on a MacBook CPU we’ll get some<br>reasonable results.</p>
<p>.. Note::<br>   If you run this notebook you can train, interrupt the kernel,<br>   evaluate, and continue training later. Comment out the lines where the<br>   encoder and decoder are initialized and run <code>trainIters</code> again.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line">encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)</span><br><span class="line">attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">trainIters(encoder1, attn_decoder1, <span class="number">75000</span>, print_every=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>15m 15s (- 213m 42s) (5000 6%) 2.8335
36m 35s (- 237m 48s) (10000 13%) 2.3080
60m 22s (- 241m 29s) (15000 20%) 1.9768
86m 20s (- 237m 26s) (20000 26%) 1.7509
103m 50s (- 207m 41s) (25000 33%) 1.5515
123m 36s (- 185m 24s) (30000 40%) 1.3811
141m 30s (- 161m 43s) (35000 46%) 1.2262
161m 12s (- 141m 3s) (40000 53%) 1.1208
180m 57s (- 120m 38s) (45000 60%) 1.0367
195m 38s (- 97m 49s) (50000 66%) 0.9097
206m 41s (- 75m 9s) (55000 73%) 0.8348
217m 46s (- 54m 26s) (60000 80%) 0.7563
228m 59s (- 35m 13s) (65000 86%) 0.7075
246m 41s (- 17m 37s) (70000 93%) 0.6615
263m 49s (- 0m 0s) (75000 100%) 0.6048
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluateRandomly(encoder1, attn_decoder1)</span><br></pre></td></tr></table></figure>
<pre><code>&gt; tu m ennuies .
= you re annoying .
&lt; you re embarrassing . &lt;EOS&gt;

&gt; il est maintenant etudiant a la fac .
= he s now a college student .
&lt; he s now a student now . &lt;EOS&gt;

&gt; je ne suis pas votre ami .
= i m not your friend .
&lt; i m not your friend . &lt;EOS&gt;

&gt; je suis tres fatiguee par le dur labeur .
= i am very tired from the hard work .
&lt; i am very interested in the next . . &lt;EOS&gt;

&gt; ce sont des illets .
= they re carnations .
&lt; they re carnations . &lt;EOS&gt;

&gt; il est toujours en train de se plaindre .
= he is constantly complaining .
&lt; he is always complaining . &lt;EOS&gt;

&gt; je suis submerge de travail .
= i am swamped with work .
&lt; i am swamped with work . &lt;EOS&gt;

&gt; tu es mon meilleur ami .
= you re my best friend .
&lt; you are my best friend . &lt;EOS&gt;

&gt; je vous suis reconnaissant pour votre aide .
= i am grateful to you for your help .
&lt; i am grateful for your help . &lt;EOS&gt;

&gt; je vais te conter un secret .
= i m going to tell you a secret .
&lt; i m going to tell you a secret . &lt;EOS&gt;
</code></pre><h2 id="Visualizing-Attention"><a href="#Visualizing-Attention" class="headerlink" title="Visualizing Attention"></a>Visualizing Attention</h2><p>A useful property of the attention mechanism is its highly interpretable<br>outputs. Because it is used to weight specific encoder outputs of the<br>input sequence, we can imagine looking where the network is focused most<br>at each time step.</p>
<p>You could simply run <code>plt.matshow(attentions)</code> to see attention output<br>displayed as a matrix, with the columns being input steps and rows being<br>output steps:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output_words, attentions = evaluate(</span><br><span class="line">    encoder1, attn_decoder1, <span class="string">"je suis trop froid ."</span>)</span><br><span class="line">plt.matshow(attentions.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x2424cc9a438&gt;
</code></pre><p>For a better viewing experience we will do the extra work of adding axes<br>and labels:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showAttention</span><span class="params">(input_sentence, output_words, attentions)</span>:</span></span><br><span class="line">    <span class="comment"># Set up figure with colorbar</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    cax = ax.matshow(attentions.numpy(), cmap=<span class="string">'bone'</span>)</span><br><span class="line">    fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up axes</span></span><br><span class="line">    ax.set_xticklabels([<span class="string">''</span>] + input_sentence.split(<span class="string">' '</span>) +</span><br><span class="line">                       [<span class="string">'&lt;EOS&gt;'</span>], rotation=<span class="number">90</span>)</span><br><span class="line">    ax.set_yticklabels([<span class="string">''</span>] + output_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show label at every tick</span></span><br><span class="line">    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateAndShowAttention</span><span class="params">(input_sentence)</span>:</span></span><br><span class="line">    output_words, attentions = evaluate(</span><br><span class="line">        encoder1, attn_decoder1, input_sentence)</span><br><span class="line">    print(<span class="string">'input ='</span>, input_sentence)</span><br><span class="line">    print(<span class="string">'output ='</span>, <span class="string">' '</span>.join(output_words))</span><br><span class="line">    showAttention(input_sentence, output_words, attentions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle a cinq ans de moins que moi ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle est trop petit ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"je ne crains pas de mourir ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"c est un jeune directeur plein de talent ."</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input = elle a cinq ans de moins que moi .
output = she is five years younger than me . &lt;EOS&gt;
input = elle est trop petit .
output = she is too short . &lt;EOS&gt;
input = je ne crains pas de mourir .
output = i m not scared to die . &lt;EOS&gt;


C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator
  # Remove the CWD from sys.path while we load stuff.
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator
  # This is added back by InteractiveShellApp.init_path()
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.


input = c est un jeune directeur plein de talent .
output = he s a talented and . &lt;EOS&gt;
</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset</p>
<ul>
<li>Another language pair</li>
<li>Human → Machine (e.g. IOT commands)</li>
<li>Chat → Response</li>
<li>Question → Answer</li>
</ul>
</li>
<li><p>Replace the embeddings with pre-trained word embeddings such as word2vec or<br>GloVe</p>
</li>
<li>Try with more layers, more hidden units, and more sentences. Compare<br>the training time and results.</li>
<li><p>If you use a translation file where pairs have two of the same phrase<br>(<code>I am test \t I am test</code>), you can use this as an autoencoder. Try<br>this:</p>
<ul>
<li>Train as an autoencoder</li>
<li>Save only the Encoder network</li>
<li>Train a new Decoder for translation from there</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-TORCHTEXT的文本分类</title>
    <url>/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>Pytorch-Text-TORCHTEXT的文本分类:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Text-Classification-with-TorchText"><a href="#Text-Classification-with-TorchText" class="headerlink" title="Text Classification with TorchText"></a>Text Classification with TorchText</h1><p>This tutorial shows how to use the text classification datasets<br>in <code>torchtext</code>, including</p>
<p>::</p>
<ul>
<li>AG_NEWS,</li>
<li>SogouNews,</li>
<li>DBpedia,</li>
<li>YelpReviewPolarity,</li>
<li>YelpReviewFull,</li>
<li>YahooAnswers,</li>
<li>AmazonReviewPolarity,</li>
<li>AmazonReviewFull</li>
</ul>
<p>This example shows how to train a supervised learning algorithm for<br>classification using one of these <code>TextClassification</code> datasets.</p>
<h2 id="Load-data-with-ngrams"><a href="#Load-data-with-ngrams" class="headerlink" title="Load data with ngrams"></a>Load data with ngrams</h2><p>A bag of ngrams feature is applied to capture some partial information<br>about the local word order. In practice, bi-gram or tri-gram are applied<br>to provide more benefits as word groups than only one word. An example:</p>
<p>::</p>
<p>   “load data with ngrams”<br>   Bi-grams results: “load data”, “data with”, “with ngrams”<br>   Tri-grams results: “load data with”, “data with ngrams”</p>
<p><code>TextClassification</code> Dataset supports the ngrams method. By setting<br>ngrams to 2, the example text in the dataset will be a list of single<br>words plus bi-grams string.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> text_classification</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> torchtext.utils <span class="keyword">import</span> extract_archive, unicode_csv_reader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> _csv_iterator,_create_data_from_iterator</span><br><span class="line">NGRAMS = <span class="number">2</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./.data'</span>):</span><br><span class="line">	os.mkdir(<span class="string">'./.data'</span>)</span><br><span class="line"><span class="comment"># train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](</span></span><br><span class="line"><span class="comment">#     root='./.data', ngrams=NGRAMS, vocab=None)</span></span><br><span class="line"> <span class="comment">#定义创建数据集函数，原函数在torchtext.datasets.text_classification文件中，本教程所需参数直接设成了默认值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_setup_datasets</span><span class="params">(dataset_tar=<span class="string">'./.data/ag_news_csv.tar.gz'</span>,dataset_name=<span class="string">"AG_NEWS"</span>, root=<span class="string">'./.data'</span>, ngrams=NGRAMS, vocab=None, include_unk=False)</span>:</span></span><br><span class="line">    <span class="comment"># 注释掉下载数据的代码</span></span><br><span class="line">    <span class="comment">#     dataset_tar = download_from_url(URLS[dataset_name], root=root)</span></span><br><span class="line">    extracted_files = extract_archive(dataset_tar)  <span class="comment">#解压数据文件</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> extracted_files:</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'train.csv'</span>):</span><br><span class="line">            train_csv_path = fname</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'test.csv'</span>):</span><br><span class="line">            test_csv_path = fname</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        logging.info(<span class="string">'Building Vocab based on &#123;&#125;'</span>.format(train_csv_path))</span><br><span class="line">        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams)) <span class="comment">#创建词典</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(vocab, Vocab):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"Passed vocabulary is not of type Vocab"</span>)</span><br><span class="line">    logging.info(<span class="string">'Vocab has &#123;&#125; entries'</span>.format(len(vocab)))</span><br><span class="line">    logging.info(<span class="string">'Creating training data'</span>)</span><br><span class="line">    train_data, train_labels = _create_data_from_iterator(   <span class="comment">#创建训练数据</span></span><br><span class="line">        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk) </span><br><span class="line">    logging.info(<span class="string">'Creating testing data'</span>)</span><br><span class="line">    test_data, test_labels = _create_data_from_iterator(   <span class="comment">#创建测试数据</span></span><br><span class="line">        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk)</span><br><span class="line">    <span class="keyword">if</span> len(train_labels ^ test_labels) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Training and test labels don't match"</span>)</span><br><span class="line">    <span class="keyword">return</span> (TextClassificationDataset(vocab, train_data, train_labels),  <span class="comment">#返回数据集实例</span></span><br><span class="line">            TextClassificationDataset(vocab, test_data, test_labels))</span><br><span class="line">train_dataset, test_dataset = _setup_datasets()</span><br><span class="line">BATCH_SIZE = <span class="number">16</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>120000lines [00:21, 5495.55lines/s]
120000lines [00:54, 2186.84lines/s]
7600lines [00:03, 1978.44lines/s]
</code></pre><h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>The model is composed of the<br><code>EmbeddingBag &lt;https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag&gt;</code>__<br>layer and the linear layer (see the figure below). <code>nn.EmbeddingBag</code><br>computes the mean value of a “bag” of embeddings. The text entries here<br>have different lengths. <code>nn.EmbeddingBag</code> requires no padding here<br>since the text lengths are saved in offsets.</p>
<p>Additionally, since <code>nn.EmbeddingBag</code> accumulates the average across<br>the embeddings on the fly, <code>nn.EmbeddingBag</code> can enhance the<br>performance and memory efficiency to process a sequence of tensors.</p>
<p><img src="https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextSentiment</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_dim, num_class)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(embed_dim, num_class)</span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.5</span></span><br><span class="line">        self.embedding.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, offsets)</span>:</span></span><br><span class="line">        embedded = self.embedding(text, offsets)</span><br><span class="line">        <span class="keyword">return</span> self.fc(embedded)</span><br></pre></td></tr></table></figure>
<h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The AG_NEWS dataset has four labels and therefore the number of classes<br>is four.</p>
<p>::</p>
<p>   1 : World<br>   2 : Sports<br>   3 : Business<br>   4 : Sci/Tec</p>
<p>The vocab size is equal to the length of vocab (including single word<br>and ngrams). The number of classes is equal to the number of labels,<br>which is four in AG_NEWS case.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">VOCAB_SIZE = len(train_dataset.get_vocab())</span><br><span class="line">EMBED_DIM = <span class="number">32</span></span><br><span class="line">NUN_CLASS = len(train_dataset.get_labels())</span><br><span class="line">model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Functions-used-to-generate-batch"><a href="#Functions-used-to-generate-batch" class="headerlink" title="Functions used to generate batch"></a>Functions used to generate batch</h2><p>Since the text entries have different lengths, a custom function<br>generate_batch() is used to generate data batches and offsets. The<br>function is passed to <code>collate_fn</code> in <code>torch.utils.data.DataLoader</code>.<br>The input to <code>collate_fn</code> is a list of tensors with the size of<br>batch_size, and the <code>collate_fn</code> function packs them into a<br>mini-batch. Pay attention here and make sure that <code>collate_fn</code> is<br>declared as a top level def. This ensures that the function is available<br>in each worker.</p>
<p>The text entries in the original data batch input are packed into a list<br>and concatenated as a single tensor as the input of <code>nn.EmbeddingBag</code>.<br>The offsets is a tensor of delimiters to represent the beginning index<br>of the individual sequence in the text tensor. Label is a tensor saving<br>the labels of individual text entries.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch)</span>:</span></span><br><span class="line">    label = torch.tensor([entry[<span class="number">0</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch])</span><br><span class="line">    text = [entry[<span class="number">1</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch]</span><br><span class="line">    offsets = [<span class="number">0</span>] + [len(entry) <span class="keyword">for</span> entry <span class="keyword">in</span> text]</span><br><span class="line">    <span class="comment"># torch.Tensor.cumsum returns the cumulative sum</span></span><br><span class="line">    <span class="comment"># of elements in the dimension dim.</span></span><br><span class="line">    <span class="comment"># torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)</span></span><br><span class="line"></span><br><span class="line">    offsets = torch.tensor(offsets[:<span class="number">-1</span>]).cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    text = torch.cat(text)</span><br><span class="line">    <span class="keyword">return</span> text, offsets, label</span><br></pre></td></tr></table></figure>
<h2 id="Define-functions-to-train-the-model-and-evaluate-results"><a href="#Define-functions-to-train-the-model-and-evaluate-results" class="headerlink" title="Define functions to train the model and evaluate results."></a>Define functions to train the model and evaluate results.</h2><p><code>torch.utils.data.DataLoader &lt;https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader&gt;</code><strong><br>is recommended for PyTorch users, and it makes data loading in parallel<br>easily (a tutorial is<br><code>here &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code></strong>).<br>We use <code>DataLoader</code> here to load AG_NEWS datasets and send it to the<br>model for training/validation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_func</span><span class="params">(sub_train_)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the model</span></span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    train_acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>,</span><br><span class="line">                      collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> i, (text, offsets, cls) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        output = model(text, offsets)</span><br><span class="line">        loss = criterion(output, cls)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adjust the learning rate</span></span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_loss / len(sub_train_), train_acc / len(sub_train_)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(data_)</span>:</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> text, offsets, cls <span class="keyword">in</span> data:</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            output = model(text, offsets)</span><br><span class="line">            loss = criterion(output, cls)</span><br><span class="line">            loss += loss.item()</span><br><span class="line">            acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss / len(data_), acc / len(data_)</span><br></pre></td></tr></table></figure>
<h2 id="Split-the-dataset-and-run-the-model"><a href="#Split-the-dataset-and-run-the-model" class="headerlink" title="Split the dataset and run the model"></a>Split the dataset and run the model</h2><p>Since the original AG_NEWS has no valid dataset, we split the training<br>dataset into train/valid sets with a split ratio of 0.95 (train) and<br>0.05 (valid). Here we use<br><code>torch.utils.data.dataset.random_split &lt;https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split&gt;</code>__<br>function in PyTorch core library.</p>
<p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.<br>It is useful when training a classification problem with C classes.<br><code>SGD &lt;https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html&gt;</code></strong><br>implements stochastic gradient descent method as optimizer. The initial<br>learning rate is set to 4.0.<br><code>StepLR &lt;https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR&gt;</code>__<br>is used here to adjust the learning rate through epochs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> random_split</span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">min_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().to(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">4.0</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1</span>, gamma=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">train_len = int(len(train_dataset) * <span class="number">0.95</span>)</span><br><span class="line">sub_train_, sub_valid_ = \</span><br><span class="line">    random_split(train_dataset, [train_len, len(train_dataset) - train_len])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train_func(sub_train_)</span><br><span class="line">    valid_loss, valid_acc = test(sub_valid_)</span><br><span class="line"></span><br><span class="line">    secs = int(time.time() - start_time)</span><br><span class="line">    mins = secs / <span class="number">60</span></span><br><span class="line">    secs = secs % <span class="number">60</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch: %d'</span> %(epoch + <span class="number">1</span>), <span class="string">" | time in %d minutes, %d seconds"</span> %(mins, secs))</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>(train)\t|\tAcc: <span class="subst">&#123;train_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(train)'</span>)</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;valid_loss:<span class="number">.4</span>f&#125;</span>(valid)\t|\tAcc: <span class="subst">&#123;valid_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(valid)'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 1  | time in 1 minutes, 36 seconds
    Loss: 0.0261(train)    |    Acc: 84.8%(train)
    Loss: 0.0001(valid)    |    Acc: 90.5%(valid)
Epoch: 2  | time in 1 minutes, 57 seconds
    Loss: 0.0118(train)    |    Acc: 93.8%(train)
    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)
Epoch: 3  | time in 1 minutes, 35 seconds
    Loss: 0.0069(train)    |    Acc: 96.4%(train)
    Loss: 0.0001(valid)    |    Acc: 89.9%(valid)
Epoch: 4  | time in 1 minutes, 36 seconds
    Loss: 0.0038(train)    |    Acc: 98.1%(train)
    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)
Epoch: 5  | time in 1 minutes, 37 seconds
    Loss: 0.0023(train)    |    Acc: 99.0%(train)
    Loss: 0.0001(valid)    |    Acc: 91.5%(valid)
</code></pre><p>Running the model on GPU with the following information:</p>
<p>Epoch: 1 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0263(train)     |       Acc: 84.5%(train)
   Loss: 0.0001(valid)     |       Acc: 89.0%(valid)
</code></pre><p>Epoch: 2 | time in 0 minutes, 10 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0119(train)     |       Acc: 93.6%(train)
   Loss: 0.0000(valid)     |       Acc: 89.6%(valid)
</code></pre><p>Epoch: 3 | time in 0 minutes, 9 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0069(train)     |       Acc: 96.4%(train)
   Loss: 0.0000(valid)     |       Acc: 90.5%(valid)
</code></pre><p>Epoch: 4 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0038(train)     |       Acc: 98.2%(train)
   Loss: 0.0000(valid)     |       Acc: 90.4%(valid)
</code></pre><p>Epoch: 5 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0022(train)     |       Acc: 99.0%(train)
   Loss: 0.0000(valid)     |       Acc: 91.0%(valid)
</code></pre><h2 id="Evaluate-the-model-with-test-dataset"><a href="#Evaluate-the-model-with-test-dataset" class="headerlink" title="Evaluate the model with test dataset"></a>Evaluate the model with test dataset</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Checking the results of test dataset...'</span>)</span><br><span class="line">test_loss, test_acc = test(test_dataset)</span><br><span class="line">print(<span class="string">f'\tLoss: <span class="subst">&#123;test_loss:<span class="number">.4</span>f&#125;</span>(test)\t|\tAcc: <span class="subst">&#123;test_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(test)'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Checking the results of test dataset...
    Loss: 0.0003(test)    |    Acc: 90.5%(test)
</code></pre><p>Checking the results of test dataset…</p>
<p>::</p>
<pre><code>   Loss: 0.0237(test)      |       Acc: 90.5%(test)
</code></pre><h2 id="Test-on-a-random-news"><a href="#Test-on-a-random-news" class="headerlink" title="Test on a random news"></a>Test on a random news</h2><p>Use the best model so far and test a golf news. The label information is<br>available<br><code>here &lt;https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> ngrams_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line">ag_news_label = &#123;<span class="number">1</span> : <span class="string">"World"</span>,</span><br><span class="line">                 <span class="number">2</span> : <span class="string">"Sports"</span>,</span><br><span class="line">                 <span class="number">3</span> : <span class="string">"Business"</span>,</span><br><span class="line">                 <span class="number">4</span> : <span class="string">"Sci/Tec"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(text, model, vocab, ngrams)</span>:</span></span><br><span class="line">    tokenizer = get_tokenizer(<span class="string">"basic_english"</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        text = torch.tensor([vocab[token]</span><br><span class="line">                            <span class="keyword">for</span> token <span class="keyword">in</span> ngrams_iterator(tokenizer(text), ngrams)])</span><br><span class="line">        output = model(text, torch.tensor([<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output.argmax(<span class="number">1</span>).item() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">ex_text_str = <span class="string">"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \</span></span><br><span class="line"><span class="string">    enduring the season’s worst weather conditions on Sunday at The \</span></span><br><span class="line"><span class="string">    Open on his way to a closing 75 at Royal Portrush, which \</span></span><br><span class="line"><span class="string">    considering the wind and the rain was a respectable showing. \</span></span><br><span class="line"><span class="string">    Thursday’s first round at the WGC-FedEx St. Jude Invitational \</span></span><br><span class="line"><span class="string">    was another story. With temperatures in the mid-80s and hardly any \</span></span><br><span class="line"><span class="string">    wind, the Spaniard was 13 strokes better in a flawless round. \</span></span><br><span class="line"><span class="string">    Thanks to his best putting performance on the PGA Tour, Rahm \</span></span><br><span class="line"><span class="string">    finished with an 8-under 62 for a three-stroke lead, which \</span></span><br><span class="line"><span class="string">    was even more impressive considering he’d never played the \</span></span><br><span class="line"><span class="string">    front nine at TPC Southwind."</span></span><br><span class="line"></span><br><span class="line">vocab = train_dataset.get_vocab()</span><br><span class="line">model = model.to(<span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"This is a %s news"</span> %ag_news_label[predict(ex_text_str, model, vocab, <span class="number">2</span>)])</span><br></pre></td></tr></table></figure>
<pre><code>This is a Sports news
</code></pre><p>This is a Sports news</p>
<p>You can find the code examples displayed in this note<br><code>here &lt;https://github.com/pytorch/text/tree/master/examples/text_classification&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-使用字符级RNN对名称进行分类</title>
    <url>/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>Pytorch-Text-使用字符级RNN对名称进行分类:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Classifying Names with a Character-Level RNN</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>We will be building and training a basic character-level RNN to classify<br>words. This tutorial, along with the following two, show how to do<br>preprocess data for NLP modeling “from scratch”, in particular not using<br>many of the convenience functions of <code>torchtext</code>, so you can see how<br>preprocessing for NLP modeling works at a low level.</p>
<p>A character-level RNN reads words as a series of characters -<br>outputting a prediction and “hidden state” at each step, feeding its<br>previous hidden state into each next step. We take the final prediction<br>to be the output, i.e. which class the word belongs to.</p>
<p>Specifically, we’ll train on a few thousand surnames from 18 languages<br>of origin, and predict which language a name is from based on the<br>spelling:</p>
<p>::</p>
<pre><code>$ python predict.py Hinton
(-0.47) Scottish
(-1.52) English
(-3.57) Irish

$ python predict.py Schmidhuber
(-0.19) German
(-2.48) Czech
(-2.68) Dutch
</code></pre><p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about RNNs and how they work:</p>
<ul>
<li><code>The Unreasonable Effectiveness of Recurrent Neural
Networks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li>
<li><code>Understanding LSTM
Networks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li>
</ul>
<h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>Included in the <code>data/names</code> directory are 18 text files named as<br>“[Language].txt”. Each file contains a bunch of names, one name per<br>line, mostly romanized (but we still need to convert from Unicode to<br>ASCII).</p>
<p>We’ll end up with a dictionary of lists of names per language,<br><code>{language: [names ...]}</code>. The generic variables “category” and “line”<br>(for language and name in our case) are used for later extensibility.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># glob.glob返回所有匹配的文件路径列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line">print(findFiles(<span class="string">'data/names/*.txt'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></span><br><span class="line">n_letters = len(all_letters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># 在Unicode中，某些字符能够用多个合法的编码表示，在需要比较字符串的程序中使用字符的多种表示会产生问题。 </span></span><br><span class="line"><span class="comment"># 为了修正这个问题，你可以使用unicodedata模块先将文本标准化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">print(unicodeToAscii(<span class="string">'Ślusàrski'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of names per language</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;data/names\\Arabic.txt&#39;, &#39;data/names\\Chinese.txt&#39;, &#39;data/names\\Czech.txt&#39;, &#39;data/names\\Dutch.txt&#39;, &#39;data/names\\English.txt&#39;, &#39;data/names\\French.txt&#39;, &#39;data/names\\German.txt&#39;, &#39;data/names\\Greek.txt&#39;, &#39;data/names\\Irish.txt&#39;, &#39;data/names\\Italian.txt&#39;, &#39;data/names\\Japanese.txt&#39;, &#39;data/names\\Korean.txt&#39;, &#39;data/names\\Polish.txt&#39;, &#39;data/names\\Portuguese.txt&#39;, &#39;data/names\\Russian.txt&#39;, &#39;data/names\\Scottish.txt&#39;, &#39;data/names\\Spanish.txt&#39;, &#39;data/names\\Vietnamese.txt&#39;]
Slusarski
</code></pre><p>Now we have <code>category_lines</code>, a dictionary mapping each category<br>(language) to a list of lines (names). We also kept track of<br><code>all_categories</code> (just a list of languages) and <code>n_categories</code> for<br>later reference.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(category_lines[<span class="string">'Italian'</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;Abandonato&#39;, &#39;Abatangelo&#39;, &#39;Abatantuono&#39;, &#39;Abate&#39;, &#39;Abategiovanni&#39;]
</code></pre><h2 id="Turning-Names-into-Tensors"><a href="#Turning-Names-into-Tensors" class="headerlink" title="Turning Names into Tensors"></a>Turning Names into Tensors</h2><p>Now that we have all the names organized, we need to turn them into<br>Tensors to make any use of them.</p>
<p>To represent a single letter, we use a “one-hot vector” of size<br><code>&lt;1 x n_letters&gt;</code>. A one-hot vector is filled with 0s except for a 1<br>at index of the current letter, e.g. <code>&quot;b&quot; = &lt;0 1 0 0 0 ...&gt;</code>.</p>
<p>To make a word we join a bunch of those into a 2D matrix<br><code>&lt;line_length x 1 x n_letters&gt;</code>.</p>
<p>That extra 1 dimension is because PyTorch assumes everything is in<br>batches - we’re just using a batch size of 1 here.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find letter index from all_letters, e.g. "a" = 0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToIndex</span><span class="params">(letter)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> all_letters.find(letter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToTensor</span><span class="params">(letter)</span>:</span></span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_letters)</span><br><span class="line">    tensor[<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a line into a &lt;line_length x 1 x n_letters&gt;,</span></span><br><span class="line"><span class="comment"># or an array of one-hot letter vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lineToTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li, letter <span class="keyword">in</span> enumerate(line):</span><br><span class="line">        tensor[li][<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line">print(letterToTensor(<span class="string">'J'</span>))</span><br><span class="line"></span><br><span class="line">print(lineToTensor(<span class="string">'Jones'</span>).size())</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0.]])
torch.Size([5, 1, 57])
</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>Before autograd, creating a recurrent neural network in Torch involved<br>cloning the parameters of a layer over several timesteps. The layers<br>held hidden state and gradients which are now entirely handled by the<br>graph itself. This means you can implement a RNN in a very “pure” way,<br>as regular feed-forward layers.</p>
<p>This RNN module (mostly copied from <code>the PyTorch for Torch users
tutorial &lt;https://pytorch.org/tutorials/beginner/former_torchies/
nn_tutorial.html#example-2-recurrent-net&gt;</code>__)<br>is just 2 linear layers which operate on an input and hidden state, with<br>a LogSoftmax layer after the output.<br><img src="https://i.imgur.com/Z2xbySO.png" alt></p>
<h1 id="Torch-cat"><a href="#Torch-cat" class="headerlink" title="Torch.cat()"></a>Torch.cat()</h1><p>cat是concatnate的意思：拼接，联系在一起。</p>
<p>先说cat( )的普通用法</p>
<p>如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C &#x3D; torch.cat( (A,B),0 )  #按维数0拼接（竖着拼）</span><br><span class="line">C &#x3D; torch.cat( (A,B),1 )  #按维数1拼接（横着拼）</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; A&#x3D;torch.ones(2,3)    #2x3的张量（矩阵）                                     </span><br><span class="line">&gt;&gt;&gt; A</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.]])</span><br><span class="line">&gt;&gt;&gt; B&#x3D;2*torch.ones(4,3)  #4x3的张量（矩阵）                                    </span><br><span class="line">&gt;&gt;&gt; B</span><br><span class="line">tensor([[ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,B),0)  #按维数0（行）拼接</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">         [ 1.,  1.,  1.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([6, 3])</span><br><span class="line">&gt;&gt;&gt; D&#x3D;2*torch.ones(2,4) #2x4的张量（矩阵）</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,D),1)#按维数1（列）拼接</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],</span><br><span class="line">        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([2, 7])</span><br></pre></td></tr></table></figure><br>其次，cat还可以把list中的tensor拼接起来。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line"></span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line">len(x1)</span><br><span class="line">&gt;&gt;3</span><br><span class="line"></span><br><span class="line">x1</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">[tensor([[2.],</span><br><span class="line">         [4.],</span><br><span class="line">         [6]]),tensor([[2.],[4.],[6.]]),tensor([[2.],[4.],[6.]])]</span><br><span class="line"></span><br><span class="line">x2 &#x3D; &#x3D; torch.cat((x1),1)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">&gt;&gt;tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br><span class="line"></span><br><span class="line">type(x1)</span><br><span class="line">&gt;&gt; list</span><br></pre></td></tr></table></figure><br>上面的代码可以合成一行来写：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x2 &#x3D; torch.cat([x*2 for i in range(1,4)])</span><br><span class="line">x2</span><br><span class="line">&gt;&gt;</span><br><span class="line">tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(input_size + hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        combined = torch.cat((input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(combined)</span><br><span class="line">        output = self.i2o(combined)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br><span class="line"></span><br><span class="line">n_hidden = <span class="number">128</span></span><br><span class="line">rnn = RNN(n_letters, n_hidden, n_categories)</span><br></pre></td></tr></table></figure>
<p>To run a step of this network we need to pass an input (in our case, the<br>Tensor for the current letter) and a previous hidden state (which we<br>initialize as zeros at first). We’ll get back the output (probability of<br>each language) and a next hidden state (which we keep for the next<br>step).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = letterToTensor(<span class="string">'A'</span>)</span><br><span class="line">hidden =torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input, hidden)</span><br></pre></td></tr></table></figure>
<p>For the sake of efficiency we don’t want to be creating a new Tensor for<br>every step, so we will use <code>lineToTensor</code> instead of<br><code>letterToTensor</code> and use slices. This could be further optimized by<br>pre-computing batches of Tensors.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = lineToTensor(<span class="string">'Albert'</span>)</span><br><span class="line">hidden = torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input[<span class="number">0</span>], hidden)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[-2.8013, -3.0086, -2.8838, -2.8652, -2.8300, -2.7883, -2.8614, -2.9069,
         -2.9787, -2.8336, -2.9085, -2.9029, -2.9565, -2.8683, -2.9269, -2.9332,
         -2.9334, -2.8689]], grad_fn=&lt;LogSoftmaxBackward&gt;)
</code></pre><p>As you can see the output is a <code>&lt;1 x n_categories&gt;</code> Tensor, where<br>every item is the likelihood of that category (higher is more likely).</p>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>Before going into training we should make a few helper functions. The<br>first is to interpret the output of the network, which we know to be a<br>likelihood of each category. We can use <code>Tensor.topk</code> to get the index<br>of the greatest value:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryFromOutput</span><span class="params">(output)</span>:</span></span><br><span class="line">    top_n, top_i = output.topk(<span class="number">1</span>)</span><br><span class="line">    category_i = top_i[<span class="number">0</span>].item()</span><br><span class="line">    <span class="keyword">return</span> all_categories[category_i], category_i</span><br><span class="line"></span><br><span class="line">print(categoryFromOutput(output))</span><br></pre></td></tr></table></figure>
<pre><code>(&#39;French&#39;, 5)
</code></pre><p>We will also want a quick way to get a training example (a name and its<br>language):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)</span><br><span class="line">    line_tensor = lineToTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category, line, category_tensor, line_tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    print(<span class="string">'category ='</span>, category, <span class="string">'/ line ='</span>, line)</span><br></pre></td></tr></table></figure>
<pre><code>category = German / line = Reiter
category = Spanish / line = Roldan
category = Vietnamese / line = Lieu
category = Japanese / line = Maita
category = Polish / line = Wojda
category = Greek / line = Forakis
category = Italian / line = Voltolini
category = Scottish / line = Hill
category = Portuguese / line = Nunes
category = Scottish / line = Wilson
</code></pre><h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>Now all it takes to train this network is show it a bunch of examples,<br>have it make guesses, and tell it if it’s wrong.</p>
<p>For the loss function <code>nn.NLLLoss</code> is appropriate, since the last<br>layer of the RNN is <code>nn.LogSoftmax</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br></pre></td></tr></table></figure>
<p>Each loop of training will:</p>
<ul>
<li>Create input and target tensors</li>
<li>Create a zeroed initial hidden state</li>
<li><p>Read each letter in and</p>
<ul>
<li>Keep hidden state for next letter</li>
</ul>
</li>
<li><p>Compare final output to target</p>
</li>
<li>Back-propagate</li>
<li>Return the output and loss</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.005</span> <span class="comment"># If you set this too high, it might explode. If too low, it might not learn</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    loss = criterion(output, category_tensor)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add parameters' gradients to their values, multiplied by learning rate</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item()</span><br></pre></td></tr></table></figure>
<p>Now we just have to run that with a bunch of examples. Since the<br><code>train</code> function returns both the output and loss we can print its<br>guesses and also keep track of loss for plotting. Since there are 1000s<br>of examples we print only every <code>print_every</code> examples, and take an<br>average of the loss.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep track of losses for plotting</span></span><br><span class="line">current_loss = <span class="number">0</span></span><br><span class="line">all_losses = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output, loss = train(category_tensor, line_tensor)</span><br><span class="line">    current_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print iter number, loss, name and guess</span></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        guess, guess_i = categoryFromOutput(output)</span><br><span class="line">        correct = <span class="string">'✓'</span> <span class="keyword">if</span> guess == category <span class="keyword">else</span> <span class="string">'✗ (%s)'</span> % category</span><br><span class="line">        print(<span class="string">'%d %d%% (%s) %.4f %s / %s %s'</span> % (iter, iter / n_iters * <span class="number">100</span>, timeSince(start), loss, line, guess, correct))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add current loss avg to list of losses</span></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(current_loss / plot_every)</span><br><span class="line">        current_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>5000 5% (0m 26s) 2.2285 Tsen / Chinese ✓
10000 10% (0m 45s) 1.7232 Ursler / Dutch ✗ (German)
15000 15% (1m 2s) 3.3629 Power / German ✗ (Irish)
20000 20% (1m 19s) 1.1055 Ferreiro / Portuguese ✓
25000 25% (1m 37s) 1.1813 Do / Vietnamese ✓
30000 30% (1m 53s) 1.9952 Pak / Chinese ✗ (Korean)
35000 35% (2m 11s) 1.0685 Zientek / Czech ✗ (Polish)
40000 40% (2m 34s) 0.3656 Arnoni / Italian ✓
45000 45% (2m 56s) 2.5408 Schuchardt / Czech ✗ (German)
50000 50% (3m 19s) 0.9137 Ellwood / English ✓
55000 55% (3m 43s) 2.6915 Griffiths / Greek ✗ (English)
60000 60% (4m 5s) 0.0363 Quach / Vietnamese ✓
65000 65% (4m 27s) 0.1474 Rijnders / Dutch ✓
70000 70% (4m 49s) 1.8646 Clements / Portuguese ✗ (English)
75000 75% (5m 13s) 0.3696 Bobienski / Polish ✓
80000 80% (5m 37s) 1.0411 Klerx / Dutch ✓
85000 85% (5m 58s) 2.3457 Maria / Spanish ✗ (Portuguese)
90000 90% (6m 24s) 0.5750 Echevarria / Spanish ✓
95000 95% (6m 47s) 0.0762 Ohmiya / Japanese ✓
100000 100% (7m 9s) 2.5785 Kock / Czech ✗ (German)
</code></pre><h2 id="Plotting-the-Results"><a href="#Plotting-the-Results" class="headerlink" title="Plotting the Results"></a>Plotting the Results</h2><p>Plotting the historical loss from <code>all_losses</code> shows the network<br>learning:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x18c353c04a8&gt;]
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_25_1.png" alt="png"></p>
<h1 id="Evaluating-the-Results"><a href="#Evaluating-the-Results" class="headerlink" title="Evaluating the Results"></a>Evaluating the Results</h1><p>To see how well the network performs on different categories, we will<br>create a confusion matrix, indicating for every actual language (rows)<br>which language the network guesses (columns). To calculate the confusion<br>matrix a bunch of samples are run through the network with<br><code>evaluate()</code>, which is the same as <code>train()</code> minus the backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Keep track of correct guesses in a confusion matrix</span></span><br><span class="line">confusion = torch.zeros(n_categories, n_categories)</span><br><span class="line">n_confusion = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Just return an output given a line</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># Go through a bunch of examples and record which are correctly guessed</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_confusion):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output = evaluate(line_tensor)</span><br><span class="line">    guess, guess_i = categoryFromOutput(output)</span><br><span class="line">    category_i = all_categories.index(category)</span><br><span class="line">    confusion[category_i][guess_i] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize by dividing every row by its sum</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_categories):</span><br><span class="line">    confusion[i] = confusion[i] / confusion[i].sum()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up plot</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(confusion.numpy())</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up axes</span></span><br><span class="line">ax.set_xticklabels([<span class="string">''</span>] + all_categories, rotation=<span class="number">90</span>)</span><br><span class="line">ax.set_yticklabels([<span class="string">''</span>] + all_categories)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Force label at every tick</span></span><br><span class="line">ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sphinx_gallery_thumbnail_number = 2</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:34: UserWarning: FixedFormatter should only be used together with FixedLocator
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_27_1.png" alt="png"></p>
<p>You can pick out bright spots off the main axis that show which<br>languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish<br>for Italian. It seems to do very well with Greek, and very poorly with<br>English (perhaps because of overlap with other languages).</p>
<h2 id="Running-on-User-Input"><a href="#Running-on-User-Input" class="headerlink" title="Running on User Input"></a>Running on User Input</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(input_line, n_predictions=<span class="number">3</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'\n&gt; %s'</span> % input_line)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = evaluate(lineToTensor(input_line))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get top N categories</span></span><br><span class="line">        topv, topi = output.topk(n_predictions, <span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">        predictions = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_predictions):</span><br><span class="line">            value = topv[<span class="number">0</span>][i].item()</span><br><span class="line">            category_index = topi[<span class="number">0</span>][i].item()</span><br><span class="line">            print(<span class="string">'(%.2f) %s'</span> % (value, all_categories[category_index]))</span><br><span class="line">            predictions.append([value, all_categories[category_index]])</span><br><span class="line"></span><br><span class="line">predict(<span class="string">'Dovesky'</span>)</span><br><span class="line">predict(<span class="string">'Jackson'</span>)</span><br><span class="line">predict(<span class="string">'Satoshi'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&gt; Dovesky
(-0.80) Russian
(-1.53) Czech
(-1.98) English

&gt; Jackson
(-0.33) Scottish
(-1.99) English
(-3.31) Russian

&gt; Satoshi
(-0.90) Italian
(-1.65) Japanese
(-2.18) Arabic
</code></pre><p>The final versions of the scripts <code>in the Practical PyTorch
repo &lt;https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification&gt;</code>__<br>split the above code into a few files:</p>
<ul>
<li><code>data.py</code> (loads files)</li>
<li><code>model.py</code> (defines the RNN)</li>
<li><code>train.py</code> (runs training)</li>
<li><code>predict.py</code> (runs <code>predict()</code> with command line arguments)</li>
<li><code>server.py</code> (serve prediction as a JSON API with bottle.py)</li>
</ul>
<p>Run <code>train.py</code> to train and save the network.</p>
<p>Run <code>predict.py</code> with a name to view predictions:</p>
<p>::</p>
<pre><code>$ python predict.py Hazaki
(-0.42) Japanese
(-1.39) Polish
(-3.51) Czech
</code></pre><p>Run <code>server.py</code> and visit <a href="http://localhost:5533/Yourname" target="_blank" rel="noopener">http://localhost:5533/Yourname</a> to get JSON<br>output of predictions.</p>
<h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset of line -&gt; category, for example:</p>
<ul>
<li>Any word -&gt; language</li>
<li>First name -&gt; gender</li>
<li>Character name -&gt; writer</li>
<li>Page title -&gt; blog or subreddit</li>
</ul>
</li>
<li><p>Get better results with a bigger and/or better shaped network</p>
<ul>
<li>Add more linear layers</li>
<li>Try the <code>nn.LSTM</code> and <code>nn.GRU</code> layers</li>
<li>Combine multiple of these RNNs as a higher level network</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-TORCHTEXT的语言翻译</title>
    <url>/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<p>Pytorch-Text-TORCHTEXT的语言翻译:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Language-Translation-with-TorchText"><a href="#Language-Translation-with-TorchText" class="headerlink" title="Language Translation with TorchText"></a>Language Translation with TorchText</h1><p>This tutorial shows how to use several convenience classes of <code>torchtext</code> to preprocess<br>data from a well-known dataset containing sentences in both English and German and use it to<br>train a sequence-to-sequence model with attention that can translate German sentences<br>into English.</p>
<p>It is based off of<br><code>this tutorial &lt;https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code><strong><br>from PyTorch community member <code>Ben Trevett &lt;https://github.com/bentrevett&gt;</code></strong><br>and was created by <code>Seth Weidman &lt;https://github.com/SethHWeidman/&gt;</code>__ with Ben’s permission.</p>
<p>By the end of this tutorial, you will be able to:</p>
<ul>
<li>Preprocess sentences into a commonly-used format for NLP modeling using the following <code>torchtext</code> convenience classes:<ul>
<li><code>TranslationDataset &lt;https://torchtext.readthedocs.io/en/latest/datasets.html#torchtext.datasets.TranslationDataset&gt;</code>__</li>
<li><code>Field &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field&gt;</code>__</li>
<li><code>BucketIterator &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator&gt;</code>__</li>
</ul>
</li>
</ul>
<h2 id="Field-and-TranslationDataset"><a href="#Field-and-TranslationDataset" class="headerlink" title="Field and TranslationDataset"></a><code>Field</code> and <code>TranslationDataset</code></h2><p><code>torchtext</code> has utilities for creating datasets that can be easily<br>iterated through for the purposes of creating a language translation<br>model. One key class is a<br><code>Field &lt;https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L64&gt;</code><strong>,<br>which specifies the way each sentence should be preprocessed, and another is the<br><code>TranslationDataset</code> ; <code>torchtext</code><br>has several such datasets; in this tutorial we’ll use the<br><code>Multi30k dataset &lt;https://github.com/multi30k/dataset&gt;</code></strong>, which contains about<br>30,000 sentences (averaging about 13 words in length) in both English and German.</p>
<p>Note: the tokenization in this tutorial requires <code>Spacy &lt;https://spacy.io&gt;</code><strong><br>We use Spacy because it provides strong support for tokenization in languages<br>other than English. <code>torchtext</code> provides a <code>basic_english</code> tokenizer<br>and supports other tokenizers for English (e.g.<br><code>Moses &lt;https://bitbucket.org/luismsgomes/mosestokenizer/src/default/&gt;</code></strong>)<br>but for language translation - where multiple languages are required -<br>Spacy is your best bet.</p>
<p>To run this tutorial, first install <code>spacy</code> using <code>pip</code> or <code>conda</code>.<br>Next, download the raw data for the English and German Spacy tokenizers:</p>
<p>::</p>
<p>   python -m spacy download en<br>   python -m spacy download de</p>
<p>With Spacy installed, the following code will tokenize each of the sentences<br>in the <code>TranslationDataset</code> based on the tokenizer defined in the <code>Field</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> Multi30k</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, BucketIterator</span><br><span class="line"></span><br><span class="line">SRC = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"de"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">TRG = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"en"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_data, valid_data, test_data = Multi30k.splits(exts = (<span class="string">'.de'</span>, <span class="string">'.en'</span>),</span><br><span class="line">                                                    fields = (SRC, TRG))</span><br></pre></td></tr></table></figure>
<pre><code>downloading training.tar.gz


.data\multi30k\training.tar.gz: 100%|█████████████████████████████████████████████| 1.21M/1.21M [00:35&lt;00:00, 33.8kB/s]


downloading validation.tar.gz


.data\multi30k\validation.tar.gz: 100%|███████████████████████████████████████████| 46.3k/46.3k [00:01&lt;00:00, 35.0kB/s]


downloading mmt_task1_test2016.tar.gz


.data\multi30k\mmt_task1_test2016.tar.gz: 100%|███████████████████████████████████| 66.2k/66.2k [00:02&lt;00:00, 26.5kB/s]
</code></pre><p>Now that we’ve defined <code>train_data</code>, we can see an extremely useful<br>feature of <code>torchtext</code>‘s <code>Field</code>: the <code>build_vocab</code> method<br>now allows us to create the vocabulary associated with each language</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SRC.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br><span class="line">TRG.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>Once these lines of code have been run, <code>SRC.vocab.stoi</code> will  be a<br>dictionary with the tokens in the vocabulary as keys and their<br>corresponding indices as values; <code>SRC.vocab.itos</code> will be the same<br>dictionary with the keys and values swapped. We won’t make extensive<br>use of this fact in this tutorial, but this will likely be useful in<br>other NLP tasks you’ll encounter.</p>
<h2 id="BucketIterator"><a href="#BucketIterator" class="headerlink" title="BucketIterator"></a><code>BucketIterator</code></h2><p>The last <code>torchtext</code> specific feature we’ll use is the <code>BucketIterator</code>,<br>which is easy to use since it takes a <code>TranslationDataset</code> as its<br>first argument. Specifically, as the docs say:<br>Defines an iterator that batches examples of similar lengths together.<br>Minimizes amount of padding needed while producing freshly shuffled<br>batches for each new epoch. See pool for the bucketing procedure used.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_iterator, valid_iterator, test_iterator = BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data),</span><br><span class="line">    batch_size = BATCH_SIZE,</span><br><span class="line">    device = device)</span><br></pre></td></tr></table></figure>
<p>These iterators can be called just like <code>DataLoader</code>s; below, in<br>the <code>train</code> and <code>evaluate</code> functions, they are called simply with:</p>
<p>::</p>
<p>   for i, batch in enumerate(iterator):</p>
<p>Each <code>batch</code> then has <code>src</code> and <code>trg</code> attributes:</p>
<p>::</p>
<p>   src = batch.src<br>   trg = batch.trg</p>
<h2 id="Defining-our-nn-Module-and-Optimizer"><a href="#Defining-our-nn-Module-and-Optimizer" class="headerlink" title="Defining our nn.Module and Optimizer"></a>Defining our <code>nn.Module</code> and <code>Optimizer</code></h2><p>That’s mostly it from a <code>torchtext</code> perspecive: with the dataset built<br>and the iterator defined, the rest of this tutorial simply defines our<br>model as an <code>nn.Module</code>, along with an <code>Optimizer</code>, and then trains it.</p>
<p>Our model specifically, follows the architecture described<br><code>here &lt;https://arxiv.org/abs/1409.0473&gt;</code><strong> (you can find a<br>significantly more commented version<br><code>here &lt;https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code></strong>).</p>
<p>Note: this model is just an example model that can be used for language<br>translation; we choose it because it is a standard model for the task,<br>not because it is the recommended model to use for translation. As you’re<br>likely aware, state-of-the-art models are currently based on Transformers;<br>you can see PyTorch’s capabilities for implementing Transformer layers<br><code>here &lt;https://pytorch.org/docs/stable/nn.html#transformer-layers&gt;</code>__; and<br>in particular, the “attention” used in the model below is different from<br>the multi-headed self-attention present in a transformer model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Tuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 input_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: float)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(enc_hid_dim * <span class="number">2</span>, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(src))</span><br><span class="line"></span><br><span class="line">        outputs, hidden = self.rnn(embedded)</span><br><span class="line"></span><br><span class="line">        hidden = torch.tanh(self.fc(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attn_dim: int)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn_in = (enc_hid_dim * <span class="number">2</span>) + dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn = nn.Linear(self.attn_in, attn_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        src_len = encoder_outputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        repeated_decoder_hidden = decoder_hidden.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, src_len, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        energy = torch.tanh(self.attn(torch.cat((</span><br><span class="line">            repeated_decoder_hidden,</span><br><span class="line">            encoder_outputs),</span><br><span class="line">            dim = <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">        attention = torch.sum(energy, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> F.softmax(attention, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 output_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attention: nn.Module)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.attention = attention</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU((enc_hid_dim * <span class="number">2</span>) + emb_dim, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weighted_encoder_rep</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                              decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        a = self.attention(decoder_hidden, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        a = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = torch.bmm(a, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weighted_encoder_rep</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                input: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        input = input.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(input))</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,</span><br><span class="line">                                                          encoder_outputs)</span><br><span class="line"></span><br><span class="line">        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        embedded = embedded.squeeze(<span class="number">0</span>)</span><br><span class="line">        output = output.squeeze(<span class="number">0</span>)</span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = self.out(torch.cat((output,</span><br><span class="line">                                     weighted_encoder_rep,</span><br><span class="line">                                     embedded), dim = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, decoder_hidden.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 decoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device: torch.device)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                trg: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                teacher_forcing_ratio: float = <span class="number">0.5</span>)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        batch_size = src.shape[<span class="number">1</span>]</span><br><span class="line">        max_len = trg.shape[<span class="number">0</span>]</span><br><span class="line">        trg_vocab_size = self.decoder.output_dim</span><br><span class="line"></span><br><span class="line">        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)</span><br><span class="line"></span><br><span class="line">        encoder_outputs, hidden = self.encoder(src)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># first input to the decoder is the &lt;sos&gt; token</span></span><br><span class="line">        output = trg[<span class="number">0</span>,:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, max_len):</span><br><span class="line">            output, hidden = self.decoder(output, hidden, encoder_outputs)</span><br><span class="line">            outputs[t] = output</span><br><span class="line">            teacher_force = random.random() &lt; teacher_forcing_ratio</span><br><span class="line">            top1 = output.max(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            output = (trg[t] <span class="keyword">if</span> teacher_force <span class="keyword">else</span> top1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = len(SRC.vocab)</span><br><span class="line">OUTPUT_DIM = len(TRG.vocab)</span><br><span class="line"><span class="comment"># ENC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># DEC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># ENC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># DEC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># ATTN_DIM = 64</span></span><br><span class="line"><span class="comment"># ENC_DROPOUT = 0.5</span></span><br><span class="line"><span class="comment"># DEC_DROPOUT = 0.5</span></span><br><span class="line"></span><br><span class="line">ENC_EMB_DIM = <span class="number">32</span></span><br><span class="line">DEC_EMB_DIM = <span class="number">32</span></span><br><span class="line">ENC_HID_DIM = <span class="number">64</span></span><br><span class="line">DEC_HID_DIM = <span class="number">64</span></span><br><span class="line">ATTN_DIM = <span class="number">8</span></span><br><span class="line">ENC_DROPOUT = <span class="number">0.5</span></span><br><span class="line">DEC_DROPOUT = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)</span><br><span class="line"></span><br><span class="line">attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)</span><br><span class="line"></span><br><span class="line">dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)</span><br><span class="line"></span><br><span class="line">model = Seq2Seq(enc, dec, device).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">            nn.init.normal_(param.data, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nn.init.constant_(param.data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.apply(init_weights)</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The model has 1,856,653 trainable parameters
</code></pre><p>Note: when scoring the performance of a language translation model in<br>particular, we have to tell the <code>nn.CrossEntropyLoss</code> function to<br>ignore the indices where the target is simply padding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PAD_IDX = TRG.vocab.stoi[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)</span><br></pre></td></tr></table></figure>
<p>Finally, we can train and evaluate this model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">          optimizer: optim.Optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">          criterion: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          clip: float)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">        src = batch.src</span><br><span class="line">        trg = batch.trg</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(src, trg)</span><br><span class="line"></span><br><span class="line">        output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">        trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">             iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">             criterion: nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">            src = batch.src</span><br><span class="line">            trg = batch.trg</span><br><span class="line"></span><br><span class="line">            output = model(src, trg, <span class="number">0</span>) <span class="comment">#turn off teacher forcing</span></span><br><span class="line"></span><br><span class="line">            output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">            trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time: int,</span></span></span><br><span class="line"><span class="function"><span class="params">               end_time: int)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line">CLIP = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)</span><br><span class="line">    valid_loss = evaluate(model, valid_iterator, criterion)</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train PPL: <span class="subst">&#123;math.exp(train_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. PPL: <span class="subst">&#123;math.exp(valid_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">test_loss = evaluate(model, test_iterator, criterion)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'| Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test PPL: <span class="subst">&#123;math.exp(test_loss):<span class="number">7.3</span>f&#125;</span> |'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 01 | Time: 15m 34s
    Train Loss: 5.681 | Train PPL: 293.100
     Val. Loss: 5.244 |  Val. PPL: 189.491
Epoch: 02 | Time: 18m 13s
    Train Loss: 5.039 | Train PPL: 154.341
     Val. Loss: 5.152 |  Val. PPL: 172.773
Epoch: 03 | Time: 15m 47s
    Train Loss: 4.788 | Train PPL: 120.088
     Val. Loss: 5.044 |  Val. PPL: 155.033
Epoch: 04 | Time: 15m 27s
    Train Loss: 4.619 | Train PPL: 101.417
     Val. Loss: 5.146 |  Val. PPL: 171.670
Epoch: 05 | Time: 16m 16s
    Train Loss: 4.491 | Train PPL:  89.179
     Val. Loss: 5.014 |  Val. PPL: 150.444
Epoch: 06 | Time: 17m 34s
    Train Loss: 4.394 | Train PPL:  80.928
     Val. Loss: 5.014 |  Val. PPL: 150.472
Epoch: 07 | Time: 18m 31s
    Train Loss: 4.306 | Train PPL:  74.153
     Val. Loss: 4.899 |  Val. PPL: 134.150
Epoch: 08 | Time: 18m 48s
    Train Loss: 4.255 | Train PPL:  70.459
     Val. Loss: 4.872 |  Val. PPL: 130.520
Epoch: 09 | Time: 18m 21s
    Train Loss: 4.200 | Train PPL:  66.700
     Val. Loss: 4.807 |  Val. PPL: 122.399
Epoch: 10 | Time: 18m 59s
    Train Loss: 4.142 | Train PPL:  62.920
     Val. Loss: 4.644 |  Val. PPL: 103.988
| Test Loss: 4.650 | Test PPL: 104.534 |
</code></pre><h2 id="Next-steps"><a href="#Next-steps" class="headerlink" title="Next steps"></a>Next steps</h2><ul>
<li>Check out the rest of Ben Trevett’s tutorials using <code>torchtext</code><br><code>here &lt;https://github.com/bentrevett/&gt;</code>__</li>
<li>Stay tuned for a tutorial using other <code>torchtext</code> features along<br>with <code>nn.Transformer</code> for language modeling via next word prediction!</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</title>
    <url>/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>Pytorch-Text-用NN.TRANFORMER和TORCHTEXT进行序列到序列建模:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText"><a href="#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText" class="headerlink" title="Sequence-to-Sequence Modeling with nn.Transformer and TorchText"></a>Sequence-to-Sequence Modeling with nn.Transformer and TorchText</h1><p>This is a tutorial on how to train a sequence-to-sequence model<br>that uses the<br><code>nn.Transformer &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&gt;</code>__ module.</p>
<p>PyTorch 1.2 release includes a standard transformer module based on the<br>paper <code>Attention is All You
Need &lt;https://arxiv.org/pdf/1706.03762.pdf&gt;</code><strong>. The transformer model<br>has been proved to be superior in quality for many sequence-to-sequence<br>problems while being more parallelizable. The <code>nn.Transformer</code> module<br>relies entirely on an attention mechanism (another module recently<br>implemented as <code>nn.MultiheadAttention &lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&gt;</code></strong>) to draw global dependencies<br>between input and output. The <code>nn.Transformer</code> module is now highly<br>modularized such that a single component (like <code>nn.TransformerEncoder &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&gt;</code>__<br>in this tutorial) can be easily adapted/composed.</p>
<p><img src="https://pytorch.org/tutorials/_images/transformer_architecture.jpg" alt></p>
<h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>In this tutorial, we train <code>nn.TransformerEncoder</code> model on a<br>language modeling task. The language modeling task is to assign a<br>probability for the likelihood of a given word (or a sequence of words)<br>to follow a sequence of words. A sequence of tokens are passed to the embedding<br>layer first, followed by a positional encoding layer to account for the order<br>of the word (see the next paragraph for more details). The<br><code>nn.TransformerEncoder</code> consists of multiple layers of<br><code>nn.TransformerEncoderLayer &lt;https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer&gt;</code>__. Along with the input sequence, a square<br>attention mask is required because the self-attention layers in<br><code>nn.TransformerEncoder</code> are only allowed to attend the earlier positions in<br>the sequence. For the language modeling task, any tokens on the future<br>positions should be masked. To have the actual words, the output<br>of <code>nn.TransformerEncoder</code> model is sent to the final Linear<br>layer, which is followed by a log-Softmax function.</p>
<hr>
<h2 id="torch-triu-input-diagonal-0-out-None-→-Tensor"><a href="#torch-triu-input-diagonal-0-out-None-→-Tensor" class="headerlink" title="torch.triu(input, diagonal=0, out=None) → Tensor"></a>torch.triu(input, diagonal=0, out=None) → Tensor</h2><p>返回矩阵上三角部分，其余部分定义为0。</p>
<p>Parameters:</p>
<ul>
<li>input (Tensor) – the input tensor</li>
<li>diagonal (int, optional) – the diagonal to consider</li>
<li>out (Tensor, optional) – the output tensor</li>
</ul>
<hr>
<ul>
<li>如果diagonal为空，输入矩阵保留主对角线与主对角线以上的元素；</li>
<li>如果diagonal为正数n，输入矩阵保留主对角线与主对角线以上除去n行的元素；</li>
<li>如果diagonal为负数-n，输入矩阵保留主对角线与主对角线以上与主对角线下方h行对角线的元素；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a &#x3D; torch.randn(3, 3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.3480, -0.5211, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;1)</span><br><span class="line">tensor([[ 0.0000,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000,  0.0000,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;-1)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000, -0.5211, -0.4573]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="pytorch-mask-filled用法"><a href="#pytorch-mask-filled用法" class="headerlink" title="pytorch mask_filled用法"></a>pytorch mask_filled用法</h2><p>将 mask必须是一个 ByteTensor 而且shape必须和 a一样 并且元素只能是 0或者1 ，是将 mask中为1的 元素所在的索引，在a中相同的的索引处替换为 value  ,mask value必须同为tensor<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.tensor([1,0,2,3])</span><br><span class="line">a.masked_fill(mask &#x3D; torch.ByteTensor([1,1,0,0]), value&#x3D;torch.tensor(-1e9))</span><br><span class="line"></span><br><span class="line">tensor([-1.0000e+09, -1.0000e+09,  2.0000e+00,  3.0000e+00])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ntoken, ninp, nhead, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(TransformerModel, self).__init__()</span><br><span class="line">        <span class="keyword">from</span> torch.nn <span class="keyword">import</span> TransformerEncoder, TransformerEncoderLayer</span><br><span class="line">        self.model_type = <span class="string">'Transformer'</span></span><br><span class="line">        self.src_mask = <span class="literal">None</span></span><br><span class="line">        self.pos_encoder = PositionalEncoding(ninp, dropout)</span><br><span class="line">        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)</span><br><span class="line">        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        self.ninp = ninp</span><br><span class="line">        self.decoder = nn.Linear(ninp, ntoken)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_square_subsequent_mask</span><span class="params">(self, sz)</span>:</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.src_mask <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self.src_mask.size(<span class="number">0</span>) != len(src):</span><br><span class="line">            device = src.device</span><br><span class="line">            mask = self._generate_square_subsequent_mask(len(src)).to(device)</span><br><span class="line">            self.src_mask = mask</span><br><span class="line"></span><br><span class="line">        src = self.encoder(src) * math.sqrt(self.ninp)</span><br><span class="line">        src = self.pos_encoder(src)</span><br><span class="line">        output = self.transformer_encoder(src, self.src_mask)</span><br><span class="line">        output = self.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p><code>PositionalEncoding</code> module injects some information about the<br>relative or absolute position of the tokens in the sequence. The<br>positional encodings have the same dimension as the embeddings so that<br>the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of<br>different frequencies.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="Load-and-batch-data"><a href="#Load-and-batch-data" class="headerlink" title="Load and batch data"></a>Load and batch data</h2><p>The training process uses Wikitext-2 dataset from <code>torchtext</code>. The<br>vocab object is built based on the train dataset and is used to numericalize<br>tokens into tensors. Starting from sequential data, the <code>batchify()</code><br>function arranges the dataset into columns, trimming off any tokens remaining<br>after the data has been divided into batches of size <code>batch_size</code>.<br>For instance, with the alphabet as the sequence (total length of 26)<br>and a batch size of 4, we would divide the alphabet into 4 sequences of<br>length 6:</p>
<p>\begin{align}\begin{bmatrix}<br>  \text{A} &amp; \text{B} &amp; \text{C} &amp; \ldots &amp; \text{X} &amp; \text{Y} &amp; \text{Z}<br>  \end{bmatrix}<br>  \Rightarrow<br>  \begin{bmatrix}<br>  \begin{bmatrix}\text{A} \\ \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{G} \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{M} \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{S} \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix}<br>  \end{bmatrix}\end{align}</p>
<p>These columns are treated as independent by the model, which means that<br>the dependence of <code>G</code> and <code>F</code> can not be learned, but allows more<br>efficient batch processing.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line">TEXT = torchtext.data.Field(tokenize=get_tokenizer(<span class="string">"basic_english"</span>),</span><br><span class="line">                            init_token=<span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">                            eos_token=<span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">                            lower=<span class="literal">True</span>)</span><br><span class="line">train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)</span><br><span class="line">TEXT.build_vocab(train_txt)</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    data = TEXT.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line">    <span class="comment"># Divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure>
<p>Functions to generate input and target sequence<br><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del>~~~~</p>
<p><code>get_batch()</code> function generates the input and target sequence for<br>the transformer model. It subdivides the source data into chunks of<br>length <code>bptt</code>. For the language modeling task, the model needs the<br>following words as <code>Target</code>. For example, with a <code>bptt</code> value of 2,<br>we’d get the following two Variables for <code>i</code> = 0:</p>
<p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/_static/img/transformer_input_target.png" alt></p>
<p>It should be noted that the chunks are along dimension 0, consistent<br>with the <code>S</code> dimension in the Transformer model. The batch dimension<br><code>N</code> is along dimension 1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bptt = <span class="number">35</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(source, i)</span>:</span></span><br><span class="line">    seq_len = min(bptt, len(source) - <span class="number">1</span> - i)</span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure>
<h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The model is set up with the hyperparameter below. The vocab size is<br>equal to the length of the vocab object.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ntokens = len(TEXT.vocab.stoi) <span class="comment"># the size of vocabulary</span></span><br><span class="line">emsize = <span class="number">200</span> <span class="comment"># embedding dimension</span></span><br><span class="line">nhid = <span class="number">200</span> <span class="comment"># the dimension of the feedforward network model in nn.TransformerEncoder</span></span><br><span class="line">nlayers = <span class="number">2</span> <span class="comment"># the number of nn.TransformerEncoderLayer in nn.TransformerEncoder</span></span><br><span class="line">nhead = <span class="number">2</span> <span class="comment"># the number of heads in the multiheadattention models</span></span><br><span class="line">dropout = <span class="number">0.2</span> <span class="comment"># the dropout value</span></span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Run-the-model"><a href="#Run-the-model" class="headerlink" title="Run the model"></a>Run the model</h2><p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>is applied to track the loss and<br><code>SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD&gt;</code></strong><br>implements stochastic gradient descent method as the optimizer. The initial<br>learning rate is set to 5.0. <code>StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR&gt;</code><strong> is<br>applied to adjust the learn rate through epochs. During the<br>training, we use<br><code>nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_&gt;</code></strong><br>function to scale all the gradient together to prevent exploding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">lr = <span class="number">5.0</span> <span class="comment"># learning rate</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train() <span class="comment"># Turn on the train mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> enumerate(range(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output.view(<span class="number">-1</span>, ntokens), targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        log_interval = <span class="number">200</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            print(<span class="string">'| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | '</span></span><br><span class="line">                  <span class="string">'lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | '</span></span><br><span class="line">                  <span class="string">'loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, math.exp(cur_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(eval_model, data_source)</span>:</span></span><br><span class="line">    eval_model.eval() <span class="comment"># Turn on the evaluation mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, data_source.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            data, targets = get_batch(data_source, i)</span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            output_flat = output.view(<span class="number">-1</span>, ntokens)</span><br><span class="line">            total_loss += len(data) * criterion(output_flat, targets).item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / (len(data_source) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Loop over epochs. Save the model if the validation loss is the best<br>we’ve seen so far. Adjust the learning rate after each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_val_loss = float(<span class="string">"inf"</span>)</span><br><span class="line">epochs = <span class="number">3</span> <span class="comment"># The number of epochs</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train()</span><br><span class="line">    val_loss = evaluate(model, val_data)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line">    print(<span class="string">'| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | '</span></span><br><span class="line">          <span class="string">'valid ppl &#123;:8.2f&#125;'</span>.format(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                     val_loss, math.exp(val_loss)))</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">        best_val_loss = val_loss</span><br><span class="line">        best_model = model</span><br><span class="line"></span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\torch\optim\lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  &quot;please use `get_last_lr()`.&quot;, UserWarning)


| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 2144.51 | loss  8.03 | ppl  3063.62
| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 1757.64 | loss  6.78 | ppl   876.40
| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 1952.50 | loss  6.37 | ppl   584.33
| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 1758.54 | loss  6.22 | ppl   501.41
| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 1863.17 | loss  6.11 | ppl   450.46
| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 1836.79 | loss  6.10 | ppl   443.68
| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 1795.15 | loss  6.05 | ppl   422.43
| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 1902.40 | loss  6.05 | ppl   425.49
| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 1827.80 | loss  5.96 | ppl   386.04
| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 1814.35 | loss  5.96 | ppl   388.23
| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 1845.57 | loss  5.85 | ppl   346.70
| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 1798.20 | loss  5.90 | ppl   364.75
| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 1925.91 | loss  5.90 | ppl   365.12
| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 1709.37 | loss  5.80 | ppl   331.20
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 5766.46s | valid loss  5.76 | valid ppl   317.98
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 1861.38 | loss  5.80 | ppl   330.78
| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 1823.89 | loss  5.77 | ppl   320.61
| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 1858.68 | loss  5.60 | ppl   270.75
| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 1831.67 | loss  5.63 | ppl   278.20
| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 1785.64 | loss  5.59 | ppl   266.78
| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 1808.74 | loss  5.61 | ppl   273.60
| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 1926.79 | loss  5.63 | ppl   278.59
| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 1821.76 | loss  5.67 | ppl   289.91
| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 1797.76 | loss  5.59 | ppl   267.51
| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 1764.14 | loss  5.62 | ppl   274.93
| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 1895.50 | loss  5.51 | ppl   246.68
| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 1793.70 | loss  5.58 | ppl   265.49
| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 1824.77 | loss  5.58 | ppl   266.10
| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 1891.16 | loss  5.51 | ppl   247.06
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 5701.96s | valid loss  5.59 | valid ppl   268.83
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 1818.12 | loss  5.55 | ppl   256.50
| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 1945.54 | loss  5.55 | ppl   257.43
| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 1616.58 | loss  5.36 | ppl   213.27
| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 1689.64 | loss  5.42 | ppl   224.94
| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 1608.36 | loss  5.38 | ppl   216.13
| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 1630.67 | loss  5.41 | ppl   222.92
| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 1686.69 | loss  5.44 | ppl   229.34
| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 1647.30 | loss  5.48 | ppl   239.58
| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 1633.68 | loss  5.40 | ppl   221.38
| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 1613.04 | loss  5.43 | ppl   228.14
| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 1608.29 | loss  5.32 | ppl   204.06
| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 1624.74 | loss  5.40 | ppl   220.31
| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 1609.86 | loss  5.41 | ppl   224.71
| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 1559.15 | loss  5.33 | ppl   207.22
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 6002.66s | valid loss  5.51 | valid ppl   247.75
-----------------------------------------------------------------------------------------
</code></pre><h2 id="Evaluate-the-model-with-the-test-dataset"><a href="#Evaluate-the-model-with-the-test-dataset" class="headerlink" title="Evaluate the model with the test dataset"></a>Evaluate the model with the test dataset</h2><p>Apply the best model to check the result with the test dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br><span class="line">print(<span class="string">'| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure>
<pre><code>=========================================================================================
| End of training | test loss  5.43 | test ppl   227.23
=========================================================================================
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parallelizable:可并行化</span><br><span class="line">superior:优越</span><br><span class="line">mechanism:机制</span><br><span class="line">modularized:模块化</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-用字符级RNN生成名称</title>
    <url>/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/</url>
    <content><![CDATA[<p>Pytorch-Text-用字符级RNN生成名称:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Generating Names with a Character-Level RNN</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>This is our second of three tutorials on “NLP From Scratch”.<br>In the <code>first tutorial &lt;/intermediate/char_rnn_classification_tutorial&gt;</code><br>we used a RNN to classify names into their language of origin. This time<br>we’ll turn around and generate names from languages.</p>
<p>::</p>
<pre><code>&gt; python sample.py Russian RUS
Rovakov
Uantov
Shavakov

&gt; python sample.py German GER
Gerren
Ereng
Rosher

&gt; python sample.py Spanish SPA
Salla
Parer
Allan

&gt; python sample.py Chinese CHI
Chan
Hang
Iun
</code></pre><p>We are still hand-crafting a small RNN with a few linear layers. The big<br>difference is instead of predicting a category after reading in all the<br>letters of a name, we input a category and output one letter at a time.<br>Recurrently predicting characters to form language (this could also be<br>done with words or other higher order constructs) is often referred to<br>as a “language model”.</p>
<p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about RNNs and how they work:</p>
<ul>
<li><code>The Unreasonable Effectiveness of Recurrent Neural
Networks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li>
<li><code>Understanding LSTM
Networks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li>
</ul>
<p>I also suggest the previous tutorial, :doc:<code>/intermediate/char_rnn_classification_tutorial</code></p>
<h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>See the last tutorial for more detail of this process. In short, there<br>are a bunch of plain text files <code>data/names/[Language].txt</code> with a<br>name per line. We split lines into an array, convert Unicode to ASCII,<br>and end up with a dictionary <code>{language: [names ...]}</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'-"</span></span><br><span class="line">n_letters = len(all_letters) + <span class="number">1</span> <span class="comment"># Plus EOS marker</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of lines per category</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> n_categories == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">'Data not found. Make sure that you downloaded data '</span></span><br><span class="line">        <span class="string">'from https://download.pytorch.org/tutorial/data.zip and extract it to '</span></span><br><span class="line">        <span class="string">'the current directory.'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'# categories:'</span>, n_categories, all_categories)</span><br><span class="line">print(unicodeToAscii(<span class="string">"O'Néàl"</span>))</span><br></pre></td></tr></table></figure>
<pre><code># categories: 18 [&#39;Arabic&#39;, &#39;Chinese&#39;, &#39;Czech&#39;, &#39;Dutch&#39;, &#39;English&#39;, &#39;French&#39;, &#39;German&#39;, &#39;Greek&#39;, &#39;Irish&#39;, &#39;Italian&#39;, &#39;Japanese&#39;, &#39;Korean&#39;, &#39;Polish&#39;, &#39;Portuguese&#39;, &#39;Russian&#39;, &#39;Scottish&#39;, &#39;Spanish&#39;, &#39;Vietnamese&#39;]
O&#39;Neal
</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>This network extends <code>the last tutorial&#39;s RNN &lt;#Creating-the-Network&gt;</code>__<br>with an extra argument for the category tensor, which is concatenated<br>along with the others. The category tensor is a one-hot vector just like<br>the letter input.</p>
<p>We will interpret the output as the probability of the next letter. When<br>sampling, the most likely output letter is used as the next input<br>letter.</p>
<p>I added a second linear layer <code>o2o</code> (after combining hidden and<br>output) to give it more muscle to work with. There’s also a dropout<br>layer, which <code>randomly zeros parts of its
input &lt;https://arxiv.org/abs/1207.0580&gt;</code>__ with a given probability<br>(here 0.1) and is usually used to fuzz inputs to prevent overfitting.<br>Here we’re using it towards the end of the network to purposely add some<br>chaos and increase sampling variety.</p>
<p>.. figure:: <a href="https://i.imgur.com/jzVrf7f.png" target="_blank" rel="noopener">https://i.imgur.com/jzVrf7f.png</a><br>   :alt:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)</span><br><span class="line">        self.o2o = nn.Linear(hidden_size + output_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, category, input, hidden)</span>:</span></span><br><span class="line">        input_combined = torch.cat((category, input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(input_combined)</span><br><span class="line">        output = self.i2o(input_combined)</span><br><span class="line">        output_combined = torch.cat((hidden, output), <span class="number">1</span>)</span><br><span class="line">        output = self.o2o(output_combined)</span><br><span class="line">        output = self.dropout(output)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>First of all, helper functions to get random pairs of (category, line):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Random item from a list</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a random category and random line from that category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    <span class="keyword">return</span> category, line</span><br></pre></td></tr></table></figure>
<p>For each timestep (that is, for each letter in a training word) the<br>inputs of the network will be<br><code>(category, current letter, hidden state)</code> and the outputs will be<br><code>(next letter, next hidden state)</code>. So for each training set, we’ll<br>need the category, a set of input letters, and a set of output/target<br>letters.</p>
<p>Since we are predicting the next letter from the current letter for each<br>timestep, the letter pairs are groups of consecutive letters from the<br>line - e.g. for <code>&quot;ABCD&lt;EOS&gt;&quot;</code> we would create (“A”, “B”), (“B”, “C”),<br>(“C”, “D”), (“D”, “EOS”).</p>
<p>.. figure:: <a href="https://i.imgur.com/JH58tXY.png" target="_blank" rel="noopener">https://i.imgur.com/JH58tXY.png</a><br>   :alt:</p>
<p>The category tensor is a <code>one-hot
tensor &lt;https://en.wikipedia.org/wiki/One-hot&gt;</code>__ of size<br><code>&lt;1 x n_categories&gt;</code>. When training we feed it to the network at every<br>timestep - this is a design choice, it could have been included as part<br>of initial hidden state or some other strategy.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># One-hot vector for category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryTensor</span><span class="params">(category)</span>:</span></span><br><span class="line">    li = all_categories.index(category)</span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_categories)</span><br><span class="line">    tensor[<span class="number">0</span>][li] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot matrix of first to last letters (not including EOS) for input</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> range(len(line)):</span><br><span class="line">        letter = line[li]</span><br><span class="line">        tensor[li][<span class="number">0</span>][all_letters.find(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># LongTensor of second letter to end (EOS) for target</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    letter_indexes = [all_letters.find(line[li]) <span class="keyword">for</span> li <span class="keyword">in</span> range(<span class="number">1</span>, len(line))]</span><br><span class="line">    letter_indexes.append(n_letters - <span class="number">1</span>) <span class="comment"># EOS</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(letter_indexes)</span><br></pre></td></tr></table></figure>
<p>For convenience during training we’ll make a <code>randomTrainingExample</code><br>function that fetches a random (category, line) pair and turns them into<br>the required (category, input, target) tensors.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Make category, input, and target tensors from a random category, line pair</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category, line = randomTrainingPair()</span><br><span class="line">    category_tensor = categoryTensor(category)</span><br><span class="line">    input_line_tensor = inputTensor(line)</span><br><span class="line">    target_line_tensor = targetTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category_tensor, input_line_tensor, target_line_tensor</span><br></pre></td></tr></table></figure>
<h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>In contrast to classification, where only the last output is used, we<br>are making a prediction at every step, so we are calculating loss at<br>every step.</p>
<p>The magic of autograd allows you to simply sum these losses at each step<br>and call backward at the end.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, input_line_tensor, target_line_tensor)</span>:</span></span><br><span class="line">    target_line_tensor.unsqueeze_(<span class="number">-1</span>)</span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(input_line_tensor.size(<span class="number">0</span>)):</span><br><span class="line">        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)</span><br><span class="line">        l = criterion(output, target_line_tensor[i])</span><br><span class="line">        loss += l</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item() / input_line_tensor.size(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>To keep track of how long training takes I am adding a<br><code>timeSince(timestamp)</code> function which returns a human readable string:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br></pre></td></tr></table></figure>
<p>Training is business as usual - call train a bunch of times and wait a<br>few minutes, printing the current time and loss every <code>print_every</code><br>examples, and keeping store of an average loss per <code>plot_every</code> examples<br>in <code>all_losses</code> for plotting later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rnn = RNN(n_letters, <span class="number">128</span>, n_letters)</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">500</span></span><br><span class="line">all_losses = []</span><br><span class="line">total_loss = <span class="number">0</span> <span class="comment"># Reset every plot_every iters</span></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    output, loss = train(*randomTrainingExample())</span><br><span class="line">    total_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start), iter, iter / n_iters * <span class="number">100</span>, loss))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(total_loss / plot_every)</span><br><span class="line">        total_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>0m 40s (5000 5%) 2.6821
1m 17s (10000 10%) 3.1606
1m 50s (15000 15%) 2.3541
2m 23s (20000 20%) 2.4859
2m 57s (25000 25%) 2.1573
3m 30s (30000 30%) 2.2910
4m 3s (35000 35%) 2.6906
4m 37s (40000 40%) 2.1542
20m 27s (45000 45%) 2.1909
21m 10s (50000 50%) 1.8939
21m 51s (55000 55%) 2.9425
22m 34s (60000 60%) 2.8395
23m 15s (65000 65%) 3.0346
23m 55s (70000 70%) 2.5686
24m 34s (75000 75%) 2.6037
25m 13s (80000 80%) 2.5966
25m 56s (85000 85%) 2.6650
26m 39s (90000 90%) 2.7412
27m 18s (95000 95%) 2.6140
27m 58s (100000 100%) 1.9323
</code></pre><h2 id="Plotting-the-Losses"><a href="#Plotting-the-Losses" class="headerlink" title="Plotting the Losses"></a>Plotting the Losses</h2><p>Plotting the historical loss from all_losses shows the network<br>learning:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x12e2623bba8&gt;]
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/output_18_1.png" alt="png"></p>
<h1 id="Sampling-the-Network"><a href="#Sampling-the-Network" class="headerlink" title="Sampling the Network"></a>Sampling the Network</h1><p>To sample we give the network a letter and ask what the next one is,<br>feed that in as the next letter, and repeat until the EOS token.</p>
<ul>
<li>Create tensors for input category, starting letter, and empty hidden<br>state</li>
<li>Create a string <code>output_name</code> with the starting letter</li>
<li><p>Up to a maximum output length,</p>
<ul>
<li>Feed the current letter to the network</li>
<li>Get the next letter from highest output, and next hidden state</li>
<li>If the letter is EOS, stop here</li>
<li>If a regular letter, add to <code>output_name</code> and continue</li>
</ul>
</li>
<li><p>Return the final name</p>
</li>
</ul>
<p>.. Note::<br>   Rather than having to give it a starting letter, another<br>   strategy would have been to include a “start of string” token in<br>   training and have the network choose its own starting letter.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_length = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from a category and starting letter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(category, start_letter=<span class="string">'A'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># no need to track history in sampling</span></span><br><span class="line">        category_tensor = categoryTensor(category)</span><br><span class="line">        input = inputTensor(start_letter)</span><br><span class="line">        hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">        output_name = start_letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hidden = rnn(category_tensor, input[<span class="number">0</span>], hidden)</span><br><span class="line">            topv, topi = output.topk(<span class="number">1</span>)</span><br><span class="line">            topi = topi[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> topi == n_letters - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                letter = all_letters[topi]</span><br><span class="line">                output_name += letter</span><br><span class="line">            input = inputTensor(letter)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get multiple samples from one category and multiple starting letters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">samples</span><span class="params">(category, start_letters=<span class="string">'ABC'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> start_letter <span class="keyword">in</span> start_letters:</span><br><span class="line">        print(sample(category, start_letter))</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Russian'</span>, <span class="string">'RUS'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'German'</span>, <span class="string">'GER'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Spanish'</span>, <span class="string">'SPA'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Chinese'</span>, <span class="string">'CHI'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Romankovovovoshollosh
Uantovovovokovosskoss
Shaverovovovovovoshol
Gerter
Eeller
Ronger
Sara
Pare
Aran
Chan
Han
Iou
</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset of category -&gt; line, for example:</p>
<ul>
<li>Fictional series -&gt; Character name</li>
<li>Part of speech -&gt; Word</li>
<li>Country -&gt; City</li>
</ul>
</li>
<li><p>Use a “start of sentence” token so that sampling can be done without<br>choosing a start letter</p>
</li>
<li><p>Get better results with a bigger and/or better shaped network</p>
<ul>
<li>Try the nn.LSTM and nn.GRU layers</li>
<li>Combine multiple of these RNNs as a higher level network</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
</search>
