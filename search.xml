<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BP识别数字</title>
    <url>/2020/07/09/BP%E8%AF%86%E5%88%AB%E6%95%B0%E5%AD%97/</url>
    <content><![CDATA[<p>原生BP代码简单演示<br><a id="more"></a></p>
<p>BP(Back Propagation)神经网络<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dsigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeruralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,layers)</span>:</span></span><br><span class="line">        self.V=np.random.random((layers[<span class="number">0</span>]+<span class="number">1</span>,layers[<span class="number">1</span>]+<span class="number">1</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">        self.W=np.random.random((layers[<span class="number">1</span>]+<span class="number">1</span>,layers[<span class="number">2</span>]))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,X,y,lr=<span class="number">0.11</span>,epochs=<span class="number">100000</span>)</span>:</span></span><br><span class="line">        temp = np.ones([X.shape[<span class="number">0</span>],X.shape[<span class="number">1</span>]+<span class="number">1</span>])</span><br><span class="line">        temp[:,<span class="number">0</span>:<span class="number">-1</span>]=X</span><br><span class="line">        X=temp</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(epochs+<span class="number">1</span>):</span><br><span class="line">            i = np.random.randint(X.shape[<span class="number">0</span>])</span><br><span class="line">            x=[X[i]]</span><br><span class="line">            x=np.atleast_2d(x)</span><br><span class="line">            L1=sigmoid(np.dot(x,self.V))</span><br><span class="line">            L2=sigmoid(np.dot(L1,self.W))</span><br><span class="line">            L2_delta=(y[i]-L2)*dsigmoid(L2)</span><br><span class="line">            L1_delta=L2_delta.dot((self.W.T))*dsigmoid(L1)</span><br><span class="line">            self.W+=lr*L1.T.dot(L2_delta)</span><br><span class="line">            self.V+=lr*x.T.dot(L1_delta)</span><br><span class="line">            <span class="keyword">if</span> n%<span class="number">1000</span>==<span class="number">0</span>:</span><br><span class="line">                predictions=[]</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(X_test.shape[<span class="number">0</span>]):</span><br><span class="line">                    o=self.predict(X_test[j])</span><br><span class="line">                    predictions.append(np.argmax(o))</span><br><span class="line">                accuracy=np.mean(np.equal(predictions,y_test))</span><br><span class="line">                print(<span class="string">'epoch:'</span>,n,<span class="string">'accuracy:'</span>,accuracy)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        temp=np.ones(x.shape[<span class="number">0</span>]+<span class="number">1</span>)</span><br><span class="line">        temp[<span class="number">0</span>:<span class="number">-1</span>]=x</span><br><span class="line">        x=temp</span><br><span class="line">        x=np.atleast_2d(x)</span><br><span class="line">        L1=sigmoid(np.dot(x,self.V))</span><br><span class="line">        L2=sigmoid(np.dot(L1,self.W))</span><br><span class="line">        <span class="keyword">return</span> L2</span><br><span class="line">    </span><br><span class="line">digits =load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">X -= X.min()</span><br><span class="line">X/=X.max()</span><br><span class="line"></span><br><span class="line">nm = NeruralNetwork([<span class="number">64</span>,<span class="number">100</span>,<span class="number">10</span>])</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y)</span><br><span class="line">labels_train = LabelBinarizer().fit_transform(y_train)</span><br><span class="line">labels_test = LabelBinarizer().fit_transform(y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'start'</span>)</span><br><span class="line">nm.train(X_train,labels_train,epochs=<span class="number">20000</span>)</span><br><span class="line">print(<span class="string">'end'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>start
epoch: 0 accuracy: 0.09777777777777778
epoch: 1000 accuracy: 0.5288888888888889
epoch: 2000 accuracy: 0.7333333333333333
epoch: 3000 accuracy: 0.8444444444444444
epoch: 4000 accuracy: 0.8666666666666667
epoch: 5000 accuracy: 0.92
epoch: 6000 accuracy: 0.9444444444444444
epoch: 7000 accuracy: 0.9555555555555556
epoch: 8000 accuracy: 0.9577777777777777
epoch: 9000 accuracy: 0.9488888888888889
epoch: 10000 accuracy: 0.9644444444444444
epoch: 11000 accuracy: 0.9666666666666667
epoch: 12000 accuracy: 0.96
epoch: 13000 accuracy: 0.9688888888888889
epoch: 14000 accuracy: 0.96
epoch: 15000 accuracy: 0.9644444444444444
epoch: 16000 accuracy: 0.9644444444444444
epoch: 17000 accuracy: 0.9666666666666667
epoch: 18000 accuracy: 0.9711111111111111
epoch: 19000 accuracy: 0.9688888888888889
epoch: 20000 accuracy: 0.9777777777777777
end
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>BP解决异或问题</title>
    <url>/2020/07/09/BP%E8%A7%A3%E5%86%B3%E5%BC%82%E6%88%96%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>BP解决异或代码简单演示<br><a id="more"></a></p>
<p>BP(Back Propagation)神经网络<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line">V = np.random.random((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">W = np.random.random((<span class="number">4</span>,<span class="number">1</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">print(V)</span><br><span class="line">print(W)</span><br><span class="line">lr = <span class="number">0.11</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dsigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> X,Y,W,V,lr</span><br><span class="line">    L1 = sigmoid(np.dot(X,V))</span><br><span class="line">    L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">    L2_delta = (Y.T-L2)*dsigmoid(L2)</span><br><span class="line">    L1_delta = L2_delta.dot(W.T)*dsigmoid(L1)</span><br><span class="line">    W_C=lr*L1.T.dot(L2_delta)</span><br><span class="line">    V_C=lr*X.T.dot(L1_delta)</span><br><span class="line">    W=W+W_C</span><br><span class="line">    V=V+V_C</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.34261592  0.95360188  0.04334655 -0.11015956]
 [ 0.12224398 -0.95540579  0.276879   -0.46803064]
 [-0.27212006  0.92409323  0.67230364 -0.63426355]]
[[ 0.6872099 ]
 [-0.05683569]
 [ 0.53593031]
 [ 0.6657038 ]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    update()</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">500</span>==<span class="number">0</span>:</span><br><span class="line">        L1 = sigmoid(np.dot(X,V))</span><br><span class="line">        L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">        print(<span class="string">'Error:'</span>, np.mean(np.abs(Y.T-L2)))</span><br><span class="line"></span><br><span class="line">L1 = sigmoid(np.dot(X,V))</span><br><span class="line">L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">print(L2)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">judge</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x&gt;=<span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> map(judge,L2):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>
<pre><code>Error: 0.5001898525337108
Error: 0.4934029320599436
Error: 0.476950391452333
Error: 0.43144945106972044
Error: 0.3591844985087166
Error: 0.2441743332168715
Error: 0.15809286523556107
Error: 0.11588525072232857
Error: 0.09306039789777351
Error: 0.0789263390463002
Error: 0.06928189244260918
Error: 0.06224125874868228
Error: 0.05684589474475504
Error: 0.05255899358953674
Error: 0.04905658260975497
Error: 0.04613132103488044
Error: 0.04364412894489313
Error: 0.04149806879631519
Error: 0.0396233922893207
Error: 0.03796855199862452
Error: 0.036494571680927006
Error: 0.03517139424513
Error: 0.033975439770446966
Error: 0.032887928957447604
Error: 0.031893705073012604
Error: 0.030980388990824857
Error: 0.030137761945244118
Error: 0.02935730716518244
Error: 0.028631864414259807
Error: 0.027955366108362894
Error: 0.027322633269370986
Error: 0.02672921597533191
Error: 0.026171267318935168
Error: 0.025645442893166143
Error: 0.025148819932565722
Error: 0.02467883173915275
Error: 0.024233214103394567
Error: 0.02380996121912868
Error: 0.02340728917277522
Error: 0.02302360552036342
[[0.01509335]
 [0.97846218]
 [0.97414206]
 [0.02814369]]
0
1
1
0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N笔记</title>
    <url>/2020/08/06/CS224N%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>斯坦福深度学习nlp课程笔记<br><a id="more"></a></p>
<ul>
<li><a href="#header1">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L1_L4)</a></li>
<li><a href="#header2">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L5_L10)</a></li>
<li><a href="#header3">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L11_L12)</a></li>
<li><a href="#header4">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L13_L14)</a></li>
<li><a href="#header5">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L15_L16)</a></li>
<li><a href="#header6">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L17_L20)</a></li>
</ul>
<h1 id="斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L1-L4"><a href="#斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L1-L4" class="headerlink" title="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L1_L4)"></a><span id="header1">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L1_L4)</span></h1><p><embed src="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L1_L4).pdf" width="100%" style="min-height:100vh;"></p>
<h1 id="斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L5-L10"><a href="#斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L5-L10" class="headerlink" title="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L5_L10)"></a><span id="header2">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L5_L10)</span></h1><p><embed src="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L5_L10).pdf" width="100%" style="min-height:100vh;"></p>
<h1 id="斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L11-L12"><a href="#斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L11-L12" class="headerlink" title="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L11_L12)"></a><span id="header3">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L11_L12)</span></h1><p><embed src="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L11_L12).pdf" width="100%" style="min-height:100vh;"></p>
<h1 id="斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L13-L14"><a href="#斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L13-L14" class="headerlink" title="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L13_L14)"></a><span id="header4">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L13_L14)</span></h1><p><embed src="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L13_L14).pdf" width="100%" style="min-height:100vh;"></p>
<h1 id="斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L15-L16"><a href="#斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L15-L16" class="headerlink" title="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L15_L16)"></a><span id="header5">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L15_L16)</span></h1><p><embed src="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L15_L16).pdf" width="100%" style="min-height:100vh;"></p>
<h1 id="斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L17-L20"><a href="#斯坦福NLP课程-CS224N-Winter-2019-学习笔记-L17-L20" class="headerlink" title="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L17_L20)"></a><span id="header6">斯坦福NLP课程 CS224N Winter 2019 学习笔记(L17_L20)</span></h1><embed src="斯坦福NLP课程 CS224N Winter 2019 学习笔记(L17_L20).pdf" width="100%" style="min-height:100vh;">]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CS224N</tag>
        <tag>深度学习</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>K-Means</title>
    <url>/2020/07/10/K-Means/</url>
    <content><![CDATA[<p>K-Means代码简单演示</p>
<p>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/kmeans.txt" target="_blank" rel="noopener">kmeans.txt</a><br><a id="more"></a></p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans7.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/kmeans.txt'</span>, delimiter=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclDistance</span><span class="params">(vector1,vector2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(sum((vector2-vector1)**<span class="number">2</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(data,k)</span>:</span></span><br><span class="line">    numSamples,dim = data.shape</span><br><span class="line">    centroids = np.zeros((k,dim))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        index = int(np.random.uniform(<span class="number">0</span>,numSamples))</span><br><span class="line">        centroids[i,:]=data[index,:]</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(data,k)</span>:</span></span><br><span class="line">    numSamples = data.shape[<span class="number">0</span>]</span><br><span class="line">    clusterData = np.array(np.zeros((numSamples,<span class="number">2</span>)))</span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    centroids = initCentroids(data,k)</span><br><span class="line">    <span class="keyword">while</span> clusterChanged:  </span><br><span class="line">        clusterChanged = <span class="literal">False</span>  </span><br><span class="line">        <span class="comment"># 循环每一个样本 </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  </span><br><span class="line">            <span class="comment"># 最小距离</span></span><br><span class="line">            minDist  = <span class="number">100000.0</span>  </span><br><span class="line">            <span class="comment"># 定义样本所属的簇</span></span><br><span class="line">            minIndex = <span class="number">0</span>  </span><br><span class="line">            <span class="comment"># 循环计算每一个质心与该样本的距离</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </span><br><span class="line">                <span class="comment"># 循环每一个质心和样本，计算距离</span></span><br><span class="line">                distance = euclDistance(centroids[j, :], data[i, :])  </span><br><span class="line">                <span class="comment"># 如果计算的距离小于最小距离，则更新最小距离</span></span><br><span class="line">                <span class="keyword">if</span> distance &lt; minDist:  </span><br><span class="line">                    minDist  = distance  </span><br><span class="line">                    <span class="comment"># 更新样本所属的簇</span></span><br><span class="line">                    minIndex = j  </span><br><span class="line">                    <span class="comment"># 更新最小距离</span></span><br><span class="line">                    clusterData[i, <span class="number">1</span>] = distance</span><br><span class="line">              </span><br><span class="line">            <span class="comment"># 如果样本的所属的簇发生了变化</span></span><br><span class="line">            <span class="keyword">if</span> clusterData[i, <span class="number">0</span>] != minIndex:  </span><br><span class="line">                <span class="comment"># 质心要重新计算</span></span><br><span class="line">                clusterChanged = <span class="literal">True</span></span><br><span class="line">                <span class="comment"># 更新样本的簇</span></span><br><span class="line">                clusterData[i, <span class="number">0</span>] = minIndex</span><br><span class="line">        <span class="comment"># 更新质心</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </span><br><span class="line">            <span class="comment"># 获取第j个簇所有的样本所在的索引</span></span><br><span class="line">            cluster_index = np.nonzero(clusterData[:, <span class="number">0</span>] == j)</span><br><span class="line">            <span class="comment"># 第j个簇所有的样本点</span></span><br><span class="line">            pointsInCluster = data[cluster_index]  </span><br><span class="line">            <span class="comment"># 计算质心</span></span><br><span class="line">            centroids[j, :] = np.mean(pointsInCluster, axis = <span class="number">0</span>) </span><br><span class="line"><span class="comment">#         showCluster(data, k, centroids, clusterData)</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterData  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(data, k, centroids, clusterData)</span>:</span>  </span><br><span class="line">    numSamples, dim = data.shape  </span><br><span class="line">    <span class="keyword">if</span> dim != <span class="number">2</span>:  </span><br><span class="line">        print(<span class="string">"dimension of your data is not 2!"</span>)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 用不同颜色形状来表示各个类别</span></span><br><span class="line">    mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]  </span><br><span class="line">    <span class="keyword">if</span> k &gt; len(mark):  </span><br><span class="line">        print(<span class="string">"Your k is too large!"</span>)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 画样本点  </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  </span><br><span class="line">        markIndex = int(clusterData[i, <span class="number">0</span>])  </span><br><span class="line">        plt.plot(data[i, <span class="number">0</span>], data[i, <span class="number">1</span>], mark[markIndex])  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 用不同颜色形状来表示各个类别</span></span><br><span class="line">    mark = [<span class="string">'*r'</span>, <span class="string">'*b'</span>, <span class="string">'*g'</span>, <span class="string">'*k'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]  </span><br><span class="line">    <span class="comment"># 画质心点 </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):  </span><br><span class="line">        plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize = <span class="number">20</span>)  </span><br><span class="line">  </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_lost=[]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">10</span>):</span><br><span class="line">    min_loss = <span class="number">10000</span></span><br><span class="line">    min_loss_centroids = np.array([])</span><br><span class="line">    min_loss_clusterData = np.array([])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        centroids,clusterData = kmeans(data,k)</span><br><span class="line">        loss = sum(clusterData[:,<span class="number">1</span>])/data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> loss&lt;min_loss:</span><br><span class="line">            min_loss = loss</span><br><span class="line">            min_loss_centroids = centroids</span><br><span class="line">            min_loss_clusterData = clusterData</span><br><span class="line">    list_lost.append(min_loss)</span><br></pre></td></tr></table></figure>
<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\numpy\core\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\numpy\core\_methods.py:154: RuntimeWarning: invalid value encountered in true_divide
  ret, rcount, out=ret, casting=&#39;unsafe&#39;, subok=False)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_lost</span><br></pre></td></tr></table></figure>
<pre><code>[2.9811811738953176,
 1.9708559728104191,
 1.1675654672086735,
 1.0712368269135584,
 1.0019034362200374,
 0.9470283294527311,
 0.8835789709731454,
 0.8393052369848919]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(range(<span class="number">2</span>,<span class="number">10</span>),list_lost)</span><br><span class="line">plt.xlabel(<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/10/K-Means/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test = [<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k=<span class="number">6</span></span><br><span class="line">min_loss = <span class="number">10000</span></span><br><span class="line">min_loss_centroids = np.array([])</span><br><span class="line">min_loss_clusterData = np.array([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">    centroids,clusterData = kmeans(data,k)</span><br><span class="line">    loss = sum(clusterData[:,<span class="number">1</span>])/data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> loss&lt;min_loss:</span><br><span class="line">        min_loss = loss</span><br><span class="line">        min_loss_centroids = centroids</span><br><span class="line">        min_loss_clusterData = clusterData</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(datas)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array([np.argmin(((np.tile(data,(k,<span class="number">1</span>))-centroids)**<span class="number">2</span>).sum(axis=<span class="number">1</span>)) <span class="keyword">for</span> data <span class="keyword">in</span> datas])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取数据值所在的范围</span></span><br><span class="line">x_min, x_max = data[:, <span class="number">0</span>].min() - <span class="number">1</span>, data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = data[:, <span class="number">1</span>].min() - <span class="number">1</span>, data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成网格矩阵</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">z = predict(np.c_[xx.ravel(), yy.ravel()])<span class="comment"># ravel与flatten类似，多维数据转一维。flatten不会改变原始数据，ravel会改变原始数据</span></span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># 等高线图</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">showCluster(data, k, centroids, clusterData)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/10/K-Means/output_9_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title>KNN</title>
    <url>/2020/07/10/KNN/</url>
    <content><![CDATA[<p>KNN代码简单演示</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line">x_data = np.array([[<span class="number">3</span>,<span class="number">104</span>],[<span class="number">2</span>,<span class="number">100</span>],[<span class="number">1</span>,<span class="number">81</span>],[<span class="number">101</span>,<span class="number">10</span>],[<span class="number">99</span>,<span class="number">5</span>],[<span class="number">81</span>,<span class="number">2</span>]])</span><br><span class="line">y_data = np.array([<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>])</span><br><span class="line">x_test = np.array([<span class="number">18</span>,<span class="number">90</span>])</span><br><span class="line"></span><br><span class="line">x_data_size = x_data.shape[<span class="number">0</span>]</span><br><span class="line">x_data_size</span><br></pre></td></tr></table></figure>
<pre><code>6
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.tile(x_test,(x_data_size,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code>array([[18, 90],
       [18, 90],
       [18, 90],
       [18, 90],
       [18, 90],
       [18, 90]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">diffMat = np.tile(x_test,(x_data_size,<span class="number">1</span>)) - x_data</span><br><span class="line">diffMat</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 15, -14],
       [ 16, -10],
       [ 17,   9],
       [-83,  80],
       [-81,  85],
       [-63,  88]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">sqDiffMat</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 225,  196],
       [ 256,  100],
       [ 289,   81],
       [6889, 6400],
       [6561, 7225],
       [3969, 7744]], dtype=int32)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">sqDistances</span><br></pre></td></tr></table></figure>
<pre><code>array([  421,   356,   370, 13289, 13786, 11713], dtype=int32)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distance = sqDistances**<span class="number">0.5</span></span><br><span class="line">distance</span><br></pre></td></tr></table></figure>
<pre><code>array([ 20.51828453,  18.86796226,  19.23538406, 115.27792503,
       117.41379817, 108.2266141 ])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">sortedDistances = distance.argsort()</span><br><span class="line">sortedDistances</span><br></pre></td></tr></table></figure>
<pre><code>array([1, 2, 0, 5, 3, 4], dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classCount = &#123;&#125;</span><br><span class="line">k = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    votelabel = y_data[sortedDistances[i]]</span><br><span class="line">    classCount[votelabel] = classCount.get(votelabel,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">classCount</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;A&#39;: 3, &#39;B&#39;: 2}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">classCount.items(),sortedClassCount</span><br></pre></td></tr></table></figure>
<pre><code>(dict_items([(&#39;A&#39;, 3), (&#39;B&#39;, 2)]), [(&#39;A&#39;, 3), (&#39;B&#39;, 2)])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knnclass = sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">knnclass</span><br></pre></td></tr></table></figure>
<pre><code>&#39;A&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report,confusion_matrix</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(x_test,x_data,y_data,k)</span>:</span></span><br><span class="line">    x_data_size = x_data.shape[<span class="number">0</span>]</span><br><span class="line">    np.tile(x_test,(x_data_size,<span class="number">1</span>))</span><br><span class="line">    diffMat = np.tile(x_test,(x_data_size,<span class="number">1</span>))-x_data</span><br><span class="line">    sqDiffMat=diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances=sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances=sqDistances**<span class="number">0.5</span></span><br><span class="line">    sortedDistances=distances.argsort()</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        votelabel = y_data[sortedDistances[i]]</span><br><span class="line">        classCount[votelabel]=classCount.get(votelabel,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">data_size=iris.data.shape[<span class="number">0</span>]</span><br><span class="line">index = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(data_size)]</span><br><span class="line">random.shuffle(index)</span><br><span class="line">iris.data = iris.data[index]</span><br><span class="line">iris.target = iris.target[index]</span><br><span class="line"></span><br><span class="line">test_size = <span class="number">40</span></span><br><span class="line">x_train = iris.data[test_size:]</span><br><span class="line">x_test = iris.data[:test_size]</span><br><span class="line">y_train = iris.target[test_size:]</span><br><span class="line">y_test = iris.target[:test_size]</span><br><span class="line"></span><br><span class="line">predictions = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x_test.shape[<span class="number">0</span>]):</span><br><span class="line">    predictions.append(knn(x_test[i],x_train,y_train,<span class="number">5</span>))</span><br><span class="line">    </span><br><span class="line">print(classification_report(y_test,predictions))</span><br></pre></td></tr></table></figure>
<pre><code>              precision    recall  f1-score   support

           0       1.00      1.00      1.00        13
           1       1.00      1.00      1.00        11
           2       1.00      1.00      1.00        16

    accuracy                           1.00        40
   macro avg       1.00      1.00      1.00        40
weighted avg       1.00      1.00      1.00        40
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(confusion_matrix(y_test,predictions))</span><br></pre></td></tr></table></figure>
<pre><code>[[13  0  0]
 [ 0 11  0]
 [ 0  0 16]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>最近邻算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown基础语法</title>
    <url>/2020/07/08/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul>
<li>1.<a href="#header1">标题</a></li>
<li>2.<a href="#header2">字体</a></li>
<li>3.<a href="#header3">图片</a></li>
<li>4.<a href="#header4">超链接</a></li>
<li>5.<a href="#header5">锚点</a></li>
<li>6.<a href="#header6">引用</a></li>
<li>7.<a href="#header7">代码</a></li>
<li>8.<a href="#header8">列表</a></li>
<li>9.<a href="#header9">分割线</a></li>
<li>10.<a href="#header10">表格</a></li>
<li>11.<a href="#header11">换行</a></li>
<li>12.<a href="#header12">流程图</a></li>
</ul>
<a id="more"></a>
<h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a><span id="header1">标题</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 标题1</span><br><span class="line">## 标题2</span><br></pre></td></tr></table></figure>
<p>效果</p>
<h1 id="标题1"><a href="#标题1" class="headerlink" title="标题1"></a>标题1</h1><h2 id="标题2"><a href="#标题2" class="headerlink" title="标题2"></a>标题2</h2><h1 id="字体"><a href="#字体" class="headerlink" title="字体"></a><span id="header2">字体</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**这是加粗的文字**</span><br><span class="line">*这是倾斜的文字*&#96;</span><br><span class="line">***这是斜体加粗的文字***</span><br><span class="line">~~这是加删除线的文字~~</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><strong>这是加粗的文字</strong><br><em>这是倾斜的文字</em>`<br><strong><em>这是斜体加粗的文字</em></strong><br><del>这是加删除线的文字</del></p>
<h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a><span id="header3">图片</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![我是ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif)</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><img src="https://www.baidu.com/img/dong_54209c0ff3da32eecc31f340c08a18f6.gif" alt="我是ALT"></p>
<h1 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a><span id="header4">超链接</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[我是百度](www.baidu.com)</span><br><span class="line">[![我是ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif)](![我是ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif))</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><a href="http://www.baidu.com" target="_blank" rel="noopener">点我或者图片均可跳转到百度</a><br><a href="http://www.baidu.com" target="_blank" rel="noopener"><img src="https://www.baidu.com/img/dong_54209c0ff3da32eecc31f340c08a18f6.gif" alt="我是ALT"></a></p>
<h1 id="锚点"><a href="#锚点" class="headerlink" title="锚点"></a><span id="header5">锚点</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[点我跳转](#mao)</span><br><span class="line">&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</span><br><span class="line">&lt;span id&#x3D;&quot;ma0&quot;&gt;跳到了我&lt;&#x2F;span&gt;</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p><a href="#mao">点我跳转</a><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><span id="mao">跳到了我</span></p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a><span id="header6">引用</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;我是引用1</span><br><span class="line">&gt;&gt;我是引用2</span><br><span class="line">&gt;&gt;&gt;我是引用3</span><br><span class="line">&gt;&gt;&gt;&gt;我是引用4</span><br></pre></td></tr></table></figure>
<p>效果</p>
<blockquote>
<p>我是引用1</p>
<blockquote>
<p>我是引用2</p>
<blockquote>
<p>我是引用3</p>
<blockquote>
<p>我是引用4</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a><span id="header7">代码</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#96;&#96;&#96;</span><br><span class="line">console.log(1)</span><br><span class="line">&#96;&#96;&#96;</span><br></pre></td></tr></table></figure>
<p>效果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">console.log(1)</span><br></pre></td></tr></table></figure>
<h1 id="列表"><a href="#列表" class="headerlink" title="列表"></a><span id="header8">列表</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* 无序列表1</span><br><span class="line">    * 无序列表1-1</span><br><span class="line">    * 无序列表1-2</span><br><span class="line">    * 无序列表1-3</span><br><span class="line">* 无序列表2</span><br><span class="line">* 无序列表3</span><br><span class="line">* 无序列表4</span><br><span class="line">* 无序列表5</span><br><span class="line">+ 无序列表6</span><br><span class="line">    + 无序列表6-1</span><br><span class="line">        + 无序列表6-1-1</span><br><span class="line">            + 无序列表6-1-1-1</span><br><span class="line">                + 无序列表6-1-1-1-1</span><br><span class="line">+ 无序列表7</span><br><span class="line">- 无序列表8</span><br><span class="line">- 无序列表9</span><br><span class="line">    * 无序列表9-1</span><br><span class="line">    * 无序列表9-2</span><br><span class="line">    * 无序列表9-3</span><br><span class="line">    * 无序列表9-4</span><br><span class="line">- 无序列表10</span><br><span class="line">- 1.有序列表</span><br><span class="line">    - 1.1有序列表</span><br><span class="line">    - 1.2有序列表</span><br><span class="line">    - 1.3有序列表</span><br><span class="line">- 2.有序列表</span><br><span class="line">- 3.有序列表</span><br><span class="line">- 4.有序列表</span><br><span class="line">- 5.有序列表</span><br></pre></td></tr></table></figure>
<p>效果</p>
<ul>
<li>无序列表1<ul>
<li>无序列表1-1</li>
<li>无序列表1-2</li>
<li>无序列表1-3</li>
</ul>
</li>
<li>无序列表2</li>
<li>无序列表3</li>
<li>无序列表4</li>
<li>无序列表5</li>
</ul>
<ul>
<li>无序列表6<ul>
<li>无序列表6-1<ul>
<li>无序列表6-1-1<ul>
<li>无序列表6-1-1-1<ul>
<li>无序列表6-1-1-1-1</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>无序列表7</li>
</ul>
<ul>
<li>无序列表8</li>
<li>无序列表9<ul>
<li>无序列表9-1</li>
<li>无序列表9-2</li>
<li>无序列表9-3</li>
<li>无序列表9-4</li>
</ul>
</li>
<li>无序列表10</li>
<li>1.有序列表<ul>
<li>1.1有序列表</li>
<li>1.2有序列表</li>
<li>1.3有序列表</li>
</ul>
</li>
<li>2.有序列表</li>
<li>3.有序列表</li>
<li>4.有序列表</li>
<li>5.有序列表</li>
</ul>
<h1 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a><span id="header9">分割线</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">****</span><br><span class="line">*****</span><br><span class="line">******</span><br><span class="line">----</span><br><span class="line">-----</span><br><span class="line">------</span><br></pre></td></tr></table></figure>
<p>效果</p>
<hr>
<hr>
<h2 id><a href="#" class="headerlink" title="**"></a><strong>**</strong></h2><h2 id="——"><a href="#——" class="headerlink" title="——-"></a>——-</h2><h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a><span id="header10">表格</span></h1><ul>
<li>第二行分割表头和内容。</li>
<li>- 有一个就行，为了对齐，多加了几个</li>
<li>文字默认居左</li>
<li>-两边加：表示文字居中</li>
<li>-右边加：表示文字居右</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">姓名|性别|年龄|战斗力</span><br><span class="line">:-:|:-|-:|-</span><br><span class="line">张飞123|男boy|100|2001</span><br><span class="line">刘备45|男girl|10|101</span><br><span class="line">关羽67890|男啊啊啊|1000|5001</span><br></pre></td></tr></table></figure>
<p>效果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">姓名</th>
<th style="text-align:left">性别</th>
<th style="text-align:right">年龄</th>
<th>战斗力</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">张飞123</td>
<td style="text-align:left">男boy</td>
<td style="text-align:right">100</td>
<td>2001</td>
</tr>
<tr>
<td style="text-align:center">刘备45</td>
<td style="text-align:left">男girl</td>
<td style="text-align:right">10</td>
<td>101</td>
</tr>
<tr>
<td style="text-align:center">关羽67890</td>
<td style="text-align:left">男啊啊啊</td>
<td style="text-align:right">1000</td>
<td>5001</td>
</tr>
</tbody>
</table>
</div>
<h1 id="换行"><a href="#换行" class="headerlink" title="换行"></a><span id="header11">换行</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">abcd</span><br><span class="line">efg</span><br><span class="line">&lt;br&gt;</span><br><span class="line">abcd</span><br><span class="line"></span><br><span class="line">efg</span><br><span class="line">&lt;br&gt;</span><br><span class="line">abcd&lt;br&gt;</span><br><span class="line">efg</span><br></pre></td></tr></table></figure>
<p>效果</p>
<p>abcd<br>efg<br><br><br>abcd</p>
<p>efg<br><br><br>abcd<br><br>efg</p>
<h1 id="流程图-部分网站如github不支持-VS-code的MD预览插件不支持"><a href="#流程图-部分网站如github不支持-VS-code的MD预览插件不支持" class="headerlink" title="流程图(部分网站如github不支持,VS code的MD预览插件不支持)"></a><span id="header12">流程图(部分网站如github不支持,VS code的MD预览插件不支持)</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flow</span><br><span class="line">st&#x3D;&gt;start: Start</span><br><span class="line">op&#x3D;&gt;operation: Your Operation</span><br><span class="line">cond&#x3D;&gt;condition: Yes or No?</span><br><span class="line">e&#x3D;&gt;end</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>
<p>flow<br>st=&gt;start: Start<br>op=&gt;operation: Your Operation<br>cond=&gt;condition: Yes or No?<br>e=&gt;end<br>st-&gt;op-&gt;cond<br>cond(yes)-&gt;e<br>cond(no)-&gt;op</p>
]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2020/07/11/PCA/</url>
    <content><![CDATA[<p>PCA简单代码展示:<br><a id="more"></a></p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA7.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA9.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA10.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report,confusion_matrix</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">digits = load_digits()</span><br><span class="line">x_data = digits.data</span><br><span class="line">y_data = digits.target</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(1797, 64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">100</span>,<span class="number">50</span>), max_iter=<span class="number">500</span>)</span><br><span class="line">mlp.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>
<pre><code>MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100, 50), learning_rate=&#39;constant&#39;,
              learning_rate_init=0.001, max_fun=15000, max_iter=500,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=None, shuffle=True, solver=&#39;adam&#39;,
              tol=0.0001, validation_fraction=0.1, verbose=False,
              warm_start=False)
</code></pre><p>协方差代码形式<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">函数原型：def cov(m, y&#x3D;None, rowvar&#x3D;True, bias&#x3D;False, ddof&#x3D;None, fweights&#x3D;None,aweights&#x3D;None)</span><br><span class="line">m:一维或则二维的数组，默认情况下每一行代表一个变量（属性），每一列代表一个观测</span><br><span class="line">y:与m具有一样的形式的一组数据</span><br><span class="line">rowvar:默认为True,此时每一行代表一个变量（属性），每一列代表一个观测；为False时，则反之</span><br><span class="line">bias:默认为False,此时标准化时除以n-1；反之为n。其中n为观测数</span><br><span class="line">ddof:类型是int，当其值非None时，bias参数作用将失效。当ddof&#x3D;1时，将会返回无偏估计（除以n-1），即使指定了fweights和aweights参数；当ddof&#x3D;0时，则返回简单平均值。</span><br><span class="line">frequency weights:一维数组，代表每个观测要重复的次数（相当于给观测赋予权重）</span><br><span class="line">analytic weights:一维数组，代表观测矢量权重。对于被认为“重要”的观察,这些相对权重通常很大,而对于被认为不太重要的观察,这些相对权重较小。如果ddof &#x3D; 0,则可以使用权重数组将概率分配给观测向量。</span><br></pre></td></tr></table></figure><br>代码示例<br>基本使用<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 计算协方差的时候，一行代表一个特征</span><br><span class="line"># 下面计算cov(T, S, M)</span><br><span class="line">T &#x3D; np.array([9, 15, 25, 14, 10, 18, 0, 16, 5, 19, 16, 20])</span><br><span class="line">S &#x3D; np.array([39, 56, 93, 61, 50, 75, 32, 85, 42, 70, 66, 80])</span><br><span class="line">M &#x3D; np.asarray([38, 56, 90, 63, 56, 77, 30, 80, 41, 79, 64, 88])</span><br><span class="line">X &#x3D; np.vstack((T, S, M))</span><br><span class="line"># X每行代表一个属性</span><br><span class="line">#  每列代表一个示例，或者说观测</span><br><span class="line">print(np.cov(X))</span><br><span class="line"></span><br><span class="line"># [[ 47.71969697 122.9469697  129.59090909]</span><br><span class="line">#  [122.9469697  370.08333333 374.59090909]</span><br><span class="line">#  [129.59090909 374.59090909 399.        ]]</span><br></pre></td></tr></table></figure><br>重点：协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间。拿到一个样本矩阵，首先要明确的就是行代表什么，列代表什么。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zeroMean</span><span class="params">(dataMat)</span>:</span></span><br><span class="line">    meanVal = np.mean(dataMat, axis=<span class="number">0</span>)</span><br><span class="line">    newData = dataMat - meanVal</span><br><span class="line">    <span class="keyword">return</span> newData, meanVal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(dataMat,top)</span>:</span></span><br><span class="line">    newData, meanVal = zeroMean(dataMat)</span><br><span class="line">    covMat = np.cov(newData,rowvar=<span class="number">0</span>) <span class="comment"># 求样本的协方差矩阵。</span></span><br><span class="line">    eigVals,eigVects = np.linalg.eig(np.mat(covMat)) <span class="comment"># 对协方差1m 𝑋𝑋𝑇矩阵做特征值分解。</span></span><br><span class="line">    eigValIndice = np.argsort(eigVals)</span><br><span class="line">    n_eigValIndice = eigValIndice[<span class="number">-1</span>:-(top+<span class="number">1</span>):<span class="number">-1</span>] <span class="comment"># 选出最大的k个特征值对应的k个特征向量。</span></span><br><span class="line">    n_eigVect = eigVects[:,n_eigValIndice]</span><br><span class="line">    lowDDataMat = newData*n_eigVect <span class="comment"># 将原始数据投影到选取的特征向量上。</span></span><br><span class="line">    reconMat = (lowDDataMat*n_eigVect.T)+meanVal</span><br><span class="line">    <span class="keyword">return</span> lowDDataMat,reconMat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowDDataMat,reconMat = pca(x_data,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x,y,c=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/11/PCA/output_8_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = mlp.predict(x_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x,y,c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/11/PCA/output_10_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowDDataMat,reconMat = pca(x_data,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">z = np.array(lowDDataMat)[:,<span class="number">2</span>]</span><br><span class="line">ax = plt.figure().add_subplot(<span class="number">111</span>,projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(x,y,z,c=y_data,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/11/PCA/output_12_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>主成分分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Audio-torchaudio</title>
    <url>/2020/07/24/Pytorch-Audio-torchaudio/</url>
    <content><![CDATA[<p>Pytorch-Audio-torchaudio:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line">pip install torchaudio</span><br></pre></td></tr></table></figure>
<pre><code>Collecting torchaudio
[?25l  Downloading https://files.pythonhosted.org/packages/e9/0a/40e53c686c2af65b2a4e818d11d9b76fa79178440caf99f3ceb2a32c3b04/torchaudio-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)
[K     |████████████████████████████████| 3.2MB 2.8MB/s 
[?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)
Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (0.16.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (1.18.5)
Installing collected packages: torchaudio
Successfully installed torchaudio-0.5.1
</code></pre><h1 id="torchaudio-Tutorial"><a href="#torchaudio-Tutorial" class="headerlink" title="torchaudio Tutorial"></a>torchaudio Tutorial</h1><p>PyTorch is an open source deep learning platform that provides a<br>seamless path from research prototyping to production deployment with<br>GPU support.</p>
<p>Significant effort in solving machine learning problems goes into data<br>preparation. <code>torchaudio</code> leverages PyTorch’s GPU support, and provides<br>many tools to make data loading easy and more readable. In this<br>tutorial, we will see how to load and preprocess data from a simple<br>dataset.</p>
<p>For this tutorial, please make sure the <code>matplotlib</code> package is<br>installed for easier visualization.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchaudio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="Opening-a-file"><a href="#Opening-a-file" class="headerlink" title="Opening a file"></a>Opening a file</h2><p><code>torchaudio</code> also supports loading sound files in the wav and mp3 format. We<br>call waveform the resulting raw audio signal.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename = <span class="string">"test.mp3"</span></span><br><span class="line">waveform, sample_rate = torchaudio.load(filename)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of waveform: &#123;&#125;"</span>.format(waveform.size()))</span><br><span class="line">print(<span class="string">"Sample rate of waveform: &#123;&#125;"</span>.format(sample_rate))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of waveform: torch.Size([2, 10857600])
Sample rate of waveform: 44100





[&lt;matplotlib.lines.Line2D at 0x7f91dd44a4a8&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dd44a5c0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_5_2.png" alt="png"></p>
<p>When you load a file in <code>torchaudio</code>, you can optionally specify the backend to use either<br><code>SoX &lt;https://pypi.org/project/sox/&gt;</code>_ or <code>SoundFile &lt;https://pypi.org/project/SoundFile/&gt;</code>_<br>via <code>torchaudio.set_audio_backend</code>. These backends are loaded lazily when needed.</p>
<p><code>torchaudio</code> also makes JIT compilation optional for functions, and uses <code>nn.Module</code> where possible.</p>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p><code>torchaudio</code> supports a growing list of<br><code>transformations &lt;https://pytorch.org/audio/transforms.html&gt;</code>_.</p>
<ul>
<li><strong>Resample</strong>: Resample waveform to a different sample rate.</li>
<li><strong>Spectrogram</strong>: Create a spectrogram from a waveform.</li>
<li><strong>GriffinLim</strong>: Compute waveform from a linear scale magnitude spectrogram using<br>the Griffin-Lim transformation.</li>
<li><strong>ComputeDeltas</strong>: Compute delta coefficients of a tensor, usually a spectrogram.</li>
<li><strong>ComplexNorm</strong>: Compute the norm of a complex tensor.</li>
<li><strong>MelScale</strong>: This turns a normal STFT into a Mel-frequency STFT,<br>using a conversion matrix.</li>
<li><strong>AmplitudeToDB</strong>: This turns a spectrogram from the<br>power/amplitude scale to the decibel scale.</li>
<li><strong>MFCC</strong>: Create the Mel-frequency cepstrum coefficients from a<br>waveform.</li>
<li><strong>MelSpectrogram</strong>: Create MEL Spectrograms from a waveform using the<br>STFT function in PyTorch.</li>
<li><strong>MuLawEncoding</strong>: Encode waveform based on mu-law companding.</li>
<li><strong>MuLawDecoding</strong>: Decode mu-law encoded waveform.</li>
<li><strong>TimeStretch</strong>: Stretch a spectrogram in time without modifying pitch for a given rate.</li>
<li><strong>FrequencyMasking</strong>: Apply masking to a spectrogram in the frequency domain.</li>
<li><strong>TimeMasking</strong>: Apply masking to a spectrogram in the time domain.</li>
</ul>
<p>Each transform supports batching: you can perform a transform on a single raw<br>audio signal or spectrogram, or many of the same shape.</p>
<p>Since all transforms are <code>nn.Modules</code> or <code>jit.ScriptModules</code>, they can be<br>used as part of a neural network at any point.</p>
<p>To start, we can look at the log of the spectrogram on a log scale.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">specgram = torchaudio.transforms.Spectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([2, 201, 54289])





&lt;matplotlib.image.AxesImage at 0x7f91dcf1ec50&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_9_2.png" alt="png"></p>
<p>Or we can look at the Mel Spectrogram on a log scale.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">specgram = torchaudio.transforms.MelSpectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">p = plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([2, 128, 54289])
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_11_1.png" alt="png"></p>
<p>We can resample the waveform, one channel at a time.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_sample_rate = sample_rate/<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Since Resample applies to a single channel, we resample first channel here</span></span><br><span class="line">channel = <span class="number">0</span></span><br><span class="line">transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([1, 1085760])





[&lt;matplotlib.lines.Line2D at 0x7f91dce23240&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_13_2.png" alt="png"></p>
<p>As another example of transformations, we can encode the signal based on<br>Mu-Law enconding. But to do so, we need the signal to be between -1 and</p>
<ol>
<li>Since the tensor is just a regular PyTorch tensor, we can apply<br>standard operators on it.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Let's check if the tensor is in the interval [-1,1]</span></span><br><span class="line">print(<span class="string">"Min of waveform: &#123;&#125;\nMax of waveform: &#123;&#125;\nMean of waveform: &#123;&#125;"</span>.format(waveform.min(), waveform.max(), waveform.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Min of waveform: -1.0
Max of waveform: 1.0
Mean of waveform: -4.018312756670639e-05
</code></pre><p>Since the waveform is already between -1 and 1, we do not need to<br>normalize it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    <span class="comment"># Subtract the mean, and scale to the interval [-1,1]</span></span><br><span class="line">    tensor_minusmean = tensor - tensor.mean()</span><br><span class="line">    <span class="keyword">return</span> tensor_minusmean/tensor_minusmean.abs().max()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's normalize to the full interval [-1,1]</span></span><br><span class="line"><span class="comment"># waveform = normalize(waveform)</span></span><br></pre></td></tr></table></figure>
<p>Let’s apply encode the waveform.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transformed = torchaudio.transforms.MuLawEncoding()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dce02b38&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_19_2.png" alt="png"></p>
<p>And now decode.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reconstructed = torchaudio.transforms.MuLawDecoding()(transformed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of recovered waveform: &#123;&#125;"</span>.format(reconstructed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(reconstructed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of recovered waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dcd62ef0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_21_2.png" alt="png"></p>
<p>We can finally compare the original waveform with its reconstructed<br>version.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Compute median relative difference</span></span><br><span class="line">err = ((waveform-reconstructed).abs() / waveform.abs()).median()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Median relative difference between original and MuLaw reconstucted signals: &#123;:.2%&#125;"</span>.format(err))</span><br></pre></td></tr></table></figure>
<pre><code>Median relative difference between original and MuLaw reconstucted signals: 1.20%
</code></pre><h2 id="Functional"><a href="#Functional" class="headerlink" title="Functional"></a>Functional</h2><p>The transformations seen above rely on lower level stateless functions for their computations.<br>These functions are available under <code>torchaudio.functional</code>. The complete list is available<br><code>here &lt;https://pytorch.org/audio/functional.html&gt;</code>_ and includes:</p>
<ul>
<li><strong>istft</strong>: Inverse short time Fourier Transform.</li>
<li><strong>gain</strong>: Applies amplification or attenuation to the whole waveform.</li>
<li><strong>dither</strong>: Increases the perceived dynamic range of audio stored at a<br>particular bit-depth.</li>
<li><strong>compute_deltas</strong>: Compute delta coefficients of a tensor.</li>
<li><strong>equalizer_biquad</strong>: Design biquad peaking equalizer filter and perform filtering.</li>
<li><strong>lowpass_biquad</strong>: Design biquad lowpass filter and perform filtering.</li>
<li><strong>highpass_biquad</strong>:Design biquad highpass filter and perform filtering.</li>
</ul>
<p>For example, let’s try the <code>mu_law_encoding</code> functional:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mu_law_encoding_waveform = torchaudio.functional.mu_law_encoding(waveform, quantization_channels=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(mu_law_encoding_waveform.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(mu_law_encoding_waveform[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dcd545f8&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_25_2.png" alt="png"></p>
<p>You can see how the output fron <code>torchaudio.functional.mu_law_encoding</code> is the same as<br>the output from <code>torchaudio.transforms.MuLawEncoding</code>.</p>
<p>Now let’s experiment with a few of the other functionals and visualize their output. Taking our<br>spectogram, we can compute it’s deltas:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">computed = torchaudio.functional.compute_deltas(specgram.contiguous(), win_length=<span class="number">3</span>)</span><br><span class="line">print(<span class="string">"Shape of computed deltas: &#123;&#125;"</span>.format(computed.shape))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(computed.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of computed deltas: torch.Size([2, 128, 54289])





&lt;matplotlib.image.AxesImage at 0x7f91dccb0b70&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_27_2.png" alt="png"></p>
<p>We can take the original waveform and apply different effects to it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gain_waveform = torchaudio.functional.gain(waveform, gain_db=<span class="number">5.0</span>)</span><br><span class="line">print(<span class="string">"Min of gain_waveform: &#123;&#125;\nMax of gain_waveform: &#123;&#125;\nMean of gain_waveform: &#123;&#125;"</span>.format(gain_waveform.min(), gain_waveform.max(), gain_waveform.mean()))</span><br><span class="line"></span><br><span class="line">dither_waveform = torchaudio.functional.dither(waveform)</span><br><span class="line">print(<span class="string">"Min of dither_waveform: &#123;&#125;\nMax of dither_waveform: &#123;&#125;\nMean of dither_waveform: &#123;&#125;"</span>.format(dither_waveform.min(), dither_waveform.max(), dither_waveform.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Min of gain_waveform: -1.778279423713684
Max of gain_waveform: 1.778279423713684
Mean of gain_waveform: -7.145693234633654e-05
Min of dither_waveform: -0.99993896484375
Max of dither_waveform: 0.999969482421875
Mean of dither_waveform: -2.492486237315461e-05
</code></pre><p>Another example of the capabilities in <code>torchaudio.functional</code> are applying filters to our<br>waveform. Applying the lowpass biquad filter to our waveform will output a new waveform with<br>the signal of the frequency modified.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowpass_waveform = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of lowpass_waveform: &#123;&#125;\nMax of lowpass_waveform: &#123;&#125;\nMean of lowpass_waveform: &#123;&#125;"</span>.format(lowpass_waveform.min(), lowpass_waveform.max(), lowpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(lowpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Min of lowpass_waveform: -1.0
Max of lowpass_waveform: 1.0
Mean of lowpass_waveform: -4.02079094783403e-05





[&lt;matplotlib.lines.Line2D at 0x7f91dcb8e278&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dcb8e390&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_31_2.png" alt="png"></p>
<p>We can also visualize a waveform with the highpass biquad filter.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">highpass_waveform = torchaudio.functional.highpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of highpass_waveform: &#123;&#125;\nMax of highpass_waveform: &#123;&#125;\nMean of highpass_waveform: &#123;&#125;"</span>.format(highpass_waveform.min(), highpass_waveform.max(), highpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(highpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Min of highpass_waveform: -0.8367071151733398
Max of highpass_waveform: 0.7935813069343567
Mean of highpass_waveform: -9.841002679422672e-09





[&lt;matplotlib.lines.Line2D at 0x7f91dcaf3e48&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dcaf3f60&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_33_2.png" alt="png"></p>
<h2 id="Migrating-to-torchaudio-from-Kaldi"><a href="#Migrating-to-torchaudio-from-Kaldi" class="headerlink" title="Migrating to torchaudio from Kaldi"></a>Migrating to torchaudio from Kaldi</h2><p>Users may be familiar with<br><code>Kaldi &lt;http://github.com/kaldi-asr/kaldi&gt;</code>_, a toolkit for speech<br>recognition. <code>torchaudio</code> offers compatibility with it in<br><code>torchaudio.kaldi_io</code>. It can indeed read from kaldi scp, or ark file<br>or streams with:</p>
<ul>
<li>read_vec_int_ark</li>
<li>read_vec_flt_scp</li>
<li>read_vec_flt_arkfile/stream</li>
<li>read_mat_scp</li>
<li>read_mat_ark</li>
</ul>
<p><code>torchaudio</code> provides Kaldi-compatible transforms for <code>spectrogram</code>,<br><code>fbank</code>, <code>mfcc</code>, and <code>`resample_waveform with the benefit of GPU support, see</code>here <compliance.kaldi.html>`__ for more information.</compliance.kaldi.html></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_fft = <span class="number">400.0</span></span><br><span class="line">frame_length = n_fft / sample_rate * <span class="number">1000.0</span></span><br><span class="line">frame_shift = frame_length / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">"channel"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"dither"</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">"window_type"</span>: <span class="string">"hanning"</span>,</span><br><span class="line">    <span class="string">"frame_length"</span>: frame_length,</span><br><span class="line">    <span class="string">"frame_shift"</span>: frame_shift,</span><br><span class="line">    <span class="string">"remove_dc_offset"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"round_to_power_of_two"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"sample_frequency"</span>: sample_rate,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([54287, 201])





&lt;matplotlib.image.AxesImage at 0x7f91dca6d240&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_35_2.png" alt="png"></p>
<p>We also support computing the filterbank features from waveforms,<br>matching Kaldi’s implementation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fbank = torchaudio.compliance.kaldi.fbank(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of fbank: &#123;&#125;"</span>.format(fbank.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(fbank.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of fbank: torch.Size([54287, 23])





&lt;matplotlib.image.AxesImage at 0x7f91dca440f0&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_37_2.png" alt="png"></p>
<p>You can create mel frequency cepstral coefficients from a raw audio signal<br>This matches the input/output of Kaldi’s compute-mfcc-feats.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mfcc = torchaudio.compliance.kaldi.mfcc(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of mfcc: &#123;&#125;"</span>.format(mfcc.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(mfcc.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of mfcc: torch.Size([54287, 13])





&lt;matplotlib.image.AxesImage at 0x7f91dca16828&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_39_2.png" alt="png"></p>
<h2 id="Available-Datasets"><a href="#Available-Datasets" class="headerlink" title="Available Datasets"></a>Available Datasets</h2><p>If you do not want to create your own dataset to train your model, <code>torchaudio</code> offers a<br>unified dataset interface. This interface supports lazy-loading of files to memory, download<br>and extract functions, and datasets to build models.</p>
<p>The datasets <code>torchaudio</code> currently supports are:</p>
<ul>
<li><strong>VCTK</strong>: Speech data uttered by 109 native speakers of English with various accents<br>(<code>Read more here &lt;https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html&gt;</code>_).</li>
<li><strong>Yesno</strong>: Sixty recordings of one individual saying yes or no in Hebrew; each<br>recording is eight words long (<code>Read more here &lt;https://www.openslr.org/1/&gt;</code>_).</li>
<li><strong>Common Voice</strong>: An open source, multi-language dataset of voices that anyone can use<br>to train speech-enabled applications (<code>Read more here &lt;https://voice.mozilla.org/en/datasets&gt;</code>_).</li>
<li><strong>LibriSpeech</strong>: Large-scale (1000 hours) corpus of read English speech (<code>Read more here &lt;http://www.openslr.org/12&gt;</code>_).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yesno_data = torchaudio.datasets.YESNO(<span class="string">'./'</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A data point in Yesno is a tuple (waveform, sample_rate, labels) where labels is a list of integers with 1 for yes and 0 for no.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pick data point number 3 to see an example of the the yesno_data:</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">waveform, sample_rate, labels = yesno_data[n]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Waveform: &#123;&#125;\nSample rate: &#123;&#125;\nLabels: &#123;&#125;"</span>.format(waveform, sample_rate, labels))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=4703754.0), HTML(value=&#39;&#39;)))



Waveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -8.5449e-04,
         -1.0986e-03, -8.8501e-04]])
Sample rate: 8000
Labels: [1, 0, 0, 0, 1, 0, 0, 1]





[&lt;matplotlib.lines.Line2D at 0x7f91dbcfb1d0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_41_3.png" alt="png"></p>
<p>Now, whenever you ask for a sound file from the dataset, it is loaded in memory only when you ask for it.<br>Meaning, the dataset only loads and keeps in memory the items that you want and use, saving on memory.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We used an example raw audio signal, or waveform, to illustrate how to<br>open an audio file using <code>torchaudio</code>, and how to pre-process,<br>transform, and apply functions to such waveform. We also demonstrated how<br>to use familiar Kaldi functions, as well as utilize built-in datasets to<br>construct our models. Given that <code>torchaudio</code> is built on PyTorch,<br>these techniques can be used as building blocks for more advanced audio<br>applications, such as speech recognition, while leveraging GPUs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Audio</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-计算机视觉迁移学习</title>
    <url>/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>Pytorch-Image-计算机视觉迁移学习:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Transfer-Learning-for-Computer-Vision-Tutorial"><a href="#Transfer-Learning-for-Computer-Vision-Tutorial" class="headerlink" title="Transfer Learning for Computer Vision Tutorial"></a>Transfer Learning for Computer Vision Tutorial</h1><p><strong>Author</strong>: <code>Sasank Chilamkurthy &lt;https://chsasank.github.io&gt;</code>_</p>
<p>In this tutorial, you will learn how to train a convolutional neural network for<br>image classification using transfer learning. You can read more about the transfer<br>learning at <code>cs231n notes &lt;https://cs231n.github.io/transfer-learning/&gt;</code>__</p>
<p>Quoting these notes,</p>
<pre><code>In practice, very few people train an entire Convolutional Network
from scratch (with random initialization), because it is relatively
rare to have a dataset of sufficient size. Instead, it is common to
pretrain a ConvNet on a very large dataset (e.g. ImageNet, which
contains 1.2 million images with 1000 categories), and then use the
ConvNet either as an initialization or a fixed feature extractor for
the task of interest.
</code></pre><p>These two major transfer learning scenarios look as follows:</p>
<ul>
<li><strong>Finetuning the convnet</strong>: Instead of random initializaion, we<br>initialize the network with a pretrained network, like the one that is<br>trained on imagenet 1000 dataset. Rest of the training looks as<br>usual.</li>
<li><strong>ConvNet as fixed feature extractor</strong>: Here, we will freeze the weights<br>for all of the network except that of the final fully connected<br>layer. This last fully connected layer is replaced with a new one<br>with random weights and only this layer is trained.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># License: BSD</span></span><br><span class="line"><span class="comment"># Author: Sasank Chilamkurthy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure>
<h2 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h2><p>We will use torchvision and torch.utils.data packages for loading the<br>data.</p>
<p>The problem we’re going to solve today is to train a model to classify<br><strong>ants</strong> and <strong>bees</strong>. We have about 120 training images each for ants and bees.<br>There are 75 validation images for each class. Usually, this is a very<br>small dataset to generalize upon, if trained from scratch. Since we<br>are using transfer learning, we should be able to generalize reasonably<br>well.</p>
<p>This dataset is a very small subset of imagenet.</p>
<p>.. Note ::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/hymenoptera_data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Data augmentation and normalization for training</span></span><br><span class="line"><span class="comment"># Just normalization for validation</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">'val'</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">'data/hymenoptera_data'</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">                                          data_transforms[x])</span><br><span class="line">                  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataset_sizes = &#123;x: len(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">class_names = image_datasets[<span class="string">'train'</span>].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<p>Visualize a few images</p>
<p>Let’s visualize a few training images so as to understand the data<br>augmentations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch of training data</span></span><br><span class="line">inputs, classes = next(iter(dataloaders[<span class="string">'train'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a grid from batch</span></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=[class_names[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_6_0.png" alt="png"></p>
<h2 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h2><p>Now, let’s write a general function to train a model. Here, we will<br>illustrate:</p>
<ul>
<li>Scheduling the learning rate</li>
<li>Saving the best model</li>
</ul>
<p>In the following, parameter <code>scheduler</code> is an LR scheduler object from<br><code>torch.optim.lr_scheduler</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter gradients</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># statistics</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>Visualizing the model predictions</p>
<p>Generic function to display predictions for a few images</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span><span class="params">(model, num_images=<span class="number">6</span>)</span>:</span></span><br><span class="line">    was_training = model.training</span><br><span class="line">    model.eval()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(dataloaders[<span class="string">'val'</span>]):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(inputs.size()[<span class="number">0</span>]):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">'off'</span>)</span><br><span class="line">                ax.set_title(<span class="string">'predicted: &#123;&#125;'</span>.format(class_names[preds[j]]))</span><br><span class="line">                imshow(inputs.cpu().data[j])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train(mode=was_training)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train(mode=was_training)</span><br></pre></td></tr></table></figure>
<h2 id="Finetuning-the-convnet"><a href="#Finetuning-the-convnet" class="headerlink" title="Finetuning the convnet"></a>Finetuning the convnet</h2><p>Load a pretrained model and reset final fully connected layer.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"><span class="comment"># Here the size of each output sample is set to 2.</span></span><br><span class="line"><span class="comment"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 0/24
----------
train Loss: 0.5563 Acc: 0.7131
val Loss: 0.3475 Acc: 0.8693

Epoch 1/24
----------
train Loss: 0.4089 Acc: 0.8156
val Loss: 0.2445 Acc: 0.9085

Epoch 2/24
----------
train Loss: 0.5546 Acc: 0.7910
val Loss: 0.2798 Acc: 0.8758

Epoch 3/24
----------
train Loss: 0.4420 Acc: 0.8238
val Loss: 0.2220 Acc: 0.9085

Epoch 4/24
----------
train Loss: 0.5138 Acc: 0.8279
val Loss: 0.4449 Acc: 0.8497

Epoch 5/24
----------
train Loss: 0.4944 Acc: 0.8320
val Loss: 0.2964 Acc: 0.9281

Epoch 6/24
----------
train Loss: 0.6059 Acc: 0.7541
val Loss: 0.2792 Acc: 0.8889

Epoch 7/24
----------
train Loss: 0.4492 Acc: 0.8279
val Loss: 0.2148 Acc: 0.9085

Epoch 8/24
----------
train Loss: 0.3162 Acc: 0.8730
val Loss: 0.2214 Acc: 0.9281

Epoch 9/24
----------
train Loss: 0.2760 Acc: 0.8730
val Loss: 0.2317 Acc: 0.9281

Epoch 10/24
----------
train Loss: 0.2800 Acc: 0.8811
val Loss: 0.2063 Acc: 0.9216

Epoch 11/24
----------
train Loss: 0.2789 Acc: 0.8975
val Loss: 0.2132 Acc: 0.9281

Epoch 12/24
----------
train Loss: 0.2112 Acc: 0.9180
val Loss: 0.2114 Acc: 0.9346

Epoch 13/24
----------
train Loss: 0.3116 Acc: 0.8811
val Loss: 0.2009 Acc: 0.9346

Epoch 14/24
----------
train Loss: 0.2907 Acc: 0.8975
val Loss: 0.1990 Acc: 0.9346

Epoch 15/24
----------
train Loss: 0.2431 Acc: 0.9098
val Loss: 0.2149 Acc: 0.9346

Epoch 16/24
----------
train Loss: 0.2203 Acc: 0.9180
val Loss: 0.2014 Acc: 0.9346

Epoch 17/24
----------
train Loss: 0.2727 Acc: 0.8689
val Loss: 0.1924 Acc: 0.9346

Epoch 18/24
----------
train Loss: 0.2276 Acc: 0.9139
val Loss: 0.1987 Acc: 0.9281

Epoch 19/24
----------
train Loss: 0.1850 Acc: 0.9180
val Loss: 0.2287 Acc: 0.9346

Epoch 20/24
----------
train Loss: 0.2624 Acc: 0.8893
val Loss: 0.2368 Acc: 0.9281

Epoch 21/24
----------
train Loss: 0.2524 Acc: 0.8975
val Loss: 0.2231 Acc: 0.9346

Epoch 22/24
----------
train Loss: 0.2732 Acc: 0.8730
val Loss: 0.1966 Acc: 0.9346

Epoch 23/24
----------
train Loss: 0.3067 Acc: 0.8811
val Loss: 0.1995 Acc: 0.9346

Epoch 24/24
----------
train Loss: 0.2301 Acc: 0.8934
val Loss: 0.1958 Acc: 0.9281

Training complete in 80m 47s
Best val Acc: 0.934641
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_0.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_1.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_2.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_3.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_4.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_5.png" alt="png"></p>
<h2 id="ConvNet-as-fixed-feature-extractor"><a href="#ConvNet-as-fixed-feature-extractor" class="headerlink" title="ConvNet as fixed feature extractor"></a>ConvNet as fixed feature extractor</h2><p>Here, we need to freeze all the network except the final layer. We need<br>to set <code>requires_grad == False</code> to freeze the parameters so that the<br>gradients are not computed in <code>backward()</code>.</p>
<p>You can read more about this in the documentation<br><code>here &lt;https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></span><br><span class="line"><span class="comment"># opposed to before.</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>Train and evaluate</p>
<p>On CPU this will take about half the time compared to previous scenario.<br>This is expected as gradients don’t need to be computed for most of the<br>network. However, forward does need to be computed.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 0/24
----------
train Loss: 0.6604 Acc: 0.6516
val Loss: 0.4662 Acc: 0.7582

Epoch 1/24
----------
train Loss: 0.4774 Acc: 0.7910
val Loss: 0.2465 Acc: 0.9020

Epoch 2/24
----------
train Loss: 0.7743 Acc: 0.7049
val Loss: 0.3536 Acc: 0.8431

Epoch 3/24
----------
train Loss: 0.4459 Acc: 0.8033
val Loss: 0.1802 Acc: 0.9346

Epoch 4/24
----------
train Loss: 0.8349 Acc: 0.6803
val Loss: 0.3540 Acc: 0.8627

Epoch 5/24
----------
train Loss: 0.4686 Acc: 0.8033
val Loss: 0.1727 Acc: 0.9608

Epoch 6/24
----------
train Loss: 0.5511 Acc: 0.7623
val Loss: 0.3009 Acc: 0.8889

Epoch 7/24
----------
train Loss: 0.3496 Acc: 0.8525
val Loss: 0.1738 Acc: 0.9542

Epoch 8/24
----------
train Loss: 0.3696 Acc: 0.8484
val Loss: 0.1663 Acc: 0.9542

Epoch 9/24
----------
train Loss: 0.2564 Acc: 0.8770
val Loss: 0.1647 Acc: 0.9608

Epoch 10/24
----------
train Loss: 0.3623 Acc: 0.8402
val Loss: 0.1873 Acc: 0.9346

Epoch 11/24
----------
train Loss: 0.3846 Acc: 0.8320
val Loss: 0.1770 Acc: 0.9477

Epoch 12/24
----------
train Loss: 0.3871 Acc: 0.8238
val Loss: 0.1760 Acc: 0.9477

Epoch 13/24
----------
train Loss: 0.3481 Acc: 0.8525
val Loss: 0.1711 Acc: 0.9542

Epoch 14/24
----------
train Loss: 0.3504 Acc: 0.8402
val Loss: 0.1635 Acc: 0.9477

Epoch 15/24
----------
train Loss: 0.4247 Acc: 0.8279
val Loss: 0.1630 Acc: 0.9608

Epoch 16/24
----------
train Loss: 0.3036 Acc: 0.8607
val Loss: 0.1695 Acc: 0.9608

Epoch 17/24
----------
train Loss: 0.2761 Acc: 0.8934
val Loss: 0.1709 Acc: 0.9608

Epoch 18/24
----------
train Loss: 0.4223 Acc: 0.8238
val Loss: 0.1854 Acc: 0.9412

Epoch 19/24
----------
train Loss: 0.3503 Acc: 0.8402
val Loss: 0.1845 Acc: 0.9216

Epoch 20/24
----------
train Loss: 0.2934 Acc: 0.8811
val Loss: 0.1648 Acc: 0.9412

Epoch 21/24
----------
train Loss: 0.3156 Acc: 0.8402
val Loss: 0.1775 Acc: 0.9346

Epoch 22/24
----------
train Loss: 0.4119 Acc: 0.8115
val Loss: 0.1744 Acc: 0.9477

Epoch 23/24
----------
train Loss: 0.2424 Acc: 0.8893
val Loss: 0.1738 Acc: 0.9412

Epoch 24/24
----------
train Loss: 0.3547 Acc: 0.8361
val Loss: 0.1687 Acc: 0.9412

Training complete in 46m 23s
Best val Acc: 0.960784
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_0.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_1.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_2.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_3.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_4.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_5.png" alt="png"></p>
<h2 id="Further-Learning"><a href="#Further-Learning" class="headerlink" title="Further Learning"></a>Further Learning</h2><p>If you would like to learn more about the applications of transfer learning,<br>checkout our <code>Quantized Transfer Learning for Computer Vision Tutorial &lt;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&gt;</code>_.</p>
<h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sufficient:足够的</span><br><span class="line">Finetuning:微调</span><br><span class="line">extractor:提取器</span><br><span class="line">generalize:概括</span><br><span class="line">flip:翻转</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-DCGAN教程</title>
    <url>/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>Pytorch-Image-DCGAN教程:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="DCGAN-Tutorial"><a href="#DCGAN-Tutorial" class="headerlink" title="DCGAN Tutorial"></a>DCGAN Tutorial</h1><p><strong>Author</strong>: <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This tutorial will give an introduction to DCGANs through an example. We<br>will train a generative adversarial network (GAN) to generate new<br>celebrities after showing it pictures of many real celebrities. Most of<br>the code here is from the dcgan implementation in<br><code>pytorch/examples &lt;https://github.com/pytorch/examples&gt;</code>__, and this<br>document will give a thorough explanation of the implementation and shed<br>light on how and why this model works. But don’t worry, no prior<br>knowledge of GANs is required, but it may require a first-timer to spend<br>some time reasoning about what is actually happening under the hood.<br>Also, for the sake of time it will help to have a GPU, or two. Lets<br>start from the beginning.</p>
<h2 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h2><p>What is a GAN?</p>
<p>GANs are a framework for teaching a DL model to capture the training<br>data’s distribution so we can generate new data from that same<br>distribution. GANs were invented by Ian Goodfellow in 2014 and first<br>described in the paper <code>Generative Adversarial
Nets &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__.<br>They are made of two distinct models, a <em>generator</em> and a<br><em>discriminator</em>. The job of the generator is to spawn ‘fake’ images that<br>look like the training images. The job of the discriminator is to look<br>at an image and output whether or not it is a real training image or a<br>fake image from the generator. During training, the generator is<br>constantly trying to outsmart the discriminator by generating better and<br>better fakes, while the discriminator is working to become a better<br>detective and correctly classify the real and fake images. The<br>equilibrium of this game is when the generator is generating perfect<br>fakes that look as if they came directly from the training data, and the<br>discriminator is left to always guess at 50% confidence that the<br>generator output is real or fake.</p>
<p>Now, lets define some notation to be used throughout tutorial starting<br>with the discriminator. Let $x$ be data representing an image.<br>$D(x)$ is the discriminator network which outputs the (scalar)<br>probability that $x$ came from training data rather than the<br>generator. Here, since we are dealing with images the input to<br>$D(x)$ is an image of CHW size 3x64x64. Intuitively, $D(x)$<br>should be HIGH when $x$ comes from training data and LOW when<br>$x$ comes from the generator. $D(x)$ can also be thought of<br>as a traditional binary classifier.</p>
<p>For the generator’s notation, let $z$ be a latent space vector<br>sampled from a standard normal distribution. $G(z)$ represents the<br>generator function which maps the latent vector $z$ to data-space.<br>The goal of $G$ is to estimate the distribution that the training<br>data comes from ($p_{data}$) so it can generate fake samples from<br>that estimated distribution ($p_g$).</p>
<p>So, $D(G(z))$ is the probability (scalar) that the output of the<br>generator $G$ is a real image. As described in <code>Goodfellow’s
paper &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__,<br>$D$ and $G$ play a minimax game in which $D$ tries to<br>maximize the probability it correctly classifies reals and fakes<br>($logD(x)$), and $G$ tries to minimize the probability that<br>$D$ will predict its outputs are fake ($log(1-D(G(x)))$).<br>From the paper, the GAN loss function is</p>
<p>$\begin{align}\underset{G}{\text{min}} \underset{D}{\text{max}}V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}\big[logD(x)\big] + \mathbb{E}_{z\sim p_{z}(z)}\big[log(1-D(G(z)))\big]\end{align}$</p>
<p>In theory, the solution to this minimax game is where<br>$p_g = p_{data}$, and the discriminator guesses randomly if the<br>inputs are real or fake. However, the convergence theory of GANs is<br>still being actively researched and in reality models do not always<br>train to this point.</p>
<p>What is a DCGAN?</p>
<p>A DCGAN is a direct extension of the GAN described above, except that it<br>explicitly uses convolutional and convolutional-transpose layers in the<br>discriminator and generator, respectively. It was first described by<br>Radford et. al. in the paper <code>Unsupervised Representation Learning With
Deep Convolutional Generative Adversarial
Networks &lt;https://arxiv.org/pdf/1511.06434.pdf&gt;</code>. The discriminator<br>is made up of strided<br><code>convolution &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code><br>layers, <code>batch
norm &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d&gt;</code><br>layers, and<br><code>LeakyReLU &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU&gt;</code><br>activations. The input is a 3x64x64 input image and the output is a<br>scalar probability that the input is from the real data distribution.<br>The generator is comprised of<br><code>convolutional-transpose &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d&gt;</code><br>layers, batch norm layers, and<br><code>ReLU &lt;https://pytorch.org/docs/stable/nn.html#relu&gt;</code>__ activations. The<br>input is a latent vector, $z$, that is drawn from a standard<br>normal distribution and the output is a 3x64x64 RGB image. The strided<br>conv-transpose layers allow the latent vector to be transformed into a<br>volume with the same shape as an image. In the paper, the authors also<br>give some tips about how to setup the optimizers, how to calculate the<br>loss functions, and how to initialize the model weights, all of which<br>will be explained in the coming sections.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="comment">#%matplotlib inline</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed for reproducibility</span></span><br><span class="line">manualSeed = <span class="number">999</span></span><br><span class="line"><span class="comment">#manualSeed = random.randint(1, 10000) # use if you want new results</span></span><br><span class="line">print(<span class="string">"Random Seed: "</span>, manualSeed)</span><br><span class="line">random.seed(manualSeed)</span><br><span class="line">torch.manual_seed(manualSeed)</span><br></pre></td></tr></table></figure>
<pre><code>Random Seed:  999





&lt;torch._C.Generator at 0x268c24f1e50&gt;
</code></pre><h2 id="Inputs"><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h2><p>Let’s define some inputs for the run:</p>
<ul>
<li><strong>dataroot</strong> - the path to the root of the dataset folder. We will<br>talk more about the dataset in the next section</li>
<li><strong>workers</strong> - the number of worker threads for loading the data with<br>the DataLoader</li>
<li><strong>batch_size</strong> - the batch size used in training. The DCGAN paper<br>uses a batch size of 128</li>
<li><strong>image_size</strong> - the spatial size of the images used for training.<br>This implementation defaults to 64x64. If another size is desired,<br>the structures of D and G must be changed. See<br><code>here &lt;https://github.com/pytorch/examples/issues/70&gt;</code>__ for more<br>details</li>
<li><strong>nc</strong> - number of color channels in the input images. For color<br>images this is 3</li>
<li><strong>nz</strong> - length of latent vector</li>
<li><strong>ngf</strong> - relates to the depth of feature maps carried through the<br>generator</li>
<li><strong>ndf</strong> - sets the depth of feature maps propagated through the<br>discriminator</li>
<li><strong>num_epochs</strong> - number of training epochs to run. Training for<br>longer will probably lead to better results but will also take much<br>longer</li>
<li><strong>lr</strong> - learning rate for training. As described in the DCGAN paper,<br>this number should be 0.0002</li>
<li><strong>beta1</strong> - beta1 hyperparameter for Adam optimizers. As described in<br>paper, this number should be 0.5</li>
<li><strong>ngpu</strong> - number of GPUs available. If this is 0, code will run in<br>CPU mode. If this number is greater than 0 it will run on that number<br>of GPUs</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Root directory for dataset</span></span><br><span class="line">dataroot = <span class="string">"data/celeba"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of workers for dataloader</span></span><br><span class="line">workers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch size during training</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spatial size of training images. All images will be resized to this</span></span><br><span class="line"><span class="comment">#   size using a transformer.</span></span><br><span class="line">image_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of channels in the training images. For color images this is 3</span></span><br><span class="line">nc = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of z latent vector (i.e. size of generator input)</span></span><br><span class="line">nz = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in generator</span></span><br><span class="line">ngf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in discriminator</span></span><br><span class="line">ndf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of training epochs</span></span><br><span class="line"><span class="comment"># num_epochs = 5</span></span><br><span class="line">num_epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate for optimizers</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Beta1 hyperparam for Adam optimizers</span></span><br><span class="line">beta1 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of GPUs available. Use 0 for CPU mode.</span></span><br><span class="line">ngpu = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>In this tutorial we will use the <code>Celeb-A Faces
dataset &lt;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&gt;</code><strong> which can<br>be downloaded at the linked site, or in <code>Google
Drive &lt;https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg&gt;</code></strong>.<br>The dataset will download as a file named <em>img_align_celeba.zip</em>. Once<br>downloaded, create a directory named <em>celeba</em> and extract the zip file<br>into that directory. Then, set the <em>dataroot</em> input for this notebook to<br>the <em>celeba</em> directory you just created. The resulting directory<br>structure should be:</p>
<p>::</p>
<p>   /path/to/celeba<br>       -&gt; img_align_celeba<br>           -&gt; 188242.jpg<br>           -&gt; 173822.jpg<br>           -&gt; 284702.jpg<br>           -&gt; 537394.jpg<br>              …</p>
<p>This is an important step because we will be using the ImageFolder<br>dataset class, which requires there to be subdirectories in the<br>dataset’s root folder. Now, we can create the dataset, create the<br>dataloader, set the device to run on, and finally visualize some of the<br>training data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We can use an image folder dataset the way we have it setup.</span></span><br><span class="line"><span class="comment"># Create the dataset</span></span><br><span class="line">dataset = dset.ImageFolder(root=dataroot,</span><br><span class="line">                           transform=transforms.Compose([</span><br><span class="line">                               transforms.Resize(image_size),</span><br><span class="line">                               transforms.CenterCrop(image_size),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">                           ]))</span><br><span class="line"><span class="comment"># Create the dataloader</span></span><br><span class="line">dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>, num_workers=workers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot some training images</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Training Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">2</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x268c49247f0&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/output_7_1.png" alt="png"></p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>With our input parameters set and the dataset prepared, we can now get<br>into the implementation. We will start with the weigth initialization<br>strategy, then talk about the generator, discriminator, loss functions,<br>and training loop in detail.</p>
<p>Weight Initialization</p>
<p>From the DCGAN paper, the authors specify that all model weights shall<br>be randomly initialized from a Normal distribution with mean=0,<br>stdev=0.02. The <code>weights_init</code> function takes an initialized model as<br>input and reinitializes all convolutional, convolutional-transpose, and<br>batch normalization layers to meet this criteria. This function is<br>applied to the models immediately after initialization.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># custom weights initialization called on netG and netD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        nn.init.constant_(m.bias.data, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>Generator</p>
<p>The generator, $G$, is designed to map the latent space vector<br>($z$) to data-space. Since our data are images, converting<br>$z$ to data-space means ultimately creating a RGB image with the<br>same size as the training images (i.e. 3x64x64). In practice, this is<br>accomplished through a series of strided two dimensional convolutional<br>transpose layers, each paired with a 2d batch norm layer and a relu<br>activation. The output of the generator is fed through a tanh function<br>to return it to the input data range of $[-1,1]$. It is worth<br>noting the existence of the batch norm functions after the<br>conv-transpose layers, as this is a critical contribution of the DCGAN<br>paper. These layers help with the flow of gradients during training. An<br>image of the generator from the DCGAN paper is shown below.</p>
<p><img src="https://pytorch.org/tutorials/_images/dcgan_generator.png" alt></p>
<p>Notice, the how the inputs we set in the input section (<em>nz</em>, <em>ngf</em>, and<br><em>nc</em>) influence the generator architecture in code. <em>nz</em> is the length<br>of the z input vector, <em>ngf</em> relates to the size of the feature maps<br>that are propagated through the generator, and <em>nc</em> is the number of<br>channels in the output image (set to 3 for RGB images). Below is the<br>code for the generator.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generator Code</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is Z, going into a convolution</span></span><br><span class="line">            nn.ConvTranspose2d( nz, ngf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">8</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*8) x 4 x 4</span></span><br><span class="line">            nn.ConvTranspose2d(ngf * <span class="number">8</span>, ngf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">4</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*4) x 8 x 8</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">4</span>, ngf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*2) x 16 x 16</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">2</span>, ngf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf) x 32 x 32</span></span><br><span class="line">            nn.ConvTranspose2d( ngf, nc, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">            <span class="comment"># state size. (nc) x 64 x 64</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure>
<p>Now, we can instantiate the generator and apply the <code>weights_init</code><br>function. Check out the printed model to see how the generator object is<br>structured.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the generator</span></span><br><span class="line">netG = Generator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netG = nn.DataParallel(netG, list(range(ngpu)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netG.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netG)</span><br></pre></td></tr></table></figure>
<pre><code>Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
</code></pre><p>Discriminator</p>
<p>As mentioned, the discriminator, $D$, is a binary classification<br>network that takes an image as input and outputs a scalar probability<br>that the input image is real (as opposed to fake). Here, $D$ takes<br>a 3x64x64 input image, processes it through a series of Conv2d,<br>BatchNorm2d, and LeakyReLU layers, and outputs the final probability<br>through a Sigmoid activation function. This architecture can be extended<br>with more layers if necessary for the problem, but there is significance<br>to the use of the strided convolution, BatchNorm, and LeakyReLUs. The<br>DCGAN paper mentions it is a good practice to use strided convolution<br>rather than pooling to downsample because it lets the network learn its<br>own pooling function. Also batch norm and leaky relu functions promote<br>healthy gradient flow which is critical for the learning process of both<br>$G$ and $D$.</p>
<p>Discriminator Code</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is (nc) x 64 x 64</span></span><br><span class="line">            nn.Conv2d(nc, ndf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf) x 32 x 32</span></span><br><span class="line">            nn.Conv2d(ndf, ndf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">2</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*2) x 16 x 16</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">2</span>, ndf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">4</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*4) x 8 x 8</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">4</span>, ndf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">8</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*8) x 4 x 4</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">8</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure>
<p>Now, as with the generator, we can create the discriminator, apply the<br><code>weights_init</code> function, and print the model’s structure.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the Discriminator</span></span><br><span class="line">netD = Discriminator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netD = nn.DataParallel(netD, list(range(ngpu)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netD.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netD)</span><br></pre></td></tr></table></figure>
<pre><code>Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (12): Sigmoid()
  )
)
</code></pre><p>Loss Functions and Optimizers</p>
<p>With $D$ and $G$ setup, we can specify how they learn<br>through the loss functions and optimizers. We will use the Binary Cross<br>Entropy loss<br>(<code>BCELoss &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss&gt;</code>__)<br>function which is defined in PyTorch as:</p>
<p>\begin{align}\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]\end{align}</p>
<p>Notice how this function provides the calculation of both log components<br>in the objective function (i.e. $log(D(x))$ and<br>$log(1-D(G(z)))$). We can specify what part of the BCE equation to<br>use with the $y$ input. This is accomplished in the training loop<br>which is coming up soon, but it is important to understand how we can<br>choose which component we wish to calculate just by changing $y$<br>(i.e. GT labels).</p>
<p>Next, we define our real label as 1 and the fake label as 0. These<br>labels will be used when calculating the losses of $D$ and<br>$G$, and this is also the convention used in the original GAN<br>paper. Finally, we set up two separate optimizers, one for $D$ and<br>one for $G$. As specified in the DCGAN paper, both are Adam<br>optimizers with learning rate 0.0002 and Beta1 = 0.5. For keeping track<br>of the generator’s learning progression, we will generate a fixed batch<br>of latent vectors that are drawn from a Gaussian distribution<br>(i.e. fixed_noise) . In the training loop, we will periodically input<br>this fixed_noise into $G$, and over the iterations we will see<br>images form out of the noise.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize BCELoss function</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create batch of latent vectors that we will use to visualize</span></span><br><span class="line"><span class="comment">#  the progression of the generator</span></span><br><span class="line">fixed_noise = torch.randn(<span class="number">64</span>, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Establish convention for real and fake labels during training</span></span><br><span class="line">real_label = <span class="number">1</span></span><br><span class="line">fake_label = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup Adam optimizers for both G and D</span></span><br><span class="line">optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure>
<p>Training</p>
<p>Finally, now that we have all of the parts of the GAN framework defined,<br>we can train it. Be mindful that training GANs is somewhat of an art<br>form, as incorrect hyperparameter settings lead to mode collapse with<br>little explanation of what went wrong. Here, we will closely follow<br>Algorithm 1 from Goodfellow’s paper, while abiding by some of the best<br>practices shown in <code>ganhacks &lt;https://github.com/soumith/ganhacks&gt;</code>__.<br>Namely, we will “construct different mini-batches for real and fake”<br>images, and also adjust G’s objective function to maximize<br>$logD(G(z))$. Training is split up into two main parts. Part 1<br>updates the Discriminator and Part 2 updates the Generator.</p>
<p><strong>Part 1 - Train the Discriminator</strong></p>
<p>Recall, the goal of training the discriminator is to maximize the<br>probability of correctly classifying a given input as real or fake. In<br>terms of Goodfellow, we wish to “update the discriminator by ascending<br>its stochastic gradient”. Practically, we want to maximize<br>$log(D(x)) + log(1-D(G(z)))$. Due to the separate mini-batch<br>suggestion from ganhacks, we will calculate this in two steps. First, we<br>will construct a batch of real samples from the training set, forward<br>pass through $D$, calculate the loss ($log(D(x))$), then<br>calculate the gradients in a backward pass. Secondly, we will construct<br>a batch of fake samples with the current generator, forward pass this<br>batch through $D$, calculate the loss ($log(1-D(G(z)))$),<br>and <em>accumulate</em> the gradients with a backward pass. Now, with the<br>gradients accumulated from both the all-real and all-fake batches, we<br>call a step of the Discriminator’s optimizer.</p>
<p><strong>Part 2 - Train the Generator</strong></p>
<p>As stated in the original paper, we want to train the Generator by<br>minimizing $log(1-D(G(z)))$ in an effort to generate better fakes.<br>As mentioned, this was shown by Goodfellow to not provide sufficient<br>gradients, especially early in the learning process. As a fix, we<br>instead wish to maximize $log(D(G(z)))$. In the code we accomplish<br>this by: classifying the Generator output from Part 1 with the<br>Discriminator, computing G’s loss <em>using real labels as GT</em>, computing<br>G’s gradients in a backward pass, and finally updating G’s parameters<br>with an optimizer step. It may seem counter-intuitive to use the real<br>labels as GT labels for the loss function, but this allows us to use the<br>$log(x)$ part of the BCELoss (rather than the $log(1-x)$<br>part) which is exactly what we want.</p>
<p>Finally, we will do some statistic reporting and at the end of each<br>epoch we will push our fixed_noise batch through the generator to<br>visually track the progress of G’s training. The training statistics<br>reported are:</p>
<ul>
<li><strong>Loss_D</strong> - discriminator loss calculated as the sum of losses for<br>the all real and all fake batches ($log(D(x)) + log(D(G(z)))$).</li>
<li><strong>Loss_G</strong> - generator loss calculated as $log(D(G(z)))$</li>
<li><strong>D(x)</strong> - the average output (across the batch) of the discriminator<br>for the all real batch. This should start close to 1 then<br>theoretically converge to 0.5 when G gets better. Think about why<br>this is.</li>
<li><strong>D(G(z))</strong> - average discriminator outputs for the all fake batch.<br>The first number is before D is updated and the second number is<br>after D is updated. These numbers should start near 0 and converge to<br>0.5 as G gets better. Think about why this is.</li>
</ul>
<p><strong>Note:</strong> This step might take a while, depending on how many epochs you<br>run and if you removed some data from the dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Training Loop</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Lists to keep track of progress</span></span><br><span class="line">img_list = []</span><br><span class="line">G_losses = []</span><br><span class="line">D_losses = []</span><br><span class="line">iters = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Starting Training Loop..."</span>)</span><br><span class="line"><span class="comment"># For each epoch</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># For each batch in the dataloader</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(dataloader, <span class="number">0</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        <span class="comment">## Train with all-real batch</span></span><br><span class="line">        netD.zero_grad()</span><br><span class="line">        <span class="comment"># Format batch</span></span><br><span class="line">        real_cpu = data[<span class="number">0</span>].to(device)</span><br><span class="line">        b_size = real_cpu.size(<span class="number">0</span>)</span><br><span class="line">        label = torch.full((b_size,), real_label, device=device)</span><br><span class="line">        <span class="comment"># Forward pass real batch through D</span></span><br><span class="line">        output = netD(real_cpu).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate loss on all-real batch</span></span><br><span class="line">        errD_real = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for D in backward pass</span></span><br><span class="line">        errD_real.backward()</span><br><span class="line">        D_x = output.mean().item()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Train with all-fake batch</span></span><br><span class="line">        <span class="comment"># Generate batch of latent vectors</span></span><br><span class="line">        noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        <span class="comment"># Generate fake image batch with G</span></span><br><span class="line">        fake = netG(noise)</span><br><span class="line">        label.fill_(fake_label)</span><br><span class="line">        <span class="comment"># Classify all fake batch with D</span></span><br><span class="line">        output = netD(fake.detach()).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate D's loss on the all-fake batch</span></span><br><span class="line">        errD_fake = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate the gradients for this batch</span></span><br><span class="line">        errD_fake.backward()</span><br><span class="line">        D_G_z1 = output.mean().item()</span><br><span class="line">        <span class="comment"># Add the gradients from the all-real and all-fake batches</span></span><br><span class="line">        errD = errD_real + errD_fake</span><br><span class="line">        <span class="comment"># Update D</span></span><br><span class="line">        optimizerD.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (2) Update G network: maximize log(D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        label.fill_(real_label)  <span class="comment"># fake labels are real for generator cost</span></span><br><span class="line">        <span class="comment"># Since we just updated D, perform another forward pass of all-fake batch through D</span></span><br><span class="line">        output = netD(fake).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate G's loss based on this output</span></span><br><span class="line">        errG = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for G</span></span><br><span class="line">        errG.backward()</span><br><span class="line">        D_G_z2 = output.mean().item()</span><br><span class="line">        <span class="comment"># Update G</span></span><br><span class="line">        optimizerG.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Output training stats</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'</span></span><br><span class="line">                  % (epoch, num_epochs, i, len(dataloader),</span><br><span class="line">                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Save Losses for plotting later</span></span><br><span class="line">        G_losses.append(errG.item())</span><br><span class="line">        D_losses.append(errD.item())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check how the generator is doing by saving G's output on fixed_noise</span></span><br><span class="line">        <span class="keyword">if</span> (iters % <span class="number">500</span> == <span class="number">0</span>) <span class="keyword">or</span> ((epoch == num_epochs<span class="number">-1</span>) <span class="keyword">and</span> (i == len(dataloader)<span class="number">-1</span>)):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = netG(fixed_noise).detach().cpu()</span><br><span class="line">            img_list.append(vutils.make_grid(fake, padding=<span class="number">2</span>, normalize=<span class="literal">True</span>))</span><br><span class="line">            </span><br><span class="line">        iters += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>Starting Training Loop...


..\aten\src\ATen\native\TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.


[0/2][0/1583]    Loss_D: 1.8664    Loss_G: 4.9949    D(x): 0.5050    D(G(z)): 0.5928 / 0.0106
[0/2][50/1583]    Loss_D: 0.1046    Loss_G: 7.1177    D(x): 0.9758    D(G(z)): 0.0124 / 0.0086
[0/2][100/1583]    Loss_D: 1.0915    Loss_G: 12.3498    D(x): 0.9553    D(G(z)): 0.4722 / 0.0000
[0/2][150/1583]    Loss_D: 1.7593    Loss_G: 5.9544    D(x): 0.3933    D(G(z)): 0.0053 / 0.0070
[0/2][200/1583]    Loss_D: 0.8020    Loss_G: 5.9020    D(x): 0.6117    D(G(z)): 0.0295 / 0.0069
[0/2][250/1583]    Loss_D: 0.6124    Loss_G: 4.1050    D(x): 0.7546    D(G(z)): 0.1926 / 0.0268
[0/2][300/1583]    Loss_D: 0.9607    Loss_G: 1.8866    D(x): 0.5101    D(G(z)): 0.0519 / 0.2133
[0/2][350/1583]    Loss_D: 0.5478    Loss_G: 3.3292    D(x): 0.7691    D(G(z)): 0.1973 / 0.0492
[0/2][400/1583]    Loss_D: 1.2509    Loss_G: 1.3775    D(x): 0.4456    D(G(z)): 0.1254 / 0.2993
[0/2][450/1583]    Loss_D: 0.4384    Loss_G: 5.5121    D(x): 0.7491    D(G(z)): 0.0298 / 0.0068
[0/2][500/1583]    Loss_D: 0.4170    Loss_G: 3.6389    D(x): 0.7676    D(G(z)): 0.0610 / 0.0396
[0/2][550/1583]    Loss_D: 0.5595    Loss_G: 4.2165    D(x): 0.8009    D(G(z)): 0.1784 / 0.0296
[0/2][600/1583]    Loss_D: 1.2295    Loss_G: 3.3066    D(x): 0.5244    D(G(z)): 0.0907 / 0.0752
[0/2][650/1583]    Loss_D: 0.5091    Loss_G: 4.7135    D(x): 0.7118    D(G(z)): 0.0292 / 0.0203
[0/2][700/1583]    Loss_D: 0.3912    Loss_G: 2.7194    D(x): 0.8198    D(G(z)): 0.1221 / 0.1098
[0/2][750/1583]    Loss_D: 0.7578    Loss_G: 7.3482    D(x): 0.9471    D(G(z)): 0.4270 / 0.0015
[0/2][800/1583]    Loss_D: 0.7080    Loss_G: 5.8282    D(x): 0.9395    D(G(z)): 0.4118 / 0.0053
[0/2][850/1583]    Loss_D: 0.5755    Loss_G: 3.2657    D(x): 0.6967    D(G(z)): 0.0784 / 0.0637
[0/2][900/1583]    Loss_D: 1.0720    Loss_G: 6.0690    D(x): 0.8734    D(G(z)): 0.5405 / 0.0047
[0/2][950/1583]    Loss_D: 0.6901    Loss_G: 5.7612    D(x): 0.9313    D(G(z)): 0.3993 / 0.0064
[0/2][1000/1583]    Loss_D: 0.2473    Loss_G: 5.5474    D(x): 0.8535    D(G(z)): 0.0574 / 0.0123
[0/2][1050/1583]    Loss_D: 0.6581    Loss_G: 5.6885    D(x): 0.9537    D(G(z)): 0.4081 / 0.0069
[0/2][1100/1583]    Loss_D: 0.5447    Loss_G: 2.8289    D(x): 0.7523    D(G(z)): 0.1252 / 0.0966
[0/2][1150/1583]    Loss_D: 0.3936    Loss_G: 3.4201    D(x): 0.8383    D(G(z)): 0.1639 / 0.0528
[0/2][1200/1583]    Loss_D: 0.7676    Loss_G: 5.3321    D(x): 0.8581    D(G(z)): 0.3643 / 0.0112
[0/2][1250/1583]    Loss_D: 0.4362    Loss_G: 3.9937    D(x): 0.8223    D(G(z)): 0.1671 / 0.0321
[0/2][1300/1583]    Loss_D: 0.6602    Loss_G: 5.5204    D(x): 0.9106    D(G(z)): 0.3806 / 0.0072
[0/2][1350/1583]    Loss_D: 0.6352    Loss_G: 3.2255    D(x): 0.7404    D(G(z)): 0.1788 / 0.0708
[0/2][1400/1583]    Loss_D: 0.7936    Loss_G: 3.5125    D(x): 0.7248    D(G(z)): 0.2855 / 0.0474
[0/2][1450/1583]    Loss_D: 1.0550    Loss_G: 1.2678    D(x): 0.7036    D(G(z)): 0.3793 / 0.3957
[0/2][1500/1583]    Loss_D: 0.4235    Loss_G: 2.8506    D(x): 0.7756    D(G(z)): 0.1001 / 0.0801
[0/2][1550/1583]    Loss_D: 0.4839    Loss_G: 4.1835    D(x): 0.8839    D(G(z)): 0.2465 / 0.0254
[1/2][0/1583]    Loss_D: 0.7391    Loss_G: 3.9735    D(x): 0.8016    D(G(z)): 0.3263 / 0.0284
[1/2][50/1583]    Loss_D: 0.5051    Loss_G: 3.8654    D(x): 0.7839    D(G(z)): 0.1756 / 0.0337
[1/2][100/1583]    Loss_D: 0.4857    Loss_G: 4.5489    D(x): 0.8677    D(G(z)): 0.2420 / 0.0209
[1/2][150/1583]    Loss_D: 0.6025    Loss_G: 4.4404    D(x): 0.8519    D(G(z)): 0.2920 / 0.0212
[1/2][200/1583]    Loss_D: 0.4301    Loss_G: 4.4767    D(x): 0.8909    D(G(z)): 0.2399 / 0.0190
[1/2][250/1583]    Loss_D: 1.2600    Loss_G: 7.6782    D(x): 0.9744    D(G(z)): 0.6415 / 0.0013
[1/2][300/1583]    Loss_D: 0.5044    Loss_G: 3.7002    D(x): 0.8408    D(G(z)): 0.2446 / 0.0375
[1/2][350/1583]    Loss_D: 0.4184    Loss_G: 3.2221    D(x): 0.7736    D(G(z)): 0.0924 / 0.0649
[1/2][400/1583]    Loss_D: 0.5320    Loss_G: 4.6695    D(x): 0.9051    D(G(z)): 0.3072 / 0.0150
[1/2][450/1583]    Loss_D: 0.3804    Loss_G: 3.3363    D(x): 0.7888    D(G(z)): 0.0978 / 0.0636
[1/2][500/1583]    Loss_D: 0.4293    Loss_G: 4.2911    D(x): 0.9014    D(G(z)): 0.2399 / 0.0226
[1/2][550/1583]    Loss_D: 0.3940    Loss_G: 2.7648    D(x): 0.7634    D(G(z)): 0.0777 / 0.0929
[1/2][600/1583]    Loss_D: 0.4044    Loss_G: 3.3666    D(x): 0.8438    D(G(z)): 0.1664 / 0.0598
[1/2][650/1583]    Loss_D: 0.3879    Loss_G: 3.4838    D(x): 0.8517    D(G(z)): 0.1754 / 0.0455
[1/2][700/1583]    Loss_D: 0.4487    Loss_G: 3.6364    D(x): 0.8773    D(G(z)): 0.2434 / 0.0370
[1/2][750/1583]    Loss_D: 0.7588    Loss_G: 2.0882    D(x): 0.6144    D(G(z)): 0.1150 / 0.1773
[1/2][800/1583]    Loss_D: 0.6134    Loss_G: 4.0046    D(x): 0.9100    D(G(z)): 0.3546 / 0.0278
[1/2][850/1583]    Loss_D: 0.5061    Loss_G: 2.2267    D(x): 0.7046    D(G(z)): 0.0860 / 0.1488
[1/2][900/1583]    Loss_D: 0.6032    Loss_G: 1.8834    D(x): 0.6518    D(G(z)): 0.0847 / 0.2023
[1/2][950/1583]    Loss_D: 1.1199    Loss_G: 2.2135    D(x): 0.4166    D(G(z)): 0.0332 / 0.1791
[1/2][1000/1583]    Loss_D: 0.8061    Loss_G: 2.2557    D(x): 0.5479    D(G(z)): 0.0442 / 0.1506
[1/2][1050/1583]    Loss_D: 0.7723    Loss_G: 2.7941    D(x): 0.5652    D(G(z)): 0.0532 / 0.0968
[1/2][1100/1583]    Loss_D: 0.6160    Loss_G: 1.4266    D(x): 0.6152    D(G(z)): 0.0460 / 0.2874
[1/2][1150/1583]    Loss_D: 1.1706    Loss_G: 5.4761    D(x): 0.9509    D(G(z)): 0.6143 / 0.0088
[1/2][1200/1583]    Loss_D: 0.5637    Loss_G: 2.2863    D(x): 0.7523    D(G(z)): 0.1901 / 0.1335
[1/2][1250/1583]    Loss_D: 0.4913    Loss_G: 2.1290    D(x): 0.7336    D(G(z)): 0.1155 / 0.1592
[1/2][1300/1583]    Loss_D: 0.4753    Loss_G: 2.9672    D(x): 0.8157    D(G(z)): 0.1986 / 0.0763
[1/2][1350/1583]    Loss_D: 0.6133    Loss_G: 2.9954    D(x): 0.8253    D(G(z)): 0.2826 / 0.0687
[1/2][1400/1583]    Loss_D: 0.4921    Loss_G: 3.1019    D(x): 0.8035    D(G(z)): 0.1985 / 0.0676
</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Finally, lets check out how we did. Here, we will look at three<br>different results. First, we will see how D and G’s losses changed<br>during training. Second, we will visualize G’s output on the fixed_noise<br>batch for every epoch. And third, we will look at a batch of real data<br>next to a batch of fake data from G.</p>
<p><strong>Loss versus training iteration</strong></p>
<p>Below is a plot of D &amp; G’s losses versus training iterations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">"Generator and Discriminator Loss During Training"</span>)</span><br><span class="line">plt.plot(G_losses,label=<span class="string">"G"</span>)</span><br><span class="line">plt.plot(D_losses,label=<span class="string">"D"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"iterations"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>Visualization of G’s progression</strong></p>
<p>Remember how we saved the generator’s output on the fixed_noise batch<br>after every epoch of training. Now, we can visualize the training<br>progression of G with an animation. Press the play button to start the<br>animation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#%%capture</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">ims = [[plt.imshow(np.transpose(i,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)), animated=<span class="literal">True</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> img_list]</span><br><span class="line">ani = animation.ArtistAnimation(fig, ims, interval=<span class="number">1000</span>, repeat_delay=<span class="number">1000</span>, blit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">HTML(ani.to_jshtml())</span><br></pre></td></tr></table></figure>
<p><strong>Real Images vs. Fake Images</strong></p>
<p>Finally, lets take a look at some real images and fake images side by<br>side.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Grab a batch of real images from the dataloader</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the real images</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Real Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">5</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the fake images from the last epoch</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Fake Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(img_list[<span class="number">-1</span>],(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Where-to-Go-Next"><a href="#Where-to-Go-Next" class="headerlink" title="Where to Go Next"></a>Where to Go Next</h2><p>We have reached the end of our journey, but there are several places you<br>could go from here. You could:</p>
<ul>
<li>Train for longer to see how good the results get</li>
<li>Modify this model to take a different dataset and possibly change the<br>size of the images and the model architecture</li>
<li>Check out some other cool GAN projects<br><code>here &lt;https://github.com/nashory/gans-awesome-applications&gt;</code>__</li>
<li>Create GANs that generate<br><code>music &lt;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&gt;</code>__</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 我不认识的单词</span></span><br></pre></td></tr></table></figure>
<p>Intuitively:直观地<br><code>
</code></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-对抗样本生成</title>
    <url>/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<p>Pytorch-Image-对抗样本生成:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Adversarial-Example-Generation"><a href="#Adversarial-Example-Generation" class="headerlink" title="Adversarial Example Generation"></a>Adversarial Example Generation</h1><p><strong>Author:</strong> <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p>
<p>If you are reading this, hopefully you can appreciate how effective some<br>machine learning models are. Research is constantly pushing ML models to<br>be faster, more accurate, and more efficient. However, an often<br>overlooked aspect of designing and training models is security and<br>robustness, especially in the face of an adversary who wishes to fool<br>the model.</p>
<p>This tutorial will raise your awareness to the security vulnerabilities<br>of ML models, and will give insight into the hot topic of adversarial<br>machine learning. You may be surprised to find that adding imperceptible<br>perturbations to an image <em>can</em> cause drastically different model<br>performance. Given that this is a tutorial, we will explore the topic<br>via example on an image classifier. Specifically we will use one of the<br>first and most popular attack methods, the Fast Gradient Sign Attack<br>(FGSM), to fool an MNIST classifier.</p>
<h2 id="Threat-Model"><a href="#Threat-Model" class="headerlink" title="Threat Model"></a>Threat Model</h2><p>For context, there are many categories of adversarial attacks, each with<br>a different goal and assumption of the attacker’s knowledge. However, in<br>general the overarching goal is to add the least amount of perturbation<br>to the input data to cause the desired misclassification. There are<br>several kinds of assumptions of the attacker’s knowledge, two of which<br>are: <strong>white-box</strong> and <strong>black-box</strong>. A <em>white-box</em> attack assumes the<br>attacker has full knowledge and access to the model, including<br>architecture, inputs, outputs, and weights. A <em>black-box</em> attack assumes<br>the attacker only has access to the inputs and outputs of the model, and<br>knows nothing about the underlying architecture or weights. There are<br>also several types of goals, including <strong>misclassification</strong> and<br><strong>source/target misclassification</strong>. A goal of <em>misclassification</em> means<br>the adversary only wants the output classification to be wrong but does<br>not care what the new classification is. A <em>source/target<br>misclassification</em> means the adversary wants to alter an image that is<br>originally of a specific source class so that it is classified as a<br>specific target class.</p>
<p>In this case, the FGSM attack is a <em>white-box</em> attack with the goal of<br><em>misclassification</em>. With this background information, we can now<br>discuss the attack in detail.</p>
<p>Fast Gradient Sign Attack</p>
<p>One of the first and most popular adversarial attacks to date is<br>referred to as the <em>Fast Gradient Sign Attack (FGSM)</em> and is described<br>by Goodfellow et. al. in <code>Explaining and Harnessing Adversarial
Examples &lt;https://arxiv.org/abs/1412.6572&gt;</code>__. The attack is remarkably<br>powerful, and yet intuitive. It is designed to attack neural networks by<br>leveraging the way they learn, <em>gradients</em>. The idea is simple, rather<br>than working to minimize the loss by adjusting the weights based on the<br>backpropagated gradients, the attack <em>adjusts the input data to maximize<br>the loss</em> based on the same backpropagated gradients. In other words,<br>the attack uses the gradient of the loss w.r.t the input data, then<br>adjusts the input data to maximize the loss.</p>
<p>Before we jump into the code, let’s look at the famous<br><code>FGSM &lt;https://arxiv.org/abs/1412.6572&gt;</code>__ panda example and extract<br>some notation.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_images/fgsm_panda_image.png" alt></p>
<p>From the figure, $\mathbf{x}$ is the original input image<br>correctly classified as a “panda”, $y$ is the ground truth label<br>for $\mathbf{x}$, $\mathbf{\theta}$ represents the model<br>parameters, and $J(\mathbf{\theta}, \mathbf{x}, y)$ is the loss<br>that is used to train the network. The attack backpropagates the<br>gradient back to the input data to calculate<br>$\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$. Then, it adjusts<br>the input data by a small step ($\epsilon$ or $0.007$ in the<br>picture) in the direction (i.e.<br>$sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))$) that will<br>maximize the loss. The resulting perturbed image, $x’$, is then<br><em>misclassified</em> by the target network as a “gibbon” when it is still<br>clearly a “panda”.</p>
<p>Hopefully now the motivation for this tutorial is clear, so lets jump<br>into the implementation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>In this section, we will discuss the input parameters for the tutorial,<br>define the model under attack, then code the attack and run some tests.</p>
<p>Inputs</p>
<p>There are only three inputs for this tutorial, and are defined as<br>follows:</p>
<ul>
<li><p><strong>epsilons</strong> - List of epsilon values to use for the run. It is<br>important to keep 0 in the list because it represents the model<br>performance on the original test set. Also, intuitively we would<br>expect the larger the epsilon, the more noticeable the perturbations<br>but the more effective the attack in terms of degrading model<br>accuracy. Since the data range here is $[0,1]$, no epsilon<br>value should exceed 1.</p>
</li>
<li><p><strong>pretrained_model</strong> - path to the pretrained MNIST model which was<br>trained with<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code><strong>.<br>For simplicity, download the pretrained model <code>here &lt;https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing&gt;</code></strong>.</p>
</li>
<li><p><strong>use_cuda</strong> - boolean flag to use CUDA if desired and available.<br>Note, a GPU with CUDA is not critical for this tutorial as a CPU will<br>not take much time.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilons = [<span class="number">0</span>, <span class="number">.05</span>, <span class="number">.1</span>, <span class="number">.15</span>, <span class="number">.2</span>, <span class="number">.25</span>, <span class="number">.3</span>]</span><br><span class="line">pretrained_model = <span class="string">"data/lenet_mnist_model.pth"</span></span><br><span class="line">use_cuda=<span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>Model Under Attack</p>
<p>As mentioned, the model under attack is the same MNIST model from<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code>__.<br>You may train and save your own MNIST model or you can download and use<br>the provided model. The <em>Net</em> definition and test dataloader here have<br>been copied from the MNIST example. The purpose of this section is to<br>define the model and dataloader, then initialize the model and load the<br>pretrained weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># LeNet Model definition</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MNIST Test dataset and dataloader declaration</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            ])), </span><br><span class="line">        batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define what device we are using</span></span><br><span class="line">print(<span class="string">"CUDA Available: "</span>,torch.cuda.is_available())</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> (use_cuda <span class="keyword">and</span> torch.cuda.is_available()) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the network</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pretrained model</span></span><br><span class="line">model.load_state_dict(torch.load(pretrained_model, map_location=<span class="string">'cpu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the model in evaluation mode. In this case this is for the Dropout layers</span></span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>
<pre><code>Using downloaded and verified file: data\MNIST\raw\train-images-idx3-ubyte.gz
Extracting data\MNIST\raw\train-images-idx3-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\train-labels-idx1-ubyte.gz
Extracting data\MNIST\raw\train-labels-idx1-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\t10k-images-idx3-ubyte.gz
Extracting data\MNIST\raw\t10k-images-idx3-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\t10k-labels-idx1-ubyte.gz
Extracting data\MNIST\raw\t10k-labels-idx1-ubyte.gz
Processing...
Done!
CUDA Available:  False





Net(
  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))
  (conv2_drop): Dropout2d(p=0.5)
  (fc1): Linear(in_features=320, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=10, bias=True)
)
</code></pre><p>FGSM Attack</p>
<p>Now, we can define the function that creates the adversarial examples by<br>perturbing the original inputs. The <code>fgsm_attack</code> function takes three<br>inputs, <em>image</em> is the original clean image ($x$), <em>epsilon</em> is<br>the pixel-wise perturbation amount ($\epsilon$), and <em>data_grad</em><br>is gradient of the loss w.r.t the input image<br>($\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$). The function<br>then creates perturbed image as</p>
<p>\begin{align}perturbed_image = image + epsilon<em>sign(data_grad) = x + \epsilon </em> sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))\end{align}</p>
<p>Finally, in order to maintain the original range of the data, the<br>perturbed image is clipped to range $[0,1]$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># FGSM attack code</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fgsm_attack</span><span class="params">(image, epsilon, data_grad)</span>:</span></span><br><span class="line">    <span class="comment"># Collect the element-wise sign of the data gradient</span></span><br><span class="line">    sign_data_grad = data_grad.sign()</span><br><span class="line">    <span class="comment"># Create the perturbed image by adjusting each pixel of the input image</span></span><br><span class="line">    perturbed_image = image + epsilon*sign_data_grad</span><br><span class="line">    <span class="comment"># Adding clipping to maintain [0,1] range</span></span><br><span class="line">    perturbed_image = torch.clamp(perturbed_image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Return the perturbed image</span></span><br><span class="line">    <span class="keyword">return</span> perturbed_image</span><br></pre></td></tr></table></figure>
<p>Testing Function</p>
<p>Finally, the central result of this tutorial comes from the <code>test</code><br>function. Each call to this test function performs a full test step on<br>the MNIST test set and reports a final accuracy. However, notice that<br>this function also takes an <em>epsilon</em> input. This is because the<br><code>test</code> function reports the accuracy of a model that is under attack<br>from an adversary with strength $\epsilon$. More specifically, for<br>each sample in the test set, the function computes the gradient of the<br>loss w.r.t the input data ($data_grad$), creates a perturbed<br>image with <code>fgsm_attack</code> ($perturbed_data$), then checks to see<br>if the perturbed example is adversarial. In addition to testing the<br>accuracy of the model, the function also saves and returns some<br>successful adversarial examples to be visualized later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">( model, device, test_loader, epsilon )</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Accuracy counter</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    adv_examples = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop over all examples in test set</span></span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Send the data and label to the device</span></span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set requires_grad attribute of tensor. Important for Attack</span></span><br><span class="line">        data.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass the data through the model</span></span><br><span class="line">        output = model(data)</span><br><span class="line">        init_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the initial prediction is wrong, dont bother attacking, just move on</span></span><br><span class="line">        <span class="keyword">if</span> init_pred.item() != target.item():</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the loss</span></span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero all existing gradients</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate gradients of model in backward pass</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Collect datagrad</span></span><br><span class="line">        data_grad = data.grad.data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Call FGSM Attack</span></span><br><span class="line">        perturbed_data = fgsm_attack(data, epsilon, data_grad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Re-classify the perturbed image</span></span><br><span class="line">        output = model(perturbed_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for success</span></span><br><span class="line">        final_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">        <span class="keyword">if</span> final_pred.item() == target.item():</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Special case for saving 0 epsilon examples</span></span><br><span class="line">            <span class="keyword">if</span> (epsilon == <span class="number">0</span>) <span class="keyword">and</span> (len(adv_examples) &lt; <span class="number">5</span>):</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Save some adv examples for visualization later</span></span><br><span class="line">            <span class="keyword">if</span> len(adv_examples) &lt; <span class="number">5</span>:</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate final accuracy for this epsilon</span></span><br><span class="line">    final_acc = correct/float(len(test_loader))</span><br><span class="line">    print(<span class="string">"Epsilon: &#123;&#125;\tTest Accuracy = &#123;&#125; / &#123;&#125; = &#123;&#125;"</span>.format(epsilon, correct, len(test_loader), final_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the accuracy and an adversarial example</span></span><br><span class="line">    <span class="keyword">return</span> final_acc, adv_examples</span><br></pre></td></tr></table></figure>
<p>Run Attack</p>
<p>The last part of the implementation is to actually run the attack. Here,<br>we run a full test step for each epsilon value in the <em>epsilons</em> input.<br>For each epsilon we also save the final accuracy and some successful<br>adversarial examples to be plotted in the coming sections. Notice how<br>the printed accuracies decrease as the epsilon value increases. Also,<br>note the $\epsilon=0$ case represents the original test accuracy,<br>with no attack.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracies = []</span><br><span class="line">examples = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run test for each epsilon</span></span><br><span class="line"><span class="keyword">for</span> eps <span class="keyword">in</span> epsilons:</span><br><span class="line">    acc, ex = test(model, device, test_loader, eps)</span><br><span class="line">    accuracies.append(acc)</span><br><span class="line">    examples.append(ex)</span><br></pre></td></tr></table></figure>
<pre><code>Epsilon: 0    Test Accuracy = 9810 / 10000 = 0.981
Epsilon: 0.05    Test Accuracy = 9426 / 10000 = 0.9426
Epsilon: 0.1    Test Accuracy = 8510 / 10000 = 0.851
Epsilon: 0.15    Test Accuracy = 6826 / 10000 = 0.6826
Epsilon: 0.2    Test Accuracy = 4301 / 10000 = 0.4301
Epsilon: 0.25    Test Accuracy = 2082 / 10000 = 0.2082
Epsilon: 0.3    Test Accuracy = 869 / 10000 = 0.0869
</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Accuracy vs Epsilon</p>
<p>The first result is the accuracy versus epsilon plot. As alluded to<br>earlier, as epsilon increases we expect the test accuracy to decrease.<br>This is because larger epsilons mean we take a larger step in the<br>direction that will maximize the loss. Notice the trend in the curve is<br>not linear even though the epsilon values are linearly spaced. For<br>example, the accuracy at $\epsilon=0.05$ is only about 4% lower<br>than $\epsilon=0$, but the accuracy at $\epsilon=0.2$ is 25%<br>lower than $\epsilon=0.15$. Also, notice the accuracy of the model<br>hits random accuracy for a 10-class classifier between<br>$\epsilon=0.25$ and $\epsilon=0.3$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(epsilons, accuracies, <span class="string">"*-"</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">1.1</span>, step=<span class="number">0.1</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, <span class="number">.35</span>, step=<span class="number">0.05</span>))</span><br><span class="line">plt.title(<span class="string">"Accuracy vs Epsilon"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epsilon"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_15_0.png" alt="png"></p>
<p>Sample Adversarial Examples</p>
<p>Remember the idea of no free lunch? In this case, as epsilon increases<br>the test accuracy decreases <strong>BUT</strong> the perturbations become more easily<br>perceptible. In reality, there is a tradeoff between accuracy<br>degredation and perceptibility that an attacker must consider. Here, we<br>show some examples of successful adversarial examples at each epsilon<br>value. Each row of the plot shows a different epsilon value. The first<br>row is the $\epsilon=0$ examples which represent the original<br>“clean” images with no perturbation. The title of each image shows the<br>“original classification -&gt; adversarial classification.” Notice, the<br>perturbations start to become evident at $\epsilon=0.15$ and are<br>quite evident at $\epsilon=0.3$. However, in all cases humans are<br>still capable of identifying the correct class despite the added noise.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Plot several examples of adversarial samples at each epsilon</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(epsilons)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(examples[i])):</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.subplot(len(epsilons),len(examples[<span class="number">0</span>]),cnt)</span><br><span class="line">        plt.xticks([], [])</span><br><span class="line">        plt.yticks([], [])</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            plt.ylabel(<span class="string">"Eps: &#123;&#125;"</span>.format(epsilons[i]), fontsize=<span class="number">14</span>)</span><br><span class="line">        orig,adv,ex = examples[i][j]</span><br><span class="line">        plt.title(<span class="string">"&#123;&#125; -&gt; &#123;&#125;"</span>.format(orig, adv))</span><br><span class="line">        plt.imshow(ex, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_17_0.png" alt="png"></p>
<h2 id="Where-to-go-next"><a href="#Where-to-go-next" class="headerlink" title="Where to go next?"></a>Where to go next?</h2><p>Hopefully this tutorial gives some insight into the topic of adversarial<br>machine learning. There are many potential directions to go from here.<br>This attack represents the very beginning of adversarial attack research<br>and since there have been many subsequent ideas for how to attack and<br>defend ML models from an adversary. In fact, at NIPS 2017 there was an<br>adversarial attack and defense competition and many of the methods used<br>in the competition are described in this paper: <code>Adversarial Attacks and
Defences Competition &lt;https://arxiv.org/pdf/1804.00097.pdf&gt;</code>__. The work<br>on defense also leads into the idea of making machine learning models<br>more <em>robust</em> in general, to both naturally perturbed and adversarially<br>crafted inputs.</p>
<p>Another direction to go is adversarial attacks and defense in different<br>domains. Adversarial research is not limited to the image domain, check<br>out <code>this &lt;https://arxiv.org/pdf/1801.01944.pdf&gt;</code>__ attack on<br>speech-to-text models. But perhaps the best way to learn more about<br>adversarial machine learning is to get your hands dirty. Try to<br>implement a different attack from the NIPS 2017 competition, and see how<br>it differs from FGSM. Then, try to defend the model from your own<br>attacks.</p>
<h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">overlooked:被忽视</span><br><span class="line">robustness:健壮性&#x2F;鲁棒性</span><br><span class="line">vulnerabilities:漏洞</span><br><span class="line">imperceptible:不可察觉的</span><br><span class="line">drastically:剧烈地</span><br><span class="line">via:通过</span><br><span class="line">remarkably:显着地</span><br><span class="line">intuitive:直觉的</span><br><span class="line">tradeoff:交易</span><br><span class="line">perturbations:摄动,扰动</span><br><span class="line">adversarial:对抗的</span><br><span class="line">However, in general the overarching goal is to add the least amount of perturbation to the input data to cause the desired misclassification.:一般来说，总体目标是向输入数据添加最少的扰动量，从而导致所需的错误分类</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-autograd</title>
    <url>/2020/07/23/Pytorch-Learning-autograd/</url>
    <content><![CDATA[<p>Pytorch-Learning-autograd:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="Autograd-Automatic-Differentiation"><a href="#Autograd-Automatic-Differentiation" class="headerlink" title="Autograd: Automatic Differentiation"></a>Autograd: Automatic Differentiation</h1><p>Central to all neural networks in PyTorch is the <code>autograd</code> package.<br>Let’s first briefly visit this, and we will then go to training our<br>first neural network.</p>
<p>The <code>autograd</code> package provides automatic differentiation for all operations<br>on Tensors. It is a define-by-run framework, which means that your backprop is<br>defined by how your code is run, and that every single iteration can be<br>different.</p>
<p>Let us see this in more simple terms with some examples.</p>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><code>torch.Tensor</code> is the central class of the package. If you set its attribute<br><code>.requires_grad</code> as <code>True</code>, it starts to track all operations on it. When<br>you finish your computation you can call <code>.backward()</code> and have all the<br>gradients computed automatically. The gradient for this tensor will be<br>accumulated into <code>.grad</code> attribute.</p>
<p>To stop a tensor from tracking history, you can call <code>.detach()</code> to detach<br>it from the computation history, and to prevent future computation from being<br>tracked.</p>
<p>To prevent tracking history (and using memory), you can also wrap the code block<br>in <code>with torch.no_grad():</code>. This can be particularly helpful when evaluating a<br>model because the model may have trainable parameters with<br><code>requires_grad=True</code>, but for which we don’t need the gradients.</p>
<p>There’s one more class which is very important for autograd<br>implementation - a <code>Function</code>.</p>
<p><code>Tensor</code> and <code>Function</code> are interconnected and build up an acyclic<br>graph, that encodes a complete history of computation. Each tensor has<br>a <code>.grad_fn</code> attribute that references a <code>Function</code> that has created<br>the <code>Tensor</code> (except for Tensors created by the user - their<br><code>grad_fn is None</code>).</p>
<p>If you want to compute the derivatives, you can call <code>.backward()</code> on<br>a <code>Tensor</code>. If <code>Tensor</code> is a scalar (i.e. it holds a one element<br>data), you don’t need to specify any arguments to <code>backward()</code>,<br>however if it has more elements, you need to specify a <code>gradient</code><br>argument that is a tensor of matching shape.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>Create a tensor and set <code>requires_grad=True</code> to track computation with it</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
</code></pre><p>Do a tensor operation:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
</code></pre><p><code>y</code> was created as a result of an operation, so it has a <code>grad_fn</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;AddBackward0 object at 0x0000015370CAB438&gt;
</code></pre><p>Do more operations on <code>y</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)
</code></pre><p><code>.requires_grad_( ... )</code> changes an existing Tensor’s <code>requires_grad</code><br>flag in-place. The input flag defaults to <code>False</code> if not given.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>False
True
&lt;SumBackward0 object at 0x000001536D5A24E0&gt;
</code></pre><h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>Let’s backprop now.<br>Because <code>out</code> contains a single scalar, <code>out.backward()</code> is<br>equivalent to <code>out.backward(torch.tensor(1.))</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>
<p>Print gradients d(out)/dx</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
</code></pre><p>You should have got a matrix of <code>4.5</code>. Let’s call the <code>out</code><br><em>Tensor</em> “$o$”.<br>We have that $o = \frac{1}{4}\sum_i z_i$,<br>$z_i = 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$.<br>Therefore,<br>$\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)$, hence<br>$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$.</p>
<p>Mathematically, if you have a vector valued function $\vec{y}=f(\vec{x})$,<br>then the gradient of $\vec{y}$ with respect to $\vec{x}$<br>is a Jacobian matrix:</p>
<p>\begin{align}J=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\<br>   \vdots &amp; \ddots &amp; \vdots\\<br>   \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p>
<p>Generally speaking, <code>torch.autograd</code> is an engine for computing<br>vector-Jacobian product. That is, given any vector<br>$v=\left(\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m}\end{array}\right)^{T}$,<br>compute the product $v^{T}\cdot J$. If $v$ happens to be<br>the gradient of a scalar function $l=g\left(\vec{y}\right)$,<br>that is,<br>$v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$,<br>then by the chain rule, the vector-Jacobian product would be the<br>gradient of $l$ with respect to $\vec{x}$:</p>
<p>\begin{align}J^{T}\cdot v=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}\\<br>   \vdots &amp; \ddots &amp; \vdots\\<br>   \frac{\partial y_{1}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\left(\begin{array}{c}<br>   \frac{\partial l}{\partial y_{1}}\\<br>   \vdots\\<br>   \frac{\partial l}{\partial y_{m}}<br>   \end{array}\right)=\left(\begin{array}{c}<br>   \frac{\partial l}{\partial x_{1}}\\<br>   \vdots\\<br>   \frac{\partial l}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p>
<p>(Note that $v^{T}\cdot J$ gives a row vector which can be<br>treated as a column vector by taking $J^{T}\cdot v$.)</p>
<p>This characteristic of vector-Jacobian product makes it very<br>convenient to feed external gradients into a model that has<br>non-scalar output.</p>
<p>Now let’s take a look at an example of vector-Jacobian product:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([849.9269, 245.4694, 633.8903], grad_fn=&lt;MulBackward0&gt;)
</code></pre><p>Now in this case <code>y</code> is no longer a scalar. <code>torch.autograd</code><br>could not compute the full Jacobian directly, but if we just<br>want the vector-Jacobian product, simply pass the vector to<br><code>backward</code> as argument:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])
</code></pre><p>You can also stop autograd from tracking history on Tensors<br>with <code>.requires_grad=True</code> either by wrapping the code block in<br><code>with torch.no_grad():</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">	print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>
<pre><code>True
True
False
</code></pre><p>Or by using <code>.detach()</code> to get a new Tensor with the same<br>content but that does not require gradients:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">y = x.detach()</span><br><span class="line">print(y.requires_grad)</span><br><span class="line">print(x.eq(y).all())</span><br></pre></td></tr></table></figure>
<pre><code>True
False
tensor(1, dtype=torch.uint8)
</code></pre><p><strong>Read Later:</strong></p>
<p>Document about <code>autograd.Function</code> is at<br><a href="https://pytorch.org/docs/stable/autograd.html#function" target="_blank" rel="noopener">https://pytorch.org/docs/stable/autograd.html#function</a></p>
<h2 id="不认识的单词"><a href="#不认识的单词" class="headerlink" title="不认识的单词"></a>不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Automatic:自动的</span><br><span class="line">briefly:短暂的</span><br><span class="line">track:跟踪</span><br><span class="line">specify:指明</span><br><span class="line">accumulate:累加</span><br><span class="line">particularly:尤其,格外的</span><br><span class="line">interconnect:互相连接</span><br><span class="line">acyclic:非循环的,无环的</span><br><span class="line">Mathematically:数学上</span><br><span class="line">Jacobian:雅可比</span><br><span class="line">scalar:标量</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-cifar10tutorial-visualizing</title>
    <url>/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/</url>
    <content><![CDATA[<p>Pytorch-Learning-cifar10tutorial-visualizing<br><a id="more"></a></p>
<div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div>
* 1.[Learning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/)
    * 1.1[Pytorch-Learning-tensor](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/)
    * 1.2[Pytorch-Learning-autograd](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/)
    * 1.3[Pytorch-Learning-neural_newworks](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/)
    * 1.4[Pytorch-Learning-examples](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/)
    * 1.5[Pytorch-Learning-torch.nn](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/)
    * 1.6[Pytorch-Learning-cifar10tutorial-visualizing](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/)
* 2.[Image](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/)
    * 2.1[微调TorchVision对象检测](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/)
    * 2.2[计算机视觉迁移学习](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/)
    * 2.3[对抗样本生成](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/
    * 2.4[DCGAN教程](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/)
* 3.[Audio](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/)
    * 3.1[torchaudio](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/)
* 4.[Text](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/)
    * 4.1[用NN.TRANFORMER和TORCHTEXT进行序列到序列建模](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/)
    * 4.2[使用字符级RNN对名称进行分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/)
    * 4.3[用字符级RNN生成名称](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/)
    * 4.4[使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/)
    * 4.5[TORCHTEXT的文本分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)
    * 4.6[TORCHTEXT的语言翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/)
* 5.[ReinforcementLearning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/)
* 5.1[Reinforcement-Learning](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/)


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


Training a Classifier
=====================

This is it. You have seen how to define neural networks, compute loss and make
updates to the weights of the network.

Now you might be thinking,

What about data?
----------------

Generally, when you have to deal with image, text, audio or video data,
you can use standard python packages that load data into a numpy array.
Then you can convert this array into a ``torch.*Tensor``.

-  For images, packages such as Pillow, OpenCV are useful
-  For audio, packages such as scipy and librosa
-  For text, either raw Python or Cython based loading, or NLTK and
   SpaCy are useful

Specifically for vision, we have created a package called
``torchvision``, that has data loaders for common datasets such as
Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,
``torchvision.datasets`` and ``torch.utils.data.DataLoader``.

This provides a huge convenience and avoids writing boilerplate code.

For this tutorial, we will use the CIFAR10 dataset.
It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,
‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of
size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.

![](https://pytorch.org/tutorials/_images/cifar10.png)

   cifar10


Training an image classifier
----------------------------

We will do the following steps in order:

1. Load and normalizing the CIFAR10 training and test datasets using
   ``torchvision``
2. Define a Convolutional Neural Network
3. Define a loss function
4. Train the network on the training data
5. Test the network on the test data

1. Loading and normalizing CIFAR10

----------------------------

Using ``torchvision``, it’s extremely easy to load CIFAR10.




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>

The output of torchvision datasets are PILImage images of range [0, 1].
We transform them to Tensors of normalized range [-1, 1].
<div class="alert alert-info"><h4>Note</h4><p>If running on Windows and you get a BrokenPipeError, try setting
    the num_worker of torch.utils.data.DataLoader() to 0.</p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre><p>Let us show some of the training images, for fun.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_6_0.png" alt="png"></p>
<pre><code>horse   cat  deer   cat
</code></pre><h2 id="2-Define-a-Convolutional-Neural-Network"><a href="#2-Define-a-Convolutional-Neural-Network" class="headerlink" title="2. Define a Convolutional Neural Network"></a>2. Define a Convolutional Neural Network</h2><p>Copy the neural network from the Neural Networks section before and modify it to<br>take 3-channel images (instead of 1-channel images as it was defined).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<h2 id="3-Define-a-Loss-function-and-optimizer"><a href="#3-Define-a-Loss-function-and-optimizer" class="headerlink" title="3. Define a Loss function and optimizer"></a>3. Define a Loss function and optimizer</h2><p>Let’s use a Classification Cross-Entropy loss and SGD with momentum.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h2 id="4-Train-the-network"><a href="#4-Train-the-network" class="headerlink" title="4. Train the network"></a>4. Train the network</h2><p>This is when things start to get interesting.<br>We simply have to loop over our data iterator, and feed the inputs to the<br>network and optimize.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[1,  2000] loss: 2.173
[1,  4000] loss: 1.818
[1,  6000] loss: 1.647
[1,  8000] loss: 1.545
[1, 10000] loss: 1.490
[1, 12000] loss: 1.436
[2,  2000] loss: 1.384
[2,  4000] loss: 1.348
[2,  6000] loss: 1.341
[2,  8000] loss: 1.306
[2, 10000] loss: 1.292
[2, 12000] loss: 1.283
Finished Training
</code></pre><p>Let’s quickly save our trained model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">'./cifar_net.pth'</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>
<p>See <code>here &lt;https://pytorch.org/docs/stable/notes/serialization.html&gt;</code>_<br>for more details on saving PyTorch models.</p>
<h2 id="5-Test-the-network-on-the-test-data"><a href="#5-Test-the-network-on-the-test-data" class="headerlink" title="5. Test the network on the test data"></a>5. Test the network on the test data</h2><p>We have trained the network for 2 passes over the training dataset.<br>But we need to check if the network has learnt anything at all.</p>
<p>We will check this by predicting the class label that the neural network<br>outputs, and checking it against the ground-truth. If the prediction is<br>correct, we add the sample to the list of correct predictions.</p>
<p>Okay, first step. Let us display an image from the test set to get familiar.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_16_0.png" alt="png"></p>
<pre><code>GroundTruth:    cat  ship  ship plane
</code></pre><p>Next, let’s load back in our saved model (note: saving and re-loading the model<br>wasn’t necessary here, we only did it to illustrate how to do so):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<pre><code>IncompatibleKeys(missing_keys=[], unexpected_keys=[])
</code></pre><p>Okay, now let us see what the neural network thinks these examples above are:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure>
<p>The outputs are energies for the 10 classes.<br>The higher the energy for a class, the more the network<br>thinks that the image is of the particular class.<br>So, let’s get the index of the highest energy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>Predicted:    cat plane plane plane
</code></pre><p>The results seem pretty good.</p>
<p>Let us look at how the network performs on the whole dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of the network on the 10000 test images: 53 %
</code></pre><p>That looks way better than chance, which is 10% accuracy (randomly picking<br>a class out of 10 classes).<br>Seems like the network learnt something.</p>
<p>Hmmm, what are the classes that performed well, and the classes that did<br>not perform well:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of plane : 71 %
Accuracy of   car : 57 %
Accuracy of  bird : 26 %
Accuracy of   cat : 32 %
Accuracy of  deer : 52 %
Accuracy of   dog : 40 %
Accuracy of  frog : 72 %
Accuracy of horse : 74 %
Accuracy of  ship : 57 %
Accuracy of truck : 53 %
</code></pre><p>Okay, so what next?</p>
<p>How do we run these neural networks on the GPU?</p>
<h2 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h2><p>Just like how you transfer a Tensor onto the GPU, you transfer the neural<br>net onto the GPU.</p>
<p>Let’s first define our device as the first visible cuda device if we have<br>CUDA available:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>
<pre><code>cpu
</code></pre><p>The rest of this section assumes that <code>device</code> is a CUDA device.</p>
<p>Then these methods will recursively go over all modules and convert their<br>parameters and buffers to CUDA tensors:</p>
<p>.. code:: python</p>
<pre><code>net.to(device)
</code></pre><p>Remember that you will have to send the inputs and targets at every step<br>to the GPU too:</p>
<p>.. code:: python</p>
<pre><code>    inputs, labels = data[0].to(device), data[1].to(device)
</code></pre><p>Why dont I notice MASSIVE speedup compared to CPU? Because your network<br>is really small.</p>
<p><strong>Exercise:</strong> Try increasing the width of your network (argument 2 of<br>the first <code>nn.Conv2d</code>, and argument 1 of the second <code>nn.Conv2d</code> –<br>they need to be the same number), see what kind of speedup you get.</p>
<p><strong>Goals achieved</strong>:</p>
<ul>
<li>Understanding PyTorch’s Tensor library and neural networks at a high level.</li>
<li>Train a small neural network to classify images</li>
</ul>
<h2 id="Training-on-multiple-GPUs"><a href="#Training-on-multiple-GPUs" class="headerlink" title="Training on multiple GPUs"></a>Training on multiple GPUs</h2><p>If you want to see even more MASSIVE speedup using all of your GPUs,<br>please check out :doc:<code>data_parallel_tutorial</code>.</p>
<h2 id="Where-do-I-go-next"><a href="#Where-do-I-go-next" class="headerlink" title="Where do I go next?"></a>Where do I go next?</h2><ul>
<li>:doc:<code>Train neural nets to play video games &lt;/intermediate/reinforcement_q_learning&gt;</code></li>
<li><code>Train a state-of-the-art ResNet network on imagenet</code>_</li>
<li><code>Train a face generator using Generative Adversarial Networks</code>_</li>
<li><code>Train a word-level language model using Recurrent LSTM networks</code>_</li>
<li><code>More examples</code>_</li>
<li><code>More tutorials</code>_</li>
<li><code>Discuss PyTorch on the Forums</code>_</li>
<li><code>Chat with other users on Slack</code>_</li>
</ul>
<h2 id="VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD"><a href="#VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD" class="headerlink" title="VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD"></a>VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD</h2><p>In the 60 Minute Blitz, we show you how to load in data, feed it through a model we define as a subclass of nn.Module, train this model on training data, and test it on test data. To see what’s happening, we print out some statistics as the model is training to get a sense for whether training is progressing. However, we can do much better than that: PyTorch integrates with TensorBoard, a tool designed for visualizing the results of neural network training runs. This tutorial illustrates some of its functionality, using the Fashion-MNIST dataset which can be read into PyTorch using torchvision.datasets.</p>
<p>In this tutorial, we’ll learn how to:</p>
<ul>
<li>Read in data and with appropriate transforms (nearly identical to the prior tutorial).</li>
<li>Set up TensorBoard.</li>
<li>Write to TensorBoard.</li>
<li>Inspect a model architecture using TensorBoard.</li>
</ul>
<p>Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code<br>Specifically, on point #5, we’ll see:</p>
<ul>
<li>A couple of ways to inspect our training data</li>
<li>How to track our model’s performance as it trains</li>
<li>How to assess our model’s performance once it is trained.</li>
<li>We’ll begin with similar boilerplate code as in the CIFAR-10 tutorial:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># transforms</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># datasets</span></span><br><span class="line">trainset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line">testset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataloaders</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># constant for classes</span></span><br><span class="line">classes = (<span class="string">'T-shirt/top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,</span><br><span class="line">        <span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle Boot'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function to show an image</span></span><br><span class="line"><span class="comment"># (used in the `plot_classes_preds` function below)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matplotlib_imshow</span><span class="params">(img, one_channel=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        img = img.mean(dim=<span class="number">0</span>)</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        plt.imshow(npimg, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\train-images-idx3-ubyte.gz


100.0%

Extracting ./data\FashionMNIST\raw\train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gz


111.0%

Extracting ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz


100.0%

Extracting ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz


159.1%

Extracting ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz
Processing...
Done!
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. TensorBoard setup</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">'runs/fashion_mnist_experiment_1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Writing to TensorBoard</span></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">matplotlib_imshow(img_grid, one_channel=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># write to tensorboard</span></span><br><span class="line">writer.add_image(<span class="string">'four_fashion_mnist_images'</span>, img_grid)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Now running: tensorboard --logdir=runs</span></span><br><span class="line"><span class="comment"># from the command line and then navigating to https://localhost:6006</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_32_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3. Inspect the model using TensorBoard</span></span><br><span class="line">writer.add_graph(net, images)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4. Adding a “Projector” to TensorBoard</span></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_n_random</span><span class="params">(data, labels, n=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Selects n random datapoints and their corresponding labels from a dataset</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> len(data) == len(labels)</span><br><span class="line"></span><br><span class="line">    perm = torch.randperm(len(data))</span><br><span class="line">    <span class="keyword">return</span> data[perm][:n], labels[perm][:n]</span><br><span class="line"></span><br><span class="line"><span class="comment"># select random images and their target indices</span></span><br><span class="line">images, labels = select_n_random(trainset.data, trainset.targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the class labels for each image</span></span><br><span class="line">class_labels = [classes[lab] <span class="keyword">for</span> lab <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="comment"># log embeddings</span></span><br><span class="line">features = images.view(<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>)</span><br><span class="line">writer.add_embedding(features,</span><br><span class="line">                    metadata=class_labels,</span><br><span class="line">                    label_img=images.unsqueeze(<span class="number">1</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5. Tracking model training with TensorBoard</span></span><br><span class="line"><span class="comment"># helper functions</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">images_to_probs</span><span class="params">(net, images)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates predictions and corresponding probabilities from a trained</span></span><br><span class="line"><span class="string">    network and a list of images</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    output = net(images)</span><br><span class="line">    <span class="comment"># convert output probabilities to predicted class</span></span><br><span class="line">    _, preds_tensor = torch.max(output, <span class="number">1</span>)</span><br><span class="line">    preds = np.squeeze(preds_tensor.numpy())</span><br><span class="line">    <span class="keyword">return</span> preds, [F.softmax(el, dim=<span class="number">0</span>)[i].item() <span class="keyword">for</span> i, el <span class="keyword">in</span> zip(preds, output)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_classes_preds</span><span class="params">(net, images, labels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates matplotlib Figure using a trained network, along with images</span></span><br><span class="line"><span class="string">    and labels from a batch, that shows the network's top prediction along</span></span><br><span class="line"><span class="string">    with its probability, alongside the actual label, coloring this</span></span><br><span class="line"><span class="string">    information based on whether the prediction was correct or not.</span></span><br><span class="line"><span class="string">    Uses the "images_to_probs" function.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    preds, probs = images_to_probs(net, images)</span><br><span class="line">    <span class="comment"># plot the images in the batch, along with predicted and true labels</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">48</span>))</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> np.arange(<span class="number">4</span>):</span><br><span class="line">        ax = fig.add_subplot(<span class="number">1</span>, <span class="number">4</span>, idx+<span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        matplotlib_imshow(images[idx], one_channel=<span class="literal">True</span>)</span><br><span class="line">        ax.set_title(<span class="string">"&#123;0&#125;, &#123;1:.1f&#125;%\n(label: &#123;2&#125;)"</span>.format(</span><br><span class="line">            classes[preds[idx]],</span><br><span class="line">            probs[idx] * <span class="number">100.0</span>,</span><br><span class="line">            classes[labels[idx]]),</span><br><span class="line">                    color=(<span class="string">"green"</span> <span class="keyword">if</span> preds[idx]==labels[idx].item() <span class="keyword">else</span> <span class="string">"red"</span>))</span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line">running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">999</span>:    <span class="comment"># every 1000 mini-batches...</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log the running loss</span></span><br><span class="line">            writer.add_scalar(<span class="string">'training loss'</span>,</span><br><span class="line">                            running_loss / <span class="number">1000</span>,</span><br><span class="line">                            epoch * len(trainloader) + i)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log a Matplotlib Figure showing the model's predictions on a</span></span><br><span class="line">            <span class="comment"># random mini-batch</span></span><br><span class="line">            writer.add_figure(<span class="string">'predictions vs. actuals'</span>,</span><br><span class="line">                            plot_classes_preds(net, inputs, labels),</span><br><span class="line">                            global_step=epoch * len(trainloader) + i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Finished Training
</code></pre><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_1.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_2.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_3.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_4.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_5.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_6.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_7.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_8.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_9.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_10.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_11.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_12.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_13.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_14.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_15.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 6. Assessing trained models with TensorBoard</span></span><br><span class="line"><span class="comment"># 1. gets the probability predictions in a test_size x num_classes Tensor</span></span><br><span class="line"><span class="comment"># 2. gets the preds in a test_size Tensor</span></span><br><span class="line"><span class="comment"># takes ~10 seconds to run</span></span><br><span class="line">class_probs = []</span><br><span class="line">class_preds = []</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        output = net(images)</span><br><span class="line">        class_probs_batch = [F.softmax(el, dim=<span class="number">0</span>) <span class="keyword">for</span> el <span class="keyword">in</span> output]</span><br><span class="line">        _, class_preds_batch = torch.max(output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        class_probs.append(class_probs_batch)</span><br><span class="line">        class_preds.append(class_preds_batch)</span><br><span class="line"></span><br><span class="line">test_probs = torch.cat([torch.stack(batch) <span class="keyword">for</span> batch <span class="keyword">in</span> class_probs])</span><br><span class="line">test_preds = torch.cat(class_preds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_pr_curve_tensorboard</span><span class="params">(class_index, test_probs, test_preds, global_step=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Takes in a "class_index" from 0 to 9 and plots the corresponding</span></span><br><span class="line"><span class="string">    precision-recall curve</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tensorboard_preds = test_preds == class_index</span><br><span class="line">    tensorboard_probs = test_probs[:, class_index]</span><br><span class="line"></span><br><span class="line">    writer.add_pr_curve(classes[class_index],</span><br><span class="line">                        tensorboard_preds,</span><br><span class="line">                        tensorboard_probs,</span><br><span class="line">                        global_step=global_step)</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot all the pr curves</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">    add_pr_curve_tensorboard(i, test_probs, test_preds)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-examples</title>
    <url>/2020/07/23/Pytorch-Learning-examples/</url>
    <content><![CDATA[<p>Pytorch-Learning-examples<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<ul>
<li>1.<a href="#header1">warm-up:numpy</a></li>
<li>2.<a href="#header2">pytorch:tensors</a></li>
<li>3.<a href="#header3">pytorch:tensors and autograd</a></li>
<li>4.<a href="#header4">pytorch:defining new autograd functions</a></li>
<li>5.<a href="#header5">tensorflow1.x:static graphs</a></li>
<li>6.<a href="#header6">pytorch:nn</a></li>
<li>7.<a href="#header7">pytorch:optim</a></li>
<li>8.<a href="#header8">pytorch:custom nn modules</a></li>
<li>9.<a href="#header9">pytorch:control flow+weight sharing</a></li>
</ul>
<h1 id="warm-up-numpy"><a href="#warm-up-numpy" class="headerlink" title="warm-up:numpy"></a><span id="header1">warm-up:numpy</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="Warm-up-numpy"><a href="#Warm-up-numpy" class="headerlink" title="Warm-up: numpy"></a>Warm-up: numpy</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x using Euclidean error.</p>
<p>This implementation uses numpy to manually compute the forward pass, loss, and<br>backward pass.</p>
<p>A numpy array is a generic n-dimensional array; it does not know anything about<br>deep learning or gradients or computational graphs, and is just a way to perform<br>generic numeric computations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>
<pre><code>0 38502658.12178467
1 39021030.505150035
2 43783005.35946928
3 43513345.95754772
4 33130647.574302156
5 18494184.582708906
6 8309147.312092974
7 3709728.9004302337
8 1973222.0064019447
9 1292160.3020421679
10 969207.6696523366
11 777313.2166244711
12 642885.4458057204
13 539862.0964943856
14 457706.36910393136
15 390776.99980368273
16 335541.3660443882
17 289534.9698406697
18 250999.0891175047
19 218440.8922160021
20 190801.03505385082
21 167229.41111619392
22 147030.14995170798
23 129644.55669752343
24 114634.85573944401
25 101654.50910880938
26 90379.66910158034
27 80547.50420605141
28 71942.30234259031
29 64389.53897462465
30 57755.011608667795
31 51903.61478445082
32 46747.37821866218
33 42180.36670548356
34 38120.10143244453
35 34504.8367862085
36 31283.625825762603
37 28409.554826229883
38 25832.84410659271
39 23520.944901953713
40 21443.92872486158
41 19573.49619579054
42 17887.52961185431
43 16364.901917158142
44 14986.999074876061
45 13739.21782497934
46 12607.392910105698
47 11579.322653387639
48 10644.061828317645
49 9792.956168204091
50 9016.956906200578
51 8309.619387713636
52 7664.013985165039
53 7073.449800791166
54 6532.833920817042
55 6037.538464055722
56 5583.274822316345
57 5166.3150372957025
58 4783.438966887356
59 4431.265118218456
60 4107.306250671735
61 3809.173244284424
62 3534.3065526324035
63 3281.382487196609
64 3048.2720327779725
65 2832.9613638905826
66 2634.0552486543734
67 2450.1208680788013
68 2280.0038612106828
69 2122.5020348125718
70 1976.6140763769765
71 1841.480282193922
72 1716.1436144355846
73 1599.8498210515127
74 1491.925103606006
75 1391.74788163408
76 1298.700821661702
77 1212.212450782299
78 1131.7942696970626
79 1057.0430043302988
80 987.4925199287406
81 922.7460298895005
82 862.4637455820518
83 806.335290707932
84 754.0347890268129
85 705.3032590197868
86 659.8640583682109
87 617.4715043492135
88 577.9369320777199
89 541.0382711590596
90 506.62139688224494
91 474.46146460235906
92 444.4289850228761
93 416.3831714509241
94 390.1781262414818
95 365.68416156867966
96 342.7828243058609
97 321.37451033250693
98 301.34344345528007
99 282.6049616675032
100 265.0769405906856
101 248.66706624922423
102 233.31182982470165
103 218.93579583019107
104 205.47399586723316
105 192.8673639685237
106 181.05784775853311
107 169.9914161272534
108 159.62205201703807
109 149.90414548125696
110 140.7986893389537
111 132.2586907147042
112 124.25279696970381
113 116.74222194734885
114 109.70210950592357
115 103.09497001058955
116 96.89525543441061
117 91.07884065063246
118 85.6198765958778
119 80.49732520749234
120 75.6882961733577
121 71.1722154368565
122 66.93235677110607
123 62.95241596773887
124 59.21332801738676
125 55.70220136122839
126 52.40276978766286
127 49.30324491666052
128 46.39219465065453
129 43.6561631420701
130 41.08445097890545
131 38.66705716574056
132 36.39569090451589
133 34.2596896820596
134 32.25129366141559
135 30.36277332849072
136 28.58840749936592
137 26.91886404849481
138 25.348295684128104
139 23.871887568407733
140 22.482392230639924
141 21.1752612429013
142 19.94540814626695
143 18.78848359075504
144 17.699455611505552
145 16.674867891142245
146 15.710568962031482
147 14.802825886952098
148 13.94842074029503
149 13.144227723348116
150 12.387131550628439
151 11.674026481428772
152 11.002689146006661
153 10.370609949309603
154 9.775509369377593
155 9.215072226656517
156 8.687075426828947
157 8.189767782525552
158 7.7214576865624025
159 7.280233710481657
160 6.864642688917494
161 6.473008103216368
162 6.104105173833666
163 5.7564916113754085
164 5.428879681554832
165 5.120147993533515
166 4.829259142691539
167 4.555145740046442
168 4.29672213329475
169 4.053143156190888
170 3.8235214202376655
171 3.607138244094545
172 3.4031196927940224
173 3.210757574413539
174 3.0293765387034064
175 2.858454621478234
176 2.697234384156207
177 2.545177266862872
178 2.4017939117722404
179 2.2665997813870664
180 2.1391195167593566
181 2.0188693271562754
182 1.9054496369340477
183 1.7984763417415828
184 1.6975705115269373
185 1.6023729047397333
186 1.512573512380922
187 1.4278650471502181
188 1.347969970837449
189 1.272567002922919
190 1.2014204465432794
191 1.1342855863955332
192 1.0709568915888343
193 1.0111820179227746
194 0.9547746744541861
195 0.9015532749111042
196 0.8513302278674784
197 0.803922014568537
198 0.7591740066091374
199 0.7169375303267338
200 0.6770877114086349
201 0.6394607979618921
202 0.6039480138637082
203 0.5704178896870162
204 0.5387711941311117
205 0.5088920505641997
206 0.4806800135074349
207 0.45404935956164216
208 0.4289117734184146
209 0.40517486327826213
210 0.38275697069924214
211 0.36158944936278636
212 0.3416070794747825
213 0.3227344916114124
214 0.3049141167911339
215 0.28808300182326085
216 0.27219060149023244
217 0.2571773488343734
218 0.24299758077475503
219 0.22960605968706874
220 0.21696181922245522
221 0.20501866875786806
222 0.19373590402638502
223 0.1830764093023291
224 0.17300985254584955
225 0.1635004063397864
226 0.15451596252804456
227 0.14602859536330753
228 0.13801270444499963
229 0.1304395543791902
230 0.12328307118542486
231 0.1165215280650118
232 0.11013498048221826
233 0.10409915585381448
234 0.09839683048227854
235 0.09300849009893092
236 0.08791893479291592
237 0.08310796594355277
238 0.07856156604359649
239 0.07426508307950577
240 0.07020565731703925
241 0.06636946700613897
242 0.0627437660794708
243 0.059317781060088316
244 0.05608034093448906
245 0.05301955241121613
246 0.050126674069563906
247 0.04739256213401111
248 0.04480928906259292
249 0.04236691748155608
250 0.04005879599601969
251 0.03787661761938381
252 0.03581475257492655
253 0.033864928740949526
254 0.032021804201541854
255 0.030279858100986354
256 0.02863334928312954
257 0.02707658789186648
258 0.025604569332906155
259 0.024213249395320283
260 0.022897970661672578
261 0.02165425187746097
262 0.020478737432682297
263 0.01936719332160696
264 0.018316430865928372
265 0.017322619307502316
266 0.016383050617920083
267 0.015494734481326376
268 0.014655032299327592
269 0.013860819190983916
270 0.013109809002567857
271 0.0123997755042141
272 0.011728358290906498
273 0.011093355906296416
274 0.01049298268330884
275 0.009925290535664889
276 0.009388360895355642
277 0.008880549450207519
278 0.008400420664047465
279 0.007946325800609208
280 0.00751686776061394
281 0.007110688095051999
282 0.006726592064373132
283 0.00636331544306615
284 0.006019761179348237
285 0.005694800059913335
286 0.005387424438387055
287 0.005096730276146309
288 0.004821747870498604
289 0.004561741644547909
290 0.004315728687198646
291 0.004083088241466739
292 0.0038630342216639736
293 0.0036548280743543514
294 0.003457908894412565
295 0.0032716544897889873
296 0.003095480430783593
297 0.0029287903501173454
298 0.0027711283850350016
299 0.002622002726801867
300 0.0024808885834647024
301 0.0023473968087844807
302 0.00222113284310873
303 0.00210170542574699
304 0.0019887074497667136
305 0.0018817802500325769
306 0.00178065116010868
307 0.0016849552143364992
308 0.0015944091801416442
309 0.001508753706017779
310 0.0014277397746070208
311 0.0013510695821452332
312 0.0012785229777043136
313 0.0012098928983483854
314 0.0011449625641886667
315 0.0010835156717760649
316 0.001025377588941091
317 0.0009703758384736077
318 0.0009183361752611588
319 0.0008690850720838991
320 0.0008224940700348837
321 0.0007784029943592372
322 0.0007366791771033156
323 0.0006971989990605096
324 0.0006598426281655082
325 0.000624495741732001
326 0.0005910439292796761
327 0.0005593973773407745
328 0.0005294403512899198
329 0.0005010976939705757
330 0.00047427317778801455
331 0.0004488877470686361
332 0.0004248669939759654
333 0.0004021385047171281
334 0.0003806279081745162
335 0.0003602664168058468
336 0.00034099755680728995
337 0.0003227653070463328
338 0.00030550707403562225
339 0.0002891754204576711
340 0.000273721587865668
341 0.00025909406293294767
342 0.0002452471530517434
343 0.0002321429824444712
344 0.00021974540966147302
345 0.00020800723976222412
346 0.00019689755859322988
347 0.00018638458674544666
348 0.00017643522452308918
349 0.00016701530053816744
350 0.0001581003279303452
351 0.00014966407110653316
352 0.00014167707307891997
353 0.00013411790417482185
354 0.00012696356477874673
355 0.00012019230146127977
356 0.00011378157083006711
357 0.00010771346551538498
358 0.0001019705019676451
359 9.653404961343065e-05
360 9.138760257508527e-05
361 8.651709721760377e-05
362 8.19070094203869e-05
363 7.754226881202036e-05
364 7.341041687150536e-05
365 6.94995078743375e-05
366 6.579726835351665e-05
367 6.229309750532338e-05
368 5.897611236256165e-05
369 5.583565153701364e-05
370 5.286297313654044e-05
371 5.004834708849745e-05
372 4.738444170826265e-05
373 4.486241750382573e-05
374 4.2474866618345255e-05
375 4.0215019660048885e-05
376 3.807518640145299e-05
377 3.604963907599714e-05
378 3.4131911558009186e-05
379 3.231655671510322e-05
380 3.059770246224207e-05
381 2.8970757618958097e-05
382 2.743057310133526e-05
383 2.5972006538400987e-05
384 2.459140169431031e-05
385 2.3284245013094282e-05
386 2.2046767101248254e-05
387 2.0874974705148337e-05
388 1.976572632623195e-05
389 1.8715764940418173e-05
390 1.7721415862570632e-05
391 1.6779971953419194e-05
392 1.588871637152896e-05
393 1.5044836588187851e-05
394 1.4245828226986497e-05
395 1.3489410778399332e-05
396 1.2773364399028685e-05
397 1.2095139789484804e-05
398 1.145299443496586e-05
399 1.0845077903682161e-05
400 1.0269508212283454e-05
401 9.724527789368986e-06
402 9.208527492374383e-06
403 8.720045436070135e-06
404 8.2573575243078e-06
405 7.819257808270196e-06
406 7.404463436549593e-06
407 7.011821523303816e-06
408 6.639943046336263e-06
409 6.2878326555936834e-06
410 5.954457206733875e-06
411 5.638725815406013e-06
412 5.339791165439266e-06
413 5.056690012109548e-06
414 4.7886913275527965e-06
415 4.5348871524205895e-06
416 4.294550962428522e-06
417 4.0669989344986414e-06
418 3.851507524029607e-06
419 3.6474147842461815e-06
420 3.4541567135768535e-06
421 3.2712066098541254e-06
422 3.0979501309951946e-06
423 2.9338623570734417e-06
424 2.778486859288849e-06
425 2.6313579995304816e-06
426 2.492005877958706e-06
427 2.3600400344687442e-06
428 2.235117848833567e-06
429 2.1168180481419086e-06
430 2.0047497870126444e-06
431 1.8986381787503396e-06
432 1.7981531121369732e-06
433 1.7029874912083225e-06
434 1.6128687459205246e-06
435 1.5275476848984372e-06
436 1.4467329944515942e-06
437 1.3701865914534269e-06
438 1.2977020898043846e-06
439 1.2290629088171652e-06
440 1.164051733130194e-06
441 1.1024831847468408e-06
442 1.0441903271914875e-06
443 9.889751182032595e-07
444 9.366772125412096e-07
445 8.871553290035977e-07
446 8.402520019677647e-07
447 7.958308104042071e-07
448 7.53763518098307e-07
449 7.139244346971557e-07
450 6.761960655207478e-07
451 6.404595540956439e-07
452 6.066124906303541e-07
453 5.74556854302957e-07
454 5.442030621013854e-07
455 5.154497792919255e-07
456 4.882203577154002e-07
457 4.6243074105866146e-07
458 4.380049551012981e-07
459 4.1486836737937236e-07
460 3.929552794591328e-07
461 3.722082406282778e-07
462 3.525512434892848e-07
463 3.339350935587304e-07
464 3.163035075274776e-07
465 2.9960560558970377e-07
466 2.8378833766961356e-07
467 2.688083814724989e-07
468 2.5461985965457267e-07
469 2.4117912639785815e-07
470 2.2845076123088963e-07
471 2.1639404311547854e-07
472 2.049768154586423e-07
473 1.9416035947365062e-07
474 1.8391578287894605e-07
475 1.742122640542393e-07
476 1.6502075323866398e-07
477 1.5631602307949188e-07
478 1.4807025488750153e-07
479 1.4026124873333745e-07
480 1.3286339655833968e-07
481 1.2585529347481789e-07
482 1.1921849306789818e-07
483 1.1293151084301514e-07
484 1.069766166825837e-07
485 1.0133619692710931e-07
486 9.599472852904284e-08
487 9.093341825332011e-08
488 8.613939196599216e-08
489 8.159862515209676e-08
490 7.729751975546753e-08
491 7.322345528572764e-08
492 6.936479915203619e-08
493 6.57099781685289e-08
494 6.224692745752194e-08
495 5.896657665795129e-08
496 5.585950062511546e-08
497 5.291656163586972e-08
498 5.012926872369238e-08
499 4.748859133659835e-08
</code></pre><h1 id="pytorch-tensors"><a href="#pytorch-tensors" class="headerlink" title="pytorch:tensors"></a><span id="header2">pytorch:tensors</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p>
<p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p>
<p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>
<pre><code>99 784.6785888671875
199 5.850834846496582
299 0.07988587021827698
399 0.0017072007758542895
499 0.00015852594515308738
</code></pre><h1 id="pytorch-tensors-and-autograd"><a href="#pytorch-tensors-and-autograd" class="headerlink" title="pytorch:tensors and autograd"></a><span id="header3">pytorch:tensors and autograd</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-Tensors-and-autograd"><a href="#PyTorch-Tensors-and-autograd" class="headerlink" title="PyTorch: Tensors and autograd"></a>PyTorch: Tensors and autograd</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation computes the forward pass using operations on PyTorch<br>Tensors, and uses PyTorch autograd to compute gradients.</p>
<p>A PyTorch Tensor represents a node in a computational graph. If <code>x</code> is a<br>Tensor that has <code>x.requires_grad=True</code> then <code>x.grad</code> is another Tensor<br>holding the gradient of <code>x</code> with respect to some scalar value.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></span><br><span class="line"><span class="comment"># with respect to these Tensors during the backward pass.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Tensors during the backward pass.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Tensors; these</span></span><br><span class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></span><br><span class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></span><br><span class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss using operations on Tensors.</span></span><br><span class="line">    <span class="comment"># Now loss is a Tensor of shape (1,)</span></span><br><span class="line">    <span class="comment"># loss.item() gets the scalar value held in the loss.</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></span><br><span class="line">    <span class="comment"># gradient of loss with respect to all Tensors with requires_grad=True.</span></span><br><span class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Tensors holding the gradient</span></span><br><span class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually update weights using gradient descent. Wrap in torch.no_grad()</span></span><br><span class="line">    <span class="comment"># because weights have requires_grad=True, but we don't need to track this</span></span><br><span class="line">    <span class="comment"># in autograd.</span></span><br><span class="line">    <span class="comment"># An alternative way is to operate on weight.data and weight.grad.data.</span></span><br><span class="line">    <span class="comment"># Recall that tensor.data gives a tensor that shares the storage with</span></span><br><span class="line">    <span class="comment"># tensor, but doesn't track history.</span></span><br><span class="line">    <span class="comment"># You can also use torch.optim.SGD to achieve this.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure>
<pre><code>99 850.6499633789062
199 5.497010231018066
299 0.0542689710855484
399 0.0009686618577688932
499 0.000102342150057666
</code></pre><h1 id="pytorch-defining-new-autograd-functions"><a href="#pytorch-defining-new-autograd-functions" class="headerlink" title="pytorch:defining new autograd functions"></a><span id="header4">pytorch:defining new autograd functions</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-Defining-New-autograd-Functions"><a href="#PyTorch-Defining-New-autograd-Functions" class="headerlink" title="PyTorch: Defining New autograd Functions"></a>PyTorch: Defining New autograd Functions</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation computes the forward pass using operations on PyTorch<br>Variables, and uses PyTorch autograd to compute gradients.</p>
<p>In this implementation we implement our own custom autograd function to perform<br>the ReLU function.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># To apply our Function, we use Function.apply method. We alias this as 'relu'.</span></span><br><span class="line">    relu = MyReLU.apply</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure>
<pre><code>99 173.57586669921875
199 0.16617316007614136
299 0.0004797253059223294
399 3.693650069180876e-05
499 1.2812281056540087e-05
</code></pre><h1 id="pytorch-static-graphs"><a href="#pytorch-static-graphs" class="headerlink" title="pytorch:static graphs"></a><span id="header5">pytorch:static graphs</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-Tensors-1"><a href="#PyTorch-Tensors-1" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p>
<p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p>
<p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>
<pre><code>99 784.6785888671875
199 5.850834846496582
299 0.07988587021827698
399 0.0017072007758542895
499 0.00015852594515308738
</code></pre><h1 id="pytorch-nn"><a href="#pytorch-nn" class="headerlink" title="pytorch:nn"></a><span id="header6">pytorch:nn</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation uses the nn package from PyTorch to build the network.<br>PyTorch autograd makes it easy to define computational graphs and take gradients,<br>but raw autograd can be a bit too low-level for defining complex neural networks;<br>this is where the nn package can help. The nn package defines a set of Modules,<br>which you can think of as a neural network layer that has produces output from<br>input and may have some trainable weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></span><br><span class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></span><br><span class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></span><br><span class="line"><span class="comment"># linear function, and holds internal Tensors for its weight and bias.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></span><br><span class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></span><br><span class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></span><br><span class="line">    <span class="comment"># doing so you pass a Tensor of input data to the Module and it produces</span></span><br><span class="line">    <span class="comment"># a Tensor of output data.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss. We pass Tensors containing the predicted and true</span></span><br><span class="line">    <span class="comment"># values of y, and the loss function returns a Tensor containing the</span></span><br><span class="line">    <span class="comment"># loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></span><br><span class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></span><br><span class="line">    <span class="comment"># in Tensors with requires_grad=True, so this call will compute gradients for</span></span><br><span class="line">    <span class="comment"># all learnable parameters in the model.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Tensor, so</span></span><br><span class="line">    <span class="comment"># we can access its gradients like we did before.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param -= learning_rate * param.grad</span><br></pre></td></tr></table></figure>
<pre><code>99 2.456005573272705
199 0.04037925601005554
299 0.001298694172874093
399 5.4667791118845344e-05
499 2.6507393613428576e-06
</code></pre><h1 id="pytorch-optim"><a href="#pytorch-optim" class="headerlink" title="pytorch:optim"></a><span id="header7">pytorch:optim</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation uses the nn package from PyTorch to build the network.</p>
<p>Rather than manually updating the weights of the model as we have been doing,<br>we use the optim package to define an Optimizer that will update the weights<br>for us. The optim package defines many optimization algorithms that are commonly<br>used for deep learning, including SGD+momentum, RMSProp, Adam, etc.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></span><br><span class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></span><br><span class="line"><span class="comment"># optimization algorithms. The first argument to the Adam constructor tells the</span></span><br><span class="line"><span class="comment"># optimizer which Tensors it should update.</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></span><br><span class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable</span></span><br><span class="line">    <span class="comment"># weights of the model). This is because by default, gradients are</span></span><br><span class="line">    <span class="comment"># accumulated in buffers( i.e, not overwritten) whenever .backward()</span></span><br><span class="line">    <span class="comment"># is called. Checkout docs of torch.autograd.backward for more details.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<pre><code>99 40.16399383544922
199 0.3977137506008148
299 0.001604456570930779
399 1.6438591046608053e-05
499 8.815557350771996e-08
</code></pre><h1 id="pytorch-custom-nn-modules"><a href="#pytorch-custom-nn-modules" class="headerlink" title="pytorch:custom nn modules"></a><span id="header8">pytorch:custom nn modules</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch: Custom nn Modules"></a>PyTorch: Custom nn Modules</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation defines the model as a custom Module subclass. Whenever you<br>want a model more complex than a simple sequence of existing Modules you will<br>need to define your model this way.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as</span></span><br><span class="line"><span class="string">        member variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward function we accept a Tensor of input data and we must return</span></span><br><span class="line"><span class="string">        a Tensor of output data. We can use Modules defined in the constructor as</span></span><br><span class="line"><span class="string">        well as arbitrary operators on Tensors.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.linear2(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></span><br><span class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></span><br><span class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h1 id="pytorch-control-flow-weight-sharing"><a href="#pytorch-control-flow-weight-sharing" class="headerlink" title="pytorch:control flow+weight sharing"></a><span id="header9">pytorch:control flow+weight sharing</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch: Control Flow + Weight Sharing"></a>PyTorch: Control Flow + Weight Sharing</h2><p>To showcase the power of PyTorch dynamic graphs, we will implement a very strange<br>model: a fully-connected ReLU network that on each forward pass randomly chooses<br>a number between 1 and 4 and has that many hidden layers, reusing the same<br>weights multiple times to compute the innermost hidden layers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we construct three nn.Linear instances that we will use</span></span><br><span class="line"><span class="string">        in the forward pass.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(DynamicNet, self).__init__()</span><br><span class="line">        self.input_linear = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.middle_linear = torch.nn.Linear(H, H)</span><br><span class="line">        self.output_linear = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</span></span><br><span class="line"><span class="string">        and reuse the middle_linear Module that many times to compute hidden layer</span></span><br><span class="line"><span class="string">        representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Since each forward pass builds a dynamic computation graph, we can use normal</span></span><br><span class="line"><span class="string">        Python control-flow operators like loops or conditional statements when</span></span><br><span class="line"><span class="string">        defining the forward pass of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Here we also see that it is perfectly safe to reuse the same Module many</span></span><br><span class="line"><span class="string">        times when defining a computational graph. This is a big improvement from Lua</span></span><br><span class="line"><span class="string">        Torch, where each Module could be used only once.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</span><br><span class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.output_linear(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = DynamicNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></span><br><span class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<pre><code>99 18.901199340820312
199 7.5054731369018555
299 1.1003623008728027
399 0.8731748461723328
499 2.003668785095215
</code></pre>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-neural_newworks</title>
    <url>/2020/07/23/Pytorch-Learning-neural-newworks/</url>
    <content><![CDATA[<p>Pytorch-Learning-autograd:<br><a id="more"></a></p>
<div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div>
* 1.[Learning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/)
    * 1.1[Pytorch-Learning-tensor](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/)
    * 1.2[Pytorch-Learning-autograd](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/)
    * 1.3[Pytorch-Learning-neural_newworks](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/)
    * 1.4[Pytorch-Learning-examples](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/)
    * 1.5[Pytorch-Learning-torch.nn](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/)
    * 1.6[Pytorch-Learning-cifar10tutorial-visualizing](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/)
* 2.[Image](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/)
    * 2.1[微调TorchVision对象检测](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/)
    * 2.2[计算机视觉迁移学习](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/)
    * 2.3[对抗样本生成](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/
    * 2.4[DCGAN教程](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/)
* 3.[Audio](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/)
    * 3.1[torchaudio](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/)
* 4.[Text](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/)
    * 4.1[用NN.TRANFORMER和TORCHTEXT进行序列到序列建模](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/)
    * 4.2[使用字符级RNN对名称进行分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/)
    * 4.3[用字符级RNN生成名称](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/)
    * 4.4[使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/)
    * 4.5[TORCHTEXT的文本分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)
    * 4.6[TORCHTEXT的语言翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/)
* 5.[ReinforcementLearning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/)
* 5.1[Reinforcement-Learning](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/)


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


Neural Networks
===============

Neural networks can be constructed using the ``torch.nn`` package.

Now that you had a glimpse of ``autograd``, ``nn`` depends on
``autograd`` to define models and differentiate them.
An ``nn.Module`` contains layers, and a method ``forward(input)`` that
returns the ``output``.

For example, look at this network that classifies digit images:

![](https://pytorch.org/tutorials/_images/mnist.png)

   convnet

It is a simple feed-forward network. It takes the input, feeds it
through several layers one after the other, and then finally gives the
output.

A typical training procedure for a neural network is as follows:

- Define the neural network that has some learnable parameters (or
  weights)
- Iterate over a dataset of inputs
- Process input through the network
- Compute the loss (how far is the output from being correct)
- Propagate gradients back into the network’s parameters
- Update the weights of the network, typically using a simple update rule:
  ``weight = weight - learning_rate * gradient``

Define the network
------------------

Let’s define this network:




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)  <span class="comment"># 6*6 from image dimension </span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>

    Net(
      (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
      (fc1): Linear(in_features=576, out_features=120, bias=True)
      (fc2): Linear(in_features=120, out_features=84, bias=True)
      (fc3): Linear(in_features=84, out_features=10, bias=True)
    )


You just have to define the ``forward`` function, and the ``backward``
function (where gradients are computed) is automatically defined for you
using ``autograd``.
You can use any of the Tensor operations in the ``forward`` function.

The learnable parameters of a model are returned by ``net.parameters()``




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's .weight</span></span><br></pre></td></tr></table></figure>

    10
    torch.Size([6, 1, 3, 3])


Let's try a random 32x32 input.
Note: expected input size of this net (LeNet) is 32x32. To use this net on
the MNIST dataset, please resize the images from the dataset to 32x32.




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>

    tensor([[ 0.0416,  0.0926, -0.0761, -0.0135, -0.0745, -0.0158,  0.0696, -0.0040,
             -0.0099, -0.1799]], grad_fn=<AddmmBackward>)


Zero the gradient buffers of all parameters and backprops with random
gradients:




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<div class="alert alert-info">
    <h4>Note</h4>

    torch.nn only supports mini-batches. The entire torch.nn
    package only supports inputs that are a mini-batch of samples, and not
    a single sample.

    For example, nn.Conv2d will take in a 4D Tensor of
    nSamples x nChannels x Height x Width.

    If you have a single sample, just use input.unsqueeze(0) to add
    a fake batch dimension.
</div>

<p>Before proceeding further, let’s recap all the classes you’ve seen so far.</p>
<p><strong>Recap:</strong></p>
<ul>
<li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd<br>operations like <code>backward()</code>. Also <em>holds the gradient</em> w.r.t. the<br>tensor.</li>
<li><code>nn.Module</code> - Neural network module. <em>Convenient way of<br>encapsulating parameters</em>, with helpers for moving them to GPU,<br>exporting, loading, etc.</li>
<li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically<br>registered as a parameter when assigned as an attribute to a</em><br><code>Module</code>.</li>
<li><code>autograd.Function</code> - Implements <em>forward and backward definitions<br>of an autograd operation</em>. Every <code>Tensor</code> operation creates at<br>least a single <code>Function</code> node that connects to functions that<br>created a <code>Tensor</code> and <em>encodes its history</em>.</li>
</ul>
<p><strong>At this point, we covered:</strong></p>
<ul>
<li>Defining a neural network</li>
<li>Processing inputs and calling backward</li>
</ul>
<p><strong>Still Left:</strong></p>
<ul>
<li>Computing the loss</li>
<li>Updating the weights of the network</li>
</ul>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>A loss function takes the (output, target) pair of inputs, and computes a<br>value that estimates how far away the output is from the target.</p>
<p>There are several different<br><code>loss functions &lt;https://pytorch.org/docs/nn.html#loss-functions&gt;</code>_ under the<br>nn package .<br>A simple loss is: <code>nn.MSELoss</code> which computes the mean-squared error<br>between the input and the target.</p>
<p>For example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.9128, grad_fn=&lt;MseLossBackward&gt;)
</code></pre><p>Now, if you follow <code>loss</code> in the backward direction, using its<br><code>.grad_fn</code> attribute, you will see a graph of computations that looks<br>like this:</p>
<p>::</p>
<pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
      -&gt; MSELoss
      -&gt; loss
</code></pre><p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated<br>w.r.t. the loss, and all Tensors in the graph that has <code>requires_grad=True</code><br>will have their <code>.grad</code> Tensor accumulated with the gradient.</p>
<p>For illustration, let us follow a few steps backward:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure>
<pre><code>&lt;MseLossBackward object at 0x000001F109346C88&gt;
&lt;AddmmBackward object at 0x000001F109346EB8&gt;
&lt;AccumulateGrad object at 0x000001F109346C88&gt;
</code></pre><h2 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a>Backprop</h2><p>To backpropagate the error all we have to do is to <code>loss.backward()</code>.<br>You need to clear the existing gradients though, else gradients will be<br>accumulated to existing gradients.</p>
<p>Now we shall call <code>loss.backward()</code>, and have a look at conv1’s bias<br>gradients before and after the backward.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>
<pre><code>conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([-0.0032, -0.0131,  0.0148,  0.0334, -0.0327, -0.0073])
</code></pre><p>Now, we have seen how to use loss functions.</p>
<p><strong>Read Later:</strong></p>
<p>  The neural network package contains various modules and loss functions<br>  that form the building blocks of deep neural networks. A full list with<br>  documentation is <code>here &lt;https://pytorch.org/docs/nn&gt;</code>_.</p>
<p><strong>The only thing left to learn is:</strong></p>
<ul>
<li>Updating the weights of the network</li>
</ul>
<h2 id="Update-the-weights"><a href="#Update-the-weights" class="headerlink" title="Update the weights"></a>Update the weights</h2><p>The simplest update rule used in practice is the Stochastic Gradient<br>Descent (SGD):</p>
<pre><code> ``weight = weight - learning_rate * gradient``
</code></pre><p>We can implement this using simple Python code:</p>
<p>.. code:: python</p>
<pre><code>learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
</code></pre><p>However, as you use neural networks, you want to use various different<br>update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.<br>To enable this, we built a small package: <code>torch.optim</code> that<br>implements all these methods. Using it is very simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>
<p>.. Note::</p>
<pre><code>  Observe how gradient buffers had to be manually set to zero using
  ``optimizer.zero_grad()``. This is because gradients are accumulated
  as explained in the `Backprop`_ section.
</code></pre><h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">feed-forward:前向</span><br><span class="line">glimpse:一瞥</span><br><span class="line">proce:进行</span><br><span class="line">recap:回顾</span><br><span class="line">encapsulat:封装</span><br><span class="line">assign:分配</span><br><span class="line">For illustration:为了说明</span><br></pre></td></tr></table></figure>
</AddmmBackward>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-torch.nn</title>
    <url>/2020/07/23/Pytorch-Learning-torch-nn/</url>
    <content><![CDATA[<p>Pytorch-Learning-torch.nn<br><a id="more"></a></p>
<div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div>
* 1.[Learning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/)
    * 1.1[Pytorch-Learning-tensor](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/)
    * 1.2[Pytorch-Learning-autograd](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/)
    * 1.3[Pytorch-Learning-neural_newworks](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/)
    * 1.4[Pytorch-Learning-examples](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/)
    * 1.5[Pytorch-Learning-torch.nn](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/)
    * 1.6[Pytorch-Learning-cifar10tutorial-visualizing](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/)
* 2.[Image](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/)
    * 2.1[微调TorchVision对象检测](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/)
    * 2.2[计算机视觉迁移学习](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/)
    * 2.3[对抗样本生成](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/
    * 2.4[DCGAN教程](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/)
* 3.[Audio](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/)
    * 3.1[torchaudio](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/)
* 4.[Text](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/)
    * 4.1[用NN.TRANFORMER和TORCHTEXT进行序列到序列建模](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/)
    * 4.2[使用字符级RNN对名称进行分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/)
    * 4.3[用字符级RNN生成名称](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/)
    * 4.4[使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/)
    * 4.5[TORCHTEXT的文本分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)
    * 4.6[TORCHTEXT的语言翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/)
* 5.[ReinforcementLearning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/)
* 5.1[Reinforcement-Learning](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/)

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


What is `torch.nn` *really*?
============================
by Jeremy Howard, `fast.ai <https: www.fast.ai>`_. Thanks to Rachel Thomas and Francisco Ingham.



We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,
click the link at the top of the page.

PyTorch provides the elegantly designed modules and classes `torch.nn <https: pytorch.org docs stable nn.html>`_ ,
`torch.optim <https: pytorch.org docs stable optim.html>`_ ,
`Dataset <https: pytorch.org docs stable data.html?highlight="dataset#torch.utils.data.Dataset">`_ ,
and `DataLoader <https: pytorch.org docs stable data.html?highlight="dataloader#torch.utils.data.DataLoader">`_
to help you create and train neural networks.
In order to fully utilize their power and customize
them for your problem, you need to really understand exactly what they're
doing. To develop this understanding, we will first train basic neural net
on the MNIST data set without using any features from these models; we will
initially only use the most basic PyTorch tensor functionality. Then, we will
incrementally add one feature from ``torch.nn``, ``torch.optim``, ``Dataset``, or
``DataLoader`` at a time, showing exactly what each piece does, and how it
works to make the code either more concise, or more flexible.

**This tutorial assumes you already have PyTorch installed, and are familiar
with the basics of tensor operations.** (If you're familiar with Numpy array
operations, you'll find the PyTorch tensor operations used here nearly identical).

MNIST data setup
----------------

We will use the classic `MNIST <http: deeplearning.net data mnist>`_ dataset,
which consists of black-and-white images of hand-drawn digits (between 0 and 9).

We will use `pathlib <https: 3 docs.python.org library pathlib.html>`_
for dealing with paths (part of the Python 3 standard library), and will
download the dataset using
`requests <http: docs.python-requests.org en master>`_. We will only
import modules when we use them, so you can see exactly what's being
used at each point.




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">"data"</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">"mnist"</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">"http://deeplearning.net/data/mnist/"</span></span><br><span class="line">FILENAME = <span class="string">"mnist.pkl.gz"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).open(<span class="string">"wb"</span>).write(content)</span><br></pre></td></tr></table></figure>

This dataset is in numpy array format, and has been stored using pickle,
a python-specific format for serializing data.




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.open((PATH / FILENAME).as_posix(), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">"latin-1"</span>)</span><br></pre></td></tr></table></figure>

Each image is 28 x 28, and is being stored as a flattened row of length
784 (=28x28). Let's take a look at one; we need to reshape it to 2d
first.




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">"gray"</span>)</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>

    (50000, 784)



![png](output_7_1.png)


PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to
convert our data.




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = map(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line">x_train, x_train.shape, y_train.min(), y_train.max()</span><br><span class="line">print(x_train, y_train)</span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.min(), y_train.max())</span><br></pre></td></tr></table></figure>

    tensor([[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])
    torch.Size([50000, 784])
    tensor(0) tensor(9)


Neural net from scratch (no torch.nn)
---------------------------------------------

Let's first create a model using nothing but PyTorch tensor operations. We're assuming
you're already familiar with the basics of neural networks. (If you're not, you can
learn them at `course.fast.ai <https: course.fast.ai>`_).

PyTorch provides methods to create random or zero-filled tensors, which we will
use to create our weights and bias for a simple linear model. These are just regular
tensors, with one very special addition: we tell PyTorch that they require a
gradient. This causes PyTorch to record all of the operations done on the tensor,
so that it can calculate the gradient during back-propagation *automatically*!

For the weights, we set ``requires_grad`` **after** the initialization, since we
don't want that step included in the gradient. (Note that a trailling ``_`` in
PyTorch signifies that the operation is performed in-place.)

<div class="alert alert-info"><h4>Note</h4><p>We are initializing the weights here with
   `Xavier initialisation <http: proceedings.mlr.press v9 glorot10a glorot10a.pdf>`_
   (by multiplying with 1/sqrt(n)).</http:></p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Thanks to PyTorch’s ability to calculate gradients automatically, we can<br>use any standard Python function (or callable object) as a model! So<br>let’s just write a plain matrix multiplication and broadcasted addition<br>to create a simple linear model. We also need an activation function, so<br>we’ll write <code>log_softmax</code> and use it. Remember: although PyTorch<br>provides lots of pre-written loss functions, activation functions, and<br>so forth, you can easily write your own using plain python. PyTorch will<br>even create fast GPU or vectorized CPU code for your function<br>automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x - x.exp().sum(<span class="number">-1</span>).log().unsqueeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>
<p>In the above, the <code>@</code> stands for the dot product operation. We will call<br>our function on one batch of data (in this case, 64 images).  This is<br>one <em>forward pass</em>.  Note that our predictions won’t be any better than<br>random at this stage, since we start with random weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bs = <span class="number">64</span>  <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs]  <span class="comment"># a mini-batch from x</span></span><br><span class="line">preds = model(xb)  <span class="comment"># predictions</span></span><br><span class="line">preds[<span class="number">0</span>], preds.shape</span><br><span class="line">print(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([-2.2669, -2.6024, -2.8454, -1.5665, -2.7687, -2.2455, -2.6885, -2.4918,
        -2.1065, -2.1682], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])
</code></pre><p>As you see, the <code>preds</code> tensor contains not only the tensor values, but also a<br>gradient function. We’ll use this later to do backprop.</p>
<p>Let’s implement negative log-likelihood to use as the loss function<br>(again, we can just use standard Python):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nll</span><span class="params">(input, target)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -input[range(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>
<p>Let’s check our loss with our random model, so we can see if we improve<br>after a backprop pass later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line">print(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(2.2549, grad_fn=&lt;NegBackward&gt;)
</code></pre><p>Let’s also implement a function to calculate the accuracy of our model.<br>For each prediction, if the index with the largest value matches the<br>target value, then the prediction was correct.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(out, yb)</span>:</span></span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).float().mean()</span><br></pre></td></tr></table></figure>
<p>Let’s check the accuracy of our random model, so we can see if our<br>accuracy improves as our loss improves.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.1562)
</code></pre><p>We can now run a training loop.  For each iteration, we will:</p>
<ul>
<li>select a mini-batch of data (of size <code>bs</code>)</li>
<li>use the model to make predictions</li>
<li>calculate the loss</li>
<li><code>loss.backward()</code> updates the gradients of the model, in this case, <code>weights</code><br>and <code>bias</code>.</li>
</ul>
<p>We now use these gradients to update the weights and bias.  We do this<br>within the <code>torch.no_grad()</code> context manager, because we do not want these<br>actions to be recorded for our next calculation of the gradient.  You can read<br>more about how PyTorch’s Autograd records operations<br><code>here &lt;https://pytorch.org/docs/stable/notes/autograd.html&gt;</code>_.</p>
<p>We then set the<br>gradients to zero, so that we are ready for the next loop.<br>Otherwise, our gradients would record a running tally of all the operations<br>that had happened (i.e. <code>loss.backward()</code> <em>adds</em> the gradients to whatever is<br>already stored, rather than replacing them).</p>
<p>.. tip:: You can use the standard python debugger to step through PyTorch<br>   code, allowing you to check the various variable values at each step.<br>   Uncomment <code>set_trace()</code> below to try it out.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># how many epochs to train for</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line"><span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>That’s it: we’ve created and trained a minimal neural network (in this case, a<br>logistic regression, since we have no hidden layers) entirely from scratch!</p>
<p>Let’s check the loss and accuracy and compare those to what we got<br>earlier. We expect that the loss will have decreased and accuracy to<br>have increased, and they have.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.0781, grad_fn=&lt;NegBackward&gt;) tensor(1.)
</code></pre><h2 id="Using-torch-nn-functional"><a href="#Using-torch-nn-functional" class="headerlink" title="Using torch.nn.functional"></a>Using torch.nn.functional</h2><p>We will now refactor our code, so that it does the same thing as before, only<br>we’ll start taking advantage of PyTorch’s <code>nn</code> classes to make it more concise<br>and flexible. At each step from here, we should be making our code one or more<br>of: shorter, more understandable, and/or more flexible.</p>
<p>The first and easiest step is to make our code shorter by replacing our<br>hand-written activation and loss functions with those from <code>torch.nn.functional</code><br>(which is generally imported into the namespace <code>F</code> by convention). This module<br>contains all the functions in the <code>torch.nn</code> library (whereas other parts of the<br>library contain classes). As well as a wide range of loss and activation<br>functions, you’ll also find here some convenient functions for creating neural<br>nets, such as pooling functions. (There are also functions for doing convolutions,<br>linear layers, etc, but as we’ll see, these are usually better handled using<br>other parts of the library.)</p>
<p>If you’re using negative log likelihood loss and log softmax activation,<br>then Pytorch provides a single function <code>F.cross_entropy</code> that combines<br>the two. So we can even remove the activation function from our model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure>
<p>Note that we no longer call <code>log_softmax</code> in the <code>model</code> function. Let’s<br>confirm that our loss and accuracy are the same as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.0781, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)
</code></pre><h2 id="Refactor-using-nn-Module"><a href="#Refactor-using-nn-Module" class="headerlink" title="Refactor using nn.Module"></a>Refactor using nn.Module</h2><p>Next up, we’ll use <code>nn.Module</code> and <code>nn.Parameter</code>, for a clearer and more<br>concise training loop. We subclass <code>nn.Module</code> (which itself is a class and<br>able to keep track of state).  In this case, we want to create a class that<br>holds our weights, bias, and method for the forward step.  <code>nn.Module</code> has a<br>number of attributes and methods (such as <code>.parameters()</code> and <code>.zero_grad()</code>)<br>which we will be using.</p>
<div class="alert alert-info"><h4>Note</h4><p>``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a
   class we'll be using a lot. ``nn.Module`` is not to be confused with the Python
   concept of a (lowercase ``m``) `module <https: 3 docs.python.org tutorial modules.html>`_,
   which is a file of Python code that can be imported.</https:></p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>
<p>Since we’re now using an object instead of just using a function, we<br>first have to instantiate our model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure>
<p>Now we can calculate the loss in the same way as before. Note that<br><code>nn.Module</code> objects are used as if they are functions (i.e they are<br><em>callable</em>), but behind the scenes Pytorch will call our <code>forward</code><br>method automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(2.4768, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><p>Previously for our training loop we had to update the values for each parameter<br>by name, and manually zero out the grads for each parameter separately, like this:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    weights -&#x3D; weights.grad * lr</span><br><span class="line">    bias -&#x3D; bias.grad * lr</span><br><span class="line">    weights.grad.zero_()</span><br><span class="line">    bias.grad.zero_()</span><br></pre></td></tr></table></figure></p>
<p>Now we can take advantage of model.parameters() and model.zero_grad() (which<br>are both defined by PyTorch for <code>nn.Module</code>) to make those steps more concise<br>and less prone to the error of forgetting some of our parameters, particularly<br>if we had a more complicated model:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure></p>
<p>We’ll wrap our little training loop in a <code>fit</code> function so we can run it<br>again later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>
<p>Let’s double-check that our loss has gone down:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.0860, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><h2 id="Refactor-using-nn-Linear"><a href="#Refactor-using-nn-Linear" class="headerlink" title="Refactor using nn.Linear"></a>Refactor using nn.Linear</h2><p>We continue to refactor our code.  Instead of manually defining and<br>initializing <code>self.weights</code> and <code>self.bias</code>, and calculating <code>xb  @
self.weights + self.bias</code>, we will instead use the Pytorch class<br><code>nn.Linear &lt;https://pytorch.org/docs/stable/nn.html#linear-layers&gt;</code>_ for a<br>linear layer, which does all that for us. Pytorch has many types of<br>predefined layers that can greatly simplify our code, and often makes it<br>faster too.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure>
<p>We instantiate our model and calculate the loss in the same way as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(2.3752, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><p>We are still able to use our same <code>fit</code> method as before.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.0814, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><h2 id="Refactor-using-optim"><a href="#Refactor-using-optim" class="headerlink" title="Refactor using optim"></a>Refactor using optim</h2><p>Pytorch also has a package with various optimization algorithms, <code>torch.optim</code>.<br>We can use the <code>step</code> method from our optimizer to take a forward step, instead<br>of manually updating each parameter.</p>
<p>This will let us replace our previous manually coded optimization step:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure><br>and instead use just:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">opt.step()</span><br><span class="line">opt.zero_grad()</span><br></pre></td></tr></table></figure></p>
<p>(<code>optim.zero_grad()</code> resets the gradient to 0 and we need to call it before<br>computing the gradient for the next minibatch.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>
<p>We’ll define a little function to create our model and optimizer so we<br>can reuse it in the future.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(2.2501, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.0822, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><h2 id="Refactor-using-Dataset"><a href="#Refactor-using-Dataset" class="headerlink" title="Refactor using Dataset"></a>Refactor using Dataset</h2><p>PyTorch has an abstract Dataset class.  A Dataset can be anything that has<br>a <code>__len__</code> function (called by Python’s standard <code>len</code> function) and<br>a <code>__getitem__</code> function as a way of indexing into it.<br><code>This tutorial &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code>_<br>walks through a nice example of creating a custom <code>FacialLandmarkDataset</code> class<br>as a subclass of <code>Dataset</code>.</p>
<p>PyTorch’s <code>TensorDataset &lt;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset&gt;</code>_<br>is a Dataset wrapping tensors. By defining a length and way of indexing,<br>this also gives us a way to iterate, index, and slice along the first<br>dimension of a tensor. This will make it easier to access both the<br>independent and dependent variables in the same line as we train.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br></pre></td></tr></table></figure>
<p>Both <code>x_train</code> and <code>y_train</code> can be combined in a single <code>TensorDataset</code>,<br>which will be easier to iterate over and slice.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>
<p>Previously, we had to iterate through minibatches of x and y values separately:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xb &#x3D; x_train[start_i:end_i]</span><br><span class="line">yb &#x3D; y_train[start_i:end_i]</span><br></pre></td></tr></table></figure></p>
<p>Now, we can do these two steps together:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xb,yb &#x3D; train_ds[i*bs : i*bs+bs]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.0801, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><h2 id="Refactor-using-DataLoader"><a href="#Refactor-using-DataLoader" class="headerlink" title="Refactor using DataLoader"></a>Refactor using DataLoader</h2><p>Pytorch’s <code>DataLoader</code> is responsible for managing batches. You can<br>create a <code>DataLoader</code> from any <code>Dataset</code>. <code>DataLoader</code> makes it easier<br>to iterate over batches. Rather than having to use <code>train_ds[i*bs : i*bs+bs]</code>,<br>the DataLoader gives us each minibatch automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure>
<p>Previously, our loop iterated over batches (xb, yb) like this:<br>::<br>      for i in range((n-1)//bs + 1):<br>          xb,yb = train_ds[i<em>bs : i</em>bs+bs]<br>          pred = model(xb)</p>
<p>Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:<br>::<br>      for xb,yb in train_dl:<br>          pred = model(xb)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.0824, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><p>Thanks to Pytorch’s <code>nn.Module</code>, <code>nn.Parameter</code>, <code>Dataset</code>, and <code>DataLoader</code>,<br>our training loop is now dramatically smaller and easier to understand. Let’s<br>now try to add the basic features necessary to create effecive models in practice.</p>
<h2 id="Add-validation"><a href="#Add-validation" class="headerlink" title="Add validation"></a>Add validation</h2><p>In section 1, we were just trying to get a reasonable training loop set up for<br>use on our training data.  In reality, you <strong>always</strong> should also have<br>a <code>validation set &lt;https://www.fast.ai/2017/11/13/validation-sets/&gt;</code>_, in order<br>to identify if you are overfitting.</p>
<p>Shuffling the training data is<br><code>important &lt;https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks&gt;</code>_<br>to prevent correlation between batches and overfitting. On the other hand, the<br>validation loss will be identical whether we shuffle the validation set or not.<br>Since shuffling takes extra time, it makes no sense to shuffle the validation data.</p>
<p>We’ll use a batch size for the validation set that is twice as large as<br>that for the training set. This is because the validation set does not<br>need backpropagation and thus takes less memory (it doesn’t need to<br>store the gradients). We take advantage of this to use a larger batch<br>size and compute the loss more quickly.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>We will calculate and print the validation loss at the end of each epoch.</p>
<p>(Note that we always call <code>model.train()</code> before training, and <code>model.eval()</code><br>before inference, because these are used by layers such as <code>nn.BatchNorm2d</code><br>and <code>nn.Dropout</code> to ensure appropriate behaviour for these different phases.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = sum(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line"></span><br><span class="line">    print(epoch, valid_loss / len(valid_dl))</span><br></pre></td></tr></table></figure>
<pre><code>0 tensor(0.3169)
1 tensor(0.4910)
</code></pre><h2 id="Create-fit-and-get-data"><a href="#Create-fit-and-get-data" class="headerlink" title="Create fit() and get_data()"></a>Create fit() and get_data()</h2><p>We’ll now do a little refactoring of our own. Since we go through a similar<br>process twice of calculating the loss for both the training set and the<br>validation set, let’s make that into its own function, <code>loss_batch</code>, which<br>computes the loss for one batch.</p>
<p>We pass an optimizer in for the training set, and use it to perform<br>backprop.  For the validation set, we don’t pass an optimizer, so the<br>method doesn’t perform backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_batch</span><span class="params">(model, loss_func, xb, yb, opt=None)</span>:</span></span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), len(xb)</span><br></pre></td></tr></table></figure>
<p><code>fit</code> runs the necessary operations to train our model and compute the<br>training and validation losses for each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(epochs, model, loss_func, opt, train_dl, valid_dl)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = zip(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)</span><br><span class="line"></span><br><span class="line">        print(epoch, val_loss)</span><br></pre></td></tr></table></figure>
<p><code>get_data</code> returns dataloaders for the training and validation sets.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(train_ds, valid_ds, bs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>Now, our whole process of obtaining the data loaders and fitting the<br>model can be run in 3 lines of code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<pre><code>0 0.30705235414505005
1 0.31455935287475584
</code></pre><p>You can use these basic 3 lines of code to train a wide variety of models.<br>Let’s see if we can use them to train a convolutional neural network (CNN)!</p>
<h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>We are now going to build our neural network with three convolutional layers.<br>Because none of the functions in the previous section assume anything about<br>the model form, we’ll be able to use them to train a CNN without any modification.</p>
<p>We will use Pytorch’s predefined<br><code>Conv2d &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code>_ class<br>as our convolutional layer. We define a CNN with 3 convolutional layers.<br>Each convolution is followed by a ReLU.  At the end, we perform an<br>average pooling.  (Note that <code>view</code> is PyTorch’s version of numpy’s<br><code>reshape</code>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        xb = xb.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> xb.view(<span class="number">-1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<p><code>Momentum &lt;https://cs231n.github.io/neural-networks-3/#sgd&gt;</code>_ is a variation on<br>stochastic gradient descent that takes previous updates into account as well<br>and generally leads to faster training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<pre><code>0 0.8898362560272217
1 0.7796085683822632
</code></pre><h2 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h2><p><code>torch.nn</code> has another handy class we can use to simply our code:<br><code>Sequential &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential&gt;</code>_ .<br>A <code>Sequential</code> object runs each of the modules contained within it, in a<br>sequential manner. This is a simpler way of writing our neural network.</p>
<p>To take advantage of this, we need to be able to easily define a<br><strong>custom layer</strong> from a given function.  For instance, PyTorch doesn’t<br>have a <code>view</code> layer, and we need to create one for our network. <code>Lambda</code><br>will create a layer that we can then use when defining a network with<br><code>Sequential</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure>
<p>The model created with <code>Sequential</code> is simply:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<pre><code>0 0.4746176950454712
1 0.3015344765663147
</code></pre><h2 id="Wrapping-DataLoader"><a href="#Wrapping-DataLoader" class="headerlink" title="Wrapping DataLoader"></a>Wrapping DataLoader</h2><p>Our CNN is fairly concise, but it only works with MNIST, because:</p>
<ul>
<li>It assumes the input is a 28*28 long vector</li>
<li>It assumes that the final CNN grid size is 4*4 (since that’s the average<br>pooling kernel size we used)</li>
</ul>
<p>Let’s get rid of these two assumptions, so our model works with any 2d<br>single channel image. First, we can remove the initial Lambda layer but<br>moving the data preprocessing into a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WrappedDataLoader</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, func)</span>:</span></span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        batches = iter(self.dl)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> batches:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>Next, we can replace <code>nn.AvgPool2d</code> with <code>nn.AdaptiveAvgPool2d</code>, which<br>allows us to define the size of the <em>output</em> tensor we want, rather than<br>the <em>input</em> tensor we have. As a result, our model will work with any<br>size input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>Let’s try it out:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<pre><code>0 0.43946951394081113
1 0.2360862446308136
</code></pre><h2 id="Using-your-GPU"><a href="#Using-your-GPU" class="headerlink" title="Using your GPU"></a>Using your GPU</h2><p>If you’re lucky enough to have access to a CUDA-capable GPU (you can<br>rent one for about $0.50/hour from most cloud providers) you can<br>use it to speed up your code. First check that your GPU is working in<br>Pytorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<pre><code>False
</code></pre><p>And then create a device object for it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dev = torch.device(</span><br><span class="line">    <span class="string">"cuda"</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<p>Let’s update <code>preprocess</code> to move batches to the GPU:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(dev), y.to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>Finally, we can move our model to the GPU.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>You should find it runs faster now:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<pre><code>0 0.20087482118606567
1 0.21629996614456176
</code></pre><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>We now have a general data pipeline and training loop which you can use for<br>training many types of models using Pytorch. To see how simple training a model<br>can now be, take a look at the <code>mnist_sample</code> sample notebook.</p>
<p>Of course, there are many things you’ll want to add, such as data augmentation,<br>hyperparameter tuning, monitoring training, transfer learning, and so forth.<br>These features are available in the fastai library, which has been developed<br>using the same design approach shown in this tutorial, providing a natural<br>next step for practitioners looking to take their models further.</p>
<p>We promised at the start of this tutorial we’d explain through example each of<br><code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, and <code>DataLoader</code>. So let’s summarize<br>what we’ve seen:</p>
<ul>
<li><p><strong>torch.nn</strong></p>
<ul>
<li><code>Module</code>: creates a callable which behaves like a function, but can also<br>contain state(such as neural net layer weights). It knows what <code>Parameter</code> (s) it<br>contains and can zero all their gradients, loop through them for weight updates, etc.</li>
<li><code>Parameter</code>: a wrapper for a tensor that tells a <code>Module</code> that it has weights<br>that need updating during backprop. Only tensors with the <code>requires_grad</code> attribute set are updated</li>
<li><code>functional</code>: a module(usually imported into the <code>F</code> namespace by convention)<br>which contains activation functions, loss functions, etc, as well as non-stateful<br>versions of layers such as convolutional and linear layers.</li>
</ul>
</li>
<li><code>torch.optim</code>: Contains optimizers such as <code>SGD</code>, which update the weights<br>of <code>Parameter</code> during the backward step</li>
<li><code>Dataset</code>: An abstract interface of objects with a <code>__len__</code> and a <code>__getitem__</code>,<br>including classes provided with Pytorch such as <code>TensorDataset</code></li>
<li><code>DataLoader</code>: Takes any <code>Dataset</code> and creates an iterator which returns batches of data.</li>
</ul>
<h2 id="我不会的单词"><a href="#我不会的单词" class="headerlink" title="我不会的单词"></a>我不会的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">utilize:利用</span><br><span class="line">customize:定制</span><br><span class="line">refactor:重构</span><br><span class="line">identical:相同的</span><br></pre></td></tr></table></figure>
</https:></http:></https:></http:></https:></https:></https:></https:></https:>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-tensor</title>
    <url>/2020/07/23/Pytorch-Learning-tensor/</url>
    <content><![CDATA[<p>Pytorch-Learning-tensor:<br><a id="more"></a></p>
<div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div>
* 1.[Learning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/)
    * 1.1[Pytorch-Learning-tensor](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/)
    * 1.2[Pytorch-Learning-autograd](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/)
    * 1.3[Pytorch-Learning-neural_newworks](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/)
    * 1.4[Pytorch-Learning-examples](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/)
    * 1.5[Pytorch-Learning-torch.nn](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/)
    * 1.6[Pytorch-Learning-cifar10tutorial-visualizing](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/)
* 2.[Image](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/)
    * 2.1[微调TorchVision对象检测](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/)
    * 2.2[计算机视觉迁移学习](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/)
    * 2.3[对抗样本生成](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/
    * 2.4[DCGAN教程](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/)
* 3.[Audio](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/)
    * 3.1[torchaudio](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/)
* 4.[Text](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/)
    * 4.1[用NN.TRANFORMER和TORCHTEXT进行序列到序列建模](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/)
    * 4.2[使用字符级RNN对名称进行分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/)
    * 4.3[用字符级RNN生成名称](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/)
    * 4.4[使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/)
    * 4.5[TORCHTEXT的文本分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)
    * 4.6[TORCHTEXT的语言翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/)
* 5.[ReinforcementLearning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/)
* 5.1[Reinforcement-Learning](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/)

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


What is PyTorch?
================

It’s a Python-based scientific computing package targeted at two sets of
audiences:

-  A replacement for NumPy to use the power of GPUs
-  a deep learning research platform that provides maximum flexibility
   and speed

Getting Started
---------------

Tensors

Tensors are similar to NumPy’s ndarrays, with the addition being that
Tensors can also be used on a GPU to accelerate computing.




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<div class="alert alert-info"><h4>Note</h4><p>An uninitialized matrix is declared,
    but does not contain definite known
    values before it is used. When an
    uninitialized matrix is created,
    whatever values were in the allocated
    memory at the time will appear as the initial values.</p></div>



<p>Construct a 5x3 matrix, uninitialized:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[-3.4374e-14,  7.1046e-43, -3.4374e-14],
        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],
        [-3.4374e-14,  7.1046e-43, -3.4374e-14],
        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],
        [-3.4374e-14,  7.1046e-43, -3.4374e-14]])
</code></pre><p>Construct a randomly initialized matrix:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.6385, 0.8264, 0.0737],
        [0.1567, 0.5029, 0.7141],
        [0.8297, 0.3453, 0.2860],
        [0.0158, 0.3826, 0.7823],
        [0.3434, 0.0977, 0.1530]])
</code></pre><p>Construct a matrix filled zeros and of dtype long:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
</code></pre><p>Construct a tensor directly from data:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([5.5000, 3.0000])
</code></pre><p>or create a tensor based on an existing tensor. These methods<br>will reuse properties of the input tensor, e.g. dtype, unless<br>new values are provided by user</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)      <span class="comment"># new_* methods take in sizes</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)    <span class="comment"># override dtype!</span></span><br><span class="line">print(x)                                      <span class="comment"># result has the same size</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 2.0072,  0.0294,  0.1776],
        [-0.3961, -1.7436, -0.1741],
        [ 0.7820,  0.5535, -0.0059],
        [-1.9826, -0.7387, -0.3942],
        [ 0.3501,  0.5796, -1.3633]])
</code></pre><p>Get its size:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([5, 3])
</code></pre><div class="alert alert-info"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div>

<p>Operations</p>
<p>There are multiple syntaxes for operations. In the following<br>example, we will take a look at the addition operation.</p>
<p>Addition: syntax 1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])
</code></pre><p>Addition: syntax 2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])
</code></pre><p>Addition: providing an output tensor as argument</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])
</code></pre><p>Addition: in-place</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])
</code></pre><div class="alert alert-info"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.
    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div>

<p>You can use standard NumPy-like indexing with all bells and whistles!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>tensor([ 0.0294, -1.7436,  0.5535, -0.7387,  0.5796])
</code></pre><p>Resizing: If you want to resize/reshape tensor, you can use <code>torch.view</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
</code></pre><p>If you have a one element tensor, use <code>.item()</code> to get the value as a<br>Python number</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure>
<pre><code>tensor([1.0191])
1.0191349983215332
</code></pre><p><strong>Read later:</strong></p>
<p>  100+ Tensor operations, including transposing, indexing, slicing,<br>  mathematical operations, linear algebra, random numbers, etc.,<br>  are described<br>  <code>here &lt;https://pytorch.org/docs/torch&gt;</code>_.</p>
<h2 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h2><p>Converting a Torch Tensor to a NumPy array and vice versa is a breeze.</p>
<p>The Torch Tensor and NumPy array will share their underlying memory<br>locations (if the Torch Tensor is on CPU), and changing one will change<br>the other.</p>
<p>Converting a Torch Tensor to a NumPy Array</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([1., 1., 1., 1., 1.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>[1. 1. 1. 1. 1.]
</code></pre><p>See how the numpy array changed in value.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
</code></pre><p>Converting NumPy Array to Torch Tensor</p>
<p>See how changing the np array changed the Torch Tensor automatically</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
</code></pre><p>All the Tensors on the CPU except a CharTensor support converting to<br>NumPy and back.</p>
<h2 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h2><p>Tensors can be moved onto any device using the <code>.to</code> method.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="comment"># We will use ``torch.device`` objects to move tensors in and out of GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># a CUDA device object</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` can also change dtype together!</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">allocate:分配</span><br><span class="line">mutate:变异</span><br><span class="line">Bridge:桥</span><br><span class="line">vice versa:反之亦然</span><br><span class="line">underlying:底层的</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-TORCHTEXT的文本分类</title>
    <url>/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>Pytorch-Text-TORCHTEXT的文本分类:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Text-Classification-with-TorchText"><a href="#Text-Classification-with-TorchText" class="headerlink" title="Text Classification with TorchText"></a>Text Classification with TorchText</h1><p>This tutorial shows how to use the text classification datasets<br>in <code>torchtext</code>, including</p>
<p>::</p>
<ul>
<li>AG_NEWS,</li>
<li>SogouNews,</li>
<li>DBpedia,</li>
<li>YelpReviewPolarity,</li>
<li>YelpReviewFull,</li>
<li>YahooAnswers,</li>
<li>AmazonReviewPolarity,</li>
<li>AmazonReviewFull</li>
</ul>
<p>This example shows how to train a supervised learning algorithm for<br>classification using one of these <code>TextClassification</code> datasets.</p>
<h2 id="Load-data-with-ngrams"><a href="#Load-data-with-ngrams" class="headerlink" title="Load data with ngrams"></a>Load data with ngrams</h2><p>A bag of ngrams feature is applied to capture some partial information<br>about the local word order. In practice, bi-gram or tri-gram are applied<br>to provide more benefits as word groups than only one word. An example:</p>
<p>::</p>
<p>   “load data with ngrams”<br>   Bi-grams results: “load data”, “data with”, “with ngrams”<br>   Tri-grams results: “load data with”, “data with ngrams”</p>
<p><code>TextClassification</code> Dataset supports the ngrams method. By setting<br>ngrams to 2, the example text in the dataset will be a list of single<br>words plus bi-grams string.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> text_classification</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> torchtext.utils <span class="keyword">import</span> extract_archive, unicode_csv_reader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> _csv_iterator,_create_data_from_iterator</span><br><span class="line">NGRAMS = <span class="number">2</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./.data'</span>):</span><br><span class="line">	os.mkdir(<span class="string">'./.data'</span>)</span><br><span class="line"><span class="comment"># train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](</span></span><br><span class="line"><span class="comment">#     root='./.data', ngrams=NGRAMS, vocab=None)</span></span><br><span class="line"> <span class="comment">#定义创建数据集函数，原函数在torchtext.datasets.text_classification文件中，本教程所需参数直接设成了默认值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_setup_datasets</span><span class="params">(dataset_tar=<span class="string">'./.data/ag_news_csv.tar.gz'</span>,dataset_name=<span class="string">"AG_NEWS"</span>, root=<span class="string">'./.data'</span>, ngrams=NGRAMS, vocab=None, include_unk=False)</span>:</span></span><br><span class="line">    <span class="comment"># 注释掉下载数据的代码</span></span><br><span class="line">    <span class="comment">#     dataset_tar = download_from_url(URLS[dataset_name], root=root)</span></span><br><span class="line">    extracted_files = extract_archive(dataset_tar)  <span class="comment">#解压数据文件</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> extracted_files:</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'train.csv'</span>):</span><br><span class="line">            train_csv_path = fname</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'test.csv'</span>):</span><br><span class="line">            test_csv_path = fname</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        logging.info(<span class="string">'Building Vocab based on &#123;&#125;'</span>.format(train_csv_path))</span><br><span class="line">        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams)) <span class="comment">#创建词典</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(vocab, Vocab):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"Passed vocabulary is not of type Vocab"</span>)</span><br><span class="line">    logging.info(<span class="string">'Vocab has &#123;&#125; entries'</span>.format(len(vocab)))</span><br><span class="line">    logging.info(<span class="string">'Creating training data'</span>)</span><br><span class="line">    train_data, train_labels = _create_data_from_iterator(   <span class="comment">#创建训练数据</span></span><br><span class="line">        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk) </span><br><span class="line">    logging.info(<span class="string">'Creating testing data'</span>)</span><br><span class="line">    test_data, test_labels = _create_data_from_iterator(   <span class="comment">#创建测试数据</span></span><br><span class="line">        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk)</span><br><span class="line">    <span class="keyword">if</span> len(train_labels ^ test_labels) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Training and test labels don't match"</span>)</span><br><span class="line">    <span class="keyword">return</span> (TextClassificationDataset(vocab, train_data, train_labels),  <span class="comment">#返回数据集实例</span></span><br><span class="line">            TextClassificationDataset(vocab, test_data, test_labels))</span><br><span class="line">train_dataset, test_dataset = _setup_datasets()</span><br><span class="line">BATCH_SIZE = <span class="number">16</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>120000lines [00:21, 5495.55lines/s]
120000lines [00:54, 2186.84lines/s]
7600lines [00:03, 1978.44lines/s]
</code></pre><h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>The model is composed of the<br><code>EmbeddingBag &lt;https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag&gt;</code>__<br>layer and the linear layer (see the figure below). <code>nn.EmbeddingBag</code><br>computes the mean value of a “bag” of embeddings. The text entries here<br>have different lengths. <code>nn.EmbeddingBag</code> requires no padding here<br>since the text lengths are saved in offsets.</p>
<p>Additionally, since <code>nn.EmbeddingBag</code> accumulates the average across<br>the embeddings on the fly, <code>nn.EmbeddingBag</code> can enhance the<br>performance and memory efficiency to process a sequence of tensors.</p>
<p><img src="https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextSentiment</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_dim, num_class)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(embed_dim, num_class)</span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.5</span></span><br><span class="line">        self.embedding.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, offsets)</span>:</span></span><br><span class="line">        embedded = self.embedding(text, offsets)</span><br><span class="line">        <span class="keyword">return</span> self.fc(embedded)</span><br></pre></td></tr></table></figure>
<h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The AG_NEWS dataset has four labels and therefore the number of classes<br>is four.</p>
<p>::</p>
<p>   1 : World<br>   2 : Sports<br>   3 : Business<br>   4 : Sci/Tec</p>
<p>The vocab size is equal to the length of vocab (including single word<br>and ngrams). The number of classes is equal to the number of labels,<br>which is four in AG_NEWS case.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">VOCAB_SIZE = len(train_dataset.get_vocab())</span><br><span class="line">EMBED_DIM = <span class="number">32</span></span><br><span class="line">NUN_CLASS = len(train_dataset.get_labels())</span><br><span class="line">model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Functions-used-to-generate-batch"><a href="#Functions-used-to-generate-batch" class="headerlink" title="Functions used to generate batch"></a>Functions used to generate batch</h2><p>Since the text entries have different lengths, a custom function<br>generate_batch() is used to generate data batches and offsets. The<br>function is passed to <code>collate_fn</code> in <code>torch.utils.data.DataLoader</code>.<br>The input to <code>collate_fn</code> is a list of tensors with the size of<br>batch_size, and the <code>collate_fn</code> function packs them into a<br>mini-batch. Pay attention here and make sure that <code>collate_fn</code> is<br>declared as a top level def. This ensures that the function is available<br>in each worker.</p>
<p>The text entries in the original data batch input are packed into a list<br>and concatenated as a single tensor as the input of <code>nn.EmbeddingBag</code>.<br>The offsets is a tensor of delimiters to represent the beginning index<br>of the individual sequence in the text tensor. Label is a tensor saving<br>the labels of individual text entries.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch)</span>:</span></span><br><span class="line">    label = torch.tensor([entry[<span class="number">0</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch])</span><br><span class="line">    text = [entry[<span class="number">1</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch]</span><br><span class="line">    offsets = [<span class="number">0</span>] + [len(entry) <span class="keyword">for</span> entry <span class="keyword">in</span> text]</span><br><span class="line">    <span class="comment"># torch.Tensor.cumsum returns the cumulative sum</span></span><br><span class="line">    <span class="comment"># of elements in the dimension dim.</span></span><br><span class="line">    <span class="comment"># torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)</span></span><br><span class="line"></span><br><span class="line">    offsets = torch.tensor(offsets[:<span class="number">-1</span>]).cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    text = torch.cat(text)</span><br><span class="line">    <span class="keyword">return</span> text, offsets, label</span><br></pre></td></tr></table></figure>
<h2 id="Define-functions-to-train-the-model-and-evaluate-results"><a href="#Define-functions-to-train-the-model-and-evaluate-results" class="headerlink" title="Define functions to train the model and evaluate results."></a>Define functions to train the model and evaluate results.</h2><p><code>torch.utils.data.DataLoader &lt;https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader&gt;</code><strong><br>is recommended for PyTorch users, and it makes data loading in parallel<br>easily (a tutorial is<br><code>here &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code></strong>).<br>We use <code>DataLoader</code> here to load AG_NEWS datasets and send it to the<br>model for training/validation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_func</span><span class="params">(sub_train_)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the model</span></span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    train_acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>,</span><br><span class="line">                      collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> i, (text, offsets, cls) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        output = model(text, offsets)</span><br><span class="line">        loss = criterion(output, cls)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adjust the learning rate</span></span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_loss / len(sub_train_), train_acc / len(sub_train_)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(data_)</span>:</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> text, offsets, cls <span class="keyword">in</span> data:</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            output = model(text, offsets)</span><br><span class="line">            loss = criterion(output, cls)</span><br><span class="line">            loss += loss.item()</span><br><span class="line">            acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss / len(data_), acc / len(data_)</span><br></pre></td></tr></table></figure>
<h2 id="Split-the-dataset-and-run-the-model"><a href="#Split-the-dataset-and-run-the-model" class="headerlink" title="Split the dataset and run the model"></a>Split the dataset and run the model</h2><p>Since the original AG_NEWS has no valid dataset, we split the training<br>dataset into train/valid sets with a split ratio of 0.95 (train) and<br>0.05 (valid). Here we use<br><code>torch.utils.data.dataset.random_split &lt;https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split&gt;</code>__<br>function in PyTorch core library.</p>
<p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.<br>It is useful when training a classification problem with C classes.<br><code>SGD &lt;https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html&gt;</code></strong><br>implements stochastic gradient descent method as optimizer. The initial<br>learning rate is set to 4.0.<br><code>StepLR &lt;https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR&gt;</code>__<br>is used here to adjust the learning rate through epochs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> random_split</span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">min_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().to(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">4.0</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1</span>, gamma=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">train_len = int(len(train_dataset) * <span class="number">0.95</span>)</span><br><span class="line">sub_train_, sub_valid_ = \</span><br><span class="line">    random_split(train_dataset, [train_len, len(train_dataset) - train_len])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train_func(sub_train_)</span><br><span class="line">    valid_loss, valid_acc = test(sub_valid_)</span><br><span class="line"></span><br><span class="line">    secs = int(time.time() - start_time)</span><br><span class="line">    mins = secs / <span class="number">60</span></span><br><span class="line">    secs = secs % <span class="number">60</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch: %d'</span> %(epoch + <span class="number">1</span>), <span class="string">" | time in %d minutes, %d seconds"</span> %(mins, secs))</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>(train)\t|\tAcc: <span class="subst">&#123;train_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(train)'</span>)</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;valid_loss:<span class="number">.4</span>f&#125;</span>(valid)\t|\tAcc: <span class="subst">&#123;valid_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(valid)'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 1  | time in 1 minutes, 36 seconds
    Loss: 0.0261(train)    |    Acc: 84.8%(train)
    Loss: 0.0001(valid)    |    Acc: 90.5%(valid)
Epoch: 2  | time in 1 minutes, 57 seconds
    Loss: 0.0118(train)    |    Acc: 93.8%(train)
    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)
Epoch: 3  | time in 1 minutes, 35 seconds
    Loss: 0.0069(train)    |    Acc: 96.4%(train)
    Loss: 0.0001(valid)    |    Acc: 89.9%(valid)
Epoch: 4  | time in 1 minutes, 36 seconds
    Loss: 0.0038(train)    |    Acc: 98.1%(train)
    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)
Epoch: 5  | time in 1 minutes, 37 seconds
    Loss: 0.0023(train)    |    Acc: 99.0%(train)
    Loss: 0.0001(valid)    |    Acc: 91.5%(valid)
</code></pre><p>Running the model on GPU with the following information:</p>
<p>Epoch: 1 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0263(train)     |       Acc: 84.5%(train)
   Loss: 0.0001(valid)     |       Acc: 89.0%(valid)
</code></pre><p>Epoch: 2 | time in 0 minutes, 10 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0119(train)     |       Acc: 93.6%(train)
   Loss: 0.0000(valid)     |       Acc: 89.6%(valid)
</code></pre><p>Epoch: 3 | time in 0 minutes, 9 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0069(train)     |       Acc: 96.4%(train)
   Loss: 0.0000(valid)     |       Acc: 90.5%(valid)
</code></pre><p>Epoch: 4 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0038(train)     |       Acc: 98.2%(train)
   Loss: 0.0000(valid)     |       Acc: 90.4%(valid)
</code></pre><p>Epoch: 5 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0022(train)     |       Acc: 99.0%(train)
   Loss: 0.0000(valid)     |       Acc: 91.0%(valid)
</code></pre><h2 id="Evaluate-the-model-with-test-dataset"><a href="#Evaluate-the-model-with-test-dataset" class="headerlink" title="Evaluate the model with test dataset"></a>Evaluate the model with test dataset</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Checking the results of test dataset...'</span>)</span><br><span class="line">test_loss, test_acc = test(test_dataset)</span><br><span class="line">print(<span class="string">f'\tLoss: <span class="subst">&#123;test_loss:<span class="number">.4</span>f&#125;</span>(test)\t|\tAcc: <span class="subst">&#123;test_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(test)'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Checking the results of test dataset...
    Loss: 0.0003(test)    |    Acc: 90.5%(test)
</code></pre><p>Checking the results of test dataset…</p>
<p>::</p>
<pre><code>   Loss: 0.0237(test)      |       Acc: 90.5%(test)
</code></pre><h2 id="Test-on-a-random-news"><a href="#Test-on-a-random-news" class="headerlink" title="Test on a random news"></a>Test on a random news</h2><p>Use the best model so far and test a golf news. The label information is<br>available<br><code>here &lt;https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> ngrams_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line">ag_news_label = &#123;<span class="number">1</span> : <span class="string">"World"</span>,</span><br><span class="line">                 <span class="number">2</span> : <span class="string">"Sports"</span>,</span><br><span class="line">                 <span class="number">3</span> : <span class="string">"Business"</span>,</span><br><span class="line">                 <span class="number">4</span> : <span class="string">"Sci/Tec"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(text, model, vocab, ngrams)</span>:</span></span><br><span class="line">    tokenizer = get_tokenizer(<span class="string">"basic_english"</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        text = torch.tensor([vocab[token]</span><br><span class="line">                            <span class="keyword">for</span> token <span class="keyword">in</span> ngrams_iterator(tokenizer(text), ngrams)])</span><br><span class="line">        output = model(text, torch.tensor([<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output.argmax(<span class="number">1</span>).item() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">ex_text_str = <span class="string">"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \</span></span><br><span class="line"><span class="string">    enduring the season’s worst weather conditions on Sunday at The \</span></span><br><span class="line"><span class="string">    Open on his way to a closing 75 at Royal Portrush, which \</span></span><br><span class="line"><span class="string">    considering the wind and the rain was a respectable showing. \</span></span><br><span class="line"><span class="string">    Thursday’s first round at the WGC-FedEx St. Jude Invitational \</span></span><br><span class="line"><span class="string">    was another story. With temperatures in the mid-80s and hardly any \</span></span><br><span class="line"><span class="string">    wind, the Spaniard was 13 strokes better in a flawless round. \</span></span><br><span class="line"><span class="string">    Thanks to his best putting performance on the PGA Tour, Rahm \</span></span><br><span class="line"><span class="string">    finished with an 8-under 62 for a three-stroke lead, which \</span></span><br><span class="line"><span class="string">    was even more impressive considering he’d never played the \</span></span><br><span class="line"><span class="string">    front nine at TPC Southwind."</span></span><br><span class="line"></span><br><span class="line">vocab = train_dataset.get_vocab()</span><br><span class="line">model = model.to(<span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"This is a %s news"</span> %ag_news_label[predict(ex_text_str, model, vocab, <span class="number">2</span>)])</span><br></pre></td></tr></table></figure>
<pre><code>This is a Sports news
</code></pre><p>This is a Sports news</p>
<p>You can find the code examples displayed in this note<br><code>here &lt;https://github.com/pytorch/text/tree/master/examples/text_classification&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Reinforcement-Learning</title>
    <url>/2020/07/25/Pytorch-Reinforcement-Learning/</url>
    <content><![CDATA[<p>Pytorch-Reinforcement-Learning:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Reinforcement-Learning-DQN-Tutorial"><a href="#Reinforcement-Learning-DQN-Tutorial" class="headerlink" title="Reinforcement Learning (DQN) Tutorial"></a>Reinforcement Learning (DQN) Tutorial</h1><p><strong>Author</strong>: <code>Adam Paszke &lt;https://github.com/apaszke&gt;</code>_</p>
<p>This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent<br>on the CartPole-v0 task from the <code>OpenAI Gym &lt;https://gym.openai.com/&gt;</code>__.</p>
<p><strong>Task</strong></p>
<p>The agent has to decide between two actions - moving the cart left or<br>right - so that the pole attached to it stays upright. You can find an<br>official leaderboard with various algorithms and visualizations at the<br><code>Gym website &lt;https://gym.openai.com/envs/CartPole-v0&gt;</code>__.</p>
<p><img src="https://pytorch.org/tutorials/_images/cartpole.gif" alt></p>
<p>   cartpole</p>
<p>As the agent observes the current state of the environment and chooses<br>an action, the environment <em>transitions</em> to a new state, and also<br>returns a reward that indicates the consequences of the action. In this<br>task, rewards are +1 for every incremental timestep and the environment<br>terminates if the pole falls over too far or the cart moves more then 2.4<br>units away from center. This means better performing scenarios will run<br>for longer duration, accumulating larger return.</p>
<p>The CartPole task is designed so that the inputs to the agent are 4 real<br>values representing the environment state (position, velocity, etc.).<br>However, neural networks can solve the task purely by looking at the<br>scene, so we’ll use a patch of the screen centered on the cart as an<br>input. Because of this, our results aren’t directly comparable to the<br>ones from the official leaderboard - our task is much harder.<br>Unfortunately this does slow down the training, because we have to<br>render all the frames.</p>
<p>Strictly speaking, we will present the state as the difference between<br>the current screen patch and the previous one. This will allow the agent<br>to take the velocity of the pole into account from one image.</p>
<p><strong>Packages</strong></p>
<p>First, let’s import needed packages. Firstly, we need<br><code>gym &lt;https://gym.openai.com/docs&gt;</code>__ for the environment<br>(Install using <code>pip install gym</code>).<br>We’ll also use the following from PyTorch:</p>
<ul>
<li>neural networks (<code>torch.nn</code>)</li>
<li>optimization (<code>torch.optim</code>)</li>
<li>automatic differentiation (<code>torch.autograd</code>)</li>
<li>utilities for vision tasks (<code>torchvision</code> - <code>a separate
package &lt;https://github.com/pytorch/vision&gt;</code>__).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line"><span class="comment"># set up matplotlib</span></span><br><span class="line">is_ipython = <span class="string">'inline'</span> <span class="keyword">in</span> matplotlib.get_backend()</span><br><span class="line"><span class="keyword">if</span> is_ipython:</span><br><span class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="comment"># if gpu is to be used</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><p>We’ll be using experience replay memory for training our DQN. It stores<br>the transitions that the agent observes, allowing us to reuse this data<br>later. By sampling from it randomly, the transitions that build up a<br>batch are decorrelated. It has been shown that this greatly stabilizes<br>and improves the DQN training procedure.</p>
<p>For this, we’re going to need two classses:</p>
<ul>
<li><code>Transition</code> - a named tuple representing a single transition in<br>our environment. It essentially maps (state, action) pairs<br>to their (next_state, reward) result, with the state being the<br>screen difference image as described later on.</li>
<li><code>ReplayMemory</code> - a cyclic buffer of bounded size that holds the<br>transitions observed recently. It also implements a <code>.sample()</code><br>method for selecting a random batch of transitions for training.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>,</span><br><span class="line">                        (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.memory = []</span><br><span class="line">        self.position = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Saves a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.memory) &lt; self.capacity:</span><br><span class="line">            self.memory.append(<span class="literal">None</span>)</span><br><span class="line">        self.memory[self.position] = Transition(*args)</span><br><span class="line">        self.position = (self.position + <span class="number">1</span>) % self.capacity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.memory, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.memory)</span><br></pre></td></tr></table></figure>
<p>Now, let’s define our model. But first, let quickly recap what a DQN is.</p>
<h2 id="DQN-algorithm"><a href="#DQN-algorithm" class="headerlink" title="DQN algorithm"></a>DQN algorithm</h2><p>Our environment is deterministic, so all equations presented here are<br>also formulated deterministically for the sake of simplicity. In the<br>reinforcement learning literature, they would also contain expectations<br>over stochastic transitions in the environment.</p>
<p>Our aim will be to train a policy that tries to maximize the discounted,<br>cumulative reward<br>$R_{t_0} = \sum_{t=t_0}^{\infty} \gamma^{t - t_0} r_t$, where<br>$R_{t_0}$ is also known as the <em>return</em>. The discount,<br>$\gamma$, should be a constant between $0$ and $1$<br>that ensures the sum converges. It makes rewards from the uncertain far<br>future less important for our agent than the ones in the near future<br>that it can be fairly confident about.</p>
<p>The main idea behind Q-learning is that if we had a function<br>$Q^*: State \times Action \rightarrow \mathbb{R}$, that could tell<br>us what our return would be, if we were to take an action in a given<br>state, then we could easily construct a policy that maximizes our<br>rewards:</p>
<p>\begin{align}\pi^<em>(s) = \arg!\max_a \ Q^</em>(s, a)\end{align}</p>
<p>However, we don’t know everything about the world, so we don’t have<br>access to $Q^<em>$. But, since neural networks are universal function<br>approximators, we can simply create one and train it to resemble<br>$Q^</em>$.</p>
<p>For our training update rule, we’ll use a fact that every $Q$<br>function for some policy obeys the Bellman equation:</p>
<p>\begin{align}Q^{\pi}(s, a) = r + \gamma Q^{\pi}(s’, \pi(s’))\end{align}</p>
<p>The difference between the two sides of the equality is known as the<br>temporal difference error, $\delta$:</p>
<p>\begin{align}\delta = Q(s, a) - (r + \gamma \max_a Q(s’, a))\end{align}</p>
<p>To minimise this error, we will use the <code>Huber
loss &lt;https://en.wikipedia.org/wiki/Huber_loss&gt;</code>__. The Huber loss acts<br>like the mean squared error when the error is small, but like the mean<br>absolute error when the error is large - this makes it more robust to<br>outliers when the estimates of $Q$ are very noisy. We calculate<br>this over a batch of transitions, $B$, sampled from the replay<br>memory:</p>
<p>\begin{align}\mathcal{L} = \frac{1}{|B|}\sum_{(s, a, s’, r) \ \in \ B} \mathcal{L}(\delta)\end{align}</p>
<p>\begin{align}\text{where} \quad \mathcal{L}(\delta) = \begin{cases}<br>     \frac{1}{2}{\delta^2}  &amp; \text{for } |\delta| \le 1, \\<br>     |\delta| - \frac{1}{2} &amp; \text{otherwise.}<br>   \end{cases}\end{align}</p>
<p>Q-network</p>
<p>Our model will be a convolutional neural network that takes in the<br>difference between the current and previous screen patches. It has two<br>outputs, representing $Q(s, \mathrm{left})$ and<br>$Q(s, \mathrm{right})$ (where $s$ is the input to the<br>network). In effect, the network is trying to predict the <em>expected return</em> of<br>taking each action given the current input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, w, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Number of Linear input connections depends on output of conv2d layers</span></span><br><span class="line">        <span class="comment"># and therefore the input image size, so compute it.</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv2d_size_out</span><span class="params">(size, kernel_size = <span class="number">5</span>, stride = <span class="number">2</span>)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> (size - (kernel_size - <span class="number">1</span>) - <span class="number">1</span>) // stride  + <span class="number">1</span></span><br><span class="line">        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))</span><br><span class="line">        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))</span><br><span class="line">        linear_input_size = convw * convh * <span class="number">32</span></span><br><span class="line">        self.head = nn.Linear(linear_input_size, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Called with either one element to determine next action, or a batch</span></span><br><span class="line">    <span class="comment"># during optimization. Returns tensor([[left0exp,right0exp]...]).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        <span class="keyword">return</span> self.head(x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<p>Input extraction</p>
<p>The code below are utilities for extracting and processing rendered<br>images from the environment. It uses the <code>torchvision</code> package, which<br>makes it easy to compose image transforms. Once you run the cell it will<br>display an example patch that it extracted.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">resize = T.Compose([T.ToPILImage(),</span><br><span class="line">                    T.Resize(<span class="number">40</span>, interpolation=Image.CUBIC),</span><br><span class="line">                    T.ToTensor()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cart_location</span><span class="params">(screen_width)</span>:</span></span><br><span class="line">    world_width = env.x_threshold * <span class="number">2</span></span><br><span class="line">    scale = screen_width / world_width</span><br><span class="line">    <span class="keyword">return</span> int(env.state[<span class="number">0</span>] * scale + screen_width / <span class="number">2.0</span>)  <span class="comment"># MIDDLE OF CART</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_screen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Returned screen requested by gym is 400x600x3, but is sometimes larger</span></span><br><span class="line">    <span class="comment"># such as 800x1200x3. Transpose it into torch order (CHW).</span></span><br><span class="line">    screen = env.render(mode=<span class="string">'rgb_array'</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Cart is in the lower half, so strip off the top and bottom of the screen</span></span><br><span class="line">    _, screen_height, screen_width = screen.shape</span><br><span class="line">    screen = screen[:, int(screen_height*<span class="number">0.4</span>):int(screen_height * <span class="number">0.8</span>)]</span><br><span class="line">    view_width = int(screen_width * <span class="number">0.6</span>)</span><br><span class="line">    cart_location = get_cart_location(screen_width)</span><br><span class="line">    <span class="keyword">if</span> cart_location &lt; view_width // <span class="number">2</span>:</span><br><span class="line">        slice_range = slice(view_width)</span><br><span class="line">    <span class="keyword">elif</span> cart_location &gt; (screen_width - view_width // <span class="number">2</span>):</span><br><span class="line">        slice_range = slice(-view_width, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        slice_range = slice(cart_location - view_width // <span class="number">2</span>,</span><br><span class="line">                            cart_location + view_width // <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Strip off the edges, so that we have a square image centered on a cart</span></span><br><span class="line">    screen = screen[:, :, slice_range]</span><br><span class="line">    <span class="comment"># Convert to float, rescale, convert to torch tensor</span></span><br><span class="line">    <span class="comment"># (this doesn't require a copy)</span></span><br><span class="line">    screen = np.ascontiguousarray(screen, dtype=np.float32) / <span class="number">255</span></span><br><span class="line">    screen = torch.from_numpy(screen)</span><br><span class="line">    <span class="comment"># Resize, and add a batch dimension (BCHW)</span></span><br><span class="line">    <span class="keyword">return</span> resize(screen).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env.reset()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(get_screen().cpu().squeeze(<span class="number">0</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy(),</span><br><span class="line">           interpolation=<span class="string">'none'</span>)</span><br><span class="line">plt.title(<span class="string">'Example extracted screen'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/25/Pytorch-Reinforcement-Learning/output_8_0.png" alt="png"></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Hyperparameters and utilities</p>
<p>This cell instantiates our model and its optimizer, and defines some<br>utilities:</p>
<ul>
<li><code>select_action</code> - will select an action accordingly to an epsilon<br>greedy policy. Simply put, we’ll sometimes use our model for choosing<br>the action, and sometimes we’ll just sample one uniformly. The<br>probability of choosing a random action will start at <code>EPS_START</code><br>and will decay exponentially towards <code>EPS_END</code>. <code>EPS_DECAY</code><br>controls the rate of the decay.</li>
<li><code>plot_durations</code> - a helper for plotting the durations of episodes,<br>along with an average over the last 100 episodes (the measure used in<br>the official evaluations). The plot will be underneath the cell<br>containing the main training loop, and will update after every<br>episode.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.999</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get screen size so that we can initialize layers correctly based on shape</span></span><br><span class="line"><span class="comment"># returned from AI gym. Typical dimensions at this point are close to 3x40x90</span></span><br><span class="line"><span class="comment"># which is the result of a clamped and down-scaled render buffer in get_screen()</span></span><br><span class="line">init_screen = get_screen()</span><br><span class="line">_, _, screen_height, screen_width = init_screen.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get number of actions from gym action space</span></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line"></span><br><span class="line">policy_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.RMSprop(policy_net.parameters())</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * \</span><br><span class="line">        math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(state).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">episode_durations = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_durations</span><span class="params">()</span>:</span></span><br><span class="line">    plt.figure(<span class="number">2</span>)</span><br><span class="line">    plt.clf()</span><br><span class="line">    durations_t = torch.tensor(episode_durations, dtype=torch.float)</span><br><span class="line">    plt.title(<span class="string">'Training...'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Episode'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Duration'</span>)</span><br><span class="line">    plt.plot(durations_t.numpy())</span><br><span class="line">    <span class="comment"># Take 100 episode averages and plot them too</span></span><br><span class="line">    <span class="keyword">if</span> len(durations_t) &gt;= <span class="number">100</span>:</span><br><span class="line">        means = durations_t.unfold(<span class="number">0</span>, <span class="number">100</span>, <span class="number">1</span>).mean(<span class="number">1</span>).view(<span class="number">-1</span>)</span><br><span class="line">        means = torch.cat((torch.zeros(<span class="number">99</span>), means))</span><br><span class="line">        plt.plot(means.numpy())</span><br><span class="line"></span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line">    <span class="keyword">if</span> is_ipython:</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">        display.display(plt.gcf())</span><br></pre></td></tr></table></figure>
<p>Training loop</p>
<p>Finally, the code for training our model.</p>
<p>Here, you can find an <code>optimize_model</code> function that performs a<br>single step of the optimization. It first samples a batch, concatenates<br>all the tensors into a single one, computes $Q(s_t, a_t)$ and<br>$V(s_{t+1}) = \max_a Q(s_{t+1}, a)$, and combines them into our<br>loss. By defition we set $V(s) = 0$ if $s$ is a terminal<br>state. We also use a target network to compute $V(s_{t+1})$ for<br>added stability. The target network has its weights kept frozen most of<br>the time, but is updated with the policy network’s weights every so often.<br>This is usually a set number of steps but we shall use episodes for<br>simplicity.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    <span class="comment"># Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for</span></span><br><span class="line">    <span class="comment"># detailed explanation). This converts batch-array of Transitions</span></span><br><span class="line">    <span class="comment"># to Transition of batch-arrays.</span></span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute a mask of non-final states and concatenate the batch elements</span></span><br><span class="line">    <span class="comment"># (a final state would've been the one after which simulation ended)</span></span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span> s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">                                          batch.next_state)), device=device, dtype=torch.bool)</span><br><span class="line">    non_final_next_states = torch.cat([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state</span><br><span class="line">                                                <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>])</span><br><span class="line">    state_batch = torch.cat(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.cat(batch.reward)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q(s_t, a) - the model computes Q(s_t), then we select the</span></span><br><span class="line">    <span class="comment"># columns of actions taken. These are the actions which would've been taken</span></span><br><span class="line">    <span class="comment"># for each batch state according to policy_net</span></span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute V(s_&#123;t+1&#125;) for all next states.</span></span><br><span class="line">    <span class="comment"># Expected values of actions for non_final_next_states are computed based</span></span><br><span class="line">    <span class="comment"># on the "older" target_net; selecting their best reward with max(1)[0].</span></span><br><span class="line">    <span class="comment"># This is merged based on the mask, such that we'll have either the expected</span></span><br><span class="line">    <span class="comment"># state value or 0 in case the state was final.</span></span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE, device=device)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    <span class="comment"># Compute the expected Q values</span></span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Huber loss</span></span><br><span class="line">    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimize the model</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>Below, you can find the main training loop. At the beginning we reset<br>the environment and initialize the <code>state</code> Tensor. Then, we sample<br>an action, execute it, observe the next screen and the reward (always<br>1), and optimize our model once. When the episode ends (our model<br>fails), we restart the loop.</p>
<p>Below, <code>num_episodes</code> is set small. You should download<br>the notebook and run lot more epsiodes, such as 300+ for meaningful<br>duration improvements.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_episodes = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    <span class="comment"># Initialize the environment and state</span></span><br><span class="line">    env.reset()</span><br><span class="line">    last_screen = get_screen()</span><br><span class="line">    current_screen = get_screen()</span><br><span class="line">    state = current_screen - last_screen</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        <span class="comment"># Select and perform an action</span></span><br><span class="line">        action = select_action(state)</span><br><span class="line">        _, reward, done, _ = env.step(action.item())</span><br><span class="line">        reward = torch.tensor([reward], device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Observe new state</span></span><br><span class="line">        last_screen = current_screen</span><br><span class="line">        current_screen = get_screen()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> done:</span><br><span class="line">            next_state = current_screen - last_screen</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Store the transition in memory</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Move to the next state</span></span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform one step of the optimization (on the target network)</span></span><br><span class="line">        optimize_model()</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            episode_durations.append(t + <span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># Update the target network, copying all weights and biases in DQN</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Complete'</span>)</span><br><span class="line">env.render()</span><br><span class="line">env.close()</span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;


Complete



&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre><p>Here is the diagram that illustrates the overall resulting data flow.</p>
<p><img src="https://pytorch.org/tutorials/_images/reinforcement_learning_diagram.jpg" alt></p>
<p>Actions are chosen either randomly or based on a policy, getting the next<br>step sample from the gym environment. We record the results in the<br>replay memory and also run optimization step on every iteration.<br>Optimization picks a random batch from the replay memory to do training of the<br>new policy. “Older” target_net is also used in optimization to compute the<br>expected Q values; it is updated occasionally to keep it current.</p>
<h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample:采样</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-ReinforcementLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-TORCHTEXT的语言翻译</title>
    <url>/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<p>Pytorch-Text-TORCHTEXT的语言翻译:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Language-Translation-with-TorchText"><a href="#Language-Translation-with-TorchText" class="headerlink" title="Language Translation with TorchText"></a>Language Translation with TorchText</h1><p>This tutorial shows how to use several convenience classes of <code>torchtext</code> to preprocess<br>data from a well-known dataset containing sentences in both English and German and use it to<br>train a sequence-to-sequence model with attention that can translate German sentences<br>into English.</p>
<p>It is based off of<br><code>this tutorial &lt;https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code><strong><br>from PyTorch community member <code>Ben Trevett &lt;https://github.com/bentrevett&gt;</code></strong><br>and was created by <code>Seth Weidman &lt;https://github.com/SethHWeidman/&gt;</code>__ with Ben’s permission.</p>
<p>By the end of this tutorial, you will be able to:</p>
<ul>
<li>Preprocess sentences into a commonly-used format for NLP modeling using the following <code>torchtext</code> convenience classes:<ul>
<li><code>TranslationDataset &lt;https://torchtext.readthedocs.io/en/latest/datasets.html#torchtext.datasets.TranslationDataset&gt;</code>__</li>
<li><code>Field &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field&gt;</code>__</li>
<li><code>BucketIterator &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator&gt;</code>__</li>
</ul>
</li>
</ul>
<h2 id="Field-and-TranslationDataset"><a href="#Field-and-TranslationDataset" class="headerlink" title="Field and TranslationDataset"></a><code>Field</code> and <code>TranslationDataset</code></h2><p><code>torchtext</code> has utilities for creating datasets that can be easily<br>iterated through for the purposes of creating a language translation<br>model. One key class is a<br><code>Field &lt;https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L64&gt;</code><strong>,<br>which specifies the way each sentence should be preprocessed, and another is the<br><code>TranslationDataset</code> ; <code>torchtext</code><br>has several such datasets; in this tutorial we’ll use the<br><code>Multi30k dataset &lt;https://github.com/multi30k/dataset&gt;</code></strong>, which contains about<br>30,000 sentences (averaging about 13 words in length) in both English and German.</p>
<p>Note: the tokenization in this tutorial requires <code>Spacy &lt;https://spacy.io&gt;</code><strong><br>We use Spacy because it provides strong support for tokenization in languages<br>other than English. <code>torchtext</code> provides a <code>basic_english</code> tokenizer<br>and supports other tokenizers for English (e.g.<br><code>Moses &lt;https://bitbucket.org/luismsgomes/mosestokenizer/src/default/&gt;</code></strong>)<br>but for language translation - where multiple languages are required -<br>Spacy is your best bet.</p>
<p>To run this tutorial, first install <code>spacy</code> using <code>pip</code> or <code>conda</code>.<br>Next, download the raw data for the English and German Spacy tokenizers:</p>
<p>::</p>
<p>   python -m spacy download en<br>   python -m spacy download de</p>
<p>With Spacy installed, the following code will tokenize each of the sentences<br>in the <code>TranslationDataset</code> based on the tokenizer defined in the <code>Field</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> Multi30k</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, BucketIterator</span><br><span class="line"></span><br><span class="line">SRC = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"de"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">TRG = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"en"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_data, valid_data, test_data = Multi30k.splits(exts = (<span class="string">'.de'</span>, <span class="string">'.en'</span>),</span><br><span class="line">                                                    fields = (SRC, TRG))</span><br></pre></td></tr></table></figure>
<pre><code>downloading training.tar.gz


.data\multi30k\training.tar.gz: 100%|█████████████████████████████████████████████| 1.21M/1.21M [00:35&lt;00:00, 33.8kB/s]


downloading validation.tar.gz


.data\multi30k\validation.tar.gz: 100%|███████████████████████████████████████████| 46.3k/46.3k [00:01&lt;00:00, 35.0kB/s]


downloading mmt_task1_test2016.tar.gz


.data\multi30k\mmt_task1_test2016.tar.gz: 100%|███████████████████████████████████| 66.2k/66.2k [00:02&lt;00:00, 26.5kB/s]
</code></pre><p>Now that we’ve defined <code>train_data</code>, we can see an extremely useful<br>feature of <code>torchtext</code>‘s <code>Field</code>: the <code>build_vocab</code> method<br>now allows us to create the vocabulary associated with each language</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SRC.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br><span class="line">TRG.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>Once these lines of code have been run, <code>SRC.vocab.stoi</code> will  be a<br>dictionary with the tokens in the vocabulary as keys and their<br>corresponding indices as values; <code>SRC.vocab.itos</code> will be the same<br>dictionary with the keys and values swapped. We won’t make extensive<br>use of this fact in this tutorial, but this will likely be useful in<br>other NLP tasks you’ll encounter.</p>
<h2 id="BucketIterator"><a href="#BucketIterator" class="headerlink" title="BucketIterator"></a><code>BucketIterator</code></h2><p>The last <code>torchtext</code> specific feature we’ll use is the <code>BucketIterator</code>,<br>which is easy to use since it takes a <code>TranslationDataset</code> as its<br>first argument. Specifically, as the docs say:<br>Defines an iterator that batches examples of similar lengths together.<br>Minimizes amount of padding needed while producing freshly shuffled<br>batches for each new epoch. See pool for the bucketing procedure used.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_iterator, valid_iterator, test_iterator = BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data),</span><br><span class="line">    batch_size = BATCH_SIZE,</span><br><span class="line">    device = device)</span><br></pre></td></tr></table></figure>
<p>These iterators can be called just like <code>DataLoader</code>s; below, in<br>the <code>train</code> and <code>evaluate</code> functions, they are called simply with:</p>
<p>::</p>
<p>   for i, batch in enumerate(iterator):</p>
<p>Each <code>batch</code> then has <code>src</code> and <code>trg</code> attributes:</p>
<p>::</p>
<p>   src = batch.src<br>   trg = batch.trg</p>
<h2 id="Defining-our-nn-Module-and-Optimizer"><a href="#Defining-our-nn-Module-and-Optimizer" class="headerlink" title="Defining our nn.Module and Optimizer"></a>Defining our <code>nn.Module</code> and <code>Optimizer</code></h2><p>That’s mostly it from a <code>torchtext</code> perspecive: with the dataset built<br>and the iterator defined, the rest of this tutorial simply defines our<br>model as an <code>nn.Module</code>, along with an <code>Optimizer</code>, and then trains it.</p>
<p>Our model specifically, follows the architecture described<br><code>here &lt;https://arxiv.org/abs/1409.0473&gt;</code><strong> (you can find a<br>significantly more commented version<br><code>here &lt;https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code></strong>).</p>
<p>Note: this model is just an example model that can be used for language<br>translation; we choose it because it is a standard model for the task,<br>not because it is the recommended model to use for translation. As you’re<br>likely aware, state-of-the-art models are currently based on Transformers;<br>you can see PyTorch’s capabilities for implementing Transformer layers<br><code>here &lt;https://pytorch.org/docs/stable/nn.html#transformer-layers&gt;</code>__; and<br>in particular, the “attention” used in the model below is different from<br>the multi-headed self-attention present in a transformer model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Tuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 input_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: float)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(enc_hid_dim * <span class="number">2</span>, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(src))</span><br><span class="line"></span><br><span class="line">        outputs, hidden = self.rnn(embedded)</span><br><span class="line"></span><br><span class="line">        hidden = torch.tanh(self.fc(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attn_dim: int)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn_in = (enc_hid_dim * <span class="number">2</span>) + dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn = nn.Linear(self.attn_in, attn_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        src_len = encoder_outputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        repeated_decoder_hidden = decoder_hidden.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, src_len, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        energy = torch.tanh(self.attn(torch.cat((</span><br><span class="line">            repeated_decoder_hidden,</span><br><span class="line">            encoder_outputs),</span><br><span class="line">            dim = <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">        attention = torch.sum(energy, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> F.softmax(attention, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 output_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attention: nn.Module)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.attention = attention</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU((enc_hid_dim * <span class="number">2</span>) + emb_dim, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weighted_encoder_rep</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                              decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        a = self.attention(decoder_hidden, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        a = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = torch.bmm(a, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weighted_encoder_rep</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                input: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        input = input.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(input))</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,</span><br><span class="line">                                                          encoder_outputs)</span><br><span class="line"></span><br><span class="line">        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        embedded = embedded.squeeze(<span class="number">0</span>)</span><br><span class="line">        output = output.squeeze(<span class="number">0</span>)</span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = self.out(torch.cat((output,</span><br><span class="line">                                     weighted_encoder_rep,</span><br><span class="line">                                     embedded), dim = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, decoder_hidden.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 decoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device: torch.device)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                trg: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                teacher_forcing_ratio: float = <span class="number">0.5</span>)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        batch_size = src.shape[<span class="number">1</span>]</span><br><span class="line">        max_len = trg.shape[<span class="number">0</span>]</span><br><span class="line">        trg_vocab_size = self.decoder.output_dim</span><br><span class="line"></span><br><span class="line">        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)</span><br><span class="line"></span><br><span class="line">        encoder_outputs, hidden = self.encoder(src)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># first input to the decoder is the &lt;sos&gt; token</span></span><br><span class="line">        output = trg[<span class="number">0</span>,:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, max_len):</span><br><span class="line">            output, hidden = self.decoder(output, hidden, encoder_outputs)</span><br><span class="line">            outputs[t] = output</span><br><span class="line">            teacher_force = random.random() &lt; teacher_forcing_ratio</span><br><span class="line">            top1 = output.max(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            output = (trg[t] <span class="keyword">if</span> teacher_force <span class="keyword">else</span> top1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = len(SRC.vocab)</span><br><span class="line">OUTPUT_DIM = len(TRG.vocab)</span><br><span class="line"><span class="comment"># ENC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># DEC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># ENC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># DEC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># ATTN_DIM = 64</span></span><br><span class="line"><span class="comment"># ENC_DROPOUT = 0.5</span></span><br><span class="line"><span class="comment"># DEC_DROPOUT = 0.5</span></span><br><span class="line"></span><br><span class="line">ENC_EMB_DIM = <span class="number">32</span></span><br><span class="line">DEC_EMB_DIM = <span class="number">32</span></span><br><span class="line">ENC_HID_DIM = <span class="number">64</span></span><br><span class="line">DEC_HID_DIM = <span class="number">64</span></span><br><span class="line">ATTN_DIM = <span class="number">8</span></span><br><span class="line">ENC_DROPOUT = <span class="number">0.5</span></span><br><span class="line">DEC_DROPOUT = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)</span><br><span class="line"></span><br><span class="line">attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)</span><br><span class="line"></span><br><span class="line">dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)</span><br><span class="line"></span><br><span class="line">model = Seq2Seq(enc, dec, device).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">            nn.init.normal_(param.data, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nn.init.constant_(param.data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.apply(init_weights)</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The model has 1,856,653 trainable parameters
</code></pre><p>Note: when scoring the performance of a language translation model in<br>particular, we have to tell the <code>nn.CrossEntropyLoss</code> function to<br>ignore the indices where the target is simply padding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PAD_IDX = TRG.vocab.stoi[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)</span><br></pre></td></tr></table></figure>
<p>Finally, we can train and evaluate this model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">          optimizer: optim.Optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">          criterion: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          clip: float)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">        src = batch.src</span><br><span class="line">        trg = batch.trg</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(src, trg)</span><br><span class="line"></span><br><span class="line">        output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">        trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">             iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">             criterion: nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">            src = batch.src</span><br><span class="line">            trg = batch.trg</span><br><span class="line"></span><br><span class="line">            output = model(src, trg, <span class="number">0</span>) <span class="comment">#turn off teacher forcing</span></span><br><span class="line"></span><br><span class="line">            output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">            trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time: int,</span></span></span><br><span class="line"><span class="function"><span class="params">               end_time: int)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line">CLIP = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)</span><br><span class="line">    valid_loss = evaluate(model, valid_iterator, criterion)</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train PPL: <span class="subst">&#123;math.exp(train_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. PPL: <span class="subst">&#123;math.exp(valid_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">test_loss = evaluate(model, test_iterator, criterion)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'| Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test PPL: <span class="subst">&#123;math.exp(test_loss):<span class="number">7.3</span>f&#125;</span> |'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 01 | Time: 15m 34s
    Train Loss: 5.681 | Train PPL: 293.100
     Val. Loss: 5.244 |  Val. PPL: 189.491
Epoch: 02 | Time: 18m 13s
    Train Loss: 5.039 | Train PPL: 154.341
     Val. Loss: 5.152 |  Val. PPL: 172.773
Epoch: 03 | Time: 15m 47s
    Train Loss: 4.788 | Train PPL: 120.088
     Val. Loss: 5.044 |  Val. PPL: 155.033
Epoch: 04 | Time: 15m 27s
    Train Loss: 4.619 | Train PPL: 101.417
     Val. Loss: 5.146 |  Val. PPL: 171.670
Epoch: 05 | Time: 16m 16s
    Train Loss: 4.491 | Train PPL:  89.179
     Val. Loss: 5.014 |  Val. PPL: 150.444
Epoch: 06 | Time: 17m 34s
    Train Loss: 4.394 | Train PPL:  80.928
     Val. Loss: 5.014 |  Val. PPL: 150.472
Epoch: 07 | Time: 18m 31s
    Train Loss: 4.306 | Train PPL:  74.153
     Val. Loss: 4.899 |  Val. PPL: 134.150
Epoch: 08 | Time: 18m 48s
    Train Loss: 4.255 | Train PPL:  70.459
     Val. Loss: 4.872 |  Val. PPL: 130.520
Epoch: 09 | Time: 18m 21s
    Train Loss: 4.200 | Train PPL:  66.700
     Val. Loss: 4.807 |  Val. PPL: 122.399
Epoch: 10 | Time: 18m 59s
    Train Loss: 4.142 | Train PPL:  62.920
     Val. Loss: 4.644 |  Val. PPL: 103.988
| Test Loss: 4.650 | Test PPL: 104.534 |
</code></pre><h2 id="Next-steps"><a href="#Next-steps" class="headerlink" title="Next steps"></a>Next steps</h2><ul>
<li>Check out the rest of Ben Trevett’s tutorials using <code>torchtext</code><br><code>here &lt;https://github.com/bentrevett/&gt;</code>__</li>
<li>Stay tuned for a tutorial using other <code>torchtext</code> features along<br>with <code>nn.Transformer</code> for language modeling via next word prediction!</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</title>
    <url>/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<p>Pytorch-Text-使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译:<br><a id="more"></a></p>
<div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div>
* 1.[Learning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/)
    * 1.1[Pytorch-Learning-tensor](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/)
    * 1.2[Pytorch-Learning-autograd](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/)
    * 1.3[Pytorch-Learning-neural_newworks](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/)
    * 1.4[Pytorch-Learning-examples](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/)
    * 1.5[Pytorch-Learning-torch.nn](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/)
    * 1.6[Pytorch-Learning-cifar10tutorial-visualizing](https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/)
* 2.[Image](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/)
    * 2.1[微调TorchVision对象检测](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/)
    * 2.2[计算机视觉迁移学习](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/)
    * 2.3[对抗样本生成](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/
    * 2.4[DCGAN教程](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/)
* 3.[Audio](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/)
    * 3.1[torchaudio](https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/)
* 4.[Text](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/)
    * 4.1[用NN.TRANFORMER和TORCHTEXT进行序列到序列建模](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/)
    * 4.2[使用字符级RNN对名称进行分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/)
    * 4.3[用字符级RNN生成名称](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/)
    * 4.4[使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/)
    * 4.5[TORCHTEXT的文本分类](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)
    * 4.6[TORCHTEXT的语言翻译](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/)
* 5.[ReinforcementLearning](https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/)
* 5.1[Reinforcement-Learning](https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/)


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


NLP From Scratch: Translation with a Sequence to Sequence Network and Attention
*******************************************************************************
**Author**: `Sean Robertson <https: github.com spro practical-pytorch>`_

This is the third and final tutorial on doing "NLP From Scratch", where we
write our own classes and functions to preprocess the data to do our NLP
modeling tasks. We hope after you complete this tutorial that you'll proceed to
learn how `torchtext` can handle much of this preprocessing for you in the
three tutorials immediately following this one.

In this project we will be teaching a neural network to translate from
French to English.

::

    [KEY: > input, = target, < output]

    > il est en train de peindre un tableau .
    = he is painting a picture .
    < he is painting a picture .

    > pourquoi ne pas essayer ce vin delicieux ?
    = why not try that delicious wine ?
    < why not try that delicious wine ?

    > elle n est pas poete mais romanciere .
    = she is not a poet but a novelist .
    < she not not a poet but a novelist .

    > vous etes trop maigre .
    = you re too skinny .
    < you re all alone .

... to varying degrees of success.

This is made possible by the simple but powerful idea of the `sequence
to sequence network <https: arxiv.org abs 1409.3215>`__, in which two
recurrent neural networks work together to transform one sequence to
another. An encoder network condenses an input sequence into a vector,
and a decoder network unfolds that vector into a new sequence.

.. figure:: /_static/img/seq-seq-images/seq2seq.png
   :alt:

To improve upon this model we'll use an `attention
mechanism <https: arxiv.org abs 1409.0473>`__, which lets the decoder
learn to focus over a specific range of the input sequence.

**Recommended Reading:**

I assume you have at least installed PyTorch, know Python, and
understand Tensors:

-  https://pytorch.org/ For installation instructions
-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general
-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview
-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user


It would also be useful to know about Sequence to Sequence networks and
how they work:

-  `Learning Phrase Representations using RNN Encoder-Decoder for
   Statistical Machine Translation <https: arxiv.org abs 1406.1078>`__
-  `Sequence to Sequence Learning with Neural
   Networks <https: arxiv.org abs 1409.3215>`__
-  `Neural Machine Translation by Jointly Learning to Align and
   Translate <https: arxiv.org abs 1409.0473>`__
-  `A Neural Conversational Model <https: arxiv.org abs 1506.05869>`__

You will also find the previous tutorials on
:doc:`/intermediate/char_rnn_classification_tutorial`
and :doc:`/intermediate/char_rnn_generation_tutorial`
helpful as those concepts are very similar to the Encoder and Decoder
models, respectively.

And for more, read the papers that introduced these topics:

-  `Learning Phrase Representations using RNN Encoder-Decoder for
   Statistical Machine Translation <https: arxiv.org abs 1406.1078>`__
-  `Sequence to Sequence Learning with Neural
   Networks <https: arxiv.org abs 1409.3215>`__
-  `Neural Machine Translation by Jointly Learning to Align and
   Translate <https: arxiv.org abs 1409.0473>`__
-  `A Neural Conversational Model <https: arxiv.org abs 1506.05869>`__


**Requirements**




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>

Loading data files
==================

The data for this project is a set of many thousands of English to
French translation pairs.

`This question on Open Data Stack
Exchange <https: 3888 opendata.stackexchange.com questions dataset-of-sentences-translated-into-many-languages>`__
pointed me to the open translation site https://tatoeba.org/ which has
downloads available at https://tatoeba.org/eng/downloads - and better
yet, someone did the extra work of splitting language pairs into
individual text files here: https://www.manythings.org/anki/

The English to French pairs are too big to include in the repo, so
download to ``data/eng-fra.txt`` before continuing. The file is a tab
separated list of translation pairs:

::

    I am cold.    J'ai froid.

.. Note::
   Download the data from
   `here <https: download.pytorch.org tutorial data.zip>`_
   and extract it to the current directory.



Similar to the character encoding used in the character-level RNN
tutorials, we will be representing each word in a language as a one-hot
vector, or giant vector of zeros except for a single one (at the index
of the word). Compared to the dozens of characters that might exist in a
language, there are many many more words, so the encoding vector is much
larger. We will however cheat a bit and trim the data to only use a few
thousand words per language.

.. figure:: /_static/img/seq-seq-images/word-encoding.png
   :alt:





We'll need a unique index per word to use as the inputs and targets of
the networks later. To keep track of all this we will use a helper class
called ``Lang`` which has word → index (``word2index``) and index → word
(``index2word``) dictionaries, as well as a count of each word
``word2count`` to use to later replace rare words.





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SOS_token = <span class="number">0</span></span><br><span class="line">EOS_token = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lang</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        self.word2count = &#123;&#125;</span><br><span class="line">        self.index2word = &#123;<span class="number">0</span>: <span class="string">"SOS"</span>, <span class="number">1</span>: <span class="string">"EOS"</span>&#125;</span><br><span class="line">        self.n_words = <span class="number">2</span>  <span class="comment"># Count SOS and EOS</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addSentence</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</span><br><span class="line">            self.addWord(word)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2index:</span><br><span class="line">            self.word2index[word] = self.n_words</span><br><span class="line">            self.word2count[word] = <span class="number">1</span></span><br><span class="line">            self.index2word[self.n_words] = word</span><br><span class="line">            self.n_words += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.word2count[word] += <span class="number">1</span></span><br></pre></td></tr></table></figure>

The files are all in Unicode, to simplify we will turn Unicode
characters to ASCII, make everything lowercase, and trim most
punctuation.





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to</span></span><br><span class="line"><span class="comment"># https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># 在Unicode中，某些字符能够用多个合法的编码表示，在需要比较字符串的程序中使用字符的多种表示会产生问题。 </span></span><br><span class="line"><span class="comment"># 为了修正这个问题，你可以使用unicodedata模块先将文本标准化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Lowercase, trim, and remove non-letter characters</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeString</span><span class="params">(s)</span>:</span></span><br><span class="line">    s = unicodeToAscii(s.lower().strip())</span><br><span class="line">    s = re.sub(<span class="string">r"([.!?])"</span>, <span class="string">r" \1"</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">r"[^a-zA-Z.!?]+"</span>, <span class="string">r" "</span>, s)</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>

To read the data file we will split the file into lines, and then split
lines into pairs. The files are all English → Other Language, so if we
want to translate from Other Language → English I added the ``reverse``
flag to reverse the pairs.





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLangs</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    print(<span class="string">"Reading lines..."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Read the file and split into lines</span></span><br><span class="line">    lines = open(<span class="string">'data/%s-%s.txt'</span> % (lang1, lang2), encoding=<span class="string">'utf-8'</span>).\</span><br><span class="line">        read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split every line into pairs and normalize</span></span><br><span class="line">    pairs = [[normalizeString(s) <span class="keyword">for</span> s <span class="keyword">in</span> l.split(<span class="string">'\t'</span>)] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reverse pairs, make Lang instances</span></span><br><span class="line">    <span class="keyword">if</span> reverse:</span><br><span class="line">        pairs = [list(reversed(p)) <span class="keyword">for</span> p <span class="keyword">in</span> pairs]</span><br><span class="line">        input_lang = Lang(lang2)</span><br><span class="line">        output_lang = Lang(lang1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_lang = Lang(lang1)</span><br><span class="line">        output_lang = Lang(lang2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br></pre></td></tr></table></figure>

Since there are a *lot* of example sentences and we want to train
something quickly, we'll trim the data set to only relatively short and
simple sentences. Here the maximum length is 10 words (that includes
ending punctuation) and we're filtering to sentences that translate to
the form "I am" or "He is" etc. (accounting for apostrophes replaced
earlier).





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_LENGTH = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">eng_prefixes = (</span><br><span class="line">    <span class="string">"i am "</span>, <span class="string">"i m "</span>,</span><br><span class="line">    <span class="string">"he is"</span>, <span class="string">"he s "</span>,</span><br><span class="line">    <span class="string">"she is"</span>, <span class="string">"she s "</span>,</span><br><span class="line">    <span class="string">"you are"</span>, <span class="string">"you re "</span>,</span><br><span class="line">    <span class="string">"we are"</span>, <span class="string">"we re "</span>,</span><br><span class="line">    <span class="string">"they are"</span>, <span class="string">"they re "</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPair</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(p[<span class="number">0</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        len(p[<span class="number">1</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        p[<span class="number">1</span>].startswith(eng_prefixes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPairs</span><span class="params">(pairs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [pair <span class="keyword">for</span> pair <span class="keyword">in</span> pairs <span class="keyword">if</span> filterPair(pair)]</span><br></pre></td></tr></table></figure>

The full process for preparing the data is:

-  Read text file and split into lines, split lines into pairs
-  Normalize text, filter by length and content
-  Make word lists from sentences in pairs





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareData</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)</span><br><span class="line">    print(<span class="string">"Read %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    pairs = filterPairs(pairs)</span><br><span class="line">    print(<span class="string">"Trimmed to %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    print(<span class="string">"Counting words..."</span>)</span><br><span class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</span><br><span class="line">        input_lang.addSentence(pair[<span class="number">0</span>])</span><br><span class="line">        output_lang.addSentence(pair[<span class="number">1</span>])</span><br><span class="line">    print(<span class="string">"Counted words:"</span>)</span><br><span class="line">    print(input_lang.name, input_lang.n_words)</span><br><span class="line">    print(output_lang.name, output_lang.n_words)</span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_lang, output_lang, pairs = prepareData(<span class="string">'eng'</span>, <span class="string">'fra'</span>, <span class="literal">True</span>)</span><br><span class="line">print(random.choice(pairs))</span><br></pre></td></tr></table></figure>

    Reading lines...
    Read 135842 sentence pairs
    Trimmed to 10599 sentence pairs
    Counting words...
    Counted words:
    fra 4345
    eng 2803
    ['nous sommes tous sur le meme bateau .', 'we re all in the same boat .']


The Seq2Seq Model
=================

A Recurrent Neural Network, or RNN, is a network that operates on a
sequence and uses its own output as input for subsequent steps.

A `Sequence to Sequence network <https: arxiv.org abs 1409.3215>`__, or
seq2seq network, or `Encoder Decoder
network <https: arxiv.org pdf 1406.1078v3.pdf>`__, is a model
consisting of two RNNs called the encoder and decoder. The encoder reads
an input sequence and outputs a single vector, and the decoder reads
that vector to produce an output sequence.

![](https://pytorch.org/tutorials/_images/seq2seq.png)

Unlike sequence prediction with a single RNN, where every input
corresponds to an output, the seq2seq model frees us from sequence
length and order, which makes it ideal for translation between two
languages.

Consider the sentence "Je ne suis pas le chat noir" → "I am not the
black cat". Most of the words in the input sentence have a direct
translation in the output sentence, but are in slightly different
orders, e.g. "chat noir" and "black cat". Because of the "ne/pas"
construction there is also one more word in the input sentence. It would
be difficult to produce a correct translation directly from the sequence
of input words.

With a seq2seq model the encoder creates a single vector which, in the
ideal case, encodes the "meaning" of the input sequence into a single
vector — a single point in some N dimensional space of sentences.




The Encoder
-----------

The encoder of a seq2seq network is a RNN that outputs some value for
every word from the input sentence. For every input word the encoder
outputs a vector and a hidden state, and uses the hidden state for the
next input word.

![](https://pytorch.org/tutorials/_images/decoder-network.png)






<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size)</span>:</span></span><br><span class="line">        super(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = embedded</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>

The Decoder
-----------

The decoder is another RNN that takes the encoder output vector(s) and
outputs a sequence of words to create the translation.




Simple Decoder


In the simplest seq2seq decoder we use only last output of the encoder.
This last output is sometimes called the *context vector* as it encodes
context from the entire sequence. This context vector is used as the
initial hidden state of the decoder.

At every step of decoding, the decoder is given an input token and
hidden state. The initial input token is the start-of-string ``<SOS>``
token, and the first hidden state is the context vector (the encoder's
last hidden state).

![](https://pytorch.org/tutorials/_images/attention-decoder-network.png)






<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(DecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line">        self.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        output = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        output = self.softmax(self.out(output[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>

I encourage you to train and observe the results of this model, but to
save space we'll be going straight for the gold and introducing the
Attention Mechanism.




Attention Decoder


If only the context vector is passed betweeen the encoder and decoder,
that single vector carries the burden of encoding the entire sentence.

Attention allows the decoder network to "focus" on a different part of
the encoder's outputs for every step of the decoder's own outputs. First
we calculate a set of *attention weights*. These will be multiplied by
the encoder output vectors to create a weighted combination. The result
(called ``attn_applied`` in the code) should contain information about
that specific part of the input sequence, and thus help the decoder
choose the right output words.

.. figure:: https://i.imgur.com/1152PYf.png
   :alt:

Calculating the attention weights is done with another feed-forward
layer ``attn``, using the decoder's input and hidden state as inputs.
Because there are sentences of all sizes in the training data, to
actually create and train this layer we have to choose a maximum
sentence length (input length, for encoder outputs) that it can apply
to. Sentences of the maximum length will use all the attention weights,
while shorter sentences will only use the first few.

.. figure:: /_static/img/seq-seq-images/attention-decoder-network.png
   :alt:






<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnDecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">        super(AttnDecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.dropout_p = dropout_p</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</span><br><span class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(self.dropout_p)</span><br><span class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</span><br><span class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden, encoder_outputs)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line"></span><br><span class="line">        attn_weights = F.softmax(</span><br><span class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)), dim=<span class="number">1</span>)</span><br><span class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>)</span><br><span class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>

<div class="alert alert-info"><h4>Note</h4><p>There are other forms of attention that work around the length
  limitation by using a relative position approach. Read about "local
  attention" in `Effective Approaches to Attention-based Neural Machine
  Translation <https: arxiv.org abs 1508.04025>`__.</https:></p></div>

<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-Training-Data"><a href="#Preparing-Training-Data" class="headerlink" title="Preparing Training Data"></a>Preparing Training Data</h2><p>To train, for each pair we will need an input tensor (indexes of the<br>words in the input sentence) and target tensor (indexes of the words in<br>the target sentence). While creating these vectors we will append the<br>EOS token to both sequences.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">indexesFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [lang.word2index[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    indexes = indexesFromSentence(lang, sentence)</span><br><span class="line">    indexes.append(EOS_token)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(indexes, dtype=torch.long, device=device).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorsFromPair</span><span class="params">(pair)</span>:</span></span><br><span class="line">    input_tensor = tensorFromSentence(input_lang, pair[<span class="number">0</span>])</span><br><span class="line">    target_tensor = tensorFromSentence(output_lang, pair[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> (input_tensor, target_tensor)</span><br></pre></td></tr></table></figure>
<h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>To train we run the input sentence through the encoder, and keep track<br>of every output and the latest hidden state. Then the decoder is given<br>the <code>&lt;SOS&gt;</code> token as its first input, and the last hidden state of the<br>encoder as its first hidden state.</p>
<p>“Teacher forcing” is the concept of using the real target outputs as<br>each next input, instead of using the decoder’s guess as the next input.<br>Using teacher forcing causes it to converge faster but <code>when the trained
network is exploited, it may exhibit
instability &lt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&amp;rep=rep1&amp;type=pdf&gt;</code>__.</p>
<p>You can observe outputs of teacher-forced networks that read with<br>coherent grammar but wander far from the correct translation -<br>intuitively it has learned to represent the output grammar and can “pick<br>up” the meaning once the teacher tells it the first few words, but it<br>has not properly learned how to create the sentence from the translation<br>in the first place.</p>
<p>Because of the freedom PyTorch’s autograd gives us, we can randomly<br>choose to use teacher forcing or not with a simple if statement. Turn<br><code>teacher_forcing_ratio</code> up to use more of it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">teacher_forcing_ratio = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.zero_grad()</span><br><span class="line">    decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    input_length = input_tensor.size(<span class="number">0</span>)</span><br><span class="line">    target_length = target_tensor.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">        encoder_output, encoder_hidden = encoder(</span><br><span class="line">            input_tensor[ei], encoder_hidden)</span><br><span class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoder_input = torch.tensor([[SOS_token]], device=device)</span><br><span class="line"></span><br><span class="line">    decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">    use_teacher_forcing = <span class="literal">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_teacher_forcing:</span><br><span class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            decoder_input = target_tensor[di]  <span class="comment"># Teacher forcing</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            topv, topi = decoder_output.topk(<span class="number">1</span>)</span><br><span class="line">            decoder_input = topi.squeeze().detach()  <span class="comment"># detach from history as input</span></span><br><span class="line"></span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            <span class="keyword">if</span> decoder_input.item() == EOS_token:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.step()</span><br><span class="line">    decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item() / target_length</span><br></pre></td></tr></table></figure>
<p>This is a helper function to print time elapsed and estimated time<br>remaining given the current time and progress %.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asMinutes</span><span class="params">(s)</span>:</span></span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since, percent)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    es = s / (percent)</span><br><span class="line">    rs = es - s</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%s (- %s)'</span> % (asMinutes(s), asMinutes(rs))</span><br></pre></td></tr></table></figure>
<p>The whole training process looks like this:</p>
<ul>
<li>Start a timer</li>
<li>Initialize optimizers and criterion</li>
<li>Create set of training pairs</li>
<li>Start empty losses array for plotting</li>
</ul>
<p>Then we call <code>train</code> many times and occasionally print the progress (%<br>of examples, time so far, estimated time) and average loss.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainIters</span><span class="params">(encoder, decoder, n_iters, print_every=<span class="number">1000</span>, plot_every=<span class="number">100</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    plot_losses = []</span><br><span class="line">    print_loss_total = <span class="number">0</span>  <span class="comment"># Reset every print_every</span></span><br><span class="line">    plot_loss_total = <span class="number">0</span>  <span class="comment"># Reset every plot_every</span></span><br><span class="line"></span><br><span class="line">    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)</span><br><span class="line">    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)</span><br><span class="line">    training_pairs = [tensorsFromPair(random.choice(pairs))</span><br><span class="line">                      <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iters)]</span><br><span class="line">    criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">        training_pair = training_pairs[iter - <span class="number">1</span>]</span><br><span class="line">        input_tensor = training_pair[<span class="number">0</span>]</span><br><span class="line">        target_tensor = training_pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        loss = train(input_tensor, target_tensor, encoder,</span><br><span class="line">                     decoder, encoder_optimizer, decoder_optimizer, criterion)</span><br><span class="line">        print_loss_total += loss</span><br><span class="line">        plot_loss_total += loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">            print_loss_avg = print_loss_total / print_every</span><br><span class="line">            print_loss_total = <span class="number">0</span></span><br><span class="line">            print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start, iter / n_iters),</span><br><span class="line">                                         iter, iter / n_iters * <span class="number">100</span>, print_loss_avg))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">            plot_loss_avg = plot_loss_total / plot_every</span><br><span class="line">            plot_losses.append(plot_loss_avg)</span><br><span class="line">            plot_loss_total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    showPlot(plot_losses)</span><br></pre></td></tr></table></figure>
<h2 id="Plotting-results"><a href="#Plotting-results" class="headerlink" title="Plotting results"></a>Plotting results</h2><p>Plotting is done with matplotlib, using the array of loss values<br><code>plot_losses</code> saved while training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.switch_backend(<span class="string">'agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPlot</span><span class="params">(points)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># this locator puts ticks at regular intervals</span></span><br><span class="line">    loc = ticker.MultipleLocator(base=<span class="number">0.2</span>)</span><br><span class="line">    ax.yaxis.set_major_locator(loc)</span><br><span class="line">    plt.plot(points)</span><br></pre></td></tr></table></figure>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Evaluation is mostly the same as training, but there are no targets so<br>we simply feed the decoder’s predictions back to itself for each step.<br>Every time it predicts a word we add it to the output string, and if it<br>predicts the EOS token we stop there. We also store the decoder’s<br>attention outputs for display later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(encoder, decoder, sentence, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_tensor = tensorFromSentence(input_lang, sentence)</span><br><span class="line">        input_length = input_tensor.size()[<span class="number">0</span>]</span><br><span class="line">        encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">            encoder_output, encoder_hidden = encoder(input_tensor[ei],</span><br><span class="line">                                                     encoder_hidden)</span><br><span class="line">            encoder_outputs[ei] += encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        decoder_input = torch.tensor([[SOS_token]], device=device)  <span class="comment"># SOS</span></span><br><span class="line"></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">        decoded_words = []</span><br><span class="line">        decoder_attentions = torch.zeros(max_length, max_length)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(max_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            decoder_attentions[di] = decoder_attention.data</span><br><span class="line">            topv, topi = decoder_output.data.topk(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> topi.item() == EOS_token:</span><br><span class="line">                decoded_words.append(<span class="string">'&lt;EOS&gt;'</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                decoded_words.append(output_lang.index2word[topi.item()])</span><br><span class="line"></span><br><span class="line">            decoder_input = topi.squeeze().detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoded_words, decoder_attentions[:di + <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>We can evaluate random sentences from the training set and print out the<br>input, target, and output to make some subjective quality judgements:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateRandomly</span><span class="params">(encoder, decoder, n=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        pair = random.choice(pairs)</span><br><span class="line">        print(<span class="string">'&gt;'</span>, pair[<span class="number">0</span>])</span><br><span class="line">        print(<span class="string">'='</span>, pair[<span class="number">1</span>])</span><br><span class="line">        output_words, attentions = evaluate(encoder, decoder, pair[<span class="number">0</span>])</span><br><span class="line">        output_sentence = <span class="string">' '</span>.join(output_words)</span><br><span class="line">        print(<span class="string">'&lt;'</span>, output_sentence)</span><br><span class="line">        print(<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Training-and-Evaluating"><a href="#Training-and-Evaluating" class="headerlink" title="Training and Evaluating"></a>Training and Evaluating</h1><p>With all these helper functions in place (it looks like extra work, but<br>it makes it easier to run multiple experiments) we can actually<br>initialize a network and start training.</p>
<p>Remember that the input sentences were heavily filtered. For this small<br>dataset we can use relatively small networks of 256 hidden nodes and a<br>single GRU layer. After about 40 minutes on a MacBook CPU we’ll get some<br>reasonable results.</p>
<p>.. Note::<br>   If you run this notebook you can train, interrupt the kernel,<br>   evaluate, and continue training later. Comment out the lines where the<br>   encoder and decoder are initialized and run <code>trainIters</code> again.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line">encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)</span><br><span class="line">attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">trainIters(encoder1, attn_decoder1, <span class="number">75000</span>, print_every=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>15m 15s (- 213m 42s) (5000 6%) 2.8335
36m 35s (- 237m 48s) (10000 13%) 2.3080
60m 22s (- 241m 29s) (15000 20%) 1.9768
86m 20s (- 237m 26s) (20000 26%) 1.7509
103m 50s (- 207m 41s) (25000 33%) 1.5515
123m 36s (- 185m 24s) (30000 40%) 1.3811
141m 30s (- 161m 43s) (35000 46%) 1.2262
161m 12s (- 141m 3s) (40000 53%) 1.1208
180m 57s (- 120m 38s) (45000 60%) 1.0367
195m 38s (- 97m 49s) (50000 66%) 0.9097
206m 41s (- 75m 9s) (55000 73%) 0.8348
217m 46s (- 54m 26s) (60000 80%) 0.7563
228m 59s (- 35m 13s) (65000 86%) 0.7075
246m 41s (- 17m 37s) (70000 93%) 0.6615
263m 49s (- 0m 0s) (75000 100%) 0.6048
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluateRandomly(encoder1, attn_decoder1)</span><br></pre></td></tr></table></figure>
<pre><code>&gt; tu m ennuies .
= you re annoying .
&lt; you re embarrassing . &lt;EOS&gt;

&gt; il est maintenant etudiant a la fac .
= he s now a college student .
&lt; he s now a student now . &lt;EOS&gt;

&gt; je ne suis pas votre ami .
= i m not your friend .
&lt; i m not your friend . &lt;EOS&gt;

&gt; je suis tres fatiguee par le dur labeur .
= i am very tired from the hard work .
&lt; i am very interested in the next . . &lt;EOS&gt;

&gt; ce sont des illets .
= they re carnations .
&lt; they re carnations . &lt;EOS&gt;

&gt; il est toujours en train de se plaindre .
= he is constantly complaining .
&lt; he is always complaining . &lt;EOS&gt;

&gt; je suis submerge de travail .
= i am swamped with work .
&lt; i am swamped with work . &lt;EOS&gt;

&gt; tu es mon meilleur ami .
= you re my best friend .
&lt; you are my best friend . &lt;EOS&gt;

&gt; je vous suis reconnaissant pour votre aide .
= i am grateful to you for your help .
&lt; i am grateful for your help . &lt;EOS&gt;

&gt; je vais te conter un secret .
= i m going to tell you a secret .
&lt; i m going to tell you a secret . &lt;EOS&gt;
</code></pre><h2 id="Visualizing-Attention"><a href="#Visualizing-Attention" class="headerlink" title="Visualizing Attention"></a>Visualizing Attention</h2><p>A useful property of the attention mechanism is its highly interpretable<br>outputs. Because it is used to weight specific encoder outputs of the<br>input sequence, we can imagine looking where the network is focused most<br>at each time step.</p>
<p>You could simply run <code>plt.matshow(attentions)</code> to see attention output<br>displayed as a matrix, with the columns being input steps and rows being<br>output steps:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output_words, attentions = evaluate(</span><br><span class="line">    encoder1, attn_decoder1, <span class="string">"je suis trop froid ."</span>)</span><br><span class="line">plt.matshow(attentions.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x2424cc9a438&gt;
</code></pre><p>For a better viewing experience we will do the extra work of adding axes<br>and labels:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showAttention</span><span class="params">(input_sentence, output_words, attentions)</span>:</span></span><br><span class="line">    <span class="comment"># Set up figure with colorbar</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    cax = ax.matshow(attentions.numpy(), cmap=<span class="string">'bone'</span>)</span><br><span class="line">    fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up axes</span></span><br><span class="line">    ax.set_xticklabels([<span class="string">''</span>] + input_sentence.split(<span class="string">' '</span>) +</span><br><span class="line">                       [<span class="string">'&lt;EOS&gt;'</span>], rotation=<span class="number">90</span>)</span><br><span class="line">    ax.set_yticklabels([<span class="string">''</span>] + output_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show label at every tick</span></span><br><span class="line">    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateAndShowAttention</span><span class="params">(input_sentence)</span>:</span></span><br><span class="line">    output_words, attentions = evaluate(</span><br><span class="line">        encoder1, attn_decoder1, input_sentence)</span><br><span class="line">    print(<span class="string">'input ='</span>, input_sentence)</span><br><span class="line">    print(<span class="string">'output ='</span>, <span class="string">' '</span>.join(output_words))</span><br><span class="line">    showAttention(input_sentence, output_words, attentions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle a cinq ans de moins que moi ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle est trop petit ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"je ne crains pas de mourir ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"c est un jeune directeur plein de talent ."</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input = elle a cinq ans de moins que moi .
output = she is five years younger than me . &lt;EOS&gt;
input = elle est trop petit .
output = she is too short . &lt;EOS&gt;
input = je ne crains pas de mourir .
output = i m not scared to die . &lt;EOS&gt;


C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator
  # Remove the CWD from sys.path while we load stuff.
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator
  # This is added back by InteractiveShellApp.init_path()
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.


input = c est un jeune directeur plein de talent .
output = he s a talented and . &lt;EOS&gt;
</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset</p>
<ul>
<li>Another language pair</li>
<li>Human → Machine (e.g. IOT commands)</li>
<li>Chat → Response</li>
<li>Question → Answer</li>
</ul>
</li>
<li><p>Replace the embeddings with pre-trained word embeddings such as word2vec or<br>GloVe</p>
</li>
<li>Try with more layers, more hidden units, and more sentences. Compare<br>the training time and results.</li>
<li><p>If you use a translation file where pairs have two of the same phrase<br>(<code>I am test \t I am test</code>), you can use this as an autoencoder. Try<br>this:</p>
<ul>
<li>Train as an autoencoder</li>
<li>Save only the Encoder network</li>
<li>Train a new Decoder for translation from there</li>
</ul>
</li>
</ul>
</SOS></https:></https:></https:></https:></https:></https:></https:></https:></https:></https:></https:></https:></https:></https:></https:>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-使用字符级RNN对名称进行分类</title>
    <url>/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>Pytorch-Text-使用字符级RNN对名称进行分类:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Classifying Names with a Character-Level RNN</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>We will be building and training a basic character-level RNN to classify<br>words. This tutorial, along with the following two, show how to do<br>preprocess data for NLP modeling “from scratch”, in particular not using<br>many of the convenience functions of <code>torchtext</code>, so you can see how<br>preprocessing for NLP modeling works at a low level.</p>
<p>A character-level RNN reads words as a series of characters -<br>outputting a prediction and “hidden state” at each step, feeding its<br>previous hidden state into each next step. We take the final prediction<br>to be the output, i.e. which class the word belongs to.</p>
<p>Specifically, we’ll train on a few thousand surnames from 18 languages<br>of origin, and predict which language a name is from based on the<br>spelling:</p>
<p>::</p>
<pre><code>$ python predict.py Hinton
(-0.47) Scottish
(-1.52) English
(-3.57) Irish

$ python predict.py Schmidhuber
(-0.19) German
(-2.48) Czech
(-2.68) Dutch
</code></pre><p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about RNNs and how they work:</p>
<ul>
<li><code>The Unreasonable Effectiveness of Recurrent Neural
Networks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li>
<li><code>Understanding LSTM
Networks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li>
</ul>
<h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>Included in the <code>data/names</code> directory are 18 text files named as<br>“[Language].txt”. Each file contains a bunch of names, one name per<br>line, mostly romanized (but we still need to convert from Unicode to<br>ASCII).</p>
<p>We’ll end up with a dictionary of lists of names per language,<br><code>{language: [names ...]}</code>. The generic variables “category” and “line”<br>(for language and name in our case) are used for later extensibility.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># glob.glob返回所有匹配的文件路径列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line">print(findFiles(<span class="string">'data/names/*.txt'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></span><br><span class="line">n_letters = len(all_letters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># 在Unicode中，某些字符能够用多个合法的编码表示，在需要比较字符串的程序中使用字符的多种表示会产生问题。 </span></span><br><span class="line"><span class="comment"># 为了修正这个问题，你可以使用unicodedata模块先将文本标准化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">print(unicodeToAscii(<span class="string">'Ślusàrski'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of names per language</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;data/names\\Arabic.txt&#39;, &#39;data/names\\Chinese.txt&#39;, &#39;data/names\\Czech.txt&#39;, &#39;data/names\\Dutch.txt&#39;, &#39;data/names\\English.txt&#39;, &#39;data/names\\French.txt&#39;, &#39;data/names\\German.txt&#39;, &#39;data/names\\Greek.txt&#39;, &#39;data/names\\Irish.txt&#39;, &#39;data/names\\Italian.txt&#39;, &#39;data/names\\Japanese.txt&#39;, &#39;data/names\\Korean.txt&#39;, &#39;data/names\\Polish.txt&#39;, &#39;data/names\\Portuguese.txt&#39;, &#39;data/names\\Russian.txt&#39;, &#39;data/names\\Scottish.txt&#39;, &#39;data/names\\Spanish.txt&#39;, &#39;data/names\\Vietnamese.txt&#39;]
Slusarski
</code></pre><p>Now we have <code>category_lines</code>, a dictionary mapping each category<br>(language) to a list of lines (names). We also kept track of<br><code>all_categories</code> (just a list of languages) and <code>n_categories</code> for<br>later reference.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(category_lines[<span class="string">'Italian'</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;Abandonato&#39;, &#39;Abatangelo&#39;, &#39;Abatantuono&#39;, &#39;Abate&#39;, &#39;Abategiovanni&#39;]
</code></pre><h2 id="Turning-Names-into-Tensors"><a href="#Turning-Names-into-Tensors" class="headerlink" title="Turning Names into Tensors"></a>Turning Names into Tensors</h2><p>Now that we have all the names organized, we need to turn them into<br>Tensors to make any use of them.</p>
<p>To represent a single letter, we use a “one-hot vector” of size<br><code>&lt;1 x n_letters&gt;</code>. A one-hot vector is filled with 0s except for a 1<br>at index of the current letter, e.g. <code>&quot;b&quot; = &lt;0 1 0 0 0 ...&gt;</code>.</p>
<p>To make a word we join a bunch of those into a 2D matrix<br><code>&lt;line_length x 1 x n_letters&gt;</code>.</p>
<p>That extra 1 dimension is because PyTorch assumes everything is in<br>batches - we’re just using a batch size of 1 here.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find letter index from all_letters, e.g. "a" = 0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToIndex</span><span class="params">(letter)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> all_letters.find(letter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToTensor</span><span class="params">(letter)</span>:</span></span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_letters)</span><br><span class="line">    tensor[<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a line into a &lt;line_length x 1 x n_letters&gt;,</span></span><br><span class="line"><span class="comment"># or an array of one-hot letter vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lineToTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li, letter <span class="keyword">in</span> enumerate(line):</span><br><span class="line">        tensor[li][<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line">print(letterToTensor(<span class="string">'J'</span>))</span><br><span class="line"></span><br><span class="line">print(lineToTensor(<span class="string">'Jones'</span>).size())</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0.]])
torch.Size([5, 1, 57])
</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>Before autograd, creating a recurrent neural network in Torch involved<br>cloning the parameters of a layer over several timesteps. The layers<br>held hidden state and gradients which are now entirely handled by the<br>graph itself. This means you can implement a RNN in a very “pure” way,<br>as regular feed-forward layers.</p>
<p>This RNN module (mostly copied from <code>the PyTorch for Torch users
tutorial &lt;https://pytorch.org/tutorials/beginner/former_torchies/
nn_tutorial.html#example-2-recurrent-net&gt;</code>__)<br>is just 2 linear layers which operate on an input and hidden state, with<br>a LogSoftmax layer after the output.<br><img src="https://i.imgur.com/Z2xbySO.png" alt></p>
<h1 id="Torch-cat"><a href="#Torch-cat" class="headerlink" title="Torch.cat()"></a>Torch.cat()</h1><p>cat是concatnate的意思：拼接，联系在一起。</p>
<p>先说cat( )的普通用法</p>
<p>如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C &#x3D; torch.cat( (A,B),0 )  #按维数0拼接（竖着拼）</span><br><span class="line">C &#x3D; torch.cat( (A,B),1 )  #按维数1拼接（横着拼）</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; A&#x3D;torch.ones(2,3)    #2x3的张量（矩阵）                                     </span><br><span class="line">&gt;&gt;&gt; A</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.]])</span><br><span class="line">&gt;&gt;&gt; B&#x3D;2*torch.ones(4,3)  #4x3的张量（矩阵）                                    </span><br><span class="line">&gt;&gt;&gt; B</span><br><span class="line">tensor([[ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,B),0)  #按维数0（行）拼接</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">         [ 1.,  1.,  1.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([6, 3])</span><br><span class="line">&gt;&gt;&gt; D&#x3D;2*torch.ones(2,4) #2x4的张量（矩阵）</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,D),1)#按维数1（列）拼接</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],</span><br><span class="line">        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([2, 7])</span><br></pre></td></tr></table></figure><br>其次，cat还可以把list中的tensor拼接起来。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line"></span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line">len(x1)</span><br><span class="line">&gt;&gt;3</span><br><span class="line"></span><br><span class="line">x1</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">[tensor([[2.],</span><br><span class="line">         [4.],</span><br><span class="line">         [6]]),tensor([[2.],[4.],[6.]]),tensor([[2.],[4.],[6.]])]</span><br><span class="line"></span><br><span class="line">x2 &#x3D; &#x3D; torch.cat((x1),1)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">&gt;&gt;tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br><span class="line"></span><br><span class="line">type(x1)</span><br><span class="line">&gt;&gt; list</span><br></pre></td></tr></table></figure><br>上面的代码可以合成一行来写：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x2 &#x3D; torch.cat([x*2 for i in range(1,4)])</span><br><span class="line">x2</span><br><span class="line">&gt;&gt;</span><br><span class="line">tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(input_size + hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        combined = torch.cat((input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(combined)</span><br><span class="line">        output = self.i2o(combined)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br><span class="line"></span><br><span class="line">n_hidden = <span class="number">128</span></span><br><span class="line">rnn = RNN(n_letters, n_hidden, n_categories)</span><br></pre></td></tr></table></figure>
<p>To run a step of this network we need to pass an input (in our case, the<br>Tensor for the current letter) and a previous hidden state (which we<br>initialize as zeros at first). We’ll get back the output (probability of<br>each language) and a next hidden state (which we keep for the next<br>step).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = letterToTensor(<span class="string">'A'</span>)</span><br><span class="line">hidden =torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input, hidden)</span><br></pre></td></tr></table></figure>
<p>For the sake of efficiency we don’t want to be creating a new Tensor for<br>every step, so we will use <code>lineToTensor</code> instead of<br><code>letterToTensor</code> and use slices. This could be further optimized by<br>pre-computing batches of Tensors.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = lineToTensor(<span class="string">'Albert'</span>)</span><br><span class="line">hidden = torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input[<span class="number">0</span>], hidden)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[-2.8013, -3.0086, -2.8838, -2.8652, -2.8300, -2.7883, -2.8614, -2.9069,
         -2.9787, -2.8336, -2.9085, -2.9029, -2.9565, -2.8683, -2.9269, -2.9332,
         -2.9334, -2.8689]], grad_fn=&lt;LogSoftmaxBackward&gt;)
</code></pre><p>As you can see the output is a <code>&lt;1 x n_categories&gt;</code> Tensor, where<br>every item is the likelihood of that category (higher is more likely).</p>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>Before going into training we should make a few helper functions. The<br>first is to interpret the output of the network, which we know to be a<br>likelihood of each category. We can use <code>Tensor.topk</code> to get the index<br>of the greatest value:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryFromOutput</span><span class="params">(output)</span>:</span></span><br><span class="line">    top_n, top_i = output.topk(<span class="number">1</span>)</span><br><span class="line">    category_i = top_i[<span class="number">0</span>].item()</span><br><span class="line">    <span class="keyword">return</span> all_categories[category_i], category_i</span><br><span class="line"></span><br><span class="line">print(categoryFromOutput(output))</span><br></pre></td></tr></table></figure>
<pre><code>(&#39;French&#39;, 5)
</code></pre><p>We will also want a quick way to get a training example (a name and its<br>language):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)</span><br><span class="line">    line_tensor = lineToTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category, line, category_tensor, line_tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    print(<span class="string">'category ='</span>, category, <span class="string">'/ line ='</span>, line)</span><br></pre></td></tr></table></figure>
<pre><code>category = German / line = Reiter
category = Spanish / line = Roldan
category = Vietnamese / line = Lieu
category = Japanese / line = Maita
category = Polish / line = Wojda
category = Greek / line = Forakis
category = Italian / line = Voltolini
category = Scottish / line = Hill
category = Portuguese / line = Nunes
category = Scottish / line = Wilson
</code></pre><h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>Now all it takes to train this network is show it a bunch of examples,<br>have it make guesses, and tell it if it’s wrong.</p>
<p>For the loss function <code>nn.NLLLoss</code> is appropriate, since the last<br>layer of the RNN is <code>nn.LogSoftmax</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br></pre></td></tr></table></figure>
<p>Each loop of training will:</p>
<ul>
<li>Create input and target tensors</li>
<li>Create a zeroed initial hidden state</li>
<li><p>Read each letter in and</p>
<ul>
<li>Keep hidden state for next letter</li>
</ul>
</li>
<li><p>Compare final output to target</p>
</li>
<li>Back-propagate</li>
<li>Return the output and loss</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.005</span> <span class="comment"># If you set this too high, it might explode. If too low, it might not learn</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    loss = criterion(output, category_tensor)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add parameters' gradients to their values, multiplied by learning rate</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item()</span><br></pre></td></tr></table></figure>
<p>Now we just have to run that with a bunch of examples. Since the<br><code>train</code> function returns both the output and loss we can print its<br>guesses and also keep track of loss for plotting. Since there are 1000s<br>of examples we print only every <code>print_every</code> examples, and take an<br>average of the loss.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep track of losses for plotting</span></span><br><span class="line">current_loss = <span class="number">0</span></span><br><span class="line">all_losses = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output, loss = train(category_tensor, line_tensor)</span><br><span class="line">    current_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print iter number, loss, name and guess</span></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        guess, guess_i = categoryFromOutput(output)</span><br><span class="line">        correct = <span class="string">'✓'</span> <span class="keyword">if</span> guess == category <span class="keyword">else</span> <span class="string">'✗ (%s)'</span> % category</span><br><span class="line">        print(<span class="string">'%d %d%% (%s) %.4f %s / %s %s'</span> % (iter, iter / n_iters * <span class="number">100</span>, timeSince(start), loss, line, guess, correct))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add current loss avg to list of losses</span></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(current_loss / plot_every)</span><br><span class="line">        current_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>5000 5% (0m 26s) 2.2285 Tsen / Chinese ✓
10000 10% (0m 45s) 1.7232 Ursler / Dutch ✗ (German)
15000 15% (1m 2s) 3.3629 Power / German ✗ (Irish)
20000 20% (1m 19s) 1.1055 Ferreiro / Portuguese ✓
25000 25% (1m 37s) 1.1813 Do / Vietnamese ✓
30000 30% (1m 53s) 1.9952 Pak / Chinese ✗ (Korean)
35000 35% (2m 11s) 1.0685 Zientek / Czech ✗ (Polish)
40000 40% (2m 34s) 0.3656 Arnoni / Italian ✓
45000 45% (2m 56s) 2.5408 Schuchardt / Czech ✗ (German)
50000 50% (3m 19s) 0.9137 Ellwood / English ✓
55000 55% (3m 43s) 2.6915 Griffiths / Greek ✗ (English)
60000 60% (4m 5s) 0.0363 Quach / Vietnamese ✓
65000 65% (4m 27s) 0.1474 Rijnders / Dutch ✓
70000 70% (4m 49s) 1.8646 Clements / Portuguese ✗ (English)
75000 75% (5m 13s) 0.3696 Bobienski / Polish ✓
80000 80% (5m 37s) 1.0411 Klerx / Dutch ✓
85000 85% (5m 58s) 2.3457 Maria / Spanish ✗ (Portuguese)
90000 90% (6m 24s) 0.5750 Echevarria / Spanish ✓
95000 95% (6m 47s) 0.0762 Ohmiya / Japanese ✓
100000 100% (7m 9s) 2.5785 Kock / Czech ✗ (German)
</code></pre><h2 id="Plotting-the-Results"><a href="#Plotting-the-Results" class="headerlink" title="Plotting the Results"></a>Plotting the Results</h2><p>Plotting the historical loss from <code>all_losses</code> shows the network<br>learning:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x18c353c04a8&gt;]
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_25_1.png" alt="png"></p>
<h1 id="Evaluating-the-Results"><a href="#Evaluating-the-Results" class="headerlink" title="Evaluating the Results"></a>Evaluating the Results</h1><p>To see how well the network performs on different categories, we will<br>create a confusion matrix, indicating for every actual language (rows)<br>which language the network guesses (columns). To calculate the confusion<br>matrix a bunch of samples are run through the network with<br><code>evaluate()</code>, which is the same as <code>train()</code> minus the backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Keep track of correct guesses in a confusion matrix</span></span><br><span class="line">confusion = torch.zeros(n_categories, n_categories)</span><br><span class="line">n_confusion = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Just return an output given a line</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># Go through a bunch of examples and record which are correctly guessed</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_confusion):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output = evaluate(line_tensor)</span><br><span class="line">    guess, guess_i = categoryFromOutput(output)</span><br><span class="line">    category_i = all_categories.index(category)</span><br><span class="line">    confusion[category_i][guess_i] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize by dividing every row by its sum</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_categories):</span><br><span class="line">    confusion[i] = confusion[i] / confusion[i].sum()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up plot</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(confusion.numpy())</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up axes</span></span><br><span class="line">ax.set_xticklabels([<span class="string">''</span>] + all_categories, rotation=<span class="number">90</span>)</span><br><span class="line">ax.set_yticklabels([<span class="string">''</span>] + all_categories)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Force label at every tick</span></span><br><span class="line">ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sphinx_gallery_thumbnail_number = 2</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:34: UserWarning: FixedFormatter should only be used together with FixedLocator
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_27_1.png" alt="png"></p>
<p>You can pick out bright spots off the main axis that show which<br>languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish<br>for Italian. It seems to do very well with Greek, and very poorly with<br>English (perhaps because of overlap with other languages).</p>
<h2 id="Running-on-User-Input"><a href="#Running-on-User-Input" class="headerlink" title="Running on User Input"></a>Running on User Input</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(input_line, n_predictions=<span class="number">3</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'\n&gt; %s'</span> % input_line)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = evaluate(lineToTensor(input_line))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get top N categories</span></span><br><span class="line">        topv, topi = output.topk(n_predictions, <span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">        predictions = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_predictions):</span><br><span class="line">            value = topv[<span class="number">0</span>][i].item()</span><br><span class="line">            category_index = topi[<span class="number">0</span>][i].item()</span><br><span class="line">            print(<span class="string">'(%.2f) %s'</span> % (value, all_categories[category_index]))</span><br><span class="line">            predictions.append([value, all_categories[category_index]])</span><br><span class="line"></span><br><span class="line">predict(<span class="string">'Dovesky'</span>)</span><br><span class="line">predict(<span class="string">'Jackson'</span>)</span><br><span class="line">predict(<span class="string">'Satoshi'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&gt; Dovesky
(-0.80) Russian
(-1.53) Czech
(-1.98) English

&gt; Jackson
(-0.33) Scottish
(-1.99) English
(-3.31) Russian

&gt; Satoshi
(-0.90) Italian
(-1.65) Japanese
(-2.18) Arabic
</code></pre><p>The final versions of the scripts <code>in the Practical PyTorch
repo &lt;https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification&gt;</code>__<br>split the above code into a few files:</p>
<ul>
<li><code>data.py</code> (loads files)</li>
<li><code>model.py</code> (defines the RNN)</li>
<li><code>train.py</code> (runs training)</li>
<li><code>predict.py</code> (runs <code>predict()</code> with command line arguments)</li>
<li><code>server.py</code> (serve prediction as a JSON API with bottle.py)</li>
</ul>
<p>Run <code>train.py</code> to train and save the network.</p>
<p>Run <code>predict.py</code> with a name to view predictions:</p>
<p>::</p>
<pre><code>$ python predict.py Hazaki
(-0.42) Japanese
(-1.39) Polish
(-3.51) Czech
</code></pre><p>Run <code>server.py</code> and visit <a href="http://localhost:5533/Yourname" target="_blank" rel="noopener">http://localhost:5533/Yourname</a> to get JSON<br>output of predictions.</p>
<h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset of line -&gt; category, for example:</p>
<ul>
<li>Any word -&gt; language</li>
<li>First name -&gt; gender</li>
<li>Character name -&gt; writer</li>
<li>Page title -&gt; blog or subreddit</li>
</ul>
</li>
<li><p>Get better results with a bigger and/or better shaped network</p>
<ul>
<li>Add more linear layers</li>
<li>Try the <code>nn.LSTM</code> and <code>nn.GRU</code> layers</li>
<li>Combine multiple of these RNNs as a higher level network</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</title>
    <url>/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>Pytorch-Text-用NN.TRANFORMER和TORCHTEXT进行序列到序列建模:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText"><a href="#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText" class="headerlink" title="Sequence-to-Sequence Modeling with nn.Transformer and TorchText"></a>Sequence-to-Sequence Modeling with nn.Transformer and TorchText</h1><p>This is a tutorial on how to train a sequence-to-sequence model<br>that uses the<br><code>nn.Transformer &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&gt;</code>__ module.</p>
<p>PyTorch 1.2 release includes a standard transformer module based on the<br>paper <code>Attention is All You
Need &lt;https://arxiv.org/pdf/1706.03762.pdf&gt;</code><strong>. The transformer model<br>has been proved to be superior in quality for many sequence-to-sequence<br>problems while being more parallelizable. The <code>nn.Transformer</code> module<br>relies entirely on an attention mechanism (another module recently<br>implemented as <code>nn.MultiheadAttention &lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&gt;</code></strong>) to draw global dependencies<br>between input and output. The <code>nn.Transformer</code> module is now highly<br>modularized such that a single component (like <code>nn.TransformerEncoder &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&gt;</code>__<br>in this tutorial) can be easily adapted/composed.</p>
<p><img src="https://pytorch.org/tutorials/_images/transformer_architecture.jpg" alt></p>
<h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>In this tutorial, we train <code>nn.TransformerEncoder</code> model on a<br>language modeling task. The language modeling task is to assign a<br>probability for the likelihood of a given word (or a sequence of words)<br>to follow a sequence of words. A sequence of tokens are passed to the embedding<br>layer first, followed by a positional encoding layer to account for the order<br>of the word (see the next paragraph for more details). The<br><code>nn.TransformerEncoder</code> consists of multiple layers of<br><code>nn.TransformerEncoderLayer &lt;https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer&gt;</code>__. Along with the input sequence, a square<br>attention mask is required because the self-attention layers in<br><code>nn.TransformerEncoder</code> are only allowed to attend the earlier positions in<br>the sequence. For the language modeling task, any tokens on the future<br>positions should be masked. To have the actual words, the output<br>of <code>nn.TransformerEncoder</code> model is sent to the final Linear<br>layer, which is followed by a log-Softmax function.</p>
<hr>
<h2 id="torch-triu-input-diagonal-0-out-None-→-Tensor"><a href="#torch-triu-input-diagonal-0-out-None-→-Tensor" class="headerlink" title="torch.triu(input, diagonal=0, out=None) → Tensor"></a>torch.triu(input, diagonal=0, out=None) → Tensor</h2><p>返回矩阵上三角部分，其余部分定义为0。</p>
<p>Parameters:</p>
<ul>
<li>input (Tensor) – the input tensor</li>
<li>diagonal (int, optional) – the diagonal to consider</li>
<li>out (Tensor, optional) – the output tensor</li>
</ul>
<hr>
<ul>
<li>如果diagonal为空，输入矩阵保留主对角线与主对角线以上的元素；</li>
<li>如果diagonal为正数n，输入矩阵保留主对角线与主对角线以上除去n行的元素；</li>
<li>如果diagonal为负数-n，输入矩阵保留主对角线与主对角线以上与主对角线下方h行对角线的元素；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a &#x3D; torch.randn(3, 3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.3480, -0.5211, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;1)</span><br><span class="line">tensor([[ 0.0000,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000,  0.0000,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;-1)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000, -0.5211, -0.4573]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="pytorch-mask-filled用法"><a href="#pytorch-mask-filled用法" class="headerlink" title="pytorch mask_filled用法"></a>pytorch mask_filled用法</h2><p>将 mask必须是一个 ByteTensor 而且shape必须和 a一样 并且元素只能是 0或者1 ，是将 mask中为1的 元素所在的索引，在a中相同的的索引处替换为 value  ,mask value必须同为tensor<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.tensor([1,0,2,3])</span><br><span class="line">a.masked_fill(mask &#x3D; torch.ByteTensor([1,1,0,0]), value&#x3D;torch.tensor(-1e9))</span><br><span class="line"></span><br><span class="line">tensor([-1.0000e+09, -1.0000e+09,  2.0000e+00,  3.0000e+00])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ntoken, ninp, nhead, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(TransformerModel, self).__init__()</span><br><span class="line">        <span class="keyword">from</span> torch.nn <span class="keyword">import</span> TransformerEncoder, TransformerEncoderLayer</span><br><span class="line">        self.model_type = <span class="string">'Transformer'</span></span><br><span class="line">        self.src_mask = <span class="literal">None</span></span><br><span class="line">        self.pos_encoder = PositionalEncoding(ninp, dropout)</span><br><span class="line">        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)</span><br><span class="line">        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        self.ninp = ninp</span><br><span class="line">        self.decoder = nn.Linear(ninp, ntoken)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_square_subsequent_mask</span><span class="params">(self, sz)</span>:</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.src_mask <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self.src_mask.size(<span class="number">0</span>) != len(src):</span><br><span class="line">            device = src.device</span><br><span class="line">            mask = self._generate_square_subsequent_mask(len(src)).to(device)</span><br><span class="line">            self.src_mask = mask</span><br><span class="line"></span><br><span class="line">        src = self.encoder(src) * math.sqrt(self.ninp)</span><br><span class="line">        src = self.pos_encoder(src)</span><br><span class="line">        output = self.transformer_encoder(src, self.src_mask)</span><br><span class="line">        output = self.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p><code>PositionalEncoding</code> module injects some information about the<br>relative or absolute position of the tokens in the sequence. The<br>positional encodings have the same dimension as the embeddings so that<br>the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of<br>different frequencies.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="Load-and-batch-data"><a href="#Load-and-batch-data" class="headerlink" title="Load and batch data"></a>Load and batch data</h2><p>The training process uses Wikitext-2 dataset from <code>torchtext</code>. The<br>vocab object is built based on the train dataset and is used to numericalize<br>tokens into tensors. Starting from sequential data, the <code>batchify()</code><br>function arranges the dataset into columns, trimming off any tokens remaining<br>after the data has been divided into batches of size <code>batch_size</code>.<br>For instance, with the alphabet as the sequence (total length of 26)<br>and a batch size of 4, we would divide the alphabet into 4 sequences of<br>length 6:</p>
<p>\begin{align}\begin{bmatrix}<br>  \text{A} &amp; \text{B} &amp; \text{C} &amp; \ldots &amp; \text{X} &amp; \text{Y} &amp; \text{Z}<br>  \end{bmatrix}<br>  \Rightarrow<br>  \begin{bmatrix}<br>  \begin{bmatrix}\text{A} \\ \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{G} \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{M} \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{S} \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix}<br>  \end{bmatrix}\end{align}</p>
<p>These columns are treated as independent by the model, which means that<br>the dependence of <code>G</code> and <code>F</code> can not be learned, but allows more<br>efficient batch processing.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line">TEXT = torchtext.data.Field(tokenize=get_tokenizer(<span class="string">"basic_english"</span>),</span><br><span class="line">                            init_token=<span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">                            eos_token=<span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">                            lower=<span class="literal">True</span>)</span><br><span class="line">train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)</span><br><span class="line">TEXT.build_vocab(train_txt)</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    data = TEXT.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line">    <span class="comment"># Divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure>
<p>Functions to generate input and target sequence<br><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del>~~~~</p>
<p><code>get_batch()</code> function generates the input and target sequence for<br>the transformer model. It subdivides the source data into chunks of<br>length <code>bptt</code>. For the language modeling task, the model needs the<br>following words as <code>Target</code>. For example, with a <code>bptt</code> value of 2,<br>we’d get the following two Variables for <code>i</code> = 0:</p>
<p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/_static/img/transformer_input_target.png" alt></p>
<p>It should be noted that the chunks are along dimension 0, consistent<br>with the <code>S</code> dimension in the Transformer model. The batch dimension<br><code>N</code> is along dimension 1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bptt = <span class="number">35</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(source, i)</span>:</span></span><br><span class="line">    seq_len = min(bptt, len(source) - <span class="number">1</span> - i)</span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure>
<h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The model is set up with the hyperparameter below. The vocab size is<br>equal to the length of the vocab object.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ntokens = len(TEXT.vocab.stoi) <span class="comment"># the size of vocabulary</span></span><br><span class="line">emsize = <span class="number">200</span> <span class="comment"># embedding dimension</span></span><br><span class="line">nhid = <span class="number">200</span> <span class="comment"># the dimension of the feedforward network model in nn.TransformerEncoder</span></span><br><span class="line">nlayers = <span class="number">2</span> <span class="comment"># the number of nn.TransformerEncoderLayer in nn.TransformerEncoder</span></span><br><span class="line">nhead = <span class="number">2</span> <span class="comment"># the number of heads in the multiheadattention models</span></span><br><span class="line">dropout = <span class="number">0.2</span> <span class="comment"># the dropout value</span></span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Run-the-model"><a href="#Run-the-model" class="headerlink" title="Run the model"></a>Run the model</h2><p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>is applied to track the loss and<br><code>SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD&gt;</code></strong><br>implements stochastic gradient descent method as the optimizer. The initial<br>learning rate is set to 5.0. <code>StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR&gt;</code><strong> is<br>applied to adjust the learn rate through epochs. During the<br>training, we use<br><code>nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_&gt;</code></strong><br>function to scale all the gradient together to prevent exploding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">lr = <span class="number">5.0</span> <span class="comment"># learning rate</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train() <span class="comment"># Turn on the train mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> enumerate(range(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output.view(<span class="number">-1</span>, ntokens), targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        log_interval = <span class="number">200</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            print(<span class="string">'| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | '</span></span><br><span class="line">                  <span class="string">'lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | '</span></span><br><span class="line">                  <span class="string">'loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, math.exp(cur_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(eval_model, data_source)</span>:</span></span><br><span class="line">    eval_model.eval() <span class="comment"># Turn on the evaluation mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, data_source.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            data, targets = get_batch(data_source, i)</span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            output_flat = output.view(<span class="number">-1</span>, ntokens)</span><br><span class="line">            total_loss += len(data) * criterion(output_flat, targets).item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / (len(data_source) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Loop over epochs. Save the model if the validation loss is the best<br>we’ve seen so far. Adjust the learning rate after each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_val_loss = float(<span class="string">"inf"</span>)</span><br><span class="line">epochs = <span class="number">3</span> <span class="comment"># The number of epochs</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train()</span><br><span class="line">    val_loss = evaluate(model, val_data)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line">    print(<span class="string">'| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | '</span></span><br><span class="line">          <span class="string">'valid ppl &#123;:8.2f&#125;'</span>.format(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                     val_loss, math.exp(val_loss)))</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">        best_val_loss = val_loss</span><br><span class="line">        best_model = model</span><br><span class="line"></span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\torch\optim\lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  &quot;please use `get_last_lr()`.&quot;, UserWarning)


| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 2144.51 | loss  8.03 | ppl  3063.62
| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 1757.64 | loss  6.78 | ppl   876.40
| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 1952.50 | loss  6.37 | ppl   584.33
| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 1758.54 | loss  6.22 | ppl   501.41
| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 1863.17 | loss  6.11 | ppl   450.46
| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 1836.79 | loss  6.10 | ppl   443.68
| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 1795.15 | loss  6.05 | ppl   422.43
| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 1902.40 | loss  6.05 | ppl   425.49
| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 1827.80 | loss  5.96 | ppl   386.04
| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 1814.35 | loss  5.96 | ppl   388.23
| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 1845.57 | loss  5.85 | ppl   346.70
| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 1798.20 | loss  5.90 | ppl   364.75
| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 1925.91 | loss  5.90 | ppl   365.12
| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 1709.37 | loss  5.80 | ppl   331.20
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 5766.46s | valid loss  5.76 | valid ppl   317.98
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 1861.38 | loss  5.80 | ppl   330.78
| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 1823.89 | loss  5.77 | ppl   320.61
| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 1858.68 | loss  5.60 | ppl   270.75
| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 1831.67 | loss  5.63 | ppl   278.20
| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 1785.64 | loss  5.59 | ppl   266.78
| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 1808.74 | loss  5.61 | ppl   273.60
| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 1926.79 | loss  5.63 | ppl   278.59
| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 1821.76 | loss  5.67 | ppl   289.91
| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 1797.76 | loss  5.59 | ppl   267.51
| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 1764.14 | loss  5.62 | ppl   274.93
| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 1895.50 | loss  5.51 | ppl   246.68
| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 1793.70 | loss  5.58 | ppl   265.49
| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 1824.77 | loss  5.58 | ppl   266.10
| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 1891.16 | loss  5.51 | ppl   247.06
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 5701.96s | valid loss  5.59 | valid ppl   268.83
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 1818.12 | loss  5.55 | ppl   256.50
| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 1945.54 | loss  5.55 | ppl   257.43
| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 1616.58 | loss  5.36 | ppl   213.27
| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 1689.64 | loss  5.42 | ppl   224.94
| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 1608.36 | loss  5.38 | ppl   216.13
| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 1630.67 | loss  5.41 | ppl   222.92
| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 1686.69 | loss  5.44 | ppl   229.34
| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 1647.30 | loss  5.48 | ppl   239.58
| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 1633.68 | loss  5.40 | ppl   221.38
| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 1613.04 | loss  5.43 | ppl   228.14
| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 1608.29 | loss  5.32 | ppl   204.06
| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 1624.74 | loss  5.40 | ppl   220.31
| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 1609.86 | loss  5.41 | ppl   224.71
| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 1559.15 | loss  5.33 | ppl   207.22
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 6002.66s | valid loss  5.51 | valid ppl   247.75
-----------------------------------------------------------------------------------------
</code></pre><h2 id="Evaluate-the-model-with-the-test-dataset"><a href="#Evaluate-the-model-with-the-test-dataset" class="headerlink" title="Evaluate the model with the test dataset"></a>Evaluate the model with the test dataset</h2><p>Apply the best model to check the result with the test dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br><span class="line">print(<span class="string">'| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure>
<pre><code>=========================================================================================
| End of training | test loss  5.43 | test ppl   227.23
=========================================================================================
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parallelizable:可并行化</span><br><span class="line">superior:优越</span><br><span class="line">mechanism:机制</span><br><span class="line">modularized:模块化</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>mvvm原理</title>
    <url>/2020/09/05/mvvm%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>根据发布订阅模式,数据劫持等实现的模拟 mvvm 框架</p>
<a id="more"></a>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><h2 id="mvvm-html"><a href="#mvvm-html" class="headerlink" title="mvvm.html"></a>mvvm.html</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang&#x3D;&quot;en&quot;&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">        &lt;meta charset&#x3D;&quot;UTF-8&quot; &#x2F;&gt;</span><br><span class="line">        &lt;meta name&#x3D;&quot;viewport&quot; content&#x3D;&quot;width&#x3D;device-width, initial-scale&#x3D;1.0&quot; &#x2F;&gt;</span><br><span class="line">        &lt;title&gt;使用自己写的MVVM&lt;&#x2F;title&gt;</span><br><span class="line">        &lt;script src&#x3D;&quot;watcher.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">        &lt;script src&#x3D;&quot;observer.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">        &lt;script src&#x3D;&quot;compile.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">        &lt;script src&#x3D;&quot;mvvm.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">    &lt;&#x2F;head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">        &lt;div id&#x3D;&quot;app&quot;&gt;</span><br><span class="line">            &lt;input type&#x3D;&quot;text&quot; v-model&#x3D;&quot;message&quot; &#x2F;&gt;</span><br><span class="line">            &#123;&#123;message&#125;&#125;</span><br><span class="line">        &lt;&#x2F;div&gt;</span><br><span class="line">        &lt;script&gt;</span><br><span class="line">            &#x2F;&#x2F; vue中实现双向绑定 1.模板编译 2.数据劫持</span><br><span class="line">            let vm &#x3D; new MVVM(&#123;</span><br><span class="line">                el: &quot;#app&quot;,</span><br><span class="line">                data: &#123;</span><br><span class="line">                    message: &quot;hello world&quot;,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;);</span><br><span class="line">        &lt;&#x2F;script&gt;</span><br><span class="line">    &lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>
<h2 id="mvvm-js"><a href="#mvvm-js" class="headerlink" title="mvvm.js"></a>mvvm.js</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MVVM &#123;</span><br><span class="line">    constructor(options) &#123;</span><br><span class="line">        this.$el &#x3D; options.el;</span><br><span class="line">        this.$data &#x3D; options.data;</span><br><span class="line">        if (this.$el) &#123;</span><br><span class="line">            new Observer(this.$data);</span><br><span class="line">            this.proxyData(this.$data);</span><br><span class="line">            new Compile(this.el2node(this.$el), this);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; el转换为dom</span><br><span class="line">    el2node(el) &#123;</span><br><span class="line">        return this.isElementNode(el) ? el : document.querySelector(el);</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 判断是不是dom节点</span><br><span class="line">    isElementNode(node) &#123;</span><br><span class="line">        &#x2F;&#x2F; 1是元素节点 2是属性节点 3是文本节点</span><br><span class="line">        return node.nodeType &#x3D;&#x3D;&#x3D; 1;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 数据代理</span><br><span class="line">    proxyData(data) &#123;</span><br><span class="line">        Object.keys(data).forEach((key) &#x3D;&gt; &#123;</span><br><span class="line">            Object.defineProperty(this, key, &#123;</span><br><span class="line">                get() &#123;</span><br><span class="line">                    return data[key];</span><br><span class="line">                &#125;,</span><br><span class="line">                set(newValue) &#123;</span><br><span class="line">                    data[key] &#x3D; newValue;</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="compile-js"><a href="#compile-js" class="headerlink" title="compile.js"></a>compile.js</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Compile &#123;</span><br><span class="line">    constructor(el, vm) &#123;</span><br><span class="line">        this.el &#x3D; el;</span><br><span class="line">        this.vm &#x3D; vm;</span><br><span class="line">        if (this.el) &#123;</span><br><span class="line">            &#x2F;&#x2F; 1. 先把真实dom移入内存中 fragment</span><br><span class="line">            let fragment &#x3D; this.node2fragment(this.el);</span><br><span class="line">            &#x2F;&#x2F; 2. 编译 &#x3D;&gt; 提取想要的元素节点v-model和文本节点 &#123;&#123;&#125;&#125;</span><br><span class="line">            this.compile(fragment);</span><br><span class="line">            &#x2F;&#x2F; 3. 把编译后的fragment再塞回页面中</span><br><span class="line">            this.el.appendChild(fragment);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    isElementNode(node) &#123;</span><br><span class="line">        &#x2F;&#x2F; 1是元素节点 2是属性节点 3是文本节点</span><br><span class="line">        return node.nodeType &#x3D;&#x3D;&#x3D; 1;</span><br><span class="line">    &#125;</span><br><span class="line">    isDirective(name) &#123;</span><br><span class="line">        return name.includes(&quot;v-&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    node2fragment(el) &#123;</span><br><span class="line">        &#x2F;&#x2F; 将el中内容全部放在内存中</span><br><span class="line">        &#x2F;&#x2F; 文档碎片</span><br><span class="line">        let fragment &#x3D; document.createDocumentFragment();</span><br><span class="line">        let firstChild;</span><br><span class="line">        while ((firstChild &#x3D; el.firstChild)) &#123;</span><br><span class="line">            fragment.appendChild(firstChild);</span><br><span class="line">        &#125;</span><br><span class="line">        return fragment; &#x2F;&#x2F; 内存中的节点</span><br><span class="line">    &#125;</span><br><span class="line">    compile(fragment) &#123;</span><br><span class="line">        &#x2F;&#x2F; 需要递归</span><br><span class="line">        let childNodes &#x3D; fragment.childNodes;</span><br><span class="line">        Array.from(childNodes).forEach((node) &#x3D;&gt; &#123;</span><br><span class="line">            if (this.isElementNode(node)) &#123;</span><br><span class="line">                &#x2F;&#x2F; 是元素节点的话</span><br><span class="line">                this.compileElement(node);</span><br><span class="line">                &#x2F;&#x2F; 递归</span><br><span class="line">                this.compile(node);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                &#x2F;&#x2F; 是文本节点的话</span><br><span class="line">                this.compileText(node);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    compileElement(node) &#123;</span><br><span class="line">        &#x2F;&#x2F; 带v-model的</span><br><span class="line">        let attrs &#x3D; node.attributes;</span><br><span class="line">        &#x2F;&#x2F; console.log(attrs); &#x2F;&#x2F; NamedNodeMap &#123;0: type, 1: v-model, type: type, v-model: v-model, length: 2&#125;</span><br><span class="line">        &#x2F;&#x2F; console.log(Array.from(attrs)); &#x2F;&#x2F; [type, v-model]</span><br><span class="line">        Array.from(attrs).forEach((attr) &#x3D;&gt; &#123;</span><br><span class="line">            &#x2F;&#x2F; console.log(attr); &#x2F;&#x2F; v-model&#x3D;&quot;message&quot;</span><br><span class="line">            &#x2F;&#x2F; console.log(attr.name); &#x2F;&#x2F; v-model</span><br><span class="line">            &#x2F;&#x2F; console.log(attr.value); &#x2F;&#x2F; message</span><br><span class="line">            let attrName &#x3D; attr.name;</span><br><span class="line">            if (this.isDirective(attrName)) &#123;</span><br><span class="line">                &#x2F;&#x2F; 取到对应的值放到节点中</span><br><span class="line">                let expr &#x3D; attr.value;</span><br><span class="line">                let [, type] &#x3D; attrName.split(&quot;-&quot;);</span><br><span class="line">                CompileUtil[type](node, this.vm, expr);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    compileText(node) &#123;</span><br><span class="line">        &#x2F;&#x2F; 带&#123;&#123;&#125;&#125;的</span><br><span class="line">        let expr &#x3D; node.textContent;</span><br><span class="line">        let reg &#x3D; &#x2F;\&#123;\&#123;([^&#125;]+)\&#125;\&#125;&#x2F;g;</span><br><span class="line">        if (reg.test(expr)) &#123;</span><br><span class="line">            CompileUtil[&quot;text&quot;](node, this.vm, expr);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CompileUtil &#x3D; &#123;</span><br><span class="line">    getVal(vm, expr) &#123;</span><br><span class="line">        expr &#x3D; expr.split(&quot;.&quot;);</span><br><span class="line">        return expr.reduce((prev, next) &#x3D;&gt; &#123;</span><br><span class="line">            return prev[next];</span><br><span class="line">        &#125;, vm.$data);</span><br><span class="line">    &#125;,</span><br><span class="line">    getTextVal(vm, expr) &#123;</span><br><span class="line">        return expr.replace(&#x2F;\&#123;\&#123;([^&#125;]+)\&#125;\&#125;&#x2F;g, (...arguments) &#x3D;&gt; &#123;</span><br><span class="line">            return this.getVal(vm, arguments[1]);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;,</span><br><span class="line">    text(node, vm, expr) &#123;</span><br><span class="line">        let updateFn &#x3D; this.updater[&quot;textUpdater&quot;];</span><br><span class="line">        expr.replace(&#x2F;\&#123;\&#123;([^&#125;]+)\&#125;\&#125;&#x2F;g, (...arguments) &#x3D;&gt; &#123;</span><br><span class="line">            new Watcher(vm, arguments[1], (newValue) &#x3D;&gt; &#123;</span><br><span class="line">                &#x2F;&#x2F; 如果数据变化了,文本节点需要重新获取依赖的属性更新文本中的内容</span><br><span class="line">                updateFn &amp;&amp; updateFn(node, this.getTextVal(vm, expr));</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;);</span><br><span class="line">        updateFn &amp;&amp; updateFn(node, this.getTextVal(vm, expr));</span><br><span class="line">    &#125;,</span><br><span class="line">    setVal(vm, expr, value) &#123;</span><br><span class="line">        expr &#x3D; expr.split(&quot;.&quot;);</span><br><span class="line">        &#x2F;&#x2F; 收敛</span><br><span class="line">        return expr.reduce((prev, next, currentIndex) &#x3D;&gt; &#123;</span><br><span class="line">            if (currentIndex &#x3D;&#x3D;&#x3D; expr.length - 1) &#123;</span><br><span class="line">                return (prev[next] &#x3D; value);</span><br><span class="line">            &#125;</span><br><span class="line">            return prev[next];</span><br><span class="line">        &#125;, vm.$data);</span><br><span class="line">    &#125;,</span><br><span class="line">    model(node, vm, expr) &#123;</span><br><span class="line">        let updateFn &#x3D; this.updater[&quot;modelUpdater&quot;];</span><br><span class="line">        &#x2F;&#x2F; 加一个监控,数据变化了调用watch的callback</span><br><span class="line">        new Watcher(vm, expr, (newValue) &#x3D;&gt; &#123;</span><br><span class="line">            updateFn &amp;&amp; updateFn(node, newValue);</span><br><span class="line">        &#125;);</span><br><span class="line">        node.addEventListener(&quot;input&quot;, (e) &#x3D;&gt; &#123;</span><br><span class="line">            let newValue &#x3D; e.target.value;</span><br><span class="line">            this.setVal(vm, expr, newValue);</span><br><span class="line">        &#125;);</span><br><span class="line">        updateFn &amp;&amp; updateFn(node, this.getVal(vm, expr));</span><br><span class="line">    &#125;,</span><br><span class="line">    updater: &#123;</span><br><span class="line">        &#x2F;&#x2F; 文本更新</span><br><span class="line">        textUpdater(node, value) &#123;</span><br><span class="line">            node.textContent &#x3D; value;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#x2F;&#x2F; 输入框更新</span><br><span class="line">        modelUpdater(node, value) &#123;</span><br><span class="line">            node.value &#x3D; value;</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="observer-js"><a href="#observer-js" class="headerlink" title="observer.js"></a>observer.js</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Observer &#123;</span><br><span class="line">    constructor(data) &#123;</span><br><span class="line">        this.observer(data);</span><br><span class="line">    &#125;</span><br><span class="line">    observer(data) &#123;</span><br><span class="line">        if (!data || typeof data !&#x3D;&#x3D; &quot;object&quot;) &#123;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        Object.keys(data).forEach((key) &#x3D;&gt; &#123;</span><br><span class="line">            this.defineReactive(data, key, data[key]);</span><br><span class="line">            this.observer(data[key]); &#x2F;&#x2F; 深度递归劫持</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    defineReactive(obj, key, value) &#123;</span><br><span class="line">        let that &#x3D; this;</span><br><span class="line">        let dep &#x3D; new Dep(); &#x2F;&#x2F; 每个变化的数据都会对应一个数组，这个数组是存放所有更新的操作</span><br><span class="line">        Object.defineProperty(obj, key, &#123;</span><br><span class="line">            enumerable: true,</span><br><span class="line">            configurable: true,</span><br><span class="line">            get() &#123;</span><br><span class="line">                Dep.target &amp;&amp; dep.addSub(Dep.target);</span><br><span class="line">                return value;</span><br><span class="line">            &#125;,</span><br><span class="line">            set(newValue) &#123;</span><br><span class="line">                if (newValue !&#x3D; value) &#123;</span><br><span class="line">                    &#x2F;&#x2F; 这里的this不是实例</span><br><span class="line">                    that.observer(newValue);</span><br><span class="line">                    value &#x3D; newValue;</span><br><span class="line">                    dep.notify(); &#x2F;&#x2F; 通知所有人数据更新了</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Dep &#123;</span><br><span class="line">    constructor() &#123;</span><br><span class="line">        &#x2F;&#x2F; 订阅的数组</span><br><span class="line">        this.subs &#x3D; [];</span><br><span class="line">    &#125;</span><br><span class="line">    addSub(watcher) &#123;</span><br><span class="line">        this.subs.push(watcher);</span><br><span class="line">    &#125;</span><br><span class="line">    notify() &#123;</span><br><span class="line">        this.subs.forEach((watcher) &#x3D;&gt; watcher.update());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="watcher-js"><a href="#watcher-js" class="headerlink" title="watcher.js"></a>watcher.js</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Watcher &#123;</span><br><span class="line">    constructor(vm, expr, cb) &#123;</span><br><span class="line">        this.vm &#x3D; vm;</span><br><span class="line">        this.expr &#x3D; expr;</span><br><span class="line">        this.cb &#x3D; cb;</span><br><span class="line">        &#x2F;&#x2F; 先获取一下老的值</span><br><span class="line">        this.value &#x3D; this.get();</span><br><span class="line">    &#125;</span><br><span class="line">    getVal(vm, expr) &#123;</span><br><span class="line">        expr &#x3D; expr.split(&quot;.&quot;);</span><br><span class="line">        return expr.reduce((prev, next) &#x3D;&gt; &#123;</span><br><span class="line">            return prev[next];</span><br><span class="line">        &#125;, vm.$data);</span><br><span class="line">    &#125;</span><br><span class="line">    get() &#123;</span><br><span class="line">        Dep.target &#x3D; this;</span><br><span class="line">        let value &#x3D; this.getVal(this.vm, this.expr);</span><br><span class="line">        Dep.target &#x3D; null;</span><br><span class="line">        return value;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 对外暴漏的方法</span><br><span class="line">    update() &#123;</span><br><span class="line">        let newValue &#x3D; this.getVal(this.vm, this.expr);</span><br><span class="line">        let oldValue &#x3D; this.value;</span><br><span class="line">        if (newValue !&#x3D; oldValue) &#123;</span><br><span class="line">            this.cb(newValue); &#x2F;&#x2F; 对应watch的callback</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h1><h2 id="先-observer-数据"><a href="#先-observer-数据" class="headerlink" title="先 observer 数据"></a>先 observer 数据</h2><p>用 <code>Object.defineProperty</code>对 data 进行了深度递归劫持，对 get 和 get 分别增加了 addSub 和 notify,由于此时 Dep.target 还没初始化所以<code>Dep.target &amp;&amp; dep.addSub(Dep.target);</code>不执行</p>
<h2 id="再代理数据"><a href="#再代理数据" class="headerlink" title="再代理数据"></a>再代理数据</h2><p>用 <code>Object.defineProperty</code>达到对 vm 进行读取和赋值达到了对 <code>vm.$data</code>操作一样的效果</p>
<h2 id="然后-compiler"><a href="#然后-compiler" class="headerlink" title="然后 compiler"></a>然后 compiler</h2><p>对文本节点里的每个符合<code>\{\{\}\}</code>的数据都 new 一个 回调函数用来更新自己值的 Watcher,对 input 框里的 v-model 对应的数据也作此处理同时绑定 input 事件</p>
<h1 id="页面需要用到某个值"><a href="#页面需要用到某个值" class="headerlink" title="页面需要用到某个值"></a>页面需要用到某个值</h1><p>由于 observer 里进行了数据劫持，每个用到这个值的地方都会走 get 方法，Dep.target 此时为 当前的 Watcher 实例<code>Dep.target &amp;&amp; dep.addSub(Dep.target);</code>会执行，然后把对应的 Watcher 添加进行</p>
<h1 id="页面修改某个值"><a href="#页面修改某个值" class="headerlink" title="页面修改某个值"></a>页面修改某个值</h1><p>由于 observer 里进行了数据劫持,修改如何一处都会走 set，执行<code>dep.notify</code>通知所有人数据更新了,notify 里边<code>this.subs.forEach((watcher) =&gt; watcher.update());</code>会对每个之前添加的 watcher 实例都执行自己的 update 方法,然后 update 方法里<code>if (newValue != oldValue) {this.cb(newValue); // 对应watch的callback}</code>会对比变化后的值和变化前的值是否一样，不一样就会触发之前在 compiler 里绑定更新自己值的回调函数</p>
]]></content>
      <tags>
        <tag>vue</tag>
        <tag>前端</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-用字符级RNN生成名称</title>
    <url>/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/</url>
    <content><![CDATA[<p>Pytorch-Text-用字符级RNN生成名称:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Generating Names with a Character-Level RNN</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>This is our second of three tutorials on “NLP From Scratch”.<br>In the <code>first tutorial &lt;/intermediate/char_rnn_classification_tutorial&gt;</code><br>we used a RNN to classify names into their language of origin. This time<br>we’ll turn around and generate names from languages.</p>
<p>::</p>
<pre><code>&gt; python sample.py Russian RUS
Rovakov
Uantov
Shavakov

&gt; python sample.py German GER
Gerren
Ereng
Rosher

&gt; python sample.py Spanish SPA
Salla
Parer
Allan

&gt; python sample.py Chinese CHI
Chan
Hang
Iun
</code></pre><p>We are still hand-crafting a small RNN with a few linear layers. The big<br>difference is instead of predicting a category after reading in all the<br>letters of a name, we input a category and output one letter at a time.<br>Recurrently predicting characters to form language (this could also be<br>done with words or other higher order constructs) is often referred to<br>as a “language model”.</p>
<p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about RNNs and how they work:</p>
<ul>
<li><code>The Unreasonable Effectiveness of Recurrent Neural
Networks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li>
<li><code>Understanding LSTM
Networks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li>
</ul>
<p>I also suggest the previous tutorial, :doc:<code>/intermediate/char_rnn_classification_tutorial</code></p>
<h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>See the last tutorial for more detail of this process. In short, there<br>are a bunch of plain text files <code>data/names/[Language].txt</code> with a<br>name per line. We split lines into an array, convert Unicode to ASCII,<br>and end up with a dictionary <code>{language: [names ...]}</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'-"</span></span><br><span class="line">n_letters = len(all_letters) + <span class="number">1</span> <span class="comment"># Plus EOS marker</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of lines per category</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> n_categories == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">'Data not found. Make sure that you downloaded data '</span></span><br><span class="line">        <span class="string">'from https://download.pytorch.org/tutorial/data.zip and extract it to '</span></span><br><span class="line">        <span class="string">'the current directory.'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'# categories:'</span>, n_categories, all_categories)</span><br><span class="line">print(unicodeToAscii(<span class="string">"O'Néàl"</span>))</span><br></pre></td></tr></table></figure>
<pre><code># categories: 18 [&#39;Arabic&#39;, &#39;Chinese&#39;, &#39;Czech&#39;, &#39;Dutch&#39;, &#39;English&#39;, &#39;French&#39;, &#39;German&#39;, &#39;Greek&#39;, &#39;Irish&#39;, &#39;Italian&#39;, &#39;Japanese&#39;, &#39;Korean&#39;, &#39;Polish&#39;, &#39;Portuguese&#39;, &#39;Russian&#39;, &#39;Scottish&#39;, &#39;Spanish&#39;, &#39;Vietnamese&#39;]
O&#39;Neal
</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>This network extends <code>the last tutorial&#39;s RNN &lt;#Creating-the-Network&gt;</code>__<br>with an extra argument for the category tensor, which is concatenated<br>along with the others. The category tensor is a one-hot vector just like<br>the letter input.</p>
<p>We will interpret the output as the probability of the next letter. When<br>sampling, the most likely output letter is used as the next input<br>letter.</p>
<p>I added a second linear layer <code>o2o</code> (after combining hidden and<br>output) to give it more muscle to work with. There’s also a dropout<br>layer, which <code>randomly zeros parts of its
input &lt;https://arxiv.org/abs/1207.0580&gt;</code>__ with a given probability<br>(here 0.1) and is usually used to fuzz inputs to prevent overfitting.<br>Here we’re using it towards the end of the network to purposely add some<br>chaos and increase sampling variety.</p>
<p>.. figure:: <a href="https://i.imgur.com/jzVrf7f.png" target="_blank" rel="noopener">https://i.imgur.com/jzVrf7f.png</a><br>   :alt:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)</span><br><span class="line">        self.o2o = nn.Linear(hidden_size + output_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, category, input, hidden)</span>:</span></span><br><span class="line">        input_combined = torch.cat((category, input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(input_combined)</span><br><span class="line">        output = self.i2o(input_combined)</span><br><span class="line">        output_combined = torch.cat((hidden, output), <span class="number">1</span>)</span><br><span class="line">        output = self.o2o(output_combined)</span><br><span class="line">        output = self.dropout(output)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>First of all, helper functions to get random pairs of (category, line):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Random item from a list</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a random category and random line from that category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    <span class="keyword">return</span> category, line</span><br></pre></td></tr></table></figure>
<p>For each timestep (that is, for each letter in a training word) the<br>inputs of the network will be<br><code>(category, current letter, hidden state)</code> and the outputs will be<br><code>(next letter, next hidden state)</code>. So for each training set, we’ll<br>need the category, a set of input letters, and a set of output/target<br>letters.</p>
<p>Since we are predicting the next letter from the current letter for each<br>timestep, the letter pairs are groups of consecutive letters from the<br>line - e.g. for <code>&quot;ABCD&lt;EOS&gt;&quot;</code> we would create (“A”, “B”), (“B”, “C”),<br>(“C”, “D”), (“D”, “EOS”).</p>
<p>.. figure:: <a href="https://i.imgur.com/JH58tXY.png" target="_blank" rel="noopener">https://i.imgur.com/JH58tXY.png</a><br>   :alt:</p>
<p>The category tensor is a <code>one-hot
tensor &lt;https://en.wikipedia.org/wiki/One-hot&gt;</code>__ of size<br><code>&lt;1 x n_categories&gt;</code>. When training we feed it to the network at every<br>timestep - this is a design choice, it could have been included as part<br>of initial hidden state or some other strategy.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># One-hot vector for category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryTensor</span><span class="params">(category)</span>:</span></span><br><span class="line">    li = all_categories.index(category)</span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_categories)</span><br><span class="line">    tensor[<span class="number">0</span>][li] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot matrix of first to last letters (not including EOS) for input</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> range(len(line)):</span><br><span class="line">        letter = line[li]</span><br><span class="line">        tensor[li][<span class="number">0</span>][all_letters.find(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># LongTensor of second letter to end (EOS) for target</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    letter_indexes = [all_letters.find(line[li]) <span class="keyword">for</span> li <span class="keyword">in</span> range(<span class="number">1</span>, len(line))]</span><br><span class="line">    letter_indexes.append(n_letters - <span class="number">1</span>) <span class="comment"># EOS</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(letter_indexes)</span><br></pre></td></tr></table></figure>
<p>For convenience during training we’ll make a <code>randomTrainingExample</code><br>function that fetches a random (category, line) pair and turns them into<br>the required (category, input, target) tensors.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Make category, input, and target tensors from a random category, line pair</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category, line = randomTrainingPair()</span><br><span class="line">    category_tensor = categoryTensor(category)</span><br><span class="line">    input_line_tensor = inputTensor(line)</span><br><span class="line">    target_line_tensor = targetTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category_tensor, input_line_tensor, target_line_tensor</span><br></pre></td></tr></table></figure>
<h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>In contrast to classification, where only the last output is used, we<br>are making a prediction at every step, so we are calculating loss at<br>every step.</p>
<p>The magic of autograd allows you to simply sum these losses at each step<br>and call backward at the end.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, input_line_tensor, target_line_tensor)</span>:</span></span><br><span class="line">    target_line_tensor.unsqueeze_(<span class="number">-1</span>)</span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(input_line_tensor.size(<span class="number">0</span>)):</span><br><span class="line">        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)</span><br><span class="line">        l = criterion(output, target_line_tensor[i])</span><br><span class="line">        loss += l</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item() / input_line_tensor.size(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>To keep track of how long training takes I am adding a<br><code>timeSince(timestamp)</code> function which returns a human readable string:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br></pre></td></tr></table></figure>
<p>Training is business as usual - call train a bunch of times and wait a<br>few minutes, printing the current time and loss every <code>print_every</code><br>examples, and keeping store of an average loss per <code>plot_every</code> examples<br>in <code>all_losses</code> for plotting later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rnn = RNN(n_letters, <span class="number">128</span>, n_letters)</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">500</span></span><br><span class="line">all_losses = []</span><br><span class="line">total_loss = <span class="number">0</span> <span class="comment"># Reset every plot_every iters</span></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    output, loss = train(*randomTrainingExample())</span><br><span class="line">    total_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start), iter, iter / n_iters * <span class="number">100</span>, loss))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(total_loss / plot_every)</span><br><span class="line">        total_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>0m 40s (5000 5%) 2.6821
1m 17s (10000 10%) 3.1606
1m 50s (15000 15%) 2.3541
2m 23s (20000 20%) 2.4859
2m 57s (25000 25%) 2.1573
3m 30s (30000 30%) 2.2910
4m 3s (35000 35%) 2.6906
4m 37s (40000 40%) 2.1542
20m 27s (45000 45%) 2.1909
21m 10s (50000 50%) 1.8939
21m 51s (55000 55%) 2.9425
22m 34s (60000 60%) 2.8395
23m 15s (65000 65%) 3.0346
23m 55s (70000 70%) 2.5686
24m 34s (75000 75%) 2.6037
25m 13s (80000 80%) 2.5966
25m 56s (85000 85%) 2.6650
26m 39s (90000 90%) 2.7412
27m 18s (95000 95%) 2.6140
27m 58s (100000 100%) 1.9323
</code></pre><h2 id="Plotting-the-Losses"><a href="#Plotting-the-Losses" class="headerlink" title="Plotting the Losses"></a>Plotting the Losses</h2><p>Plotting the historical loss from all_losses shows the network<br>learning:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x12e2623bba8&gt;]
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/output_18_1.png" alt="png"></p>
<h1 id="Sampling-the-Network"><a href="#Sampling-the-Network" class="headerlink" title="Sampling the Network"></a>Sampling the Network</h1><p>To sample we give the network a letter and ask what the next one is,<br>feed that in as the next letter, and repeat until the EOS token.</p>
<ul>
<li>Create tensors for input category, starting letter, and empty hidden<br>state</li>
<li>Create a string <code>output_name</code> with the starting letter</li>
<li><p>Up to a maximum output length,</p>
<ul>
<li>Feed the current letter to the network</li>
<li>Get the next letter from highest output, and next hidden state</li>
<li>If the letter is EOS, stop here</li>
<li>If a regular letter, add to <code>output_name</code> and continue</li>
</ul>
</li>
<li><p>Return the final name</p>
</li>
</ul>
<p>.. Note::<br>   Rather than having to give it a starting letter, another<br>   strategy would have been to include a “start of string” token in<br>   training and have the network choose its own starting letter.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_length = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from a category and starting letter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(category, start_letter=<span class="string">'A'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># no need to track history in sampling</span></span><br><span class="line">        category_tensor = categoryTensor(category)</span><br><span class="line">        input = inputTensor(start_letter)</span><br><span class="line">        hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">        output_name = start_letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hidden = rnn(category_tensor, input[<span class="number">0</span>], hidden)</span><br><span class="line">            topv, topi = output.topk(<span class="number">1</span>)</span><br><span class="line">            topi = topi[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> topi == n_letters - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                letter = all_letters[topi]</span><br><span class="line">                output_name += letter</span><br><span class="line">            input = inputTensor(letter)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get multiple samples from one category and multiple starting letters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">samples</span><span class="params">(category, start_letters=<span class="string">'ABC'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> start_letter <span class="keyword">in</span> start_letters:</span><br><span class="line">        print(sample(category, start_letter))</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Russian'</span>, <span class="string">'RUS'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'German'</span>, <span class="string">'GER'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Spanish'</span>, <span class="string">'SPA'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Chinese'</span>, <span class="string">'CHI'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Romankovovovoshollosh
Uantovovovokovosskoss
Shaverovovovovovoshol
Gerter
Eeller
Ronger
Sara
Pare
Aran
Chan
Han
Iou
</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset of category -&gt; line, for example:</p>
<ul>
<li>Fictional series -&gt; Character name</li>
<li>Part of speech -&gt; Word</li>
<li>Country -&gt; City</li>
</ul>
</li>
<li><p>Use a “start of sentence” token so that sampling can be done without<br>choosing a start letter</p>
</li>
<li><p>Get better results with a bigger and/or better shaped network</p>
<ul>
<li>Try the nn.LSTM and nn.GRU layers</li>
<li>Combine multiple of these RNNs as a higher level network</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>deep-learning-from-scratch笔记</title>
    <url>/2020/07/29/deep-learning-from-scratch%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>deep-learning-from-scratch笔记<br><a id="more"></a></p>
<h1 id="sigmoid函数的平滑性对神经网络的学习具有重要意义。"><a href="#sigmoid函数的平滑性对神经网络的学习具有重要意义。" class="headerlink" title="sigmoid函数的平滑性对神经网络的学习具有重要意义。"></a>sigmoid函数的平滑性对神经网络的学习具有重要意义。</h1><p>识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。</p>
<p>出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。</p>
<p>阶跃函数的导数在绝大多数地方（除了0以外的地方）均为0。</p>
<p>也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微<br>小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。</p>
<p>sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。</p>
<p>另一个不同点是，相对于阶跃函数只能返回0或1，sigmoid函数可以返回0.731 …、0.880 …等实数（这一点和刚才的平滑性有关）。也就是说，感<br>知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。</p>
<p>sigmoid函数的导数在任何地方都不为0。这对神经网络的学习非常重要。得益于这个斜率不会为0的性质，神经网络的学习得以正确进行</p>
<h1 id="神经网络的激活函数必须使用非线性函数。"><a href="#神经网络的激活函数必须使用非线性函数。" class="headerlink" title="神经网络的激活函数必须使用非线性函数。"></a>神经网络的激活函数必须使用非线性函数。</h1><p>换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。</p>
<p>线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。</p>
<h1 id="实现-softmax函数时的注意事项"><a href="#实现-softmax函数时的注意事项" class="headerlink" title="实现 softmax函数时的注意事项"></a>实现 softmax函数时的注意事项</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/softmax1.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def softmax(a):</span><br><span class="line"> exp_a &#x3D; np.exp(a)</span><br><span class="line"> sum_exp_a &#x3D; np.sum(exp_a)</span><br><span class="line"> y &#x3D; exp_a &#x2F; sum_exp_a</span><br><span class="line"> return y</span><br></pre></td></tr></table></figure>
<p>上面的softmax函数的实现虽然正确描述了式，但在计算机的运算上有一定的缺陷。这个缺陷就是溢出问题。softmax函数的实现中要进行指<br>数函数的运算，但是此时指数函数的值很容易变得非常大。<br>比如，e的10次方的值会超过20000，e的1000次方会变成一个后面有40多个0的超大值，e的1000次方的结果会返回一个表示无穷大的inf。如果在这些超大值之间进行除法运算，结果会出现“不确定”的情况。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/softmax2.png" alt><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a &#x3D; np.array([1010, 1000, 990])</span><br><span class="line">&gt;&gt;&gt; np.exp(a) &#x2F; np.sum(np.exp(a)) # softmax函数的运算</span><br><span class="line">array([ nan, nan, nan]) # 没有被正确计算</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; c &#x3D; np.max(a) # 1010</span><br><span class="line">&gt;&gt;&gt; a - c</span><br><span class="line">array([ 0, -10, -20])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; np.exp(a - c) &#x2F; np.sum(np.exp(a - c))</span><br><span class="line">array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])</span><br></pre></td></tr></table></figure></p>
<p>如该例所示，通过减去输入信号中的最大值（上例中的c），我们发现原<br>本为nan（not a number，不确定）的地方，现在被正确计算了。综上，我们<br>可以像下面这样实现softmax函数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def softmax(a):</span><br><span class="line">    c &#x3D; np.max(a)</span><br><span class="line">    exp_a &#x3D; np.exp(a - c) # 溢出对策</span><br><span class="line">    sum_exp_a &#x3D; np.sum(exp_a)</span><br><span class="line">    y &#x3D; exp_a &#x2F; sum_exp_a</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure></p>
<h1 id="获得泛化能力是机器学习的最终目标。"><a href="#获得泛化能力是机器学习的最终目标。" class="headerlink" title="获得泛化能力是机器学习的最终目标。"></a>获得泛化能力是机器学习的最终目标。</h1><p>如果让我们自己来设计一个能将5正确分类的程序，就会意外地发现这是一个很难的问题。人可以简单地识别出5，但却很难明确说出是基于何种规律而识别出了5。</p>
<p>因此，与其绞尽脑汁，从零开始想出一个可以识别5的算法，不如考虑通过有效利用数据来解决这个问题。</p>
<p>一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。</p>
<p>图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。</p>
<p>机器学习的方法中，由机器从收集到的数据中找出规律性。与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担。</p>
<p>但是需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。</p>
<p>比如，为了区分狗的脸部，人们需要考虑与用于识别5的特征量不同的其他特征量。也就是说，即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量</p>
<p>神经网络的优点是对所有的问题都可以用同样的流程来解决。比如，不管要求解的问题是识别5，还是识别狗，抑或是识别人脸，神经网络都是通<br>过不断地学习所提供的数据，尝试发现待求解的问题的模式。也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端的学习。</p>
<p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的<br>能力。获得泛化能力是机器学习的最终目标。</p>
<h1 id="计算图优点"><a href="#计算图优点" class="headerlink" title="计算图优点"></a>计算图优点</h1><p>计算图到底有什么优点呢？</p>
<p>一个优点就在于前面所说的局部计算。无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题。</p>
<p>另一个优点是，利用计算图可以将中间的计算结果全部保存起来（比如，计算进行到2个苹果时的金额是200日元、加上消费税之前的金额650日元等）。</p>
<p>但是只有这些理由可能还无法令人信服。实际上，使用计算图最大的原因是，可以通过反向传播高效计算导数。</p>
<p>计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。</p>
<h1 id="推理和学习"><a href="#推理和学习" class="headerlink" title="推理和学习"></a>推理和学习</h1><p>神经网络中进行的处理有推理（inference）和学习两个阶段。</p>
<p>神经网络的推理通常不使用 Softmax层。</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%8E%A8%E7%90%86%E4%B8%8E%E5%AD%A6%E4%B9%A0.png" alt></p>
<p>比如，用上图网络进行推理时，会将最后一个 Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（Softmax层前面的 Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层。不过，神经网络的学习阶段则需要 Softmax层。</p>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Momentum.png" alt><br>式中有αv这一项。在物体不受任何力时，该项承担使物体逐渐减速的任务（α设定为0.9之类的值），对应物理上的地面摩擦或空气阻力。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Momentum:</span><br><span class="line">    def __init__(self, lr&#x3D;0.01, momentum&#x3D;0.9):</span><br><span class="line">        self.lr &#x3D; lr</span><br><span class="line">        self.momentum &#x3D; momentum</span><br><span class="line">        self.v &#x3D; None</span><br><span class="line">    def update(self, params, grads):</span><br><span class="line">        if self.v is None:</span><br><span class="line">            self.v &#x3D; &#123;&#125;</span><br><span class="line">            for key, val in params.items():</span><br><span class="line">                self.v[key] &#x3D; np.zeros_like(val)</span><br><span class="line">        for key in params.keys():</span><br><span class="line">            self.v[key] &#x3D; self.momentum*self.v[key] - self.lr*grads[key]</span><br><span class="line">            params[key] +&#x3D; self.v[key]</span><br></pre></td></tr></table></figure><br>实例变量v会保存物体的速度。初始化时，v中什么都不保存，但当第一次调用update()时，v会以字典型变量的形式保存与参数结构相同的数据。</p>
<h1 id="AdaGrad和RMSProp"><a href="#AdaGrad和RMSProp" class="headerlink" title="AdaGrad和RMSProp"></a>AdaGrad和RMSProp</h1><p>在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/AdaGrad.png" alt><br>在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。而AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值。</p>
<p>AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新。为了改善这个问题，可以使用 RMSProp方法。RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class AdaGrad:</span><br><span class="line">    def __init__(self, lr&#x3D;0.01):</span><br><span class="line">        self.lr &#x3D; lr</span><br><span class="line">        self.h &#x3D; None</span><br><span class="line">    def update(self, params, grads):</span><br><span class="line">        if self.h is None:</span><br><span class="line">            self.h &#x3D; &#123;&#125;</span><br><span class="line">            for key, val in params.items():</span><br><span class="line">                self.h[key] &#x3D; np.zeros_like(val)</span><br><span class="line">        for key in params.keys():</span><br><span class="line">            self.h[key] +&#x3D; grads[key] * grads[key]</span><br><span class="line">            params[key] -&#x3D; self.lr * grads[key] &#x2F; (np.sqrt(self.h[key]) + 1e-7)</span><br></pre></td></tr></table></figure>
<p>这里需要注意的是，最后一行加上了微小值1e-7。这是为了防止当<br>self.h[key]中有0时，将0用作除数的情况。在很多深度学习的框架中，这<br>个微小值也可以设定为参数，但这里我们用的是1e-7这个固定值。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class RMSprop:</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;RMSprop&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, lr&#x3D;0.01, decay_rate &#x3D; 0.99):</span><br><span class="line">        self.lr &#x3D; lr</span><br><span class="line">        self.decay_rate &#x3D; decay_rate</span><br><span class="line">        self.h &#x3D; None</span><br><span class="line">        </span><br><span class="line">    def update(self, params, grads):</span><br><span class="line">        if self.h is None:</span><br><span class="line">            self.h &#x3D; &#123;&#125;</span><br><span class="line">            for key, val in params.items():</span><br><span class="line">                self.h[key] &#x3D; np.zeros_like(val)</span><br><span class="line">            </span><br><span class="line">        for key in params.keys():</span><br><span class="line">            self.h[key] *&#x3D; self.decay_rate</span><br><span class="line">            self.h[key] +&#x3D; (1 - self.decay_rate) * grads[key] * grads[key]</span><br><span class="line">            params[key] -&#x3D; self.lr * grads[key] &#x2F; (np.sqrt(self.h[key]) + 1e-7)</span><br></pre></td></tr></table></figure></p>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam是2015年提出的新方法。它的理论有些复杂，直观地讲，就是融合了Momentum和AdaGrad的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。<br>此外，进行超参数的“偏置校正”也是Adam的特征。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Adam:</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;Adam (http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1412.6980v8)&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, lr&#x3D;0.001, beta1&#x3D;0.9, beta2&#x3D;0.999):</span><br><span class="line">        self.lr &#x3D; lr</span><br><span class="line">        self.beta1 &#x3D; beta1</span><br><span class="line">        self.beta2 &#x3D; beta2</span><br><span class="line">        self.iter &#x3D; 0</span><br><span class="line">        self.m &#x3D; None</span><br><span class="line">        self.v &#x3D; None</span><br><span class="line">        </span><br><span class="line">    def update(self, params, grads):</span><br><span class="line">        if self.m is None:</span><br><span class="line">            self.m, self.v &#x3D; &#123;&#125;, &#123;&#125;</span><br><span class="line">            for key, val in params.items():</span><br><span class="line">                self.m[key] &#x3D; np.zeros_like(val)</span><br><span class="line">                self.v[key] &#x3D; np.zeros_like(val)</span><br><span class="line">        </span><br><span class="line">        self.iter +&#x3D; 1</span><br><span class="line">        lr_t  &#x3D; self.lr * np.sqrt(1.0 - self.beta2**self.iter) &#x2F; (1.0 - self.beta1**self.iter)         </span><br><span class="line">        </span><br><span class="line">        for key in params.keys():</span><br><span class="line">            #self.m[key] &#x3D; self.beta1*self.m[key] + (1-self.beta1)*grads[key]</span><br><span class="line">            #self.v[key] &#x3D; self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)</span><br><span class="line">            self.m[key] +&#x3D; (1 - self.beta1) * (grads[key] - self.m[key])</span><br><span class="line">            self.v[key] +&#x3D; (1 - self.beta2) * (grads[key]**2 - self.v[key])</span><br><span class="line">            </span><br><span class="line">            params[key] -&#x3D; lr_t * self.m[key] &#x2F; (np.sqrt(self.v[key]) + 1e-7)</span><br><span class="line">            </span><br><span class="line">            #unbias_m +&#x3D; (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias</span><br><span class="line">            #unbisa_b +&#x3D; (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias</span><br><span class="line">            #params[key] +&#x3D; self.lr * unbias_m &#x2F; (np.sqrt(unbisa_b) + 1e-7)</span><br></pre></td></tr></table></figure></p>
<h1 id="可以将权重初始值设为0吗"><a href="#可以将权重初始值设为0吗" class="headerlink" title="可以将权重初始值设为0吗"></a>可以将权重初始值设为0吗</h1><p>为什么不能将权重初始值设为0呢？</p>
<p>严格地说，为什么不能将权重初始值设成一样的值呢？</p>
<p>这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。</p>
<p>比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会<br>被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新（回忆一下“乘法节点的反向传播”的内容）。</p>
<p>因此，权重被更新为相同的值，并拥有了对称的值（重复的值）。这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。</p>
<h1 id="各层的激活值的分布都要求有适当的广度"><a href="#各层的激活值的分布都要求有适当的广度" class="headerlink" title="各层的激活值的分布都要求有适当的广度"></a>各层的激活值的分布都要求有适当的广度</h1><p>各层的激活值的分布都要求有适当的广度。为什么呢？</p>
<p>因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。</p>
<p>反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。</p>
<p>激活值的分布有所偏向，说明在表现力上会有很大问题。为什么这么说呢？</p>
<p>因为如果有多个神经元都输出几乎相同的值，那它们就没有存在的意义了。</p>
<p>比如，如果100个神经元都输出几乎相同的值，那么也可以由1个神经元来表达基本相同的事情。</p>
<p>因此，激活值在分布上有所偏向会出现“表现力受限”的问题。</p>
<h1 id="Batch-Norm"><a href="#Batch-Norm" class="headerlink" title="Batch Norm"></a>Batch Norm</h1><p>为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。</p>
<ul>
<li>可以使学习快速进行（可以增大学习率）。</li>
<li>不那么依赖初始值（对于初始值不用那么神经质）。</li>
<li>抑制过拟合（降低Dropout等的必要性）。</li>
</ul>
<p>Batch Norm的思路是调整各层的激活值分布使其拥有适当的广度。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Batch%20Norm.png" alt></p>
<h1 id="权值衰减"><a href="#权值衰减" class="headerlink" title="权值衰减"></a>权值衰减</h1><p>权值衰减是一直以来经常被使用的一种抑制过拟合的方法。</p>
<p>该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。</p>
<p>很多过拟合原本就是因为权重参数取值过大才发生的。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9D%83%E5%80%BC%E8%A1%B0%E5%87%8F.png" alt></p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>作为抑制过拟合的方法，前面我们介绍了为损失函数加上权重的L2范数的权值衰减方法。</p>
<p>该方法可以简单地实现，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应对了。</p>
<p>在这种情况下，我们经常会使用Dropout方法。</p>
<p>Dropout是一种在学习的过程中随机删除神经元的方法。<br>训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递，</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Dropout.png" alt><br>训练时，每传递一次数据，就会随机选择要删除的神经元。<br>然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Dropout:</span><br><span class="line">    def __init__(self, dropout_ratio&#x3D;0.5):</span><br><span class="line">        self.dropout_ratio &#x3D; dropout_ratio</span><br><span class="line">        self.mask &#x3D; None</span><br><span class="line">    def forward(self, x, train_flg&#x3D;True):</span><br><span class="line">        if train_flg:</span><br><span class="line">            self.mask &#x3D; np.random.rand(*x.shape) &gt; self.dropout_ratio</span><br><span class="line">            return x * self.mask</span><br><span class="line">        else:</span><br><span class="line">            return x * (1.0 - self.dropout_ratio)</span><br><span class="line">        def backward(self, dout):</span><br><span class="line">            return dout * self.mask</span><br></pre></td></tr></table></figure>
<p>这里的要点是，每次正向传播时，self.mask中都会以False的形式保存要删除的神经元。</p>
<p>self.mask会随机生成和x形状相同的数组，并将值比dropout_ratio大的元素设为True。反向传播时的行为和ReLU相同。</p>
<p>也就是说，正向传播时传递了信号的神经元，反向传播时按原样传递信号；正向传播时没有传递信号的神经元，反向传播时信号将停在那里。</p>
<h1 id="验证数据"><a href="#验证数据" class="headerlink" title="验证数据"></a>验证数据</h1><p>调整超参数时，必须使用超参数专用的确认数据。</p>
<p>用于调整超参数的数据，一般称为验证数据（validation data）。</p>
<p>我们使用这个验证数据来评估超参数的好坏。</p>
<h1 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF1.png" alt><br>卷积运算对输入数据应用滤波器。</p>
<p>在这个例子中，输入数据是有高长方向的形状的数据，滤波器也一样，有高长方向上的维度。</p>
<p>假设用（height, width）表示数据和滤波器的形状，则在本例中，输入大小是(4, 4)，滤波器大小是(3, 3)，输出大小是(2, 2)。</p>
<p>另外，有的文献中也会用“核”这个词来表示这里所说的“滤波器”。</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF3.png" alt></p>
<p>应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如下图所示，应用滤波器的窗口的间隔变为2个元素。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF4.png" alt></p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF5.png" alt></p>
<p>之前的卷积运算的例子都是以有高、长方向的2维形状为对象的。</p>
<p>但是，图像是3维数据，除了高、长方向之外，还需要处理通道方向。</p>
<p>这里，我们按照与之前相同的顺序，看一下对加上了通道方向的3维数据进行卷积运算的例子。</p>
<p>图7-8是卷积运算的例子，图7-9是计算顺序。</p>
<p>这里以3通道的数据为例，展示了卷积运算的结果。和2维数据时（图7-3的例子）相比，可以发现纵深方向（通道方向）上特征图增加了。</p>
<p>通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF7.png" alt></p>
<p>将数据和滤波器结合长方体的方块来考虑，3维数据的卷积运算会很容易理解。</p>
<p>方块是如图7-10所示的3维长方体。把3维数据表示为多维数组时，书写顺序为（channel, height, width）。</p>
<p>比如，通道数为C、高度为H、长度为W的数据的形状可以写成（C, H, W）。</p>
<p>滤波器也一样，要按（channel, height, width）的顺序书写。</p>
<p>比如，通道数为C、滤波器高度为FH（Filter Height）、长度为FW（Filter Width）时，可以写成（C, FH, FW）。</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF9.png" alt></p>
<p>图7-11中，通过应用FN个滤波器，输出特征图也生成了FN个。</p>
<p>如果将这FN个特征图汇集在一起，就得到了形状为(FN, OH, OW)的方块。将这个方块传给下一层，就是CNN的处理流。</p>
<p>如图 7-11 所示，关于卷积运算的滤波器，也必须考虑滤波器的数量。</p>
<p>因此，作为4维数据，滤波器的权重数据要按(output_channel, input_channel, height, width)的顺序书写。</p>
<p>比如，通道数为3、大小为5 × 5的滤波器有20个时，可以写成(20, 3, 5, 5)。</p>
<p>卷积运算中（和全连接层一样）存在偏置。在图7-11的例子中，如果进一步追加偏置的加法运算处理，则结果如下面的图7-12所示。</p>
<p>图7-12中，每个通道只有一个偏置。这里，偏置的形状是(FN, 1, 1)，滤波器的输出结果的形状是(FN, OH, OW)。</p>
<p>这两个方块相加时，要对滤波器的输出结果(FN, OH, OW)按通道加上相同的偏置值。</p>
<p>另外，不同形状的方块相加时，可以基于NumPy的广播功能轻松实现<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF10.png" alt></p>
<p>神经网络的处理中进行了将输入数据打包的批处理。</p>
<p>之前的全连接神经网络的实现也对应了批处理，通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。</p>
<p>我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。</p>
<p>具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。</p>
<p>比如，将图7-12中的处理改成对N个数据进行批处理时，数据的形状如图7-13所示。</p>
<p>图7-13的批处理版的数据流中，在各个数据的开头添加了批用的维度。像这样，数据作为4维的形状在各层间传递。</p>
<p>这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF11.png" alt></p>
<h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%B1%A0%E5%8C%961.png" alt><br>图7-14的例子是按步幅2进行2 × 2的Max池化时的处理顺序。“Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。</p>
<p>如图所示，从2 × 2的区域中取出最大的元素。此外，这个例子中将步幅设为了2，所以2 × 2的窗口的移动间隔为2个元素。</p>
<p>另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如，3 × 3的窗口的步幅会设为3，4 × 4的窗口的步幅会设为4等。</p>
<p>池化层有以下特征。</p>
<ul>
<li>没有要学习的参数。池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。<br>通道数不发生变化</li>
<li>经过池化运算，输入数据和输出数据的通道数不会发生变化。如图7-15<br>所示，计算是按通道独立进行的。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%B1%A0%E5%8C%962.png" alt><br>对微小的位置变化具有鲁棒性（健壮）</li>
</ul>
<p>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。比如，3 × 3的池化的情况下，如图7-16所示，池化会吸收输入数据的偏差（根据数据的不同，结果有可能不一致）。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%B1%A0%E5%8C%963.png" alt></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>deep-learning-from-scratch笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo开发博客</title>
    <url>/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">从0搭建</a></li>
<li>2.<a href="#header2">遇到的坑</a></li>
<li>3.<a href="#header3">图片放置问题</a></li>
<li>4.<a href="#header4">更换主题</a></li>
<li>5.<a href="#header5">支持Latex</a></li>
</ul>
<a id="more"></a>
<h1 id="视频手把手教你从0开始搭建自己的个人博客hexo参考"><a href="#视频手把手教你从0开始搭建自己的个人博客hexo参考" class="headerlink" title="视频手把手教你从0开始搭建自己的个人博客hexo参考"></a><span id="header1">视频手把手教你从0开始搭建自己的个人博客hexo</span><a href="https://www.bilibili.com/video/BV1Yb411a7ty/?spm_id_from=333.788.videocard.4" target="_blank" rel="noopener">参考</a></h1><ul>
<li>安装node<br><a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">node官网</a>可以直接下载node,可以换源</li>
<li>npm换源(可选)<br>默认的npm源 —— <a href="https://registry.npmjs.org" target="_blank" rel="noopener">https://registry.npmjs.org</a> 比较慢,可以换成淘宝镜像 —— <a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure>
换回来同理<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npmjs.org</span><br></pre></td></tr></table></figure></li>
<li>全局安装hexo-cli(前提是安装了node,可以用npm命令)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm i -g hexo-cli</span><br></pre></td></tr></table></figure></li>
<li>初始化<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo init (文件夹名)</span><br></pre></td></tr></table></figure>
不加文件夹名就是在当前文件夹下</li>
<li><p>新建一篇文章</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo n 文章名</span><br><span class="line">或者</span><br><span class="line">hexo new 文章名</span><br></pre></td></tr></table></figure>
<p>文章内容使用<a href="https://githubzhangshuai.github.io/2020/07/08/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/" target="_blank" rel="noopener">Markdown</a>语法进行书写即可</p>
</li>
<li><p>在线预览</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server</span><br><span class="line">或者</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>浏览器窗口输入localhost:4000即可预览</p>
</li>
<li><p>部署</p>
<ul>
<li>先手动安装hexo-deployer-git<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm i --save hexo-deployer-git</span><br></pre></td></tr></table></figure></li>
<li>然后github新建仓库,仓库名为xxx.github.io,xxx为github账号名,如图,我的xxx为GitHubzhangshuai故仓库名为GitHubzhangshuai.github.io.git<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/hexo%E6%88%AA%E5%9B%BE.png" alt="示例图片"></li>
<li>修改配置文件<br>找到_config.yml文件,修改deploy部分(100行左右)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;deployment.html</span><br><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo: https:&#x2F;&#x2F;github.com&#x2F;GitHubzhangshuai&#x2F;GitHubzhangshuai.github.io.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure></li>
<li>部署到github上<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br><span class="line">或者</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></li>
<li>访问<br>浏览器地址栏输入xxx.github.io</li>
</ul>
</li>
</ul>
<h1 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a><span id="header2">遇到的坑</span></h1><ul>
<li>npm安装失败<br>检查网络问题并且换淘宝源<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li>
<li>部署时出现错误,<a href="https://blog.csdn.net/HTL2018/article/details/106876940?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">参考</a></li>
</ul>
<ul>
<li>1.错误:<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">error：spawn failed...</span><br><span class="line">或者:</span><br><span class="line">fatal: cannot lock ref &#39;HEAD&#39;: unable to resolve reference HEAD: Invalid argument error: src refspec</span><br><span class="line">或者:</span><br><span class="line">error: src refspec HEAD does not match any.</span><br></pre></td></tr></table></figure></li>
<li><ol>
<li>总结一下<br>问题大多是因为git进行push或者hexo d的时候改变了一些.deploy_git文件下的内容。</li>
</ol>
</li>
<li><ol>
<li>解决办法<ul>
<li>3-1.删除.deploy_git文件夹;</li>
<li>3-2.全局设置git的core.autocrlf为false;<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入git config --global core.autocrlf false</span><br></pre></td></tr></table></figure></li>
<li>3-2.然后，依次执行：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h1 id="hexo图片问题"><a href="#hexo图片问题" class="headerlink" title="hexo图片问题"></a><span id="header3">hexo图片问题</span></h1><ul>
<li>1.放在OSS上,我用的阿里云的oss(记得设置bucket的公共读权限)<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/2.jpg" alt="xx"></li>
<li>2.使用base64格式</li>
<li>3.使用插件hexo-asset-image<br>安装插件<br>注意:<br>使用 npm install <a href="https://github.com/CodeFalling/hexo-asset-image" target="_blank" rel="noopener">https://github.com/CodeFalling/hexo-asset-image</a> —save 安装0.0.5版本的hexo-asset-image插件。 使用 npm install hexo-asset-image —save 安装的是1.0.0版本的hexo-asset-image插件。 两者最直接的区别是映射关系不同。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install https:&#x2F;&#x2F;github.com&#x2F;CodeFalling&#x2F;hexo-asset-image --save (有效)</span><br><span class="line">npm install hexo-asset-image --save (无效)</span><br></pre></td></tr></table></figure>
在_config.yml配置文件中(44行左右)，修改为 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>
记得重启服务器(VS code的MD预览插件下无效)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a><span id="#header4">更换主题</span></h1><p><a href="https://www.jianshu.com/p/33bc0a0a6e90" target="_blank" rel="noopener">参考</a></p>
<ul>
<li>1.下载 <a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">NexT</a> 主题,将主题克隆到 themes 目录下，以下截图就是 clone 之后的结果。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &lt;博客存放的目录&gt;</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;iissnan&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure></li>
<li>2.打开 _config.yml 文件，该文件为站点配置文件,将主题修改为 next<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">theme: landscape</span><br><span class="line">改为</span><br><span class="line">theme: next</span><br></pre></td></tr></table></figure></li>
<li>3.重启服务器发现所有的点击后边都带了%20,如图<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/3.png" alt="x"><br><a href="https://blog.csdn.net/weixin_43790779/article/details/104854588" target="_blank" rel="noopener">解决</a>:<br>找到next的配置文件themes\next_config.yml,将配置文件里 ||之前所有的空格删掉,改为以下即可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: &#x2F;||home</span><br><span class="line">  #about: &#x2F;about&#x2F;||user</span><br><span class="line">  #tags: &#x2F;tags&#x2F;||tags</span><br><span class="line">  #categories: &#x2F;categories&#x2F;||th</span><br><span class="line">  archives: &#x2F;archives&#x2F;||archive</span><br><span class="line">  #schedule: &#x2F;schedule&#x2F;||calendar</span><br><span class="line">  #sitemap: &#x2F;sitemap.xml||sitemap</span><br><span class="line">  #commonweal: &#x2F;404&#x2F;||heartbeat</span><br></pre></td></tr></table></figure></li>
<li>4.设置菜单<br>菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标<ul>
<li>4.1设定菜单项的名称和链接<br>找到next的配置文件themes\next_config.yml<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">home: &#x2F;</span><br><span class="line">archives: &#x2F;archives</span><br><span class="line">about: &#x2F;about</span><br><span class="line">categories: &#x2F;categories</span><br><span class="line">tags: &#x2F;tags</span><br></pre></td></tr></table></figure></li>
<li>4.2设定菜单项的显示文本<br>在设置 菜单项的名称和链接中的名称并不会直接显示在网页上，而是会通过 NexT 主题目录下的 languages/{language}.yml 找到对应的显示文本。<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/5.jpg" alt="x"></li>
<li>4.3 设定菜单项的图标<br>对应的字段是 menu_icons。 此设定格式是 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">item name: icon name</span><br></pre></td></tr></table></figure>
，其中 item name 与上一步所配置的菜单名字对应，icon name 是 Font Awesome 图标的 名字。而 enable 可用于控制是否显示图标，你可以设置成 false 来去掉图标。</li>
</ul>
</li>
<li>5.生成子页面<br>命令行输入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new page xxx</span><br></pre></td></tr></table></figure>
然后在source里会生成和_posts同级的文件夹xxx,进入xxx编辑index.md,修改成以下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2020-07-09 00:58:42</span><br><span class="line">type: &quot;xxx&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
其中xxx可为about/categories/tags/schedule/sitemap/404</li>
<li><p>6.next选择 Scheme<br>Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是：<br>找到next的配置文件themes\next_config.yml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</span><br><span class="line">Mist - Muse 的紧凑版本，整洁有序的单栏外观</span><br><span class="line">Pisces - 双栏 Scheme，小家碧玉似的清新</span><br></pre></td></tr></table></figure>
<p>scheme: Muse<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme1.png" alt="x"><br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme1-2.png" alt="x"><br>scheme: Mist<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme2.png" alt="x"><br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme2-1.png" alt="x"><br>scheme: Pisces<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme3.png" alt="x"><br>scheme: Gemini<br>效果如下<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme4.png" alt="x"></p>
</li>
<li><p>7.添加百度/谷歌/本地 自定义站点内容搜索<br>安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
<p>编辑 站点配置文件_config.yml，新增以下内容到任意位置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
<p>编辑 主题配置文件themes\next_config.yml，启用本地搜索功能：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Local search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>8.侧边栏社交链接<br>侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 主题配置文件themes\next_config.yml 中。</p>
</li>
</ul>
<p>链接放置在 social 字段下，一行一个链接。其键值格式是 显示文本: 链接地址。</p>
<p>配置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Social links</span><br><span class="line">social:</span><br><span class="line">  GitHub: https:&#x2F;&#x2F;github.com&#x2F;your-user-name</span><br><span class="line">  Twitter: https:&#x2F;&#x2F;twitter.com&#x2F;your-user-name</span><br><span class="line">  微博: http:&#x2F;&#x2F;weibo.com&#x2F;your-user-name</span><br><span class="line">  豆瓣: http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;your-user-name</span><br><span class="line">  知乎: http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;your-user-name</span><br><span class="line">  # 等等</span><br></pre></td></tr></table></figure><br>设定链接的图标，对应的字段是 social_icons。其键值格式是 匹配键: Font Awesome 图标名称， 匹配键 与上一步所配置的链接的 显示文本 相同（大小写严格匹配），图标名称 是 Font Awesome 图标的名字（不必带 fa- 前缀）。 enable 选项用于控制是否显示图标，你可以设置成 false 来去掉图标。</p>
<p>配置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Social Icons</span><br><span class="line">social_icons:</span><br><span class="line">  enable: true</span><br><span class="line">  # Icon Mappings</span><br><span class="line">  GitHub: github</span><br><span class="line">  Twitter: twitter</span><br><span class="line">  微博: weibo</span><br></pre></td></tr></table></figure></p>
<ul>
<li>9.设置代码高亮主题<br>NexT 使用 Tomorrow Theme 作为代码高亮，共有5款主题供你选择。 NexT 默认使用的是 白色的 normal 主题，可选的值有 normal，night， night blue， night bright， night eighties：</li>
</ul>
<p>themes\next_config.yml更改 highlight_theme 字段，将其值设定成你所喜爱的高亮主题，例如：</p>
<p>高亮主题设置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Code Highlight theme</span><br><span class="line"># Available value: normal | night | night eighties | night blue | night bright</span><br><span class="line"># https:&#x2F;&#x2F;github.com&#x2F;chriskempson&#x2F;tomorrow-theme</span><br><span class="line">highlight_theme: normal</span><br></pre></td></tr></table></figure></p>
<ul>
<li>10.开启打赏功能 由 habren 贡献<br>越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 主题配置文件themes\next_config.yml 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能。</li>
</ul>
<p>打赏功能配置示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！</span><br><span class="line">wechatpay: &#x2F;path&#x2F;to&#x2F;wechat-reward-image</span><br><span class="line">alipay: &#x2F;path&#x2F;to&#x2F;alipay-reward-image</span><br></pre></td></tr></table></figure></p>
<ul>
<li>11.鼠标点击特效<br>小红心效果<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/click1.gif" alt><br>具体步骤如下：<br>在/themes/next/source/js/src下新建文件 clicklove.js ，接着把下面的代码拷贝粘贴到 clicklove.js 文件中<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &#39;&#39;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e&#x3D;0;e&lt;d.length;e++)d[e].alpha&lt;&#x3D;0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+&#x3D;.004,d[e].alpha-&#x3D;.013,d[e].el.style.cssText&#x3D;&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t&#x3D;&quot;function&quot;&#x3D;&#x3D;typeof e.onclick&amp;&amp;e.onclick;e.onclick&#x3D;function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a&#x3D;t.createElement(&quot;div&quot;);a.className&#x3D;&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a&#x3D;t.createElement(&quot;style&quot;);a.type&#x3D;&quot;text&#x2F;css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText&#x3D;e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d&#x3D;[];e.requestAnimationFrame&#x3D;function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3&#x2F;60)&#125;&#125;(),n()&#125;(window,document);</span><br></pre></td></tr></table></figure>
在\themes\next\layout_layout.swig文件末尾添加：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 页面点击小红心 --&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;src&#x2F;clicklove.js&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>另一种点击效果如图所示<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/click2.png" alt><br>在themes/next/source/js/src里面建一个叫fireworks.js的文件，代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;;</span><br></pre></td></tr></table></figure><br>打开themes/next/layout/_layout.swig,在&lt;/body&gt;上面写下如下代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% if theme.fireworks %&#125;</span><br><span class="line">   &lt;canvas class&#x3D;&quot;fireworks&quot; style&#x3D;&quot;position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;&quot; &gt;&lt;&#x2F;canvas&gt; </span><br><span class="line">   &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;&#x2F;cdn.bootcss.com&#x2F;animejs&#x2F;2.2.0&#x2F;anime.min.js&quot;&gt;&lt;&#x2F;script&gt; </span><br><span class="line">   &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;src&#x2F;fireworks.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><br>打开主题配置文件themes\next_config.yml，在里面最后写下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Fireworks</span><br><span class="line">fireworks: true</span><br></pre></td></tr></table></figure></p>
<ul>
<li>12.动态背景<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/bg.gif" alt><br>上面这种只是其中一种动态背景，新版的Next主题集成了该功能，只需要在主题配置themes\next_config.yml中设置如下即可，下面每个模块只设置其中一个为true，具体效果如何可自己尝试：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Canvas-nest</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-nest</span><br><span class="line">canvas_nest: # 网络背景</span><br><span class="line">  enable: true</span><br><span class="line">  onmobile: true # display on mobile or not</span><br><span class="line">  color: &#39;0,0,0&#39; # RGB values, use &#39;,&#39; to separate</span><br><span class="line">  opacity: 0.5 # the opacity of line: 0~1</span><br><span class="line">  zIndex: -1 # z-index property of the background</span><br><span class="line">  count: 150 # the number of lines</span><br><span class="line"></span><br><span class="line"># JavaScript 3D library.</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-three</span><br><span class="line"># three_waves</span><br><span class="line">three_waves: false</span><br><span class="line"># canvas_lines</span><br><span class="line">canvas_lines: false</span><br><span class="line"># canvas_sphere</span><br><span class="line">canvas_sphere: false</span><br><span class="line"></span><br><span class="line"># Canvas-ribbon</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-ribbon</span><br><span class="line"># size: The width of the ribbon.</span><br><span class="line"># alpha: The transparency of the ribbon.</span><br><span class="line"># zIndex: The display level of the ribbon.</span><br><span class="line">canvas_ribbon:</span><br><span class="line">  enable: false</span><br><span class="line">  size: 300</span><br><span class="line">  alpha: 0.6</span><br><span class="line">  zIndex: -1</span><br></pre></td></tr></table></figure>
另外需要在blog中下载相应资源包，具体见上面的链接，下面我给出canvas_nest的下载方式：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-nest themes&#x2F;next&#x2F;source&#x2F;lib&#x2F;canvas-nest</span><br></pre></td></tr></table></figure></li>
<li><p>13.文章结束标志<br>在路径 \themes\next\layout_macro 中新建 passage-end-tag.swig 文件,并添加以下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">        &lt;div style&#x3D;&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class&#x3D;&quot;fa fa-paw&quot;&gt;&lt;&#x2F;i&gt;感谢您的阅读-------------&lt;&#x2F;div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>
<p>接着打开\themes\next\layout_macro\post.swig文件，在post-body 之后(END POST BODY)， post-footer 之前添加如代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &#39;passage-end-tag.swig&#39; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>
<p>然后打开主题配置文件（_config.yml),在末尾添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 文章末尾添加“本文结束”标记</span><br><span class="line">passage_end_tag:</span><br><span class="line">  enabled: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>14.文字阴影效果<br>打开\themes\next\source\css_custom\custom.styl,向里面加入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 主页文章添加阴影效果</span><br><span class="line"> .post &#123;</span><br><span class="line">   margin-top: 60px;</span><br><span class="line">   margin-bottom: 60px;</span><br><span class="line">   padding: 25px;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>15.博客底部布局<br>对应主题配置文件themes\next_config.yml中的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  since: 2020 #建站时间</span><br><span class="line"></span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  icon:</span><br><span class="line">    # Icon name in fontawesome, see: https:&#x2F;&#x2F;fontawesome.com&#x2F;v4.7.0&#x2F;icons&#x2F;</span><br><span class="line">    # &#96;heart&#96; is recommended with animation in red (#ff0000).</span><br><span class="line">    name: heart  #作者图标（默认是author人像)</span><br><span class="line">    # If you want to animate the icon, set it to true.</span><br><span class="line">    animated: true #图标是否闪动</span><br><span class="line">    # Change the color of icon, using Hex Code.</span><br><span class="line">    color: &quot;#808080&quot; #图标颜色</span><br><span class="line"></span><br><span class="line">  # If not defined, &#96;author&#96; from Hexo main config will be used.</span><br><span class="line">  copyright: 张帅 #别填bool型，最后显示的东西是copyright || author，即左边没有设置的话就显示作者</span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  powered:</span><br><span class="line">    # Hexo link (Powered by Hexo).</span><br><span class="line">    enable: false #是否显示 Powered by hexo</span><br><span class="line">    # Version info of Hexo after Hexo link (vX.X.X).</span><br><span class="line">    version: false #是否显示Hexo版本</span><br><span class="line"></span><br><span class="line">  theme:</span><br><span class="line">    # Theme &amp; scheme info link (Theme - NexT.scheme).</span><br><span class="line">    enable: false #是否显示主题信息</span><br><span class="line">    # Version info of NexT after scheme info (vX.X.X).</span><br><span class="line">    version: false #是否显示主题版本</span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  # Beian icp information for Chinese users. In China, every legal website should have a beian icp in website footer.</span><br><span class="line">  # http:&#x2F;&#x2F;www.miitbeian.gov.cn</span><br><span class="line">  beian:</span><br><span class="line">    enable: false #是否显示网站备案信息</span><br><span class="line">    icp:</span><br><span class="line"></span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  # Any custom text can be defined here.</span><br><span class="line">  #custom_text: Hosted by &lt;a href&#x3D;&quot;https:&#x2F;&#x2F;pages.coding.me&quot; class&#x3D;&quot;theme-link&quot; rel&#x3D;&quot;noopener&quot; target&#x3D;&quot;_blank&quot;&gt;Coding Pages&lt;&#x2F;a&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>16.添加页面宠物<br>首先在博客目录下执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -save hexo-helper-live2d</span><br></pre></td></tr></table></figure>
<p>然后在站点配置文件中加入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  scriptFrom: local</span><br><span class="line">  pluginRootPath: live2dw&#x2F;</span><br><span class="line">  pluginJsPath: lib&#x2F;</span><br><span class="line">  pluginModelPath: assets&#x2F;</span><br><span class="line">  tagMode: false</span><br><span class="line">  model:</span><br><span class="line">    use: live2d-widget-model-wanko  #选择哪种模型</span><br><span class="line">  display: #放置位置和大小</span><br><span class="line">    position: right</span><br><span class="line">    width: 150</span><br><span class="line">    height: 300</span><br><span class="line">  mobile:</span><br><span class="line">    show: false #是否在手机端显示</span><br></pre></td></tr></table></figure>
<p>上面模型的选择可在lived2d中选择，并下载相应的模型：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install live2d-widget-model-wanko</span><br></pre></td></tr></table></figure>
</li>
<li><p>17.设置博客摘要显示<br>对于摘要显示，首先我们需要开启摘要功能，修改主题配置文件themes\next_config.yml：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Automatically scroll page to section which is under &lt;!-- more --&gt; mark.</span><br><span class="line">scroll_to_more: true #选取博客正文&lt;!--more--&gt;前的内容</span><br><span class="line"></span><br><span class="line"># Automatically saving scroll position on each post&#x2F;page in cookies.</span><br><span class="line">save_scroll: false</span><br><span class="line"></span><br><span class="line"># Automatically excerpt description in homepage as preamble text.</span><br><span class="line">excerpt_description: true #自动截取摘要</span><br><span class="line"></span><br><span class="line"># Automatically Excerpt. Not recommend.</span><br><span class="line"># Use &lt;!-- more --&gt; in the post to control excerpt accurately.</span><br><span class="line">auto_excerpt: </span><br><span class="line">  enable: false #自动截取一定程度的摘要</span><br><span class="line">  length: 150</span><br><span class="line"></span><br><span class="line"># Read more button</span><br><span class="line"># If true, the read more button would be displayed in excerpt section.</span><br><span class="line">read_more_btn: true #显示阅读全文按钮</span><br></pre></td></tr></table></figure>
</li>
<li><p>18.设置RSS订阅<br>在博客主目录下执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure>
<p>在站点配置文件中修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Extensions</span><br><span class="line">## Plugins: http:&#x2F;&#x2F;hexo.io&#x2F;plugins&#x2F;</span><br><span class="line">plugins: hexo-generate-feed</span><br></pre></td></tr></table></figure>
<p>然后设置主题配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set rss to false to disable feed link.</span><br><span class="line"># Leave rss as empty to use site&#39;s feed link.</span><br><span class="line"># Set rss to specific value if you have burned your feed already.</span><br><span class="line">rss: &#x2F;atom.xml</span><br></pre></td></tr></table></figure>
</li>
<li><p>19.修改文章链接样式<br>修改文件 themes\next\source\css_common\components\post\post.styl，在末尾添加如下css样式，：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 文章内链接文本样式</span><br><span class="line">.post-body p a&#123;</span><br><span class="line">  color: #0593d3;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  border-bottom: 1px solid #0593d3;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #fc6423;</span><br><span class="line">    border-bottom: none;</span><br><span class="line">    border-bottom: 1px solid #fc6423;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="支持Latex"><a href="#支持Latex" class="headerlink" title="支持Latex"></a><span id="header5">支持Latex</span></h1><h2 id="第一步：-安装Kramed"><a href="#第一步：-安装Kramed" class="headerlink" title="第一步： 安装Kramed"></a>第一步： 安装Kramed</h2><p>更换Hexo的默认的hexo-renderer-marked渲染引擎，改为hexo-renderer-kramed。在终端输入命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p>
<h2 id="第二步：更改文件配置"><a href="#第二步：更改文件配置" class="headerlink" title="第二步：更改文件配置"></a>第二步：更改文件配置</h2><p>打开/node_modules/hexo-renderer-kramed/lib/renderer.js,更改：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function (text) &#123;</span><br><span class="line">    &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + 1 + $$</span><br><span class="line">    return text.replace(&#x2F;&#96;$(.*?)$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>为，直接返回text<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function (text) &#123;</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="第三步-停止使用-hexo-math，并安装mathjax包"><a href="#第三步-停止使用-hexo-math，并安装mathjax包" class="headerlink" title="第三步: 停止使用 hexo-math，并安装mathjax包"></a>第三步: 停止使用 hexo-math，并安装mathjax包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure>
<h2 id="第四步-更新-Mathjax-的-配置文件"><a href="#第四步-更新-Mathjax-的-配置文件" class="headerlink" title="第四步: 更新 Mathjax 的 配置文件"></a>第四步: 更新 Mathjax 的 配置文件</h2><p>打开/node_modules/hexo-renderer-mathjax/mathjax.html ，注释掉第二个<code>&lt;script&gt;</code></p>
<h2 id="第五步-更改默认转义规则"><a href="#第五步-更改默认转义规则" class="headerlink" title="第五步: 更改默认转义规则"></a>第五步: 更改默认转义规则</h2><p>打开/node_modules/kramed/lib/rules/inline.js<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">escape: &#x2F;^\([\&#96;*&#123;&#125;[]()#$+-.!_&gt;])&#x2F;,</span><br><span class="line">更改为</span><br><span class="line">escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">    </span><br><span class="line">em: &#x2F;^b_((?:__|[sS])+?)_b|^*((?:**|[sS])+?)*(?!*)&#x2F;,</span><br><span class="line">更改为</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure></p>
<h2 id="第六步-开启mathjax"><a href="#第六步-开启mathjax" class="headerlink" title="第六步: 开启mathjax"></a>第六步: 开启mathjax</h2><p>打开你所使用主题的_config.yml文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure></p>
<h2 id="最后的最后"><a href="#最后的最后" class="headerlink" title="最后的最后"></a>最后的最后</h2><p>在每个文章的开头添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>notebook使用</title>
    <url>/2020/07/08/notebook%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">主题修改</a></li>
<li>2.<a href="#header2">密码修改</a></li>
<li>3.<a href="#header3">添加内核</a></li>
</ul>
<a id="more"></a>
<h1 id="主题修改参考"><a href="#主题修改参考" class="headerlink" title="主题修改参考"></a><span id="header1">主题修改<a href="https://blog.csdn.net/wh8514/article/details/81532286/" target="_blank" rel="noopener">参考</a></span></h1><p>安装Jupyter主题：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install  jupyterthemes</span><br></pre></td></tr></table></figure><br>更新Jupyter主题：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install --upgrade jupyterthemes</span><br></pre></td></tr></table></figure><br>装和更新成功以后，可以查看可用主题：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jt -l</span><br></pre></td></tr></table></figure><br>如图<img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/2.jpg" alt="x"><br>参数含义如下<br><img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/3.png" alt="x"><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jt -t monokai -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T -N</span><br><span class="line">-f(字体)  -fs(字体大小) -cellw(占屏比或宽度)  -ofs(输出段的字号)  -T(显示工具栏)  -N(显示自己主机名)</span><br></pre></td></tr></table></figure><br>解决输出显示不全的问题<br><br>在C:\Users\XXX.jupyter\custom\custom.css里,找到div.output_area<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">div.output_area&#123;</span><br><span class="line">    display:-webkit-box;</span><br><span class="line">&#125;</span><br><span class="line">改为</span><br><span class="line">div.output_area&#123;</span><br><span class="line">    display:-webkit-box;</span><br><span class="line">    padding:13px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="密码修改"><a href="#密码修改" class="headerlink" title="密码修改"></a><span id="header2">密码修改</span></h1><p>终端运行，输入新的密码即可<a href="https://blog.csdn.net/qq_36950604/article/details/103848631" target="_blank" rel="noopener">参考</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure><br><img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/1.jpg" alt="x"></p>
<h1 id="添加内核"><a href="#添加内核" class="headerlink" title="添加内核"></a><span id="header3">添加内核</span></h1><ul>
<li>首先添加虚拟环境,比如我要添加py版本为2.7,环境名为py2的环境<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n py2 python&#x3D;2.7</span><br></pre></td></tr></table></figure></li>
<li>然后进入当前虚拟环境下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">activate py2</span><br></pre></td></tr></table></figure></li>
<li>在虚拟环境中安装ipykernel<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install ipykernel</span><br></pre></td></tr></table></figure></li>
<li>添加内核到jupyter里:python -m ipykernel install —user —name 环境名称 —display-name “在jupyter中显示的环境名称”，注意不要忘记了双引号<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m ipykernel install --user --name py2 --display-name &quot;py2&quot;</span><br></pre></td></tr></table></figure>
成功<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/py2-1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/py2.png" alt></li>
</ul>
]]></content>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv-GUI特性</title>
    <url>/2020/07/12/opencv-GUI%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">图像入门</a></li>
<li>2.<a href="#header2">视频入门</a></li>
<li>3.<a href="#header3">绘图</a></li>
<li>4.<a href="#header4">鼠标作为画笔</a></li>
<li>5.<a href="#header5">轨迹栏作为调色板</a></li>
</ul>
<h1 id="图像入门"><a href="#图像入门" class="headerlink" title="图像入门"></a><span id="header1">图像入门</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>在这里，你将学习如何读取图像，如何显示图像以及如何将其保存回去</li>
<li>你将学习以下功能：cv.imread()，cv.imshow()，cv.imwrite()</li>
<li>(可选)你将学习如何使用Matplotlib显示图像<a id="more"></a>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h2 id="读取图像"><a href="#读取图像" class="headerlink" title="读取图像"></a>读取图像</h2><ul>
<li>使用<strong>cv.imread</strong>()函数读取图像。</li>
<li>图像应该在工作目录或图像的完整路径应给出。<br>第二个参数是一个标志，它指定了读取图像的方式。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.IMREAD_COLOR： 加载彩色图像。任何图像的透明度都会被忽视。它是默认标志。</span><br><span class="line">cv.IMREAD_GRAYSCALE：以灰度模式加载图像</span><br><span class="line">cv.IMREAD_UNCHANGED：加载图像，包括alpha通道</span><br></pre></td></tr></table></figure>
注意 除了这三个标志，你可以分别简单地传递整数1、0或-1。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img=cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img</span><br></pre></td></tr></table></figure>
<pre><code>array([[181, 247, 251, ..., 203, 177, 170],
       [217, 255, 250, ..., 211, 178, 167],
       [247, 255, 245, ..., 218, 180, 164],
       ...,
       [ 13,  13,  12, ...,   0,  16,   0],
       [ 11,  10,  10, ...,   0,   0,   0],
       [ 17,  17,  16, ...,   0,   4,   0]], dtype=uint8)
</code></pre><h2 id="显示图像"><a href="#显示图像" class="headerlink" title="显示图像"></a>显示图像</h2><ul>
<li>使用函数<strong>cv.imshow()</strong>在窗口中显示图像。窗口自动适合图像尺寸。</li>
<li>第一个参数是窗口名称，它是一个字符串。</li>
<li>第二个参数是我们的对象。你可以根据需要创建任意多个窗口，但可以使用不同的窗口名称。</li>
</ul>
<p>cv.waitKey()是一个键盘绑定函数。其参数是以毫秒为单位的时间。</p>
<p>该函数等待任何键盘事件指定的毫秒。如果您在这段时间内按下任何键，程序将继续运行。</p>
<p>如果<strong>0</strong>被传递，它将无限期地等待一次敲击键。</p>
<p>它也可以设置为检测特定的按键，例如，如果按下键 a 等</p>
<p>cv.destroyAllWindows()只会破坏我们创建的所有窗口。</p>
<p>如果要销毁任何特定的窗口，请使用函数 cv.destroyWindow()在其中传递确切的窗口名称作为参数。</p>
<p>在特殊情况下，你可以创建一个空窗口，然后再将图像加载到该窗口。在这种情况下，你可以指定窗口是否可调整大小。</p>
<p>这是通过功能<strong>cv.namedWindow</strong>()完成的。默认情况下，该标志为<strong>cv.WINDOW_AUTOSIZE</strong>。</p>
<p>但是，如果将标志指定为<strong>cv.WINDOW_NORMAL</strong>，则可以调整窗口大小。当图像尺寸过大以及向窗口添加跟踪栏时，这将很有帮助。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.namedWindow(<span class="string">'image'</span>,cv.WINDOW_NORMAL)</span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h2 id="写入图像"><a href="#写入图像" class="headerlink" title="写入图像"></a>写入图像</h2><p>使用函数<strong>cv.imwrite</strong>()保存图像。</p>
<p>第一个参数是文件名，第二个参数是要保存的图像。</p>
<p>cv.imwrite(‘messigray.png’，img)这会将图像以PNG格式保存在工作目录中。</p>
<p>在下面的程序中，以灰度加载图像，显示图像，按s保存图像并退出，或者按ESC键直接退出而不保存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">k =cv.waitKey(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> k == <span class="number">27</span>: <span class="comment"># 按下esc时</span></span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"><span class="keyword">elif</span> k == ord(<span class="string">'s'</span>): <span class="comment"># 按下s时</span></span><br><span class="line">    cv.imwrite(<span class="string">'copy1.jpg'</span>,img)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">plt.imshow(img, interpolation = <span class="string">'bicubic'</span>)</span><br><span class="line"><span class="comment"># plt.imshow(img)</span></span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv-GUI%E7%89%B9%E6%80%A7/output_13_0.png" alt="png"></p>
<h1 id="视频入门"><a href="#视频入门" class="headerlink" title="视频入门"></a><span id="header2">视频入门</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><ul>
<li>学习读取视频，显示视频和保存视频。</li>
<li>学习从相机捕捉并显示它。</li>
<li>你将学习以下功能：cv.VideoCapture()，cv.VideoWriter()<!--more-->
</li>
</ul>
<p>要捕获视频，你需要创建一个 VideoCapture 对象。</p>
<p>它的参数可以是设备索引或视频文件的名称。设备索引就是指定哪个摄像头的数字。</p>
<p>正常情况下，一个摄像头会被连接(就像我的情况一样)。所以我简单地传0(或-1)。你可以通过传递1来选择第二个相机，以此类推。</p>
<p>在此之后，你可以逐帧捕获。但是在最后，不要忘记释放俘虏。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> cap.isOpened():</span><br><span class="line">    print(<span class="string">"Cannot open camera"</span>)</span><br><span class="line">    exit()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 逐帧捕获</span></span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="comment"># 如果正确读取帧，ret为True</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 我们在框架上的操作到这里</span></span><br><span class="line">    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># 显示结果帧e</span></span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, gray)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment"># 完成所有操作后，释放捕获器</span></span><br><span class="line">cap.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<pre><code>Cannot open camera
Can&#39;t receive frame (stream end?). Exiting ...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="string">'video1.flv'</span>)</span><br><span class="line"><span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="comment"># 如果正确读取帧，ret为True</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, gray)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cap.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="string">'video1.flv'</span>)</span><br><span class="line"><span class="comment"># 定义编解码器并创建VideoWriter对象</span></span><br><span class="line">fourcc = cv.VideoWriter_fourcc(*<span class="string">'DIVX'</span>)</span><br><span class="line">out = cv.VideoWriter(<span class="string">'output.flv'</span>, fourcc, <span class="number">20.0</span>, (<span class="number">640</span>,  <span class="number">480</span>))</span><br><span class="line"><span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    frame = cv.flip(frame, <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 写翻转的框架</span></span><br><span class="line">    out.write(frame)</span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, frame)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment"># 完成工作后释放所有内容</span></span><br><span class="line">cap.release()</span><br><span class="line">out.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<pre><code>Can&#39;t receive frame (stream end?). Exiting ...
</code></pre><h1 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a><span id="header3">绘图</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><ul>
<li>学习使用OpenCV绘制不同的几何形状</li>
<li>您将学习以下功能：cv.line()，cv.circle()，cv.rectangle()，cv.ellipse()，cv.putText()等。</li>
<li>在上述所有功能中，您将看到一些常见的参数，如下所示：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img：您要绘制形状的图像</span><br><span class="line">color：形状的颜色。对于BGR，将其作为元组传递，例如：(255,0,0)对于蓝色。对于灰度，只需传递标量值即可。</span><br><span class="line">厚度：线或圆等的粗细。如果对闭合图形（如圆）传递-1 ，它将填充形状。默认厚度&#x3D; 1</span><br><span class="line">lineType：线的类型，是否为8连接线，抗锯齿线等。默认情况下，为8连接线。**cv.LINE_AA**给出了抗锯齿的线条，看起来非常适合曲线。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制一条线，您需要传递线的开始和结束坐标。我们将创建一个黑色图像，并从左上角到右下角在其上绘制一条蓝线。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line"><span class="comment"># 绘制一条厚度为5的蓝色对角线</span></span><br><span class="line">cv.line(img,(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">511</span>,<span class="number">511</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">5</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制矩形，您需要矩形的左上角和右下角。这次，我们将在图像的右上角绘制一个绿色矩形。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.rectangle(img,(<span class="number">384</span>,<span class="number">0</span>),(<span class="number">510</span>,<span class="number">128</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制一个圆，需要其中心坐标和半径。我们将在上面绘制的矩形内绘制一个圆。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.circle(img,(<span class="number">447</span>,<span class="number">63</span>), <span class="number">63</span>, (<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="number">-1</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制椭圆，我们需要传递几个参数。一个参数是中心位置（x，y）。</span></span><br><span class="line"><span class="comment"># 下一个参数是轴长度（长轴长度，短轴长度）。</span></span><br><span class="line"><span class="comment"># angle是椭圆沿逆时针方向旋转的角度。</span></span><br><span class="line"><span class="comment"># startAngle和endAngle表示从主轴沿顺时针方向测量的椭圆弧的开始和结束。即给出0和360给出完整的椭圆</span></span><br><span class="line"><span class="comment"># 下面的示例在图像的中心绘制一个椭圆形。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.ellipse(img,(<span class="number">256</span>,<span class="number">256</span>),(<span class="number">100</span>,<span class="number">50</span>),<span class="number">0</span>,<span class="number">0</span>,<span class="number">180</span>,<span class="number">255</span>,<span class="number">-1</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要绘制多边形，首先需要顶点的坐标。将这些点组成形状为ROWSx1x2的数组，其中ROWS是顶点数，并且其类型应为int32。</span></span><br><span class="line"><span class="comment"># 在这里，我们绘制了一个带有四个顶点的黄色小多边形。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">pts = np.array([[<span class="number">10</span>,<span class="number">5</span>],[<span class="number">20</span>,<span class="number">30</span>],[<span class="number">270</span>,<span class="number">20</span>],[<span class="number">150</span>,<span class="number">5</span>]], np.int32)</span><br><span class="line">pts = pts.reshape((<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">cv.polylines(img,[pts],<span class="literal">True</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果第三个参数为False，您将获得一条连接所有点的折线，而不是闭合形状。 </span></span><br><span class="line"><span class="comment"># cv.polylines()可用于绘制多条线。只需创建要绘制的所有线条的列表，然后将其传递给函数即可。</span></span><br><span class="line"><span class="comment"># 所有线条将单独绘制。与为每条线调用**cv.line**相比，绘制一组线是一种更好，更快的方法。</span></span><br><span class="line"><span class="comment"># 创建黑色的图像</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">pts = np.array([[<span class="number">10</span>,<span class="number">5</span>],[<span class="number">20</span>,<span class="number">30</span>],[<span class="number">270</span>,<span class="number">20</span>],[<span class="number">150</span>,<span class="number">5</span>]], np.int32)</span><br><span class="line">pts = pts.reshape((<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">cv.polylines(img,[pts],<span class="literal">False</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要将文本放入图像中，需要指定以下内容。 </span></span><br><span class="line"><span class="comment"># - 您要写入的文字数据 </span></span><br><span class="line"><span class="comment"># - 您要放置它的位置坐标（即数据开始的左下角）。 </span></span><br><span class="line"><span class="comment"># - 字体类型（检查**cv.putText**文档以获取受支持的字体）</span></span><br><span class="line"><span class="comment"># - 字体比例（指定字体大小） </span></span><br><span class="line"><span class="comment"># - 常规的内容，例如颜色，厚度，线条类型等。为了获得更好的外观，建议使用lineType = cv.LINE_AA。</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">font = cv.FONT_HERSHEY_SIMPLEX</span><br><span class="line">cv.putText(img,<span class="string">'OpenCV'</span>,(<span class="number">10</span>,<span class="number">500</span>), font, <span class="number">4</span>,(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>),<span class="number">2</span>,cv.LINE_AA)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h1 id="鼠标作为画笔"><a href="#鼠标作为画笔" class="headerlink" title="鼠标作为画笔"></a><span id="header4">鼠标作为画笔</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><ul>
<li>了解如何在OpenCV中处理鼠标事件</li>
<li>您将学习以下功能：cv.setMouseCallback()</li>
</ul>
<p>简单演示<br>在这里，我们创建一个简单的应用程序，无论我们在哪里双击它，都可以在图像上绘制一个圆。</p>
<p>首先，我们创建一个鼠标回调函数，该函数在发生鼠标事件时执行。</p>
<p>鼠标事件可以是与鼠标相关的任何事物，例如左键按下，左键按下，左键双击等。</p>
<p>它为我们提供了每个鼠标事件的坐标(x，y)。通过此活动和地点，我们可以做任何我们喜欢的事情。</p>
<p>要列出所有可用的可用事件，请在Python终端中运行以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">events = [i <span class="keyword">for</span> i <span class="keyword">in</span> dir(cv) <span class="keyword">if</span> <span class="string">'EVENT'</span> <span class="keyword">in</span> i]</span><br><span class="line">print( events )</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;EVENT_FLAG_ALTKEY&#39;, &#39;EVENT_FLAG_CTRLKEY&#39;, &#39;EVENT_FLAG_LBUTTON&#39;, &#39;EVENT_FLAG_MBUTTON&#39;, &#39;EVENT_FLAG_RBUTTON&#39;, &#39;EVENT_FLAG_SHIFTKEY&#39;, &#39;EVENT_LBUTTONDBLCLK&#39;, &#39;EVENT_LBUTTONDOWN&#39;, &#39;EVENT_LBUTTONUP&#39;, &#39;EVENT_MBUTTONDBLCLK&#39;, &#39;EVENT_MBUTTONDOWN&#39;, &#39;EVENT_MBUTTONUP&#39;, &#39;EVENT_MOUSEHWHEEL&#39;, &#39;EVENT_MOUSEMOVE&#39;, &#39;EVENT_MOUSEWHEEL&#39;, &#39;EVENT_RBUTTONDBLCLK&#39;, &#39;EVENT_RBUTTONDOWN&#39;, &#39;EVENT_RBUTTONUP&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在我们双击的地方绘制一个圆圈</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># 鼠标回调函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_circle</span><span class="params">(event,x,y,flags,param)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> event == cv.EVENT_LBUTTONDBLCLK:</span><br><span class="line">        cv.circle(img,(x,y),<span class="number">100</span>,(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># 创建一个黑色的图像，一个窗口，并绑定到窗口的功能</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line">cv.setMouseCallback(<span class="string">'image'</span>,draw_circle)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按m之前拖拽画青色矩形</span></span><br><span class="line"><span class="comment"># 按m之后拖拽画红色直线</span></span><br><span class="line"><span class="comment"># 按esc退出</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">drawing = <span class="literal">False</span> <span class="comment"># 如果按下鼠标，则为真</span></span><br><span class="line">mode = <span class="literal">True</span> <span class="comment"># 如果为真，绘制矩形。按 m 键可以切换到曲线</span></span><br><span class="line">ix,iy = <span class="number">-1</span>,<span class="number">-1</span></span><br><span class="line"><span class="comment"># 鼠标回调函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_circle</span><span class="params">(event,x,y,flags,param)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ix,iy,drawing,mode</span><br><span class="line">    <span class="keyword">if</span> event == cv.EVENT_LBUTTONDOWN:</span><br><span class="line">        drawing = <span class="literal">True</span></span><br><span class="line">        ix,iy = x,y</span><br><span class="line">    <span class="keyword">elif</span> event == cv.EVENT_MOUSEMOVE:</span><br><span class="line">        <span class="keyword">if</span> drawing == <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="literal">True</span>:</span><br><span class="line">                cv.rectangle(img,(ix,iy),(x,y),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cv.circle(img,(x,y),<span class="number">25</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">elif</span> event == cv.EVENT_LBUTTONUP:</span><br><span class="line">        drawing = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="literal">True</span>:</span><br><span class="line">            cv.rectangle(img,(ix,iy),(x,y),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cv.circle(img,(x,y),<span class="number">25</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">-1</span>)</span><br><span class="line">            </span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line">cv.setMouseCallback(<span class="string">'image'</span>,draw_circle)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; cv.waitKey(<span class="number">20</span>) == ord(<span class="string">'m'</span>):</span><br><span class="line">        mode = ~mode</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h1 id="轨迹栏作为调色板"><a href="#轨迹栏作为调色板" class="headerlink" title="轨迹栏作为调色板"></a><span id="header5">轨迹栏作为调色板</span></h1><h2 id="目标-4"><a href="#目标-4" class="headerlink" title="目标"></a>目标</h2><ul>
<li>了解将轨迹栏固定到OpenCV窗口</li>
<li>您将学习以下功能：cv.getTrackbarPos，<strong>cv.createTrackbar</strong>等。</li>
</ul>
<p>对于cv.getTrackbarPos()函数，</p>
<p>第一个参数是轨迹栏名称，</p>
<p>第二个参数是它附加到的窗口名称，</p>
<p>第三个参数是默认值，</p>
<p>第四个参数是最大值，</p>
<p>第五个是执行的回调函数每次跟踪栏值更改。</p>
<p>回调函数始终具有默认参数，即轨迹栏位置。</p>
<p>在我们的例子中，函数什么都不做，所以我们简单地通过。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在这里，我们将创建一个简单的应用程序，以显示您指定的颜色。</span></span><br><span class="line"><span class="comment"># 您有一个显示颜色的窗口，以及三个用于指定B、G、R颜色的跟踪栏。滑动轨迹栏，并相应地更改窗口颜色。</span></span><br><span class="line"><span class="comment"># 默认情况下，初始颜色将设置为黑色。只有在该开关为ON的情况下，该应用程序才能在其中运行，否则屏幕始终为黑色。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nothing</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># 创建一个黑色的图像，一个窗口</span></span><br><span class="line">img = np.zeros((<span class="number">300</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line"><span class="comment"># 创建颜色变化的轨迹栏</span></span><br><span class="line">cv.createTrackbar(<span class="string">'R'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line">cv.createTrackbar(<span class="string">'G'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line">cv.createTrackbar(<span class="string">'B'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line"><span class="comment"># 为 ON/OFF 功能创建开关</span></span><br><span class="line">switch = <span class="string">'0 : OFF \n1 : ON'</span></span><br><span class="line">cv.createTrackbar(switch, <span class="string">'image'</span>,<span class="number">0</span>,<span class="number">1</span>,nothing)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    k = cv.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 得到四条轨迹的当前位置</span></span><br><span class="line">    r = cv.getTrackbarPos(<span class="string">'R'</span>,<span class="string">'image'</span>)</span><br><span class="line">    g = cv.getTrackbarPos(<span class="string">'G'</span>,<span class="string">'image'</span>)</span><br><span class="line">    b = cv.getTrackbarPos(<span class="string">'B'</span>,<span class="string">'image'</span>)</span><br><span class="line">    s = cv.getTrackbarPos(switch,<span class="string">'image'</span>)</span><br><span class="line">    <span class="keyword">if</span> s == <span class="number">0</span>:</span><br><span class="line">        img[:] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img[:] = [b,g,r]</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理1</title>
    <url>/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">改变颜色空间</a></li>
<li>2.<a href="#header2">图像几何变换</a></li>
<li>3.<a href="#header3">图像阈值</a></li>
<li>4.<a href="#header4">图像平滑</a><a id="more"></a>
</li>
</ul>
<h1 id="改变颜色空间"><a href="#改变颜色空间" class="headerlink" title="改变颜色空间"></a><span id="header1">改变颜色空间</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>在本教程中，你将学习如何将图像从一个色彩空间转换到另一个，像BGR↔灰色，BGR↔HSV等</li>
<li>除此之外，我们还将创建一个应用程序，以提取视频中的彩色对象</li>
<li>你将学习以下功能：cv.cvtColor，<strong>cv.inRange</strong>等。</li>
</ul>
<h2 id="改变颜色空间-1"><a href="#改变颜色空间-1" class="headerlink" title="改变颜色空间"></a>改变颜色空间</h2><p>OpenCV中有超过150种颜色空间转换方法。但是我们将研究只有两个最广泛使用的,BGR↔灰色和BGR↔HSV。</p>
<p>对于颜色转换，我们使用cv函数。cvtColor(input_image, flag)，其中flag决定转换的类型。</p>
<p>对于BGR→灰度转换，我们使用标志cv.COLOR_BGR2GRAY。类似地，对于BGR→HSV，我们使用标志cv.COLOR_BGR2HSV。</p>
<p>要获取其他标记，只需在Python终端中运行以下命令</p>
<p>注意 HSV的色相范围为[0,179]，饱和度范围为[0,255]，值范围为[0,255]。不同的软件使用不同的规模。</p>
<p>因此，如果你要将OpenCV值和它们比较，你需要将这些范围标准化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">flags = [i <span class="keyword">for</span> i <span class="keyword">in</span> dir(cv) <span class="keyword">if</span> i.startswith(<span class="string">'COLOR_'</span>)]</span><br><span class="line">flags</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;COLOR_BAYER_BG2BGR&#39;,
 &#39;COLOR_BAYER_BG2BGRA&#39;,
 &#39;COLOR_BAYER_BG2BGR_EA&#39;,
 &#39;COLOR_BAYER_BG2BGR_VNG&#39;,
 &#39;COLOR_BAYER_BG2GRAY&#39;,
 &#39;COLOR_BAYER_BG2RGB&#39;,
 &#39;COLOR_BAYER_BG2RGBA&#39;,
 &#39;COLOR_BAYER_BG2RGB_EA&#39;,
 &#39;COLOR_BAYER_BG2RGB_VNG&#39;,
 &#39;COLOR_BAYER_GB2BGR&#39;,
 &#39;COLOR_BAYER_GB2BGRA&#39;,
 &#39;COLOR_BAYER_GB2BGR_EA&#39;,
 &#39;COLOR_BAYER_GB2BGR_VNG&#39;,
 &#39;COLOR_BAYER_GB2GRAY&#39;,
 &#39;COLOR_BAYER_GB2RGB&#39;,
 &#39;COLOR_BAYER_GB2RGBA&#39;,
 &#39;COLOR_BAYER_GB2RGB_EA&#39;,
 &#39;COLOR_BAYER_GB2RGB_VNG&#39;,
 &#39;COLOR_BAYER_GR2BGR&#39;,
 &#39;COLOR_BAYER_GR2BGRA&#39;,
 &#39;COLOR_BAYER_GR2BGR_EA&#39;,
 &#39;COLOR_BAYER_GR2BGR_VNG&#39;,
 &#39;COLOR_BAYER_GR2GRAY&#39;,
 &#39;COLOR_BAYER_GR2RGB&#39;,
 &#39;COLOR_BAYER_GR2RGBA&#39;,
 &#39;COLOR_BAYER_GR2RGB_EA&#39;,
 &#39;COLOR_BAYER_GR2RGB_VNG&#39;,
 &#39;COLOR_BAYER_RG2BGR&#39;,
 &#39;COLOR_BAYER_RG2BGRA&#39;,
 &#39;COLOR_BAYER_RG2BGR_EA&#39;,
 &#39;COLOR_BAYER_RG2BGR_VNG&#39;,
 &#39;COLOR_BAYER_RG2GRAY&#39;,
 &#39;COLOR_BAYER_RG2RGB&#39;,
 &#39;COLOR_BAYER_RG2RGBA&#39;,
 &#39;COLOR_BAYER_RG2RGB_EA&#39;,
 &#39;COLOR_BAYER_RG2RGB_VNG&#39;,
 &#39;COLOR_BGR2BGR555&#39;,
 &#39;COLOR_BGR2BGR565&#39;,
 &#39;COLOR_BGR2BGRA&#39;,
 &#39;COLOR_BGR2GRAY&#39;,
 &#39;COLOR_BGR2HLS&#39;,
 &#39;COLOR_BGR2HLS_FULL&#39;,
 &#39;COLOR_BGR2HSV&#39;,
 &#39;COLOR_BGR2HSV_FULL&#39;,
 &#39;COLOR_BGR2LAB&#39;,
 &#39;COLOR_BGR2LUV&#39;,
 &#39;COLOR_BGR2Lab&#39;,
 &#39;COLOR_BGR2Luv&#39;,
 &#39;COLOR_BGR2RGB&#39;,
 &#39;COLOR_BGR2RGBA&#39;,
 &#39;COLOR_BGR2XYZ&#39;,
 &#39;COLOR_BGR2YCR_CB&#39;,
 &#39;COLOR_BGR2YCrCb&#39;,
 &#39;COLOR_BGR2YUV&#39;,
 &#39;COLOR_BGR2YUV_I420&#39;,
 &#39;COLOR_BGR2YUV_IYUV&#39;,
 &#39;COLOR_BGR2YUV_YV12&#39;,
 &#39;COLOR_BGR5552BGR&#39;,
 &#39;COLOR_BGR5552BGRA&#39;,
 &#39;COLOR_BGR5552GRAY&#39;,
 &#39;COLOR_BGR5552RGB&#39;,
 &#39;COLOR_BGR5552RGBA&#39;,
 &#39;COLOR_BGR5652BGR&#39;,
 &#39;COLOR_BGR5652BGRA&#39;,
 &#39;COLOR_BGR5652GRAY&#39;,
 &#39;COLOR_BGR5652RGB&#39;,
 &#39;COLOR_BGR5652RGBA&#39;,
 &#39;COLOR_BGRA2BGR&#39;,
 &#39;COLOR_BGRA2BGR555&#39;,
 &#39;COLOR_BGRA2BGR565&#39;,
 &#39;COLOR_BGRA2GRAY&#39;,
 &#39;COLOR_BGRA2RGB&#39;,
 &#39;COLOR_BGRA2RGBA&#39;,
 &#39;COLOR_BGRA2YUV_I420&#39;,
 &#39;COLOR_BGRA2YUV_IYUV&#39;,
 &#39;COLOR_BGRA2YUV_YV12&#39;,
 &#39;COLOR_BayerBG2BGR&#39;,
 &#39;COLOR_BayerBG2BGRA&#39;,
 &#39;COLOR_BayerBG2BGR_EA&#39;,
 &#39;COLOR_BayerBG2BGR_VNG&#39;,
 &#39;COLOR_BayerBG2GRAY&#39;,
 &#39;COLOR_BayerBG2RGB&#39;,
 &#39;COLOR_BayerBG2RGBA&#39;,
 &#39;COLOR_BayerBG2RGB_EA&#39;,
 &#39;COLOR_BayerBG2RGB_VNG&#39;,
 &#39;COLOR_BayerGB2BGR&#39;,
 &#39;COLOR_BayerGB2BGRA&#39;,
 &#39;COLOR_BayerGB2BGR_EA&#39;,
 &#39;COLOR_BayerGB2BGR_VNG&#39;,
 &#39;COLOR_BayerGB2GRAY&#39;,
 &#39;COLOR_BayerGB2RGB&#39;,
 &#39;COLOR_BayerGB2RGBA&#39;,
 &#39;COLOR_BayerGB2RGB_EA&#39;,
 &#39;COLOR_BayerGB2RGB_VNG&#39;,
 &#39;COLOR_BayerGR2BGR&#39;,
 &#39;COLOR_BayerGR2BGRA&#39;,
 &#39;COLOR_BayerGR2BGR_EA&#39;,
 &#39;COLOR_BayerGR2BGR_VNG&#39;,
 &#39;COLOR_BayerGR2GRAY&#39;,
 &#39;COLOR_BayerGR2RGB&#39;,
 &#39;COLOR_BayerGR2RGBA&#39;,
 &#39;COLOR_BayerGR2RGB_EA&#39;,
 &#39;COLOR_BayerGR2RGB_VNG&#39;,
 &#39;COLOR_BayerRG2BGR&#39;,
 &#39;COLOR_BayerRG2BGRA&#39;,
 &#39;COLOR_BayerRG2BGR_EA&#39;,
 &#39;COLOR_BayerRG2BGR_VNG&#39;,
 &#39;COLOR_BayerRG2GRAY&#39;,
 &#39;COLOR_BayerRG2RGB&#39;,
 &#39;COLOR_BayerRG2RGBA&#39;,
 &#39;COLOR_BayerRG2RGB_EA&#39;,
 &#39;COLOR_BayerRG2RGB_VNG&#39;,
 &#39;COLOR_COLORCVT_MAX&#39;,
 &#39;COLOR_GRAY2BGR&#39;,
 &#39;COLOR_GRAY2BGR555&#39;,
 &#39;COLOR_GRAY2BGR565&#39;,
 &#39;COLOR_GRAY2BGRA&#39;,
 &#39;COLOR_GRAY2RGB&#39;,
 &#39;COLOR_GRAY2RGBA&#39;,
 &#39;COLOR_HLS2BGR&#39;,
 &#39;COLOR_HLS2BGR_FULL&#39;,
 &#39;COLOR_HLS2RGB&#39;,
 &#39;COLOR_HLS2RGB_FULL&#39;,
 &#39;COLOR_HSV2BGR&#39;,
 &#39;COLOR_HSV2BGR_FULL&#39;,
 &#39;COLOR_HSV2RGB&#39;,
 &#39;COLOR_HSV2RGB_FULL&#39;,
 &#39;COLOR_LAB2BGR&#39;,
 &#39;COLOR_LAB2LBGR&#39;,
 &#39;COLOR_LAB2LRGB&#39;,
 &#39;COLOR_LAB2RGB&#39;,
 &#39;COLOR_LBGR2LAB&#39;,
 &#39;COLOR_LBGR2LUV&#39;,
 &#39;COLOR_LBGR2Lab&#39;,
 &#39;COLOR_LBGR2Luv&#39;,
 &#39;COLOR_LRGB2LAB&#39;,
 &#39;COLOR_LRGB2LUV&#39;,
 &#39;COLOR_LRGB2Lab&#39;,
 &#39;COLOR_LRGB2Luv&#39;,
 &#39;COLOR_LUV2BGR&#39;,
 &#39;COLOR_LUV2LBGR&#39;,
 &#39;COLOR_LUV2LRGB&#39;,
 &#39;COLOR_LUV2RGB&#39;,
 &#39;COLOR_Lab2BGR&#39;,
 &#39;COLOR_Lab2LBGR&#39;,
 &#39;COLOR_Lab2LRGB&#39;,
 &#39;COLOR_Lab2RGB&#39;,
 &#39;COLOR_Luv2BGR&#39;,
 &#39;COLOR_Luv2LBGR&#39;,
 &#39;COLOR_Luv2LRGB&#39;,
 &#39;COLOR_Luv2RGB&#39;,
 &#39;COLOR_M_RGBA2RGBA&#39;,
 &#39;COLOR_RGB2BGR&#39;,
 &#39;COLOR_RGB2BGR555&#39;,
 &#39;COLOR_RGB2BGR565&#39;,
 &#39;COLOR_RGB2BGRA&#39;,
 &#39;COLOR_RGB2GRAY&#39;,
 &#39;COLOR_RGB2HLS&#39;,
 &#39;COLOR_RGB2HLS_FULL&#39;,
 &#39;COLOR_RGB2HSV&#39;,
 &#39;COLOR_RGB2HSV_FULL&#39;,
 &#39;COLOR_RGB2LAB&#39;,
 &#39;COLOR_RGB2LUV&#39;,
 &#39;COLOR_RGB2Lab&#39;,
 &#39;COLOR_RGB2Luv&#39;,
 &#39;COLOR_RGB2RGBA&#39;,
 &#39;COLOR_RGB2XYZ&#39;,
 &#39;COLOR_RGB2YCR_CB&#39;,
 &#39;COLOR_RGB2YCrCb&#39;,
 &#39;COLOR_RGB2YUV&#39;,
 &#39;COLOR_RGB2YUV_I420&#39;,
 &#39;COLOR_RGB2YUV_IYUV&#39;,
 &#39;COLOR_RGB2YUV_YV12&#39;,
 &#39;COLOR_RGBA2BGR&#39;,
 &#39;COLOR_RGBA2BGR555&#39;,
 &#39;COLOR_RGBA2BGR565&#39;,
 &#39;COLOR_RGBA2BGRA&#39;,
 &#39;COLOR_RGBA2GRAY&#39;,
 &#39;COLOR_RGBA2M_RGBA&#39;,
 &#39;COLOR_RGBA2RGB&#39;,
 &#39;COLOR_RGBA2YUV_I420&#39;,
 &#39;COLOR_RGBA2YUV_IYUV&#39;,
 &#39;COLOR_RGBA2YUV_YV12&#39;,
 &#39;COLOR_RGBA2mRGBA&#39;,
 &#39;COLOR_XYZ2BGR&#39;,
 &#39;COLOR_XYZ2RGB&#39;,
 &#39;COLOR_YCR_CB2BGR&#39;,
 &#39;COLOR_YCR_CB2RGB&#39;,
 &#39;COLOR_YCrCb2BGR&#39;,
 &#39;COLOR_YCrCb2RGB&#39;,
 &#39;COLOR_YUV2BGR&#39;,
 &#39;COLOR_YUV2BGRA_I420&#39;,
 &#39;COLOR_YUV2BGRA_IYUV&#39;,
 &#39;COLOR_YUV2BGRA_NV12&#39;,
 &#39;COLOR_YUV2BGRA_NV21&#39;,
 &#39;COLOR_YUV2BGRA_UYNV&#39;,
 &#39;COLOR_YUV2BGRA_UYVY&#39;,
 &#39;COLOR_YUV2BGRA_Y422&#39;,
 &#39;COLOR_YUV2BGRA_YUNV&#39;,
 &#39;COLOR_YUV2BGRA_YUY2&#39;,
 &#39;COLOR_YUV2BGRA_YUYV&#39;,
 &#39;COLOR_YUV2BGRA_YV12&#39;,
 &#39;COLOR_YUV2BGRA_YVYU&#39;,
 &#39;COLOR_YUV2BGR_I420&#39;,
 &#39;COLOR_YUV2BGR_IYUV&#39;,
 &#39;COLOR_YUV2BGR_NV12&#39;,
 &#39;COLOR_YUV2BGR_NV21&#39;,
 &#39;COLOR_YUV2BGR_UYNV&#39;,
 &#39;COLOR_YUV2BGR_UYVY&#39;,
 &#39;COLOR_YUV2BGR_Y422&#39;,
 &#39;COLOR_YUV2BGR_YUNV&#39;,
 &#39;COLOR_YUV2BGR_YUY2&#39;,
 &#39;COLOR_YUV2BGR_YUYV&#39;,
 &#39;COLOR_YUV2BGR_YV12&#39;,
 &#39;COLOR_YUV2BGR_YVYU&#39;,
 &#39;COLOR_YUV2GRAY_420&#39;,
 &#39;COLOR_YUV2GRAY_I420&#39;,
 &#39;COLOR_YUV2GRAY_IYUV&#39;,
 &#39;COLOR_YUV2GRAY_NV12&#39;,
 &#39;COLOR_YUV2GRAY_NV21&#39;,
 &#39;COLOR_YUV2GRAY_UYNV&#39;,
 &#39;COLOR_YUV2GRAY_UYVY&#39;,
 &#39;COLOR_YUV2GRAY_Y422&#39;,
 &#39;COLOR_YUV2GRAY_YUNV&#39;,
 &#39;COLOR_YUV2GRAY_YUY2&#39;,
 &#39;COLOR_YUV2GRAY_YUYV&#39;,
 &#39;COLOR_YUV2GRAY_YV12&#39;,
 &#39;COLOR_YUV2GRAY_YVYU&#39;,
 &#39;COLOR_YUV2RGB&#39;,
 &#39;COLOR_YUV2RGBA_I420&#39;,
 &#39;COLOR_YUV2RGBA_IYUV&#39;,
 &#39;COLOR_YUV2RGBA_NV12&#39;,
 &#39;COLOR_YUV2RGBA_NV21&#39;,
 &#39;COLOR_YUV2RGBA_UYNV&#39;,
 &#39;COLOR_YUV2RGBA_UYVY&#39;,
 &#39;COLOR_YUV2RGBA_Y422&#39;,
 &#39;COLOR_YUV2RGBA_YUNV&#39;,
 &#39;COLOR_YUV2RGBA_YUY2&#39;,
 &#39;COLOR_YUV2RGBA_YUYV&#39;,
 &#39;COLOR_YUV2RGBA_YV12&#39;,
 &#39;COLOR_YUV2RGBA_YVYU&#39;,
 &#39;COLOR_YUV2RGB_I420&#39;,
 &#39;COLOR_YUV2RGB_IYUV&#39;,
 &#39;COLOR_YUV2RGB_NV12&#39;,
 &#39;COLOR_YUV2RGB_NV21&#39;,
 &#39;COLOR_YUV2RGB_UYNV&#39;,
 &#39;COLOR_YUV2RGB_UYVY&#39;,
 &#39;COLOR_YUV2RGB_Y422&#39;,
 &#39;COLOR_YUV2RGB_YUNV&#39;,
 &#39;COLOR_YUV2RGB_YUY2&#39;,
 &#39;COLOR_YUV2RGB_YUYV&#39;,
 &#39;COLOR_YUV2RGB_YV12&#39;,
 &#39;COLOR_YUV2RGB_YVYU&#39;,
 &#39;COLOR_YUV420P2BGR&#39;,
 &#39;COLOR_YUV420P2BGRA&#39;,
 &#39;COLOR_YUV420P2GRAY&#39;,
 &#39;COLOR_YUV420P2RGB&#39;,
 &#39;COLOR_YUV420P2RGBA&#39;,
 &#39;COLOR_YUV420SP2BGR&#39;,
 &#39;COLOR_YUV420SP2BGRA&#39;,
 &#39;COLOR_YUV420SP2GRAY&#39;,
 &#39;COLOR_YUV420SP2RGB&#39;,
 &#39;COLOR_YUV420SP2RGBA&#39;,
 &#39;COLOR_YUV420p2BGR&#39;,
 &#39;COLOR_YUV420p2BGRA&#39;,
 &#39;COLOR_YUV420p2GRAY&#39;,
 &#39;COLOR_YUV420p2RGB&#39;,
 &#39;COLOR_YUV420p2RGBA&#39;,
 &#39;COLOR_YUV420sp2BGR&#39;,
 &#39;COLOR_YUV420sp2BGRA&#39;,
 &#39;COLOR_YUV420sp2GRAY&#39;,
 &#39;COLOR_YUV420sp2RGB&#39;,
 &#39;COLOR_YUV420sp2RGBA&#39;,
 &#39;COLOR_mRGBA2RGBA&#39;]
</code></pre><h2 id="如何找到要追踪的HSV值？"><a href="#如何找到要追踪的HSV值？" class="headerlink" title="如何找到要追踪的HSV值？"></a>如何找到要追踪的HSV值？</h2><p>这是在stackoverflow.com上发现的一个常见问题。它非常简单，你可以使用相同的函数<strong>cv.cvtColor()</strong>。</p>
<p>你只需传递你想要的BGR值，而不是传递图像。例如，要查找绿色的HSV值，请在Python终端中尝试以下命令</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">green = np.uint8([[[<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>]]])</span><br><span class="line">hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)</span><br><span class="line">hsv_green</span><br></pre></td></tr></table></figure>
<pre><code>array([[[ 60, 255, 255]]], dtype=uint8)
</code></pre><h1 id="图像的几何变换"><a href="#图像的几何变换" class="headerlink" title="图像的几何变换"></a><span id="header2">图像的几何变换</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>学习将不同的几何变换应用到图像上，如平移、旋转、仿射变换等。</p>
<p>你会看到这些函数: cv.getPerspectiveTransform</p>
<h2 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h2><p>OpenCV提供了两个转换函数<strong>cv.warpAffine</strong>和<strong>cv.warpPerspective</strong>，您可以使用它们进行各种转换。</p>
<p><strong>cv.warpAffine</strong>采用2x3转换矩阵，而<strong>cv.warpPerspective</strong>采用3x3转换矩阵作为输入。</p>
<h2 id="缩放"><a href="#缩放" class="headerlink" title="缩放"></a>缩放</h2><p>缩放只是调整图像的大小。为此，OpenCV带有一个函数**cv.resize()。图像的大小可以手动指定，也可以指定缩放比例。</p>
<p>也可使用不同的插值方法。首选的插值方法是<strong>cv.INTER_AREA</strong>用于缩小，<strong>cv.INTER_CUBIC（慢）和</strong>cv.INTER_LINEAR**用于缩放。</p>
<p>默认情况下，出于所有调整大小的目的，使用的插值方法为<strong>cv.INTER_LINEAR</strong>。您可以使用以下方法调整输入图像的大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">res = cv.resize(img,<span class="literal">None</span>,fx=<span class="number">2</span>, fy=<span class="number">2</span>, interpolation = cv.INTER_AREA)</span><br><span class="line">cv.imshow(<span class="string">'res1'</span>,res)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">height, width = img.shape[:<span class="number">2</span>]</span><br><span class="line">res = cv.resize(img,(<span class="number">2</span>*width, <span class="number">2</span>*height), interpolation = cv.INTER_CUBIC)</span><br><span class="line">cv.imshow(<span class="string">'res2'</span>,res)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h2 id="平移"><a href="#平移" class="headerlink" title="平移"></a>平移</h2><p>平移是物体位置的移动。如果您知道在(x,y)方向上的位移，则将其设为(tx,ty)，你可以创建转换矩阵M，如下所示：</p>
<p>M = $$<br> \left[<br> \begin{matrix}<br>   1 &amp; 0 &amp; tx \\<br>   0 &amp; 1 &amp; ty<br>  \end{matrix}<br>  \right] \</p>
<script type="math/tex; mode=display">
您可以将其放入**np.float32**类型的Numpy数组中，并将其传递给**cv.warpAffine**函数。

参见下面偏移为(100, 50)的示例：

**cv.warpAffine**函数的第三个参数是输出图像的大小，其形式应为(width，height)。记住width =列数，height =行数。


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">rows,cols = img.shape</span><br><span class="line">M = np.float32([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">100</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">50</span>]])</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

## 旋转
图像旋转角度为θ是通过以下形式的变换矩阵实现的：

M=</script><p> \left[<br> \begin{matrix}<br>   \cos \theta &amp; \sin \theta  \\<br>   \sin \theta &amp; \cos \theta<br>  \end{matrix}<br>  \right] \</p>
<script type="math/tex; mode=display">
但是OpenCV提供了可缩放的旋转以及可调整的旋转中心，因此您可以在自己喜欢的任何位置旋转。修改后的变换矩阵为</script><p> \left[<br> \begin{matrix}<br>   \alpha &amp; \beta &amp; (1-\alpha)·center·x-\beta·center·y \\<br>   -\beta &amp; \alpha &amp; \beta·center·x+(1-\alpha)·center·y<br>  \end{matrix}<br>  \right] \</p>
<script type="math/tex; mode=display">
其中：

$$α=scale⋅\cos \theta,β=scale⋅\sin \theta</script><p>为了找到此转换矩阵，OpenCV提供了一个函数<strong>cv.getRotationMatrix2D</strong>。</p>
<p>请检查以下示例，该示例将图像相对于中心旋转90度而没有任何缩放比例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">rows,cols = img.shape</span><br><span class="line"><span class="comment"># cols-1 和 rows-1 是坐标限制</span></span><br><span class="line">M = cv.getRotationMatrix2D(((cols<span class="number">-1</span>)/<span class="number">2.0</span>,(rows<span class="number">-1</span>)/<span class="number">2.0</span>),<span class="number">90</span>,<span class="number">2</span>)</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h2 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h2><p>在仿射变换中，原始图像中的所有平行线在输出图像中仍将平行。</p>
<p>为了找到变换矩阵，我们需要输入图像中的三个点及其在输出图像中的对应位置。</p>
<p>然后<strong>cv.getAffineTransform</strong>将创建一个2x3矩阵，该矩阵将传递给<strong>cv.warpAffine</strong>。</p>
<p>查看以下示例，并查看我选择的点（以绿色标记）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'drawing.png'</span>)</span><br><span class="line">rows,cols,ch = img.shape</span><br><span class="line">pts1 = np.float32([[<span class="number">50</span>,<span class="number">50</span>],[<span class="number">200</span>,<span class="number">50</span>],[<span class="number">50</span>,<span class="number">200</span>]])</span><br><span class="line">pts2 = np.float32([[<span class="number">10</span>,<span class="number">100</span>],[<span class="number">200</span>,<span class="number">50</span>],[<span class="number">100</span>,<span class="number">250</span>]])</span><br><span class="line">M = cv.getAffineTransform(pts1,pts2)</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Input'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Output'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(&lt;matplotlib.axes._subplots.AxesSubplot at 0x1bb7651dcc0&gt;,
 &lt;matplotlib.image.AxesImage at 0x1bb76bc5c18&gt;,
 Text(0.5, 1.0, &#39;Output&#39;))
</code></pre><p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_9_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>)</span><br><span class="line">rows,cols,ch = img.shape</span><br><span class="line">pts1 = np.float32([[<span class="number">56</span>,<span class="number">65</span>],[<span class="number">368</span>,<span class="number">52</span>],[<span class="number">28</span>,<span class="number">387</span>],[<span class="number">389</span>,<span class="number">390</span>]])</span><br><span class="line">pts2 = np.float32([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">300</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">300</span>],[<span class="number">300</span>,<span class="number">300</span>]])</span><br><span class="line">M = cv.getPerspectiveTransform(pts1,pts2)</span><br><span class="line">dst = cv.warpPerspective(img,M,(<span class="number">300</span>,<span class="number">300</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Input'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Output'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_10_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="图像阈值"><a href="#图像阈值" class="headerlink" title="图像阈值"></a><span id="header3">图像阈值</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在本教程中，您将学习简单阈值，自适应阈值和Otsu阈值。</p>
<p>你将学习函数<strong>cv.threshold</strong>和<strong>cv.adaptiveThreshold</strong>。</p>
<h2 id="简单阈值"><a href="#简单阈值" class="headerlink" title="简单阈值"></a>简单阈值</h2><p>在这里，问题直截了当。对于每个像素，应用相同的阈值。</p>
<p>如果像素值小于阈值，则将其设置为0，否则将其设置为最大值。函数<strong>cv.threshold</strong>用于应用阈值。</p>
<ul>
<li>第一个参数是源图像，它<strong>应该是灰度图像</strong>。</li>
<li>第二个参数是阈值，用于对像素值进行分类。</li>
<li>第三个参数是分配给超过阈值的像素值的最大值。</li>
<li>第四个参数OpenCV提供了不同类型的阈值,通过使用<strong>cv.THRESH_BINARY</strong>类型。所有简单的阈值类型为：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.THRESH_BINARY</span><br><span class="line">cv.THRESH_BINARY_INV</span><br><span class="line">cv.THRESH_TRUNC</span><br><span class="line">cv.THRESH_TOZERO</span><br><span class="line">cv.THRESH_TOZERO_INV</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>该方法返回两个输出。第一个是使用的阈值，第二个输出是<strong>阈值后的图像</strong>。</p>
<p>此代码比较了不同的简单阈值类型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'gradient.png'</span>,<span class="number">0</span>)</span><br><span class="line">ret,thresh1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line">ret,thresh2 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">ret,thresh3 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TRUNC)</span><br><span class="line">ret,thresh4 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO)</span><br><span class="line">ret,thresh5 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO_INV)</span><br><span class="line">titles = [<span class="string">'Original Image'</span>,<span class="string">'BINARY'</span>,<span class="string">'BINARY_INV'</span>,<span class="string">'TRUNC'</span>,<span class="string">'TOZERO'</span>,<span class="string">'TOZERO_INV'</span>]</span><br><span class="line">images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_2_1.png" alt="png"></p>
<h2 id="自适应阈值"><a href="#自适应阈值" class="headerlink" title="自适应阈值"></a>自适应阈值</h2><p>在上一节中，我们使用一个全局值作为阈值。但这可能并非在所有情况下都很好，例如，如果图像在不同区域具有不同的光照条件。在这种情况下，自适应阈值阈值化可以提供帮助。在此，算法基于像素周围的小区域确定像素的阈值。因此，对于同一图像的不同区域，我们获得了不同的阈值，这为光照度变化的图像提供了更好的结果。</p>
<p>除上述参数外，方法<strong>cv.adaptiveThreshold</strong>还包含三个输入参数：</p>
<p>该<strong>adaptiveMethod</strong>决定阈值是如何计算的：</p>
<p>cv.ADAPTIVE_THRESH_MEAN_C::阈值是邻近区域的平均值减去常数<strong>C</strong>。 </p>
<p>cv.ADAPTIVE_THRESH_GAUSSIAN_C:阈值是邻域值的高斯加权总和减去常数<strong>C</strong>。</p>
<p>该<strong>BLOCKSIZE</strong>确定附近区域的大小，<strong>C</strong>是从邻域像素的平均或加权总和中减去的一个常数。</p>
<p>下面的代码比较了光照变化的图像的全局阈值和自适应阈值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line">ret,th1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line">th2 = cv.adaptiveThreshold(img,<span class="number">255</span>,cv.ADAPTIVE_THRESH_MEAN_C,\</span><br><span class="line">            cv.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line">th3 = cv.adaptiveThreshold(img,<span class="number">255</span>,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\</span><br><span class="line">            cv.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line">titles = [<span class="string">'Original Image'</span>, <span class="string">'Global Thresholding (v = 127)'</span>,</span><br><span class="line">            <span class="string">'Adaptive Mean Thresholding'</span>, <span class="string">'Adaptive Gaussian Thresholding'</span>]</span><br><span class="line">images = [img, th1, th2, th3]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">2</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_4_0.png" alt="png"></p>
<h2 id="Otsu的二值化"><a href="#Otsu的二值化" class="headerlink" title="Otsu的二值化"></a>Otsu的二值化</h2><p>在全局阈值化中，我们使用任意选择的值作为阈值。相反，Otsu的方法避免了必须选择一个值并自动确定它的情况。</p>
<p>考虑仅具有两个不同图像值的图像（双峰图像），其中直方图将仅包含两个峰。一个好的阈值应该在这两个值的中间。类似地，Otsu的方法从图像直方图中确定最佳全局阈值。</p>
<p>为此，使用了<strong>cv.threshold</strong>作为附加标志传递。阈值可以任意选择。然后，算法找到最佳阈值，该阈值作为第一输出返回。</p>
<p>查看以下示例。输入图像为噪点图像。</p>
<p>在第一种情况下，采用值为127的全局阈值。</p>
<p>在第二种情况下，直接采用Otsu阈值法。</p>
<p>在第三种情况下，首先使用5x5高斯核对图像进行滤波以去除噪声，然后应用Otsu阈值处理。了解噪声滤波如何改善结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 全局阈值</span></span><br><span class="line">ret1,th1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line"><span class="comment"># Otsu阈值</span></span><br><span class="line">ret2,th2 = cv.threshold(img,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line"><span class="comment"># 高斯滤波后再采用Otsu阈值</span></span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line">ret3,th3 = cv.threshold(blur,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line"><span class="comment"># 绘制所有图像及其直方图</span></span><br><span class="line">images = [img, <span class="number">0</span>, th1,</span><br><span class="line">          img, <span class="number">0</span>, th2,</span><br><span class="line">          blur, <span class="number">0</span>, th3]</span><br><span class="line">titles = [<span class="string">'Original Noisy Image'</span>,<span class="string">'Histogram'</span>,<span class="string">'Global Thresholding (v=127)'</span>,</span><br><span class="line">          <span class="string">'Original Noisy Image'</span>,<span class="string">'Histogram'</span>,<span class="string">"Otsu's Thresholding"</span>,</span><br><span class="line">          <span class="string">'Gaussian filtered Image'</span>,<span class="string">'Histogram'</span>,<span class="string">"Otsu's Thresholding"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">1</span>),plt.imshow(images[i*<span class="number">3</span>],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">2</span>),plt.hist(images[i*<span class="number">3</span>].ravel(),<span class="number">256</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>+<span class="number">1</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">3</span>),plt.imshow(images[i*<span class="number">3</span>+<span class="number">2</span>],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>+<span class="number">2</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_6_0.png" alt="png"></p>
<h2 id="Otsu的二值化如何实现？"><a href="#Otsu的二值化如何实现？" class="headerlink" title="Otsu的二值化如何实现？"></a>Otsu的二值化如何实现？</h2><p>本节演示了Otsu二值化的Python实现，以展示其实际工作方式。</p>
<p>由于我们正在处理双峰图像，因此Otsu的算法尝试找到一个阈值(t)，该阈值将由关系式给出的<strong>加权类内方差</strong>最小化：</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Otsu.png" alt></p>
<p>实际上，它找到位于两个峰值之间的t值，以使两个类别的差异最小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 寻找归一化直方图和对应的累积分布函数</span></span><br><span class="line">hist = cv.calcHist([blur],[<span class="number">0</span>],<span class="literal">None</span>,[<span class="number">256</span>],[<span class="number">0</span>,<span class="number">256</span>])</span><br><span class="line">hist_norm = hist.ravel()/hist.max()</span><br><span class="line">Q = hist_norm.cumsum()</span><br><span class="line">bins = np.arange(<span class="number">256</span>)</span><br><span class="line">fn_min = np.inf</span><br><span class="line">thresh = <span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">256</span>):</span><br><span class="line">    p1,p2 = np.hsplit(hist_norm,[i]) <span class="comment"># 概率</span></span><br><span class="line">    q1,q2 = Q[i],Q[<span class="number">255</span>]-Q[i] <span class="comment"># 对类求和</span></span><br><span class="line">    b1,b2 = np.hsplit(bins,[i]) <span class="comment"># 权重</span></span><br><span class="line">    <span class="comment"># 寻找均值和方差</span></span><br><span class="line">    m1,m2 = np.sum(p1*b1)/q1, np.sum(p2*b2)/q2</span><br><span class="line">    v1,v2 = np.sum(((b1-m1)**<span class="number">2</span>)*p1)/q1,np.sum(((b2-m2)**<span class="number">2</span>)*p2)/q2</span><br><span class="line">    <span class="comment"># 计算最小化函数</span></span><br><span class="line">    fn = v1*q1 + v2*q2</span><br><span class="line">    <span class="keyword">if</span> fn &lt; fn_min:</span><br><span class="line">        fn_min = fn</span><br><span class="line">        thresh = i</span><br><span class="line"><span class="comment"># 使用OpenCV函数找到otsu的阈值</span></span><br><span class="line">ret, otsu = cv.threshold(blur,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line">print( <span class="string">"&#123;&#125; &#123;&#125;"</span>.format(thresh,ret) )</span><br></pre></td></tr></table></figure>
<pre><code>101 100.0


c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars
  from ipykernel import kernelapp as app
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in double_scalars
  from ipykernel import kernelapp as app
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply
  app.launch_new_instance()
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="图像平滑"><a href="#图像平滑" class="headerlink" title="图像平滑"></a><span id="header4">图像平滑</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>学会： - 使用各种低通滤镜模糊图像 - 将定制的滤镜应用于图像（2D卷积）</p>
<h2 id="2D卷积（图像过滤）"><a href="#2D卷积（图像过滤）" class="headerlink" title="2D卷积（图像过滤）"></a>2D卷积（图像过滤）</h2><p>与一维信号一样，还可以使用各种低通滤波器（LPF），高通滤波器（HPF）等对图像进行滤波。LPF有助于消除噪声，使图像模糊等。HPF滤波器有助于在图像中找到边缘。</p>
<p>OpenCV提供了一个函数<strong>cv.filter2D</strong>来将内核与图像进行卷积。例如，我们将尝试对图像进行平均滤波。5x5平均滤波器内核如下所示：</p>
<p>K=$$\frac{1}{25}<br> \left[<br> \begin{matrix}<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1<br>  \end{matrix}<br>  \right] \</p>
<script type="math/tex; mode=display">
操作如下:保持这个内核在一个像素上，将所有低于这个内核的25个像素相加，取其平均值，然后用新的平均值替换中心像素。它将对图像中的所有像素继续此操作。试试这个代码，并检查结果:


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.float32)/<span class="number">25</span></span><br><span class="line">dst = cv.filter2D(img,<span class="number">-1</span>,kernel)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Averaging'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


![png](output_2_0.png)


## 图像模糊（图像平滑）
通过将图像与低通滤波器内核进行卷积来实现图像模糊。这对于消除噪音很有用。它实际上从图像中消除了高频部分（例如噪声，边缘）。

因此，在此操作中边缘有些模糊。（有一些模糊技术也可以不模糊边缘）。OpenCV主要提供四种类型的模糊技术。

### 1.平均
这是通过将图像与归一化框滤镜进行卷积来完成的。它仅获取内核区域下所有像素的平均值，并替换中心元素。这是通过功能**cv.blur()或**cv.boxFilter()完成的。检查文档以获取有关内核的更多详细信息。我们应该指定内核的宽度和高度。3x3归一化框式过滤器如下所示：

K=$$\frac{1}{9}
 \left[
 \begin{matrix}
    1 & 1 & 1 & 1 & 1\\
    1 & 1 & 1 & 1 & 1\\
    1 & 1 & 1 & 1 & 1\\
    1 & 1 & 1 & 1 & 1\\
    1 & 1 & 1 & 1 & 1 
  \end{matrix}
  \right] \</script><p>注意 如果您不想使用标准化的框式过滤器，请使用<strong>cv.boxFilter()</strong>。将参数normalize = False传递给函数。</p>
<p>查看下面的示例演示，其内核大小为5x5：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.blur(img,(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_5_0.png" alt="png"></p>
<h3 id="2-高斯模糊"><a href="#2-高斯模糊" class="headerlink" title="2.高斯模糊"></a>2.高斯模糊</h3><p>在这种情况下，代替盒式滤波器，使用了高斯核。</p>
<p>这是通过功能<strong>cv.GaussianBlur()</strong> 完成的。我们应指定内核的宽度和高度，该宽度和高度应为正数和奇数。</p>
<p>我们还应指定X和Y方向的标准偏差，分别为sigmaX和sigmaY。如果仅指定sigmaX，则将sigmaY与sigmaX相同。</p>
<p>如果两个都为零，则根据内核大小进行计算。高斯模糊对于从图像中去除高斯噪声非常有效。</p>
<p>如果需要，可以使用函数<strong>cv.getGaussianKernel()</strong> 创建高斯内核。</p>
<p>可以修改以上代码以实现高斯模糊：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_7_0.png" alt="png"></p>
<h3 id="3-中位模糊"><a href="#3-中位模糊" class="headerlink" title="3.中位模糊"></a>3.中位模糊</h3><p>在这里，函数<strong>cv.medianBlur()</strong> 提取内核区域下所有像素的中值，并将中心元素替换为该中值。</p>
<p>这对于消除图像中的椒盐噪声非常有效。有趣的是，在上述过滤器中，中心元素是新计算的值，该值可以是图像中的像素值或新值。</p>
<p>但是在中值模糊中，中心元素总是被图像中的某些像素值代替。有效降低噪音。其内核大小应为正奇数整数。</p>
<p>在此演示中，我向原始图像添加了50％的噪声并应用了中值模糊。检查结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">median = cv.medianBlur(img,<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(median),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_9_0.png" alt="png"></p>
<h3 id="4-双边滤波"><a href="#4-双边滤波" class="headerlink" title="4.双边滤波"></a>4.双边滤波</h3><p>cv.bilateralFilter() 在去除噪声的同时保持边缘清晰锐利非常有效。</p>
<p>但是，与其他过滤器相比，该操作速度较慢。我们已经看到，高斯滤波器采用像素周围的邻域并找到其高斯加权平均值。</p>
<p>高斯滤波器仅是空间的函数，也就是说，滤波时会考虑附近的像素。它不考虑像素是否具有几乎相同的强度。它不考虑像素是否是边缘像素。因此它也模糊了边缘，这是我们不想做的。</p>
<p>双边滤波器在空间中也采用高斯滤波器，但是又有一个高斯滤波器，它是像素差的函数。</p>
<p>空间的高斯函数确保仅考虑附近像素的模糊，而强度差的高斯函数确保仅考虑强度与中心像素相似的那些像素的模糊。由于边缘的像素强度变化较大，因此可以保留边缘。</p>
<p>以下示例显示了使用双边过滤器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.bilateralFilter(img,<span class="number">9</span>,<span class="number">75</span>,<span class="number">75</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_11_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理2</title>
    <url>/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/</url>
    <content><![CDATA[<ul>
<li>5.<a href="#header1">形态转换</a></li>
<li>6.<a href="#header2">图像梯度</a></li>
<li>7.<a href="#header3">Canny边缘检测</a></li>
<li>8.<a href="#header4">图像金字塔</a><a id="more"></a>
</li>
</ul>
<h1 id="形态学转换"><a href="#形态学转换" class="headerlink" title="形态学转换"></a><span id="header1">形态学转换</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>在这一章当中， 我们将学习不同的形态学操作，例如侵蚀，膨胀，开运算，闭运算等。<br>我们将看到不同的功能，</p>
<p>例如：cv.erode(),cv.dilate(), cv.morphologyEx()等。</p>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>形态变换是一些基于图像形状的简单操作。通常在二进制图像上执行。</p>
<p>它需要两个输入，一个是我们的原始图像，第二个是决定<strong>操作性质的结构元素</strong>或<strong>内核</strong>。</p>
<p>两种基本的形态学算子是侵蚀和膨胀。</p>
<p>然后，它的变体形式（如“打开”，“关闭”，“渐变”等）也开始起作用。在下图的帮助下，我们将一一看到它们：</p>
<h2 id="1-侵蚀"><a href="#1-侵蚀" class="headerlink" title="1. 侵蚀"></a>1. 侵蚀</h2><p>侵蚀的基本思想就像土壤侵蚀一样，它侵蚀前景物体的边界(尽量使前景保持白色)。</p>
<p>它是做什么的呢?内核滑动通过图像(在2D卷积中)。</p>
<p>原始图像中的一个像素(无论是1还是0)只有当内核下的所有像素都是1时才被认为是1，否则它就会被侵蚀(变成0)。</p>
<p>结果是，根据内核的大小，边界附近的所有像素都会被丢弃。</p>
<p>因此，前景物体的厚度或大小减小，或只是图像中的白色区域减小。</p>
<p>它有助于去除小的白色噪声(正如我们在颜色空间章节中看到的)，分离两个连接的对象等。</p>
<p>在这里，作为一个例子，我将使用一个5x5内核，它包含了所有的1。</p>
<h2 id="2-扩张"><a href="#2-扩张" class="headerlink" title="2. 扩张"></a>2. 扩张</h2><p>它与侵蚀正好相反。如果内核下的至少一个像素为“ 1”，则像素元素为“ 1”。</p>
<p>因此，它会增加图像中的白色区域或增加前景对象的大小。</p>
<p>通常，在消除噪音的情况下，腐蚀后会膨胀。因为腐蚀会消除白噪声，但也会缩小物体。</p>
<p>因此，我们对其进行了扩展。由于噪音消失了，它们不会回来，但是我们的目标区域增加了。在连接对象的损坏部分时也很有用。</p>
<h2 id="3-开运算"><a href="#3-开运算" class="headerlink" title="3. 开运算"></a>3. 开运算</h2><p>开放只是<strong>侵蚀然后扩张</strong>的另一个名称。</p>
<p>如上文所述，它对于消除噪音很有用。在这里，我们使用函数<strong>cv.morphologyEx</strong>()</p>
<h2 id="4-闭运算"><a href="#4-闭运算" class="headerlink" title="4. 闭运算"></a>4. 闭运算</h2><p>闭运算与开运算相反，先扩张然后再侵蚀。</p>
<p>在关闭前景对象内部的小孔或对象上的小黑点时很有用。</p>
<h2 id="5-形态学梯度"><a href="#5-形态学梯度" class="headerlink" title="5. 形态学梯度"></a>5. 形态学梯度</h2><p>这是图像扩张和侵蚀之间的区别。</p>
<p>结果将看起来像对象的轮廓。</p>
<h2 id="6-顶帽"><a href="#6-顶帽" class="headerlink" title="6. 顶帽"></a>6. 顶帽</h2><p>它是输入图像和图像开运算之差。下面的示例针对9x9内核完成。</p>
<h2 id="7-黑帽"><a href="#7-黑帽" class="headerlink" title="7. 黑帽"></a>7. 黑帽</h2><p>这是输入图像和图像闭运算之差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl </span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>] <span class="comment"># 指定默认字体 </span></span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span> <span class="comment"># 解决保存图像是负号'-'显示为方块的问题</span></span><br><span class="line">img = cv.imread(<span class="string">'j.png'</span>,<span class="number">0</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)</span><br><span class="line">erosion = cv.erode(img,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">dilation = cv.dilate(img,kernel,iterations = <span class="number">1</span>) </span><br><span class="line">opening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel) </span><br><span class="line">closing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel) </span><br><span class="line">gradient = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel) </span><br><span class="line">tophat = cv.morphologyEx(img, cv.MORPH_TOPHAT, kernel) </span><br><span class="line">blackhat = cv.morphologyEx(img, cv.MORPH_BLACKHAT, kernel) </span><br><span class="line">imgs = [img,erosion,dilation,opening,closing,gradient,tophat,blackhat]</span><br><span class="line">titles = [<span class="string">'原图'</span>,<span class="string">'侵蚀'</span>,<span class="string">'膨胀'</span>,<span class="string">'开运算'</span>,<span class="string">'闭运算'</span>,<span class="string">'形态学梯度'</span>,<span class="string">'顶帽'</span>,<span class="string">'黑帽'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(imgs)):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">4</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(imgs[i])</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_9_0.png" alt="png"></p>
<h2 id="结构元素"><a href="#结构元素" class="headerlink" title="结构元素"></a>结构元素</h2><p>在Numpy的帮助下，我们在前面的示例中手动创建了一个结构元素。</p>
<p>它是矩形。但是在某些情况下，您可能需要椭圆形/圆形的内核。</p>
<p>因此，为此，OpenCV具有一个函数<strong>cv.getStructuringElement</strong>()。</p>
<p>您只需传递内核的形状和大小，即可获得所需的内核</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩形</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_RECT,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<pre><code>array([[1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1]], dtype=uint8)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 椭圆内核</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_ELLIPSE,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<pre><code>array([[0, 0, 1, 0, 0],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [0, 0, 1, 0, 0]], dtype=uint8)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 十字内核</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_CROSS,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<pre><code>array([[0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0],
       [1, 1, 1, 1, 1],
       [0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0]], dtype=uint8)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="图像梯度"><a href="#图像梯度" class="headerlink" title="图像梯度"></a><span id="header2">图像梯度</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习： - 查找图像梯度，边缘等 - </p>
<p>我们将看到以下函数：cv.Sobel()，cv.Scharr()，cv.Laplacian()等</p>
<h2 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h2><p>OpenCV提供三种类型的梯度滤波器或高通滤波器，即Sobel，Scharr和Laplacian。我们将看到他们每一种。</p>
<h2 id="1-Sobel-和-Scharr-算子"><a href="#1-Sobel-和-Scharr-算子" class="headerlink" title="1. Sobel 和 Scharr 算子"></a>1. Sobel 和 Scharr 算子</h2><p>Sobel算子是高斯平滑加微分运算的联合运算，因此它更抗噪声。逆可以指定要采用的导数方向，垂直或水平（分别通过参数yorder和xorder）。逆还可以通过参数ksize指定内核的大小。如果ksize = -1，则使用3x3 Scharr滤波器，比3x3 Sobel滤波器具有更好的结果。请参阅文档以了解所使用的内核。</p>
<h2 id="2-Laplacian-算子"><a href="#2-Laplacian-算子" class="headerlink" title="2. Laplacian 算子"></a>2. Laplacian 算子</h2><p>它计算了由关系Δsrc=$\frac{\delta^2 src}{\delta x^2}+\frac{\delta^2 src}{\delta y^2}$给出的图像的拉普拉斯图,它是每一阶导数通过Sobel算子计算。如果ksize = 1,然后使用以下内核用于过滤:</p>
<p>kernel=$$<br> \left[<br> \begin{matrix}<br>   0 &amp; 1 &amp; 0 \\<br>   1 &amp; -4 &amp; 1 \\<br>   0 &amp; 1 &amp; 0<br>  \end{matrix}<br>  \right] \</p>
<script type="math/tex; mode=display">
代码
下面的代码显示了单个图表中的所有算子。所有内核都是5x5大小。输出图像的深度通过-1得到结果的np.uint8型。


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'dave.png'</span>,<span class="number">0</span>)</span><br><span class="line">laplacian = cv.Laplacian(img,cv.CV_64F)</span><br><span class="line">sobelx = cv.Sobel(img,cv.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize=<span class="number">5</span>)</span><br><span class="line">sobely = cv.Sobel(img,cv.CV_64F,<span class="number">0</span>,<span class="number">1</span>,ksize=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),plt.imshow(img,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Original'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>),plt.imshow(laplacian,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Laplacian'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>),plt.imshow(sobelx,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Sobel X'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),plt.imshow(sobely,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Sobel Y'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


![png](output_1_0.png)



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


# <span id="header3">Canny边缘检测</span>
## 目标
在本章中，我们将学习 - Canny边缘检测的概念 - OpenCV函数: cv.Canny()

## 理论
Canny Edge Detection是一种流行的边缘检测算法。它由John F. Canny发明

这是一个多阶段算法，我们将经历每个阶段。

## 降噪

由于边缘检测容易受到图像中噪声的影响，因此第一步是使用5x5高斯滤波器消除图像中的噪声。我们已经在前面的章节中看到了这一点。

查找图像的强度梯度
然后使用Sobel核在水平和垂直方向上对平滑的图像进行滤波，以在水平方向(Gx)和垂直方向(Gy)上获得一阶导数。从这两张图片中，我们可以找到每个像素的边缘渐变和方向，如下所示：</script><p>Edge_Gradient \; (G) = \sqrt{G_x^2 + G_y^2} \\ Angle \; (\theta) = \tan^{-1} \bigg(\frac{G_y}{G_x}\bigg)</p>
<p>$$<br>渐变方向始终垂直于边缘。将其舍入为代表垂直，水平和两个对角线方向的四个角度之一。</p>
<p>非极大值抑制 在获得梯度大小和方向后，将对图像进行全面扫描，以去除可能不构成边缘的所有不需要的像素。</p>
<p>为此，在每个像素处，检查像素是否是其在梯度方向上附近的局部最大值。查看下面的图片：<br><img src="http://qiniu.aihubs.net/nms.jpg" alt></p>
<p>点A在边缘（垂直方向）上。渐变方向垂直于边缘。点B和C在梯度方向上。因此，将A点与B点和C点进行检查，看是否形成局部最大值。如果是这样，则考虑将其用于下一阶段，否则将其抑制（置为零）。 简而言之，你得到的结果是带有“细边”的二进制图像。</p>
<h2 id="磁滞阈值"><a href="#磁滞阈值" class="headerlink" title="磁滞阈值"></a>磁滞阈值</h2><p>该阶段确定哪些边缘全部是真正的边缘，哪些不是。为此，我们需要两个阈值minVal和maxVal。强度梯度大于maxVal的任何边缘必定是边缘，而小于minVal的那些边缘必定是非边缘，因此将其丢弃。介于这两个阈值之间的对象根据其连通性被分类为边缘或非边缘。如果将它们连接到“边缘”像素，则将它们视为边缘的一部分。否则，它们也将被丢弃。见下图：</p>
<p><img src="http://qiniu.aihubs.net/hysteresis.jpg" alt><br>边缘A在maxVal之上，因此被视为“确定边缘”。尽管边C低于maxVal，但它连接到边A，因此也被视为有效边，我们得到了完整的曲线。但是边缘B尽管在minVal之上并且与边缘C处于同一区域，但是它没有连接到任何“确保边缘”，因此被丢弃。因此，非常重要的一点是我们必须相应地选择minVal和maxVal以获得正确的结果。</p>
<p>在边缘为长线的假设下，该阶段还消除了小像素噪声。</p>
<p>因此，我们最终得到的是图像中的强边缘。</p>
<h2 id="OpenCV中的Canny-Edge检测"><a href="#OpenCV中的Canny-Edge检测" class="headerlink" title="OpenCV中的Canny Edge检测"></a>OpenCV中的Canny Edge检测</h2><p>OpenCV将以上所有内容放在单个函数<strong>cv.Canny</strong>()中。我们将看到如何使用它。</p>
<p>第一个参数是我们的输入图像。</p>
<p>第二个和第三个参数分别是我们的minVal和maxVal。</p>
<p>第三个参数是perture_size。它是用于查找图像渐变的Sobel内核的大小。默认情况下为3。</p>
<p>最后一个参数是L2gradient，它指定用于查找梯度幅度的方程式。</p>
<p>如果为True，则使用上面提到的更精确的公式，否则使用以下函数：Edge_Gradient(G)=|Gx|+|Gy|。默认情况下，它为False。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">edges = cv.Canny(img,<span class="number">100</span>,<span class="number">200</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Original Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(edges,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Edge Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="图像金字塔"><a href="#图像金字塔" class="headerlink" title="图像金字塔"></a><span id="header4">图像金字塔</span></h1><h1 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h1><p>在本章中， - 我们将学习图像金字塔 - 我们将使用图像金字塔创建一个新的水果“Orapple” - </p>
<p>我们将看到以下功能：cv.pyrUp()，cv.pyrDown()</p>
<h2 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h2><p>通常，我们过去使用的是恒定大小的图像。但是在某些情况下，我们需要使用不同分辨率的（相同）图像。</p>
<p>例如，当在图像中搜索某些东西（例如人脸）时，我们不确定对象将以多大的尺寸显示在图像中。</p>
<p>在这种情况下，我们将需要创建一组具有不同分辨率的相同图像，并在所有图像中搜索对象。</p>
<p>这些具有不同分辨率的图像集称为“图像金字塔”（因为当它们堆叠在底部时，最高分辨率的图像位于底部，最低分辨率的图像位于顶部时，看起来像金字塔）。</p>
<p>有两种图像金字塔。1）高斯金字塔<strong>和2）</strong>拉普拉斯金字塔</p>
<p>高斯金字塔中的较高级别（低分辨率）是通过删除较低级别（较高分辨率）图像中的连续行和列而形成的。</p>
<p>然后，较高级别的每个像素由基础级别的5个像素的贡献与高斯权重形成。</p>
<p>通过这样做，M×N图像变成M/2×N/2图像。因此面积减少到原始面积的四分之一。</p>
<p>它称为Octave。当我们在金字塔中越靠上时（即分辨率下降），这种模式就会继续。</p>
<p>同样，在扩展时，每个级别的面积变为4倍。</p>
<p>我们可以使用<strong>cv.pyrDown</strong>()和<strong>cv.pyrUp</strong>()函数找到高斯金字塔</p>
<p>以下是图像金字塔中的4个级别。<br><img src="http://qiniu.aihubs.net/messipyr.jpg" alt><br>现在，您可以使用<strong>cv.pyrUp</strong>()函数查看图像金字塔。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">higher_reso2 &#x3D; cv.pyrUp(lower_reso)</span><br></pre></td></tr></table></figure><br>记住，higher_reso2不等于higher_reso，因为一旦降低了分辨率，就会丢失信息。下面的图像是3层的金字塔从最小的图像在前面的情况下创建。与原图对比:<br><img src="http://qiniu.aihubs.net/messiup.jpg" alt></p>
<p>拉普拉斯金字塔由高斯金字塔形成。没有专用功能。</p>
<p>拉普拉斯金字塔图像仅像边缘图像。它的大多数元素为零。它们用于图像压缩。</p>
<p>拉普拉斯金字塔的层由高斯金字塔的层与高斯金字塔的高层的扩展版本之间的差形成。</p>
<p>拉普拉斯等级的三个等级如下所示（调整对比度以增强内容）：<br><img src="http://qiniu.aihubs.net/lap.jpg" alt></p>
<h2 id="使用金字塔进行图像融合"><a href="#使用金字塔进行图像融合" class="headerlink" title="使用金字塔进行图像融合"></a>使用金字塔进行图像融合</h2><p>金字塔的一种应用是图像融合。</p>
<p>例如，在图像拼接中，您需要将两个图像堆叠在一起，但是由于图像之间的不连续性，可能看起来不太好。</p>
<p>在这种情况下，使用金字塔混合图像可以无缝混合，而不会在图像中保留大量数据。</p>
<p>一个经典的例子是将两种水果，橙和苹果混合在一起</p>
<p>只需完成以下步骤即可：</p>
<ul>
<li>加载苹果和橙子的两个图像</li>
<li>查找苹果和橙子的高斯金字塔（在此示例中， 级别数为6）</li>
<li>在高斯金字塔中，找到其拉普拉斯金字塔</li>
<li>然后在每个拉普拉斯金字塔级别中加入苹果的左半部分和橙子的右半部分</li>
<li>最后从此联合图像金字塔中重建原始图像。<br><img src="http://qiniu.aihubs.net/orapple.jpg" alt></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np,sys</span><br><span class="line">A = cv.imread(<span class="string">'apple.png'</span>)</span><br><span class="line">B = cv.imread(<span class="string">'orange.png'</span>)</span><br><span class="line"><span class="comment"># 生成A的高斯金字塔</span></span><br><span class="line">G = A.copy()</span><br><span class="line">gpA = [G]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    G = cv.pyrDown(G)</span><br><span class="line">    gpA.append(G)</span><br><span class="line"><span class="comment"># 生成B的高斯金字塔</span></span><br><span class="line">G = B.copy()</span><br><span class="line">gpB = [G]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    G = cv.pyrDown(G)</span><br><span class="line">    gpB.append(G)</span><br><span class="line"><span class="comment"># 生成A的拉普拉斯金字塔</span></span><br><span class="line">lpA = [gpA[<span class="number">5</span>]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    GE = cv.pyrUp(gpA[i])</span><br><span class="line">    L = cv.subtract(gpA[i<span class="number">-1</span>],GE)</span><br><span class="line">    lpA.append(L)</span><br><span class="line"><span class="comment"># 生成B的拉普拉斯金字塔</span></span><br><span class="line">lpB = [gpB[<span class="number">5</span>]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    GE = cv.pyrUp(gpB[i])</span><br><span class="line">    L = cv.subtract(gpB[i<span class="number">-1</span>],GE)</span><br><span class="line">    lpB.append(L)</span><br><span class="line"><span class="comment"># 现在在每个级别中添加左右两半图像 </span></span><br><span class="line">LS = []</span><br><span class="line"><span class="keyword">for</span> la,lb <span class="keyword">in</span> zip(lpA,lpB):</span><br><span class="line">    rows,cols,dpt = la.shape</span><br><span class="line">    ls = np.hstack((la[:,<span class="number">0</span>:cols/<span class="number">2</span>], lb[:,cols/<span class="number">2</span>:]))</span><br><span class="line">    LS.append(ls)</span><br><span class="line"><span class="comment"># 现在重建</span></span><br><span class="line">ls_ = LS[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    ls_ = cv.pyrUp(ls_)</span><br><span class="line">    ls_ = cv.add(ls_, LS[i])</span><br><span class="line"><span class="comment"># 图像与直接连接的每一半</span></span><br><span class="line">real = np.hstack((A[:,:cols/<span class="number">2</span>],B[:,cols/<span class="number">2</span>:]))</span><br><span class="line">cv.imwrite(<span class="string">'Pyramid_blending2.jpg'</span>,ls_)</span><br><span class="line">cv.imwrite(<span class="string">'Direct_blending.jpg'</span>,real)</span><br><span class="line"><span class="comment">##</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv图像核心操作</title>
    <url>/2020/07/12/opencv%E5%9B%BE%E5%83%8F%E6%A0%B8%E5%BF%83%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">图片的基本操作</a></li>
<li>2.<a href="#header2">图片的算法操作</a></li>
<li>3.<a href="#header3">性能衡量和提升技术</a><a id="more"></a>
</li>
</ul>
<h1 id="图像的基本操作"><a href="#图像的基本操作" class="headerlink" title="图像的基本操作"></a><span id="header1">图像的基本操作</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>学会： - 访问像素值并修改它们 - 访问图像属性 - 设置感兴趣区域(ROI) - 分割和合并图像</p>
<p>本节中的几乎所有操作都主要与Numpy相关，而不是与OpenCV相关。要使用OpenCV编写更好的优化代码，需要Numpy的丰富知识。</p>
<h2 id="访问和修改像素值"><a href="#访问和修改像素值" class="headerlink" title="访问和修改像素值"></a>访问和修改像素值</h2><p>对于 BGR 图像，它返回一个由蓝色、绿色和红色值组成的数组。对于灰度图像，只返回相应的灰度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>) <span class="comment"># 载入彩色图像</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">px = img[<span class="number">210</span>,<span class="number">490</span>] <span class="comment"># 访问210,490点处的全部元素</span></span><br><span class="line">px</span><br></pre></td></tr></table></figure>
<pre><code>array([237, 189, 147], dtype=uint8)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blue = img[<span class="number">210</span>,<span class="number">490</span>,<span class="number">0</span>] <span class="comment"># 仅访问蓝色元素</span></span><br><span class="line">blue</span><br></pre></td></tr></table></figure>
<pre><code>237
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img[<span class="number">100</span>,<span class="number">100</span>] = [<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>]<span class="comment"># 修改像素值</span></span><br><span class="line">img[<span class="number">100</span>,<span class="number">100</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([255, 255, 255], dtype=uint8)
</code></pre><h3 id="警告"><a href="#警告" class="headerlink" title="警告"></a>警告</h3><p>Numpy是用于快速数组计算的优化库。因此，简单地访问每个像素值并对其进行修改将非常缓慢，因此不建议使用。</p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>上面的方法通常用于选择数组的区域，例如前5行和后3列。</p>
<p>对于单个像素访问，Numpy数组方法array.item()和array.itemset())被认为更好，但是它们始终返回标量。</p>
<p>如果要访问所有B，G，R值，则需要分别调用所有的array.item()。</p>
<p>下面是更好的像素访问和编辑方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">red = img.item(<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">red</span><br></pre></td></tr></table></figure>
<pre><code>255
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.itemset((<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>),<span class="number">222</span>)</span><br><span class="line">red = img.item(<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">red</span><br></pre></td></tr></table></figure>
<pre><code>222
</code></pre><h2 id="访问图像属性"><a href="#访问图像属性" class="headerlink" title="访问图像属性"></a>访问图像属性</h2><p>图像属性包括行数，列数和通道数，图像数据类型，像素数等。</p>
<p>图像的形状可通过img.shape访问。它返回行，列和通道数的元组（如果图像是彩色的）：</p>
<p>注意 如果图像是灰度的，则返回的元组仅包含行数和列数，因此这是检查加载的图像是灰度还是彩色的好方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.shape</span><br></pre></td></tr></table></figure>
<pre><code>(640, 640, 3)
</code></pre><p>像素总数可通过访问img.size：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.size</span><br></pre></td></tr></table></figure>
<pre><code>1228800
</code></pre><p>图像数据类型通过img.dtype获得：</p>
<p>注意 img.dtype在调试时非常重要，因为OpenCV-Python代码中的大量错误是由无效的数据类型引起的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.dtype</span><br></pre></td></tr></table></figure>
<pre><code>dtype(&#39;uint8&#39;)
</code></pre><h2 id="图像感兴趣区域ROI"><a href="#图像感兴趣区域ROI" class="headerlink" title="图像感兴趣区域ROI"></a>图像感兴趣区域ROI</h2><p>有时候，你不得不处理一些特定区域的图像。</p>
<p>对于图像中的眼睛检测，首先对整个图像进行人脸检测。</p>
<p>在获取人脸图像时，我们只选择人脸区域，搜索其中的眼睛，而不是搜索整个图像。</p>
<p>它提高了准确性(因为眼睛总是在面部上:D )和性能(因为我们搜索的区域很小)。</p>
<p>使用Numpy索引再次获得ROI。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ball = img[<span class="number">180</span>:<span class="number">240</span>, <span class="number">230</span>:<span class="number">290</span>]</span><br><span class="line">img[<span class="number">273</span>:<span class="number">333</span>, <span class="number">100</span>:<span class="number">160</span>] = ball </span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h2 id="拆分和合并图像通道"><a href="#拆分和合并图像通道" class="headerlink" title="拆分和合并图像通道"></a>拆分和合并图像通道</h2><p>有时你需要分别处理图像的B，G，R通道。在这种情况下，你需要将BGR图像拆分为单个通道。</p>
<p>在其他情况下，你可能需要将这些单独的频道加入BGR图片。你可以通过以下方式简单地做到这一点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b,g,r = cv.split(img)</span><br><span class="line">img = cv.merge((b,g,r))</span><br></pre></td></tr></table></figure>
<p>假设你要将所有红色像素都设置为零，则无需先拆分通道。numpy索引更快</p>
<p>警告</p>
<p>cv.split()是一项耗时的操作（就时间而言）。因此，仅在必要时才这样做。否则请进行Numpy索引。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img [:, :, <span class="number">2</span>] = <span class="number">0</span></span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h2 id="为图像设置边框（填充）"><a href="#为图像设置边框（填充）" class="headerlink" title="为图像设置边框（填充）"></a>为图像设置边框（填充）</h2><p>如果要在图像周围创建边框（如相框），则可以使用cv.copyMakeBorder()。但是它在卷积运算，零填充等方面有更多应用。此函数采用以下参数：</p>
<p>src - 输入图像</p>
<p>top，bottom，left，right 边界宽度（以相应方向上的像素数为单位）</p>
<p>borderType - 定义要添加哪种边框的标志。它可以是以下类型：</p>
<ul>
<li>cv.BORDER_CONSTANT - 添加恒定的彩色边框。该值应作为下一个参数给出。</li>
<li>cv.BORDER_REFLECT - 边框将是边框元素的镜像，如下所示： fedcba | abcdefgh | hgfedcb</li>
<li>cv.BORDER_REFLECT_101或 cv.BORDER_DEFAULT与上述相同，但略有变化，例如： gfedcb | abcdefgh | gfedcba</li>
<li>cv.BORDER_REPLICATE最后一个元素被复制，像这样： aaaaaa | abcdefgh | hhhhhhh</li>
<li>cv.BORDER_WRAP难以解释，它看起来像这样： cdefgh | abcdefgh | abcdefg</li>
</ul>
<p>value -边框的颜色，如果边框类型为<strong>cv.BORDER_CONSTANT</strong></p>
<p>下面是一个示例代码，演示了所有这些边框类型，以便更好地理解：<br>(图像与matplotlib一起显示。因此红色和蓝色通道将互换)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">BLUE = [<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">img1 = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">replicate = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REPLICATE)</span><br><span class="line">reflect = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REFLECT)</span><br><span class="line">reflect101 = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REFLECT_101)</span><br><span class="line">wrap = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_WRAP)</span><br><span class="line">constant= cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_CONSTANT,value=BLUE)</span><br><span class="line">plt.subplot(<span class="number">231</span>),plt.imshow(img1,<span class="string">'gray'</span>),plt.title(<span class="string">'ORIGINAL'</span>)</span><br><span class="line">plt.subplot(<span class="number">232</span>),plt.imshow(replicate,<span class="string">'gray'</span>),plt.title(<span class="string">'REPLICATE'</span>)</span><br><span class="line">plt.subplot(<span class="number">233</span>),plt.imshow(reflect,<span class="string">'gray'</span>),plt.title(<span class="string">'REFLECT'</span>)</span><br><span class="line">plt.subplot(<span class="number">234</span>),plt.imshow(reflect101,<span class="string">'gray'</span>),plt.title(<span class="string">'REFLECT_101'</span>)</span><br><span class="line">plt.subplot(<span class="number">235</span>),plt.imshow(wrap,<span class="string">'gray'</span>),plt.title(<span class="string">'WRAP'</span>)</span><br><span class="line">plt.subplot(<span class="number">236</span>),plt.imshow(constant,<span class="string">'gray'</span>),plt.title(<span class="string">'CONSTANT'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/12/opencv%E5%9B%BE%E5%83%8F%E6%A0%B8%E5%BF%83%E6%93%8D%E4%BD%9C/output_21_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="图像上的算术运算"><a href="#图像上的算术运算" class="headerlink" title="图像上的算术运算"></a><span id="header2">图像上的算术运算</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>学习图像的几种算术运算，例如加法，减法，按位运算等。</p>
<p>您将学习以下功能：cv.add，<strong>cv.addWeighted</strong>等。</p>
<h2 id="图像加法"><a href="#图像加法" class="headerlink" title="图像加法"></a>图像加法</h2><p>您可以通过OpenCV函数cv.add()或仅通过numpy操作res = img1 + img2添加两个图像。</p>
<p>两个图像应具有相同的深度和类型，或者第二个图像可以只是一个标量值。</p>
<p>注意 OpenCV加法和Numpy加法之间有区别。OpenCV加法是饱和运算，而Numpy加法是模运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.uint8([<span class="number">250</span>])</span><br><span class="line">y = np.uint8([<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x+y  <span class="comment"># 250+10 = 260 % 256 = 4</span></span><br></pre></td></tr></table></figure>
<pre><code>array([4], dtype=uint8)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.add(x,y) <span class="comment"># 250+10 = 260 =&gt; 255</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[255]], dtype=uint8)
</code></pre><h2 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h2><p>这也是图像加法，但是对图像赋予不同的权重，以使其具有融合或透明的感觉。根据以下等式添加图像：</p>
<p>G(x)=(1−α)f0(x)+αf1(x)<br>通过从 α 从 0→1 更改，您可以在一个图像到另一个图像之间执行很酷的过渡。</p>
<p>将两幅图像合在一起。第一幅图像的权重为0.7，第二幅图像的权重为0.3。</p>
<p>cv.addWeighted()在图像上应用以下公式。</p>
<p>dst=α⋅img1+β⋅img2+γ<br>在这里，γ 被视为零。</p>
<p>先保存一个和avatar1大小一样上下相反的图像avatar2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">newImg = img.copy() <span class="comment"># 深拷贝</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(img.shape[<span class="number">0</span>]):</span><br><span class="line">    newImg[img.shape[<span class="number">0</span>]<span class="number">-1</span>-i] = img[i]</span><br><span class="line">cv.imshow(<span class="string">'x'</span>,newImg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imwrite(<span class="string">'avatar2.jpg'</span>,newImg)</span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img1 = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">'avatar2.jpg'</span>)</span><br><span class="line">dst = cv.addWeighted(img1,<span class="number">0.7</span>,img2,<span class="number">0.3</span>,<span class="number">0</span>)</span><br><span class="line">cv.imshow(<span class="string">'dst'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h2 id="按位运算"><a href="#按位运算" class="headerlink" title="按位运算"></a>按位运算</h2><p>这包括按位 AND、 OR、NOT 和 XOR 操作。它们在提取图像的任何部分(我们将在后面的章节中看到)、定义和处理非矩形 ROI 等方面非常有用。 </p>
<p>下面我们将看到一个例子，如何改变一个图像的特定区域。 </p>
<p>我想把 OpenCV 的标志放在一个图像上面。如果我添加两个图像，它会改变颜色。如果我混合它，我得到一个透明的效果。</p>
<p>但我希望它是不透明的。如果是一个矩形区域，</p>
<p>我可以使用 ROI，就像我们在上一章中所做的那样。</p>
<p>但是 OpenCV 的 logo 不是长方形的。所以你可以使用如下的按位操作来实现:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载两张图片</span></span><br><span class="line">img1 = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line"><span class="comment"># 我想把logo放在左上角，所以我创建了ROI</span></span><br><span class="line">rows,cols,channels = img2.shape</span><br><span class="line">roi = img1[<span class="number">0</span>:rows, <span class="number">0</span>:cols ]</span><br><span class="line"><span class="comment"># 现在创建logo的掩码，并同时创建其相反掩码</span></span><br><span class="line">img2gray = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, mask = cv.threshold(img2gray, <span class="number">10</span>, <span class="number">255</span>, cv.THRESH_BINARY)</span><br><span class="line">mask_inv = cv.bitwise_not(mask)</span><br><span class="line">cv.imshow(<span class="string">'img2gray'</span>,img2gray)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'mask'</span>,mask)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'mask_inv'</span>,mask_inv)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"><span class="comment"># 现在将ROI中logo的区域涂黑</span></span><br><span class="line">img1_bg = cv.bitwise_and(roi,roi,mask = mask_inv)</span><br><span class="line"><span class="comment"># 仅从logo图像中提取logo区域</span></span><br><span class="line">img2_fg = cv.bitwise_and(img2,img2,mask = mask)</span><br><span class="line"><span class="comment"># 将logo放入ROI并修改主图像</span></span><br><span class="line">cv.imshow(<span class="string">'img1_bg'</span>,img1_bg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'img2_bg'</span>,img2_fg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">dst = cv.add(img1_bg,img2_fg)</span><br><span class="line">img1[<span class="number">0</span>:rows, <span class="number">0</span>:cols ] = dst</span><br><span class="line">cv.imshow(<span class="string">'res'</span>,img1)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<h1 id="性能衡量和提升技术"><a href="#性能衡量和提升技术" class="headerlink" title="性能衡量和提升技术"></a><span id="header3">性能衡量和提升技术</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在图像处理中，由于每秒要处理大量操作，因此必须使代码不仅提供正确的解决方案，而且还必须以最快的方式提供。</p>
<p>将学习衡量代码的性能,一些提高代码性能的技巧。</p>
<p>你将看到以下功能：cv.getTickCount，<strong>cv.getTickFrequency</strong>等。</p>
<p>除了OpenCV，Python还提供了一个模块<strong>time</strong>，这有助于衡量执行时间。</p>
<p>另一个模块<strong>profile</strong>有助于获取有关代码的详细报告，例如代码中每个函数花费了多少时间，调用了函数的次数等。</p>
<p>但是，如果你使用的是IPython，则所有这些功能都集成在用户友好的界面中方式。</p>
<h2 id="使用OpenCV衡量性能"><a href="#使用OpenCV衡量性能" class="headerlink" title="使用OpenCV衡量性能"></a>使用OpenCV衡量性能</h2><p><strong>cv.getTickCount</strong>函数返回从参考事件（如打开机器的那一刻）到调用此函数那一刻之间的时钟周期数。因此，如果在函数执行之前和之后调用它，则会获得用于执行函数的时钟周期数。</p>
<p><strong>cv.getTickFrequency</strong>函数返回时钟周期的频率或每秒的时钟周期数。因此，要找到执行时间（以秒为单位），你可以执行以下操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img1 = cv.imread(<span class="string">'avatar2gray.jpg.jpg'</span>)</span><br><span class="line">e1 = cv.getTickCount()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">49</span>,<span class="number">2</span>):</span><br><span class="line">    img1 = cv.medianBlur(img1,i)</span><br><span class="line">e2 = cv.getTickCount()</span><br><span class="line">t = (e2 - e1)/cv.getTickFrequency()</span><br><span class="line">t</span><br></pre></td></tr></table></figure>
<pre><code>0.0002413
</code></pre><h2 id="OpenCV中的默认优化"><a href="#OpenCV中的默认优化" class="headerlink" title="OpenCV中的默认优化"></a>OpenCV中的默认优化</h2><p>许多 OpenCV 函数都是使用 SSE2、 AVX 等进行优化的。 它还包含未优化的代码。</p>
<p>因此，如果我们的系统支持这些特性，我们就应该利用它们(几乎所有现代的处理器都支持它们)。</p>
<p>在编译时默认启用它。因此，如果启用了 OpenCV，它将运行优化的代码，否则它将运行未优化的代码。</p>
<p>你可以使用 cvUseoptimized 检查是否启用 / 禁用和 cvSetuseoptimized 以启用 / 禁用它。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.useOptimized() <span class="comment"># 检查是否启用了优化</span></span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit res = cv.medianBlur(img1,<span class="number">49</span>)</span><br></pre></td></tr></table></figure>
<pre><code>748 ns ± 45.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.setUseOptimized(<span class="literal">False</span>)</span><br><span class="line">print(cv.useOptimized())</span><br></pre></td></tr></table></figure>
<pre><code>False
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit res = cv.medianBlur(img1,<span class="number">49</span>)</span><br></pre></td></tr></table></figure>
<pre><code>752 ns ± 34.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.setUseOptimized(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="在IPython中衡量性能"><a href="#在IPython中衡量性能" class="headerlink" title="在IPython中衡量性能"></a>在IPython中衡量性能</h2><p>有时你可能需要比较两个类似操作的性能。</p>
<p>IPython为你提供了一个神奇的命令计时器来执行此操作。它</p>
<p>会多次运行代码以获得更准确的结果。同样，它们适用于测量单行代码。</p>
<p>例如，你知道以下哪个加法运算更好，</p>
<p>x = 5 y = x**2, </p>
<p>x = 5  y = x*x, </p>
<p>x = np.uint8([5]) y = x*x或y = np.square(x)?我们将在IPython shell中使用timeit</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=x**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<pre><code>515 ns ± 17 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=x*x</span><br></pre></td></tr></table></figure>
<pre><code>121 ns ± 5.63 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">z = np.uint8([<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=z*z</span><br></pre></td></tr></table></figure>
<pre><code>1.04 µs ± 61.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=np.square(z)</span><br></pre></td></tr></table></figure>
<pre><code>1.04 µs ± 57.5 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre><p>可以看到x = 5; y = x * x最快，比Numpy快20倍左右。如果你还考虑阵列的创建，它可能会快100倍。</p>
<p>注意 Python标量操作比Numpy标量操作快。</p>
<p>因此，对于包含一两个元素的运算，Python标量比Numpy数组好。</p>
<p>当数组大小稍大时，Numpy会占优势。</p>
<p>我们将比较<strong>cv.countNonZero</strong>和<strong>np.count_nonzero</strong>对于同一张图片的性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit z = np.count_nonzero(img1)</span><br></pre></td></tr></table></figure>
<pre><code>2.58 µs ± 321 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit z = cv.countNonZero(img1)</span><br></pre></td></tr></table></figure>
<pre><code>722 ns ± 25.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre><p>OpenCV 函数比 Numpy 函数快近25倍。</p>
<p>注意 </p>
<p>通常，OpenCV函数比Numpy函数要快。因此，对于相同的操作，首选OpenCV功能。</p>
<p>但是，可能会有例外，尤其是当Numpy处理视图而不是副本时。</p>
<h2 id="性能优化技术"><a href="#性能优化技术" class="headerlink" title="性能优化技术"></a>性能优化技术</h2><p>有几种技术和编码方法可以充分利用 Python 和 Numpy 的最大性能。</p>
<p>这里要注意的主要事情是，首先尝试以一种简单的方式实现算法。</p>
<p>一旦它运行起来，分析它，找到瓶颈并优化它们。</p>
<ul>
<li>尽量避免在Python中使用循环，尤其是双/三重循环等。它们本来就很慢。</li>
<li>由于Numpy和OpenCV已针对向量运算进行了优化，因此将算法/代码向量化到最大程度。</li>
<li>利用缓存一致性。</li>
<li>除非需要，否则切勿创建数组的副本。尝试改用视图。数组复制是一项昂贵的操作。</li>
<li>即使执行了所有这些操作后，如果你的代码仍然很慢，或者不可避免地需要使用大循环，请使用Cython等其他库来使其更快。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理3-轮廓</title>
    <url>/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%863-%E8%BD%AE%E5%BB%93/</url>
    <content><![CDATA[<ul>
<li>9.1.<a href="#header1">轮廓入门</a></li>
<li>9.2.<a href="#header2">轮廓特征</a></li>
<li>9.3.<a href="#header3">轮廓属性</a></li>
<li>9.4.<a href="#header4">轮廓更多属性</a></li>
<li>9.5.<a href="#header5">轮廓分层</a><a id="more"></a>
</li>
</ul>
<h1 id="轮廓：入门"><a href="#轮廓：入门" class="headerlink" title="轮廓：入门"></a><span id="header1">轮廓：入门</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>了解轮廓是什么。</p>
<p>学习查找轮廓，绘制轮廓等。</p>
<p>你将看到以下功能：cv.findContours()，cv.drawContours()</p>
<h2 id="什么是轮廓"><a href="#什么是轮廓" class="headerlink" title="什么是轮廓?"></a>什么是轮廓?</h2><p>轮廓可以简单地解释为连接具有相同颜色或强度的所有连续点（沿边界）的曲线。</p>
<p>轮廓是用于形状分析以及对象检测和识别的有用工具。</p>
<p>为了获得更高的准确性，请使用二进制图像。因此，在找到轮廓之前，请应用阈值或canny边缘检测。</p>
<p>从OpenCV 3.2开始，findContours()不再修改源图像。</p>
<p>在OpenCV中，找到轮廓就像从黑色背景中找到白色物体。因此请记住，要找到的对象应该是白色，背景应该是黑色。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv.threshold(imgray, <span class="number">127</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">unknown,contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)</span><br></pre></td></tr></table></figure>
<p>findcontour()函数中有三个参数，</p>
<p>第一个是源图像，</p>
<p>第二个是轮廓检索模式，</p>
<p>第三个是轮廓逼近方法。</p>
<p>输出未知量,等高线和层次结构。</p>
<p>轮廓是图像中所有轮廓的Python列表。</p>
<p>每个单独的轮廓是一个(x,y)坐标的Numpy数组的边界点的对象。</p>
<p>注意 稍后我们将详细讨论第二和第三个参数以及有关层次结构。</p>
<p>在此之前，代码示例中赋予它们的值将适用于所有图像。</p>
<h2 id="如何绘制轮廓"><a href="#如何绘制轮廓" class="headerlink" title="如何绘制轮廓?"></a>如何绘制轮廓?</h2><p>要绘制轮廓，请使用<strong>cv.drawContours</strong>函数。只要有边界点，它也可以用来绘制任何形状。</p>
<p>它的第一个参数是源图像，</p>
<p>第二个参数是应该作为Python列表传递的轮廓，</p>
<p>第三个参数是轮廓的索引（在绘制单个轮廓时有用。要绘制所有轮廓，请传递-1），其余参数是颜色，厚度等等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在图像中绘制所有轮廓：</span></span><br><span class="line">cv.drawContours(img, contours, <span class="number">-1</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制单个轮廓，如第四个轮廓：</span></span><br><span class="line">cnt = contours[<span class="number">4</span>]</span><br><span class="line">cv.drawContours(img, [cnt], <span class="number">0</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 但是在大多数情况下，以下方法会很有用：</span></span><br><span class="line">cnt = contours[<span class="number">4</span>]</span><br><span class="line">cv.drawContours(img, [cnt], <span class="number">0</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>轮廓近似方法<br>这是<strong>cv.findContours</strong>函数中的第三个参数。它实际上表示什么？</p>
<p>上面我们告诉我们轮廓是强度相同的形状的边界。</p>
<p>它存储形状边界的(x,y)坐标。但是它存储所有坐标吗？这是通过这种轮廓近似方法指定的。</p>
<p>如果传递<strong>cv.CHAIN_APPROX_NONE</strong>，则将存储所有边界点。</p>
<p>但是实际上我们需要所有这些要点吗？</p>
<p>例如，您找到了一条直线的轮廓。您是否需要线上的所有点来代表该线？</p>
<p>不，我们只需要该线的两个端点即可。</p>
<p>这就是<strong>cv.CHAIN_APPROX_SIMPLE</strong>所做的。它删除所有冗余点并压缩轮廓，从而节省内存。</p>
<p>下面的矩形图像演示了此技术。</p>
<p>只需在轮廓数组中的所有坐标上绘制一个圆（以蓝色绘制）。</p>
<p>第一幅图像显示了我用<strong>cv.CHAIN_APPROX_NONE</strong>获得的积分（734个点），</p>
<p>第二幅图像显示了我用<strong>cv.CHAIN_APPROX_SIMPLE</strong>获得的效果（只有4个点）。</p>
<p>看，它可以节省多少内存！！！</p>
<p><img src="http://qiniu.aihubs.net/none.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="轮廓特征"><a href="#轮廓特征" class="headerlink" title="轮廓特征"></a><span id="header2">轮廓特征</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本文中，我们将学习 - 如何找到轮廓的不同特征，</p>
<p>例如面积，周长，质心，边界框等。 - 您将看到大量与轮廓有关的功能。</p>
<h2 id="1-特征矩"><a href="#1-特征矩" class="headerlink" title="1. 特征矩"></a>1. 特征矩</h2><p>特征矩可以帮助您计算一些特征，例如物体的质心，物体的面积等。</p>
<p>请查看特征矩上的维基百科页面。</p>
<p>函数<strong>cv.moments</strong>()提供了所有计算出的矩值的字典。见下文：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">ret,thresh = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line">unknown,contours,hierarchy = cv.findContours(thresh, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line">M = cv.moments(cnt)</span><br><span class="line">print( M )</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;m00&#39;: 2.0, &#39;m10&#39;: 536.0, &#39;m01&#39;: 1272.0, &#39;m20&#39;: 143648.3333333333, &#39;m11&#39;: 340896.0, &#39;m02&#39;: 808992.3333333333, &#39;m30&#39;: 38497932.0, &#39;m21&#39;: 91360340.0, &#39;m12&#39;: 216809945.33333334, &#39;m03&#39;: 514519548.0, &#39;mu20&#39;: 0.3333333333139308, &#39;mu11&#39;: 0.0, &#39;mu02&#39;: 0.3333333332557231, &#39;mu30&#39;: 1.4901161193847656e-08, &#39;mu21&#39;: 1.234002411365509e-08, &#39;mu12&#39;: 3.073364496231079e-08, &#39;mu03&#39;: 1.1920928955078125e-07, &#39;nu20&#39;: 0.0833333333284827, &#39;nu11&#39;: 0.0, &#39;nu02&#39;: 0.08333333331393078, &#39;nu30&#39;: 2.634178031930877e-09, &#39;nu21&#39;: 2.1814286826927578e-09, &#39;nu12&#39;: 5.432992190857434e-09, &#39;nu03&#39;: 2.1073424255447017e-08}
</code></pre><p>从这一刻起，您可以提取有用的数据，<br>例如面积，质心等。质心由关系给出，$C_x\frac{M_{10}}{M_{00}}$ 和 $C_y\frac{M_{01}}{M_{00}}$。可以按照以下步骤进行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cx = int(M[<span class="string">'m10'</span>]/M[<span class="string">'m00'</span>])</span><br><span class="line">cy = int(M[<span class="string">'m01'</span>]/M[<span class="string">'m00'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="2-轮廓面积"><a href="#2-轮廓面积" class="headerlink" title="2. 轮廓面积"></a>2. 轮廓面积</h2><p>轮廓区域由函数<strong>cv.contourArea</strong>()或从矩M[‘m00’]中给出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">area = cv.contourArea(cnt) </span><br><span class="line">print(area)</span><br><span class="line">print(M[<span class="string">'m00'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>2.0
2.0
</code></pre><h2 id="3-轮廓周长"><a href="#3-轮廓周长" class="headerlink" title="3. 轮廓周长"></a>3. 轮廓周长</h2><p>也称为弧长。可以使用<strong>cv.arcLength</strong>()函数找到它。第二个参数指定形状是闭合轮廓(True)还是曲线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">perimeter = cv.arcLength(cnt,<span class="literal">True</span>)</span><br><span class="line">perimeter</span><br></pre></td></tr></table></figure>
<pre><code>5.656854152679443
</code></pre><h2 id="4-轮廓近似"><a href="#4-轮廓近似" class="headerlink" title="4. 轮廓近似"></a>4. 轮廓近似</h2><p>根据我们指定的精度，它可以将轮廓形状近似为顶点数量较少的其他形状。</p>
<p>它是Douglas-Peucker算法的实现。检查维基百科页面上的算法和演示。</p>
<p>为了理解这一点，假设您试图在图像中找到一个正方形，但是由于图像中的某些问题，您没有得到一个完美的正方形，而是一个“坏形状”（如下图所示）。</p>
<p>现在，您可以使用此功能来近似形状。在这种情况下，第二个参数称为epsilon，它是从轮廓到近似轮廓的最大距离。</p>
<p>它是一个精度参数。需要正确选择epsilon才能获得正确的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">0.1</span>*cv.arcLength(cnt,<span class="literal">True</span>) </span><br><span class="line">approx = cv.approxPolyDP(cnt,epsilon,<span class="literal">True</span>)</span><br><span class="line">print(epsilon)</span><br><span class="line">print(approx)</span><br></pre></td></tr></table></figure>
<pre><code>0.5656854152679444
[[[267 636]]

 [[268 635]]

 [[269 636]]

 [[268 637]]]
</code></pre><p>下面，在第二张图片中，绿线显示了ε=弧长的10％时的近似曲线。第三幅图显示了ε=弧长度的1％时的情况。第三个参数指定曲线是否闭合。<br><img src="http://qiniu.aihubs.net/approx.jpg" alt></p>
<h2 id="5-轮廓凸包"><a href="#5-轮廓凸包" class="headerlink" title="5. 轮廓凸包"></a>5. 轮廓凸包</h2><p>凸包外观看起来与轮廓逼近相似，但不相似（在某些情况下两者可能提供相同的结果）。</p>
<p>在这里，cv.convexHull()函数检查曲线是否存在凸凹缺陷并对其进行校正。</p>
<p>一般而言，凸曲线是始终凸出或至少平坦的曲线。如果在内部凸出，则称为凸度缺陷。</p>
<p>例如，检查下面的手的图像。红线显示手的凸包。双向箭头标记显示凸度缺陷，这是凸包与轮廓线之间的局部最大偏差。</p>
<p><img src="http://qiniu.aihubs.net/convexitydefects.jpg" alt></p>
<p>关于它的语法，有一些需要讨论：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hull &#x3D; cv.convexHull(points[, hull[, clockwise[, returnPoints]]</span><br></pre></td></tr></table></figure><br>参数详细信息： </p>
<ul>
<li>点**是我们传递到的轮廓。 </li>
<li><strong>凸包</strong>是输出，通常我们忽略它。 </li>
<li>**顺时针方向：方向标记。如果为True，则输出凸包为顺时针方向。否则，其方向为逆时针方向。 </li>
<li>returnPoints：默认情况下为True。然后返回凸包的坐标。如果为False，则返回与凸包点相对应的轮廓点的索引。</li>
</ul>
<p>因此，要获得如上图所示的凸包，以下内容就足够了：<br>``<br>hull = cv.convexHull(cnt)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">但是，如果要查找凸度缺陷，则需要传递returnPoints &#x3D; False。</span><br><span class="line"></span><br><span class="line">为了理解它，我们将拍摄上面的矩形图像。</span><br><span class="line"></span><br><span class="line">首先，我发现它的轮廓为cnt。现在，我发现它的带有returnPoints &#x3D; True的凸包，</span><br><span class="line"></span><br><span class="line">得到以下值：[[[234 202]]，[[51 202]]，[[51 79]]，[[234 79]]]，它们是四个角 矩形的点。</span><br><span class="line"></span><br><span class="line">现在，如果对returnPoints &#x3D; False执行相同的操作，</span><br><span class="line"></span><br><span class="line">则会得到以下结果：[[129]，[67]，[0]，[142]]。这些是轮廓中相应点的索引。</span><br><span class="line"></span><br><span class="line">例如，检查第一个值：cnt [129] &#x3D; [[234，202]]与第一个结果相同（对于其他结果依此类推）。</span><br><span class="line"></span><br><span class="line">当我们讨论凸度缺</span><br><span class="line"></span><br><span class="line">## 6. 检查凸度</span><br><span class="line">cv.isContourConvex()具有检查曲线是否凸出的功能。它只是返回True还是False。没什么大不了的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">k &#x3D; cv.isContourConvex(cnt) </span><br><span class="line">k</span><br></pre></td></tr></table></figure></p>
<pre><code>True
</code></pre><h2 id="7-边界矩形"><a href="#7-边界矩形" class="headerlink" title="7. 边界矩形"></a>7. 边界矩形</h2><p>有两种类型的边界矩形。</p>
<h3 id="7-a-直角矩形"><a href="#7-a-直角矩形" class="headerlink" title="7.a.直角矩形"></a>7.a.直角矩形</h3><p>它是一个矩形，不考虑物体的旋转。所以边界矩形的面积不是最小的。</p>
<p>它是由函数<strong>cv.boundingRect</strong>()找到的。</p>
<p>令(x，y)为矩形的左上角坐标，而(w，h)为矩形的宽度和高度。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x,y,w,h &#x3D; cv.boundingRect(cnt)</span><br><span class="line">cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)</span><br></pre></td></tr></table></figure></p>
<h3 id="7-b-旋转矩形"><a href="#7-b-旋转矩形" class="headerlink" title="7.b. 旋转矩形"></a>7.b. 旋转矩形</h3><p>这里，边界矩形是用最小面积绘制的，所以它也考虑了旋转。</p>
<p>使用的函数是<strong>cv.minAreaRect</strong>()。</p>
<p>它返回一个Box2D结构，其中包含以下细节 -(中心(x,y)，(宽度，高度)，旋转角度)。</p>
<p>但要画出这个矩形，我们需要矩形的四个角。</p>
<p>它由函数<strong>cv.boxPoints</strong>()获得<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rect &#x3D; cv.minAreaRect(cnt)</span><br><span class="line">box &#x3D; cv.boxPoints(rect)</span><br><span class="line">box &#x3D; np.int0(box)</span><br><span class="line">cv.drawContours(img,[box],0,(0,0,255),2)</span><br></pre></td></tr></table></figure><br>两个矩形都显示在一张单独的图像中。绿色矩形显示正常的边界矩形。红色矩形是旋转后的矩形。</p>
<p><img src="http://qiniu.aihubs.net/boundingrect.png" alt></p>
<h2 id="8-最小闭合圈"><a href="#8-最小闭合圈" class="headerlink" title="8. 最小闭合圈"></a>8. 最小闭合圈</h2><p>接下来，使用函数<em>*cv.minEnclosingCircle(</em>()查找对象的圆周。它是一个以最小面积完全覆盖物体的圆。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(x,y),radius &#x3D; cv.minEnclosingCircle(cnt)</span><br><span class="line">center &#x3D; (int(x),int(y))</span><br><span class="line">radius &#x3D; int(radius)</span><br><span class="line">cv.circle(img,center,radius,(0,255,0),2)</span><br><span class="line">&#96;&#96;&#96;                            </span><br><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;circumcircle.png)</span><br><span class="line"></span><br><span class="line">## 9. 拟合一个椭圆</span><br><span class="line">下一个是把一个椭圆拟合到一个物体上。它返回内接椭圆的旋转矩形。</span><br></pre></td></tr></table></figure><br>ellipse = cv.fitEllipse(cnt)<br>cv.ellipse(img,ellipse,(0,255,0),2)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;fitellipse.png)</span><br><span class="line"></span><br><span class="line">## 10. 拟合直线</span><br><span class="line">同样，我们可以将一条直线拟合到一组点。下图包含一组白点。我们可以近似一条直线。</span><br></pre></td></tr></table></figure><br>rows,cols = img.shape[:2]<br>[vx,vy,x,y] = cv.fitLine(cnt, cv.DIST_L2,0,0.01,0.01)<br>lefty = int((-x<em>vy/vx) + y)<br>righty = int(((cols-x)</em>vy/vx)+y)<br>cv.line(img,(cols-1,righty),(0,lefty),(0,255,0),2)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># &lt;span id&#x3D;&quot;header3&quot;&gt;轮廓属性&lt;&#x2F;span&gt;</span><br><span class="line">在这里，我们将学习提取一些常用的物体属性，</span><br><span class="line"></span><br><span class="line">如坚实度，等效直径，掩模图像，平均强度等。更多的功能可以在Matlab regionprops文档中找到。</span><br><span class="line"></span><br><span class="line">(注:质心、面积、周长等也属于这一类，但我们在上一章已经见过)</span><br><span class="line"></span><br><span class="line">## 1. 长宽比</span><br><span class="line">它是对象边界矩形的宽度与高度的比值。</span><br><span class="line"></span><br><span class="line">Aspect Ratio&#x3D;$\frac&#123;Width&#125;&#123;Height&#125;$</span><br></pre></td></tr></table></figure><br>x,y,w,h = cv.boundingRect(cnt)<br>aspect_ratio = float(w)/h<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 2. 范围</span><br><span class="line">范围是轮廓区域与边界矩形区域的比值。</span><br><span class="line"></span><br><span class="line">Extent&#x3D;$\frac&#123;Object Area&#125;&#123;Bounding Rectangle Area&#125;$</span><br></pre></td></tr></table></figure><br>area = cv.contourArea(cnt)<br>x,y,w,h = cv.boundingRect(cnt)<br>rect_area = w<em>h<br>extent = float(area)/rect_area<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 3. 坚实度</span><br><span class="line">坚实度是等高线面积与其凸包面积之比。</span><br><span class="line"></span><br><span class="line">Solidity&#x3D;$\frac&#123;Contour Area&#125;&#123;ConvexHull Area&#125;$</span><br></pre></td></tr></table></figure><br>area = cv.contourArea(cnt)<br>hull = cv.convexHull(cnt)<br>hull_area = cv.contourArea(hull)<br>solidity = float(area)/hull_area<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 4. 等效直径</span><br><span class="line">等效直径是面积与轮廓面积相同的圆的直径。</span><br><span class="line"></span><br><span class="line">EquivalentDiameter&#x3D;$\sqrt&#123;\frac&#123;4×ContourArea&#125;&#123;\Pi&#125;&#125;$</span><br></pre></td></tr></table></figure><br>area = cv.contourArea(cnt)<br>equi_diameter = np.sqrt(4</em>area/np.pi)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 5. 取向</span><br><span class="line">取向是物体指向的角度。以下方法还给出了主轴和副轴的长度。</span><br></pre></td></tr></table></figure><br>(x,y),(MA,ma),angle = cv.fitEllipse(cnt)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 6. 掩码和像素点</span><br><span class="line">在某些情况下，我们可能需要构成该对象的所有点。可以按照以下步骤完成：</span><br></pre></td></tr></table></figure><br>mask = np.zeros(imgray.shape,np.uint8)<br>cv.drawContours(mask,[cnt],0,255,-1)<br>pixelpoints = np.transpose(np.nonzero(mask))</p>
<h1 id="pixelpoints-cv-findNonZero-mask"><a href="#pixelpoints-cv-findNonZero-mask" class="headerlink" title="pixelpoints = cv.findNonZero(mask)"></a>pixelpoints = cv.findNonZero(mask)</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这里提供了两个方法，一个使用Numpy函数，另一个使用OpenCV函数(最后的注释行)。结果也是一样的，只是略有不同。</span><br><span class="line"></span><br><span class="line">Numpy给出的坐标是(行、列)格式，</span><br><span class="line"></span><br><span class="line">而OpenCV给出的坐标是(x,y)格式。所以基本上答案是可以互换的。注意，row &#x3D; x, column &#x3D; y。</span><br><span class="line"></span><br><span class="line">## 7. 最大值，最小值和它们的位置</span><br><span class="line">我们可以使用掩码图像找到这些参数。</span><br></pre></td></tr></table></figure>
<p>min_val, max_val, min_loc, max_loc = cv.minMaxLoc(imgray,mask = mask)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 8. 平均颜色或平均强度</span><br><span class="line">在这里，我们可以找到对象的平均颜色。或者可以是灰度模式下物体的平均强度。我们再次使用相同的掩码进行此操作。</span><br></pre></td></tr></table></figure><br>mean_val = cv.mean(im,mask = mask)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 9. 极端点</span><br><span class="line">极点是指对象的最顶部，最底部，最右侧和最左侧的点。</span><br></pre></td></tr></table></figure><br>leftmost = tuple(cnt[cnt[:,:,0].argmin()][0])<br>rightmost = tuple(cnt[cnt[:,:,0].argmax()][0])<br>topmost = tuple(cnt[cnt[:,:,1].argmin()][0])<br>bottommost = tuple(cnt[cnt[:,:,1].argmax()][0])<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">例如，如果我将其应用于印度地图，则会得到以下结果：</span><br><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;extremepoints.jpg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br></pre></td></tr></table></figure></p>
<h1 id="轮廓：更多属性"><a href="#轮廓：更多属性" class="headerlink" title="轮廓：更多属性"></a><span id="header4">轮廓：更多属性</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习 - 凸性缺陷以及如何找到它们 - 查找点到多边形的最短距离 - 匹配不同的形状</p>
<h2 id="理论和代码"><a href="#理论和代码" class="headerlink" title="理论和代码"></a>理论和代码</h2><h3 id="1-凸性缺陷"><a href="#1-凸性缺陷" class="headerlink" title="1. 凸性缺陷"></a>1. 凸性缺陷</h3><p>我们看到了关于轮廓的第二章的凸包。从这个凸包上的任何偏差都可以被认为是凸性缺陷。 OpenCV有一个函数来找到这个,cv.convexityDefects()。一个基本的函数调用如下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hull &#x3D; cv.convexHull(cnt,returnPoints &#x3D; False)</span><br><span class="line">defects &#x3D; cv.convexityDefects(cnt,hull)</span><br></pre></td></tr></table></figure><br>注意 记住,我们必须在发现凸包时,传递returnPoints= False,以找到凸性缺陷。</p>
<p>它返回一个数组，其中每行包含这些值—[起点、终点、最远点、到最远点的近似距离]。我们可以用图像把它形象化。我们画一条连接起点和终点的线，然后在最远处画一个圆。记住，返回的前三个值是cnt的索引。所以我们必须从cnt中获取这些值。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(&#39;star.jpg&#39;)</span><br><span class="line">img_gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,thresh &#x3D; cv.threshold(img_gray, 127, 255,0)</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh,2,1)</span><br><span class="line">cnt &#x3D; contours[0]</span><br><span class="line">hull &#x3D; cv.convexHull(cnt,returnPoints &#x3D; False)</span><br><span class="line">defects &#x3D; cv.convexityDefects(cnt,hull)</span><br><span class="line">for i in range(defects.shape[0]):</span><br><span class="line">    s,e,f,d &#x3D; defects[i,0]</span><br><span class="line">    start &#x3D; tuple(cnt[s][0])</span><br><span class="line">    end &#x3D; tuple(cnt[e][0])</span><br><span class="line">    far &#x3D; tuple(cnt[f][0])</span><br><span class="line">    cv.line(img,start,end,[0,255,0],2)</span><br><span class="line">    cv.circle(img,far,5,[0,0,255],-1)</span><br><span class="line">cv.imshow(&#39;img&#39;,img)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><br>查看结果：<br><img src="http://qiniu.aihubs.net/defects.jpg" alt></p>
<h3 id="2-点多边形测试"><a href="#2-点多边形测试" class="headerlink" title="2. 点多边形测试"></a>2. 点多边形测试</h3><p>这个函数找出图像中一点到轮廓线的最短距离。它返回的距离，点在轮廓线外时为负，点在轮廓线内时为正，点在轮廓线上时为零。</p>
<p>例如，我们可以检查点(50,50)如下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dist &#x3D; cv.pointPolygonTest(cnt,(50,50),True)</span><br></pre></td></tr></table></figure><br>在函数中，第三个参数是measureDist。如果它是真的，它会找到有符号的距离。如果为假，则查找该点是在轮廓线内部还是外部(分别返回+1、-1和0)。</p>
<p>注意 如果您不想找到距离，请确保第三个参数为False，因为这是一个耗时的过程。因此，将其设置为False可使速度提高2-3倍。</p>
<h3 id="3-形状匹配"><a href="#3-形状匹配" class="headerlink" title="3. 形状匹配"></a>3. 形状匹配</h3><p>OpenCV附带一个函数<strong>cv.matchShapes</strong>()，该函数使我们能够比较两个形状或两个轮廓，并返回一个显示相似性的度量。结果越低，匹配越好。它是根据矩值计算出来的。不同的测量方法在文档中有解释。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img1 &#x3D; cv.imread(&#39;star.jpg&#39;,0)</span><br><span class="line">img2 &#x3D; cv.imread(&#39;star2.jpg&#39;,0)</span><br><span class="line">ret, thresh &#x3D; cv.threshold(img1, 127, 255,0)</span><br><span class="line">ret, thresh2 &#x3D; cv.threshold(img2, 127, 255,0)</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh,2,1)</span><br><span class="line">cnt1 &#x3D; contours[0]</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh2,2,1)</span><br><span class="line">cnt2 &#x3D; contours[0]</span><br><span class="line">ret &#x3D; cv.matchShapes(cnt1,cnt2,1,0.0)</span><br><span class="line">print( ret )</span><br></pre></td></tr></table></figure><br>我尝试过匹配下面给出的不同形状的形状：<br><img src="http://qiniu.aihubs.net/matchshapes.jpg" alt></p>
<p>我得到以下结果: - 匹配的图像A与本身= 0.0 - 匹配图像A与图像B = 0.001946 - 匹配图像A与图像C = 0.326911</p>
<p>看,即使是图像旋转也不会对这个比较产生很大的影响。</p>
<p>参考 Hu矩是平移、旋转和比例不变的七个矩。第七个是无偏斜量。这些值可以使用<strong>cpu.HuMoments</strong>()函数找到。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="轮廓分层"><a href="#轮廓分层" class="headerlink" title="轮廓分层"></a><span id="header5">轮廓分层</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>这次我们学习轮廓的层次，即轮廓中的父子关系。</p>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>在前几篇关于轮廓的文章中，我们已经讨论了与OpenCV提供的轮廓相关的几个函数。</p>
<p>但是当我们使用<strong>cv.findcontour</strong>()函数在图像中找到轮廓时，我们已经传递了一个参数，轮廓检索模式。</p>
<p>我们通常通过了<strong>cv.RETR_LIST</strong>或<strong>cv.RETR_TREE</strong>，效果很好。但这到底意味着什么呢?</p>
<p>另外，在输出中，我们得到了三个数组，第一个是图像，第二个是轮廓，还有一个我们命名为<strong>hierarchy</strong>的输出(请检查前面文章中的代码)。</p>
<p>但我们从未在任何地方使用过这种层次结构。那么这个层级是什么?它是用来做什么的?它与前面提到的函数参数有什么关系?</p>
<p>这就是我们在本文中要讨论的内容。</p>
<h2 id="层次结构是什么？"><a href="#层次结构是什么？" class="headerlink" title="层次结构是什么？"></a>层次结构是什么？</h2><p>通常我们使用<strong>cv.findcontour</strong>()函数来检测图像中的对象，对吧？</p>
<p>有时对象在不同的位置。但在某些情况下，某些形状在其他形状中。</p>
<p>就像嵌套的图形一样。在这种情况下，我们把外部的称为<strong>父类</strong>，把内部的称为<strong>子类</strong>。</p>
<p>这样，图像中的轮廓就有了一定的相互关系。</p>
<p>我们可以指定一个轮廓是如何相互连接的，比如，它是另一个轮廓的子轮廓，还是父轮廓等等。这种关系的表示称为<strong>层次结构</strong>。</p>
<p><img src="http://qiniu.aihubs.net/hierarchy.png" alt><br>在这张图中，有一些形状我已经从<strong>0-5</strong>开始编号。<em>2</em>和<em>2a</em>表示最外层盒子的外部和内部轮廓。</p>
<p>这里，等高线0,1,2在<strong>外部或最外面</strong>。我们可以说，它们在<strong>层级-0</strong>中，或者简单地说，它们在<strong>同一个层级</strong>中。</p>
<p>其次是<strong>contour-2a</strong>。它可以被认为是<strong>contour-2的子级</strong>(或者反过来，contour-2是contour-2a的父级)。</p>
<p>假设它在<strong>层级-1</strong>中。类似地，contour-3是contour-2的子级，它位于下一个层次结构中。</p>
<p>最后，轮廓4,5是contour-3a的子级，他们在最后一个层级。</p>
<p>从对方框的编号来看，我认为contour-4是contour-3a的第一个子级(它也可以是contour-5)。</p>
<p>我提到这些是为了理解一些术语，比如<strong>相同层级</strong>，外部轮廓，子轮廓，父轮廓，<strong>第一个子轮廓</strong>等等。现在让我们进入OpenCV。</p>
<h2 id="OpenCV中的分级表示"><a href="#OpenCV中的分级表示" class="headerlink" title="OpenCV中的分级表示"></a>OpenCV中的分级表示</h2><p>所以每个轮廓都有它自己的信息关于它是什么层次，谁是它的孩子，谁是它的父母等等。</p>
<p>OpenCV将它表示为一个包含四个值的数组:[Next, Previous, First_Child, Parent]</p>
<p>“Next表示同一层次的下一个轮廓。”</p>
<p>例如，在我们的图片中取contour-0。谁是下一个同级别的等高线?这是contour-1。</p>
<p>简单地令Next = 1。类似地，Contour-1也是contour-2。所以Next = 2。 contour-2呢?同一水平线上没有下一条等高线。</p>
<p>简单地，让Next = -1。contour-4呢?它与contour-5处于同一级别。它的下一条等高线是contour-5，所以next = 5。</p>
<p>“Previous表示同一层次上的先前轮廓。”</p>
<p>和上面一样。contour-1之前的等值线为同级别的contour-0。</p>
<p>类似地，contour-2也是contour-1。对于contour-0，没有前项，所以设为-1。</p>
<p>“First_Child表示它的第一个子轮廓。”</p>
<p>没有必要作任何解释。对于contour-2, child是contour-2a。从而得到contour-2a对应的指标值。</p>
<p>contour-3a呢?它有两个孩子。但我们只关注第一个孩子。它是contour-4。那么First_Child = 4 对contour-3a而言。</p>
<p>“Parent表示其父轮廓的索引。”</p>
<p>它与<strong>First_Child</strong>相反。对于轮廓线-4和轮廓线-5，父轮廓线都是轮廓线-3a。对于轮廓3a，它是轮廓-3，以此类推。</p>
<p>注意 如果没有子元素或父元素，则该字段被视为-1</p>
<p>现在我们已经了解了OpenCV中使用的层次样式，我们可以借助上面给出的相同图像来检查OpenCV中的轮廓检索模式。</p>
<p>一些标志如 cv.RETR_LIST, cv.RETR_TREE,cv.RETR_CCOMP, <strong>cv.RETR_EXTERNAL</strong>等等的含义。</p>
<h2 id="轮廓检索模式"><a href="#轮廓检索模式" class="headerlink" title="轮廓检索模式"></a>轮廓检索模式</h2><h3 id="1-RETR-LIST"><a href="#1-RETR-LIST" class="headerlink" title="1. RETR_LIST"></a>1. RETR_LIST</h3><p>这是四个标志中最简单的一个(从解释的角度来看)。它只是检索所有的轮廓，但不创建任何亲子关系。</p>
<p>在这个规则下，父轮廓和子轮廓是平等的，他们只是轮廓。他们都属于同一层级。</p>
<p>这里，第3和第4项总是-1。但是很明显，下一项和上一项都有对应的值。你自己检查一下就可以了。</p>
<p>下面是我得到的结果，每一行是对应轮廓的层次细节。例如，第一行对应于轮廓0。下一条轮廓是轮廓1。所以Next = 1。</p>
<p>没有先前的轮廓，所以Previous=-1。剩下的两个，如前所述，是-1。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 1, -1, -1, -1],</span><br><span class="line">        [ 2,  0, -1, -1],</span><br><span class="line">        [ 3,  1, -1, -1],</span><br><span class="line">        [ 4,  2, -1, -1],</span><br><span class="line">        [ 5,  3, -1, -1],</span><br><span class="line">        [ 6,  4, -1, -1],</span><br><span class="line">        [ 7,  5, -1, -1],</span><br><span class="line">        [-1,  6, -1, -1]]])</span><br></pre></td></tr></table></figure><br>如果您没有使用任何层次结构特性，那么这是在您的代码中使用的最佳选择。</p>
<h3 id="2-RETR-EXTERNAL"><a href="#2-RETR-EXTERNAL" class="headerlink" title="2. RETR_EXTERNAL"></a>2. RETR_EXTERNAL</h3><p>如果使用此标志，它只返回极端外部标志。所有孩子的轮廓都被留下了。</p>
<p>我们可以说，根据这项规则，每个家庭只有长子得到关注。它不关心家庭的其他成员:)。</p>
<p>所以在我们的图像中，有多少个极端的外轮廓?在等级0级?有3个，即等值线是0 1 2，对吧?</p>
<p>现在试着用这个标志找出等高线。这里，给每个元素的值与上面相同。并与上述结果进行了比较。以下是我得到的:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 1, -1, -1, -1],</span><br><span class="line">        [ 2,  0, -1, -1],</span><br><span class="line">        [-1,  1, -1, -1]]])</span><br></pre></td></tr></table></figure><br>如果只想提取外部轮廓，可以使用此标志。它在某些情况下可能有用。</p>
<h3 id="3-RETR-CCOMP"><a href="#3-RETR-CCOMP" class="headerlink" title="3. RETR_CCOMP"></a>3. RETR_CCOMP</h3><p>此标志检索所有轮廓并将其排列为2级层次结构。物体的外部轮廓(即物体的边界)放在层次结构-1中。</p>
<p>对象内部孔洞的轮廓(如果有)放在层次结构-2中。如果其中有任何对象，则其轮廓仅在层次结构1中重新放置。以及它在层级2中的漏洞等等。</p>
<p>只需考虑在黑色背景上的“白色的零”图像。零的外圆属于第一级，零的内圆属于第二级。</p>
<p>我们可以用一个简单的图像来解释它。这里我用红色标注了等高线的顺序和它们所属的层次，用绿色标注(1或2)，顺序与OpenCV检测等高线的顺序相同。<br><img src="http://qiniu.aihubs.net/ccomp_hierarchy.png" alt><br>考虑第一个轮廓，即contour-0。这是hierarchy-1。它有两个孔，分别是等高线1和2，属于第二级。</p>
<p>因此，对于轮廓-0，在同一层次的下一个轮廓是轮廓-3。previous也没有。在hierarchy-2中，它的第一个子结点是contour-1。</p>
<p>它没有父类，因为它在hierarchy-1中。所以它的层次数组是[3，-1,1，-1]</p>
<p>现在contour-1。它在层级-2中。相同层次结构中的下一个(在contour-1的父母关系下)是contour-2。</p>
<p>没有previous。没有child，但是parent是contour-0。所以数组是[2，-1，-1,0]</p>
<p>类似的contour-2:它在hierarchy-2中。在contour-0下，同一层次结构中没有下一个轮廓。</p>
<p>所以没有Next。previous是contour-1。没有child，parent是contour0。所以数组是[-1,1，-1,0]</p>
<p>contour-3:层次-1的下一个是轮廓-5。以前是contour-0。child是contour4，没有parent。所以数组是[5,0,4，-1]</p>
<p>contour-4:它在contour-3下的层次结构2中，它没有兄弟姐妹。没有next，没有previous，没有child，parent是contour-3。</p>
<p>所以数组是[-1，-1，-1,3]</p>
<p>剩下的你可以补充。这是我得到的最终答案:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 3, -1,  1, -1],</span><br><span class="line">        [ 2, -1, -1,  0],</span><br><span class="line">        [-1,  1, -1,  0],</span><br><span class="line">        [ 5,  0,  4, -1],</span><br><span class="line">        [-1, -1, -1,  3],</span><br><span class="line">        [ 7,  3,  6, -1],</span><br><span class="line">        [-1, -1, -1,  5],</span><br><span class="line">        [ 8,  5, -1, -1],</span><br><span class="line">        [-1,  7, -1, -1]]])</span><br></pre></td></tr></table></figure></p>
<h3 id="4-RETR-TREE"><a href="#4-RETR-TREE" class="headerlink" title="4. RETR_TREE"></a>4. RETR_TREE</h3><p>这是最后一个家伙，完美先生。它检索所有的轮廓并创建一个完整的家族层次结构列表。它甚至告诉，谁是爷爷，父亲，儿子，孙子，甚至更多…:)。</p>
<p>例如，我拿上面的图片，重写了cv的代码。RETR_TREE，根据OpenCV给出的结果重新排序等高线并进行分析。</p>
<p>同样，红色的字母表示轮廓数，绿色的字母表示层次顺序。<br><img src="http://qiniu.aihubs.net/tree_hierarchy.png" alt><br>取contour-0:它在hierarchy-0中。同一层次结构的next轮廓是轮廓-7。没有previous的轮廓。child是contour-1，没有parent。所以数组是[7，-1,1，-1]</p>
<p>以contour-2为例:它在hierarchy-1中。没有轮廓在同一水平。没有previous。child是contour-3。父母是contour-1。所以数组是[-1，-1,3,1]</p>
<p>剩下的，你自己试试。以下是完整答案:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 7, -1,  1, -1],</span><br><span class="line">        [-1, -1,  2,  0],</span><br><span class="line">        [-1, -1,  3,  1],</span><br><span class="line">        [-1, -1,  4,  2],</span><br><span class="line">        [-1, -1,  5,  3],</span><br><span class="line">        [ 6, -1, -1,  4],</span><br><span class="line">        [-1,  5, -1,  4],</span><br><span class="line">        [ 8,  0, -1, -1],</span><br><span class="line">        [-1,  7, -1, -1]]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>动手实现卷积层和池化层</title>
    <url>/2020/07/29/%E5%8A%A8%E6%89%8B%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%92%8C%E6%B1%A0%E5%8C%96%E5%B1%82/</url>
    <content><![CDATA[<p>动手实现卷积层和池化层<br><a id="more"></a></p>
<h1 id="卷积层实现"><a href="#卷积层实现" class="headerlink" title="卷积层实现"></a>卷积层实现</h1><p>CNN中各层间传递的数据是4维数据。所谓4维数据，比如数据的形状是(10, 1, 28, 28)，<br>则它对应10个高为28、长为28、通道为1的数据。用Python来实现的话，如下所示。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; x &#x3D; np.random.rand(10, 1, 28, 28) # 随机生成数据</span><br><span class="line">&gt;&gt;&gt; x.shape</span><br><span class="line">(10, 1, 28, 28)</span><br></pre></td></tr></table></figure><br>这里，如果要访问第1个数据，只要写x[0]就可以了（注意Python的索引是从0开始的）。同样地，用x[1]可以访问第2个数据<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; x[0].shape # (1, 28, 28)</span><br><span class="line">&gt;&gt;&gt; x[1].shape # (1, 28, 28)</span><br></pre></td></tr></table></figure></p>
<p>如果要访问第1个数据的第1个通道的空间数据，可以写成下面这样。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; x[0, 0] # 或者x[0][0]</span><br></pre></td></tr></table></figure></p>
<p>像这样，CNN中处理的是4维数据，因此卷积运算的实现看上去会很复杂，但是通过使用下面要介绍的im2col这个技巧，问题就会变得很简单。</p>
<h2 id="基于-im2col的展开"><a href="#基于-im2col的展开" class="headerlink" title="基于 im2col的展开"></a>基于 im2col的展开</h2><p>如果老老实实地实现卷积运算，估计要重复好几层的for语句。<br>这样的实现有点麻烦，而且，NumPy中存在使用for语句后处理变慢的缺点（NumPy中，访问元素时最好不要用for语句）。</p>
<p>这里，我们不使用for语句，而是使用im2col这个便利的函数进行简单的实现。</p>
<p>im2col是一个函数，将输入数据展开以适合滤波器（权重）。</p>
<p>如图7-17所示，对3维的输入数据应用im2col后，数据转换为2维矩阵（正确地讲，是把包含批数量的4维数据转换成了2维数据）。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B01.png" alt></p>
<p>im2col会把输入数据展开以适合滤波器（权重）。</p>
<p>具体地说，如图7-18所示，对于输入数据，将应用滤波器的区域（3维方块）横向展开为1列。</p>
<p>im2col会在所有应用滤波器的地方进行这个展开处理。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B02.png" alt></p>
<p>在图7-18中，为了便于观察，将步幅设置得很大，以使滤波器的应用区域不重叠。</p>
<p>而在实际的卷积运算中，滤波器的应用区域几乎都是重叠的。</p>
<p>在滤波器的应用区域重叠的情况下，使用im2col展开后，展开后的元素个数会多于原方块的元素个数。</p>
<p>因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。</p>
<p>但是，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处。</p>
<p>比如，在矩阵计算的库（线性代数库）等中，矩阵计算的实现已被高度最优化，可以高速地进行大矩阵的乘法运算。因此，通过归结到矩阵计算<br>上，可以有效地利用线性代数库。</p>
<p>im2col这个名称是“image to column”的缩写，翻译过来就是“从图像到矩阵”的意思。</p>
<p>Caffe、Chainer 等深度学习框架中有名为im2col的函数，并且在卷积层的实现中，都使用了im2col。</p>
<p>使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可（参照图7-19）。</p>
<p>这和全连接层的Affi ne层进行的处理基本相同。<br>如图7-19所示，基于im2col方式的输出结果是2维矩阵。</p>
<p>因为CNN中数据会保存为4维数组，所以要将2维输出数据转换为合适的形状。</p>
<p>以上就是卷积层的实现流程。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B03.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def im2col(input_data, filter_h, filter_w, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据</span><br><span class="line">    filter_h : 滤波器的高</span><br><span class="line">    filter_w : 滤波器的长</span><br><span class="line">    stride : 步幅</span><br><span class="line">    pad : 填充</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    col : 2维数组</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W &#x3D; input_data.shape</span><br><span class="line">    out_h &#x3D; (H + 2*pad - filter_h)&#x2F;&#x2F;stride + 1</span><br><span class="line">    out_w &#x3D; (W + 2*pad - filter_w)&#x2F;&#x2F;stride + 1</span><br><span class="line"></span><br><span class="line">    img &#x3D; np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], &#39;constant&#39;)</span><br><span class="line">    col &#x3D; np.zeros((N, C, filter_h, filter_w, out_h, out_w))</span><br><span class="line"></span><br><span class="line">    for y in range(filter_h):</span><br><span class="line">        y_max &#x3D; y + stride*out_h</span><br><span class="line">        for x in range(filter_w):</span><br><span class="line">            x_max &#x3D; x + stride*out_w</span><br><span class="line">            col[:, :, y, x, :, :] &#x3D; img[:, :, y:y_max:stride, x:x_max:stride]</span><br><span class="line"></span><br><span class="line">    col &#x3D; col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)</span><br><span class="line">    return col</span><br></pre></td></tr></table></figure>
<p>im2col会考虑滤波器大小、步幅、填充，将输入数据展开为2维数组。<br>现在，我们来实际使用一下这个im2col。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line">from common.util import im2col</span><br><span class="line">x1 &#x3D; np.random.rand(1, 3, 7, 7)</span><br><span class="line">col1 &#x3D; im2col(x1, 5, 5, stride&#x3D;1, pad&#x3D;0)</span><br><span class="line">print(col1.shape) # (9, 75)</span><br><span class="line">x2 &#x3D; np.random.rand(10, 3, 7, 7) # 10个数据</span><br><span class="line">col2 &#x3D; im2col(x2, 5, 5, stride&#x3D;1, pad&#x3D;0)</span><br><span class="line">print(col2.shape) # (90, 75)</span><br></pre></td></tr></table></figure></p>
<p>这里举了两个例子。第一个是批大小为1、通道为3的7 × 7的数据，第二个的批大小为10，数据形状和第一个相同。</p>
<p>分别对其应用im2col函数，在这两种情形下，第2维的元素个数均为75。这是滤波器（通道为3、大小为5 × 5）的元素个数的总和。批大小为1时，im2col的结果是(9, 75)。而第2个例子中批大小为10，所以保存了10倍的数据，即(90, 75)。现在使用im2col来实现卷积层。这里我们将卷积层实现为名为Convolution的类。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Convolution:</span><br><span class="line">    def __init__(self, W, b, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">        self.W &#x3D; W</span><br><span class="line">        self.b &#x3D; b</span><br><span class="line">        self.stride &#x3D; stride</span><br><span class="line">        self.pad &#x3D; pad</span><br><span class="line">        </span><br><span class="line">        # 中间数据（backward时使用）</span><br><span class="line">        self.x &#x3D; None   </span><br><span class="line">        self.col &#x3D; None</span><br><span class="line">        self.col_W &#x3D; None</span><br><span class="line">        </span><br><span class="line">        # 权重和偏置参数的梯度</span><br><span class="line">        self.dW &#x3D; None</span><br><span class="line">        self.db &#x3D; None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        FN, C, FH, FW &#x3D; self.W.shape</span><br><span class="line">        N, C, H, W &#x3D; x.shape</span><br><span class="line">        out_h &#x3D; 1 + int((H + 2*self.pad - FH) &#x2F; self.stride)</span><br><span class="line">        out_w &#x3D; 1 + int((W + 2*self.pad - FW) &#x2F; self.stride)</span><br><span class="line"></span><br><span class="line">        col &#x3D; im2col(x, FH, FW, self.stride, self.pad)</span><br><span class="line">        col_W &#x3D; self.W.reshape(FN, -1).T</span><br><span class="line"></span><br><span class="line">        out &#x3D; np.dot(col, col_W) + self.b</span><br><span class="line">        out &#x3D; out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)</span><br><span class="line"></span><br><span class="line">        self.x &#x3D; x</span><br><span class="line">        self.col &#x3D; col</span><br><span class="line">        self.col_W &#x3D; col_W</span><br><span class="line"></span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">    def backward(self, dout):</span><br><span class="line">        FN, C, FH, FW &#x3D; self.W.shape</span><br><span class="line">        dout &#x3D; dout.transpose(0,2,3,1).reshape(-1, FN)</span><br><span class="line"></span><br><span class="line">        self.db &#x3D; np.sum(dout, axis&#x3D;0)</span><br><span class="line">        self.dW &#x3D; np.dot(self.col.T, dout)</span><br><span class="line">        self.dW &#x3D; self.dW.transpose(1, 0).reshape(FN, C, FH, FW)</span><br><span class="line"></span><br><span class="line">        dcol &#x3D; np.dot(dout, self.col_W.T)</span><br><span class="line">        dx &#x3D; col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)</span><br><span class="line"></span><br><span class="line">        return dx</span><br></pre></td></tr></table></figure>
<p>卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。</p>
<p>滤波器是 (FN, C, FH, FW)的 4 维形状。另外，FN、C、FH、FW分别是 Filter Number（滤波器数量）、Channel、Filter Height、Filter Width的缩写。</p>
<p>这里用粗体字表示Convolution层的实现中的重要部分。在这些粗体字部分，用im2col展开输入数据，并用reshape将滤波器展开为2维数组。然后，计算展开后的矩阵的乘积。</p>
<p>展开滤波器的部分（代码段中的粗体字）如图7-19所示，将各个滤波器的方块纵向展开为1列。这里通过reshape(FN,-1)将参数指定为-1，这是<br>reshape的一个便利的功能。通过在reshape时指定为-1，reshape函数会自动计算-1维度上的元素个数，以使多维数组的元素个数前后一致。比如，<br>(10, 3, 5, 5)形状的数组的元素个数共有750个，指定reshape(10,-1)后，就会转换成(10, 75)形状的数组。</p>
<p>forward的实现中，最后会将输出大小转换为合适的形状。转换时使用了NumPy的transpose函数。transpose会更改多维数组的轴的顺序。如图7-20<br>所示，通过指定从0开始的索引（编号）序列，就可以更改轴的顺序。</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B04.png" alt></p>
<p>以上就是卷积层的forward处理的实现。</p>
<p>通过使用im2col进行展开，基本上可以像实现全连接层的Affine层一样来实现。</p>
<p>接下来是卷积层的反向传播的实现，因为和Affine层的实现有很多共通的地方，所以就不再介绍了。</p>
<p>但有一点需要注意，在进行卷积层的反向传播时，必须进行im2col的逆处理。这可以使用本书提供的col2im函数<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def col2im(col, input_shape, filter_h, filter_w, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    col :</span><br><span class="line">    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）</span><br><span class="line">    filter_h :</span><br><span class="line">    filter_w</span><br><span class="line">    stride</span><br><span class="line">    pad</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W &#x3D; input_shape</span><br><span class="line">    out_h &#x3D; (H + 2*pad - filter_h)&#x2F;&#x2F;stride + 1</span><br><span class="line">    out_w &#x3D; (W + 2*pad - filter_w)&#x2F;&#x2F;stride + 1</span><br><span class="line">    col &#x3D; col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)</span><br><span class="line"></span><br><span class="line">    img &#x3D; np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))</span><br><span class="line">    for y in range(filter_h):</span><br><span class="line">        y_max &#x3D; y + stride*out_h</span><br><span class="line">        for x in range(filter_w):</span><br><span class="line">            x_max &#x3D; x + stride*out_w</span><br><span class="line">            img[:, :, y:y_max:stride, x:x_max:stride] +&#x3D; col[:, :, y, x, :, :]</span><br><span class="line"></span><br><span class="line">    return img[:, :, pad:H + pad, pad:W + pad]</span><br></pre></td></tr></table></figure><br>来进行。除了使用col2im这一点，卷积层的反向传播和Affine层的实现方式都一样</p>
<h1 id="池化层实现"><a href="#池化层实现" class="headerlink" title="池化层实现"></a>池化层实现</h1><p>池化层的实现和卷积层相同，都使用im2col展开输入数据。</p>
<p>不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同。</p>
<p>具体地讲，如图7-21所示，池化的应用区域按通道单独展开。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B05.png" alt></p>
<p>像这样展开之后，只需对展开的矩阵求各行的最大值，并转换为合适的形状即可（图7-22）。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B06.png" alt><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Pooling:</span><br><span class="line">    def __init__(self, pool_h, pool_w, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">        self.pool_h &#x3D; pool_h</span><br><span class="line">        self.pool_w &#x3D; pool_w</span><br><span class="line">        self.stride &#x3D; stride</span><br><span class="line">        self.pad &#x3D; pad</span><br><span class="line">        </span><br><span class="line">        self.x &#x3D; None</span><br><span class="line">        self.arg_max &#x3D; None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        N, C, H, W &#x3D; x.shape</span><br><span class="line">        out_h &#x3D; int(1 + (H - self.pool_h) &#x2F; self.stride)</span><br><span class="line">        out_w &#x3D; int(1 + (W - self.pool_w) &#x2F; self.stride)</span><br><span class="line"></span><br><span class="line">        col &#x3D; im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)</span><br><span class="line">        col &#x3D; col.reshape(-1, self.pool_h*self.pool_w)</span><br><span class="line"></span><br><span class="line">        arg_max &#x3D; np.argmax(col, axis&#x3D;1)</span><br><span class="line">        out &#x3D; np.max(col, axis&#x3D;1)</span><br><span class="line">        out &#x3D; out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)</span><br><span class="line"></span><br><span class="line">        self.x &#x3D; x</span><br><span class="line">        self.arg_max &#x3D; arg_max</span><br><span class="line"></span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">    def backward(self, dout):</span><br><span class="line">        dout &#x3D; dout.transpose(0, 2, 3, 1)</span><br><span class="line">        </span><br><span class="line">        pool_size &#x3D; self.pool_h * self.pool_w</span><br><span class="line">        dmax &#x3D; np.zeros((dout.size, pool_size))</span><br><span class="line">        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] &#x3D; dout.flatten()</span><br><span class="line">        dmax &#x3D; dmax.reshape(dout.shape + (pool_size,)) </span><br><span class="line">        </span><br><span class="line">        dcol &#x3D; dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)</span><br><span class="line">        dx &#x3D; col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)</span><br><span class="line">        </span><br><span class="line">        return dx</span><br></pre></td></tr></table></figure><br>池化层的实现按下面3个阶段进行。</p>
<ul>
<li>1.展开输入数据。</li>
<li>2.求各行的最大值。</li>
<li>3.转换为合适的输出大小。</li>
</ul>
<p>各阶段的实现都很简单，只有一两行代码</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>deep-learning-from-scratch笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理5</title>
    <url>/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%865/</url>
    <content><![CDATA[<ul>
<li>11.<a href="#header1">傅里叶变换</a></li>
<li>12.<a href="#header2">模板匹配</a></li>
<li>13.<a href="#header3">霍夫线变换</a></li>
<li>14.<a href="#header4">霍夫圈变换</a></li>
<li>15.<a href="#header5">图像分割与Watershed算法</a></li>
<li>16.<a href="#header6">交互式前景提取使用GrabCut算法</a><a id="more"></a>
</li>
</ul>
<h1 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a><span id="header1">傅里叶变换</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>在本节中，我们将学习 </p>
<ul>
<li>使用OpenCV查找图像的傅立叶变换 </li>
<li>利用Numpy中可用的FFT函数 </li>
<li>傅立叶变换的某些应用程序 </li>
<li>我们将看到以下函数：cv.dft()，cv.idft()等</li>
</ul>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>傅立叶变换用于分析各种滤波器的频率特性。</p>
<p>对于图像，使用<strong>2D离散傅里叶变换</strong>(DFT)查找频域。</p>
<p>一种称为<strong>快速傅立叶变换</strong>(FFT)的快速算法用于DFT的计算。</p>
<p>关于这些的详细信息可以在任何图像处理或信号处理教科书中找到。请参阅其他资源部分。</p>
<p>对于正弦信号x(t)=Asin(2πft)，我们可以说f是信号的频率，如果采用其频域，则可以看到f的尖峰。</p>
<p>如果对信号进行采样以形成离散信号，我们将获得相同的频域，但是在[−π，π]或[0,2π]范围内（对于N点DFT为[0，N]）是周期性的。</p>
<p>您可以将图像视为在两个方向上采样的信号。因此，在X和Y方向都进行傅立叶变换，可以得到图像的频率表示。</p>
<p>更直观地说，对于正弦信号，如果幅度在短时间内变化如此之快，则可以说它是高频信号。</p>
<p>如果变化缓慢，则为低频信号。您可以将相同的想法扩展到图像。</p>
<p>图像中的振幅在哪里急剧变化？在边缘点或噪声。因此，可以说边缘和噪声是图像中的高频内容。</p>
<p>如果幅度没有太大变化，则它是低频分量。（一些链接已添加到“其他资源”，其中通过示例直观地说明了频率变换）。</p>
<p>现在，我们将看到如何找到傅立叶变换。</p>
<h2 id="Numpy中的傅里叶变换"><a href="#Numpy中的傅里叶变换" class="headerlink" title="Numpy中的傅里叶变换"></a>Numpy中的傅里叶变换</h2><p>首先，我们将看到如何使用Numpy查找傅立叶变换。</p>
<p>Numpy具有FFT软件包来执行此操作。np.fft.fft2()为我们提供了频率转换，它将是一个复杂的数组。</p>
<ul>
<li>它的第一个参数是输入图像，即灰度图像。</li>
<li>第二个参数是可选的，它决定输出数组的大小。如果它大于输入图像的大小，则在计算FFT之前用零填充输入图像。如果小于输入图像，将裁切输入图像。如果未传递任何参数，则输出数组的大小将与输入的大小相同。</li>
</ul>
<p>现在，一旦获得结果，零频率分量（DC分量）将位于左上角。</p>
<p>如果要使其居中，则需要在两个方向上将结果都移动N2。</p>
<p>只需通过函数<strong>np.fft.fftshift</strong>()即可完成。（它更容易分析）。找到频率变换后，就可以找到幅度谱。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">f &#x3D; np.fft.fft2(img)</span><br><span class="line">fshift &#x3D; np.fft.fftshift(f)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(np.abs(fshift))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>Result look like below:<br>结果看起来像下面这样:<br><img src="http://qiniu.aihubs.net/fft1.jpg" alt></p>
<p>看，您可以在中心看到更多白色区域，这表明低频内容更多。</p>
<p>因此，您发现了频率变换现在，您可以在频域中进行一些操作，例如高通滤波和重建图像，即找到逆DFT。</p>
<p>为此，您只需用尺寸为60x60的矩形窗口遮罩即可消除低频。</p>
<p>然后，使用<strong>np.fft.ifftshift</strong>()应用反向移位，以使DC分量再次出现在左上角。</p>
<p>然后使用<strong>np.ifft2</strong>()函数找到逆FFT。同样，结果将是一个复数。您可以采用其绝对值。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;&#x2F;2 , cols&#x2F;&#x2F;2</span><br><span class="line">fshift[crow-30:crow+31, ccol-30:ccol+31] &#x3D; 0</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; np.fft.ifft2(f_ishift)</span><br><span class="line">img_back &#x3D; np.real(img_back)</span><br><span class="line">plt.subplot(131),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(132),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Image after HPF&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(133),plt.imshow(img_back)</span><br><span class="line">plt.title(&#39;Result in JET&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>结果看起来像下面这样：<br><img src="http://qiniu.aihubs.net/fft2.jpg" alt></p>
<p>结果表明高通滤波是边缘检测操作。</p>
<p>这就是我们在“图像渐变”一章中看到的。</p>
<p>这也表明大多数图像数据都存在于频谱的低频区域。</p>
<p>无论如何，我们已经看到了如何在Numpy中找到DFT，IDFT等。</p>
<p>现在，让我们看看如何在OpenCV中进行操作。 </p>
<p>如果您仔细观察结果，尤其是最后一张JET颜色的图像，您会看到一些伪像（我用红色箭头标记的一个实例）。</p>
<p>它在那里显示出一些波纹状结构，称为<strong>振铃效应</strong>。</p>
<p>这是由我们用于遮罩的矩形窗口引起的。此掩码转换为正弦形状，从而导致此问题。</p>
<p>因此，矩形窗口不用于过滤。更好的选择是高斯窗口。</p>
<h2 id="OpenCV中的傅里叶变换"><a href="#OpenCV中的傅里叶变换" class="headerlink" title="OpenCV中的傅里叶变换"></a>OpenCV中的傅里叶变换</h2><p>OpenCV为此提供了<strong>cv.dft</strong>()和<strong>cv.idft</strong>()函数。它返回与前一个相同的结果，但是有两个通道。</p>
<p>第一个通道是结果的实部，第二个通道是结果的虚部。输入图像首先应转换为np.float32。我们来看看怎么做。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">dft &#x3D; cv.dft(np.float32(img),flags &#x3D; cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">dft_shift &#x3D; np.fft.fftshift(dft)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(cv.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>注意 您还可以使用<strong>cv.cartToPolar</strong>()，它在单个镜头中同时返回幅值和相位</p>
<p>现在我们要做DFT的逆变换。在上一节中，我们创建了一个HPF，这次我们将看到如何删除图像中的高频内容，即我们将LPF应用到图像中。</p>
<p>它实际上模糊了图像。为此，我们首先创建一个高值(1)在低频部分，即我们过滤低频内容，0在高频区。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;2 , cols&#x2F;2</span><br><span class="line"># 首先创建一个掩码，中心正方形为1，其余全为零</span><br><span class="line">mask &#x3D; np.zeros((rows,cols,2),np.uint8)</span><br><span class="line">mask[crow-30:crow+30, ccol-30:ccol+30] &#x3D; 1</span><br><span class="line"># 应用掩码和逆DFT</span><br><span class="line">fshift &#x3D; dft_shift*mask</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; cv.idft(f_ishift)</span><br><span class="line">img_back &#x3D; cv.magnitude(img_back[:,:,0],img_back[:,:,1])</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/fft4.jpg" alt></p>
<p>注意 通常，OpenCV函数<strong>cv.dft</strong>()和<strong>cv.idft</strong>()比Numpy函数更快。但是Numpy函数更容易使用。有关性能问题的更多细节，请参见下面的部分。</p>
<h2 id="DFT的性能优化"><a href="#DFT的性能优化" class="headerlink" title="DFT的性能优化"></a>DFT的性能优化</h2><p>对于某些数组尺寸，DFT的计算性能较好。当数组大小为2的幂时，速度最快。</p>
<p>对于大小为2、3和5的乘积的数组，也可以非常有效地进行处理。</p>
<p>因此，如果您担心代码的性能，可以在找到DFT之前将数组的大小修改为任何最佳大小(通过填充零)。对</p>
<p>于OpenCV，您必须手动填充零。但是对于Numpy，您指定FFT计算的新大小，它将自动为您填充零。</p>
<p>那么如何找到最优的大小呢?OpenCV为此提供了一个函数，cv.getOptimalDFTSize()。它同时适用于<strong>cv.dft</strong>()和<strong>np.fft.fft2</strong>()。让我们使用IPython魔术命令timeit来检查它们的性能。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [16]: img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">In [17]: rows,cols &#x3D; img.shape</span><br><span class="line">In [18]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(rows,cols))</span><br><span class="line">342 548</span><br><span class="line">In [19]: nrows &#x3D; cv.getOptimalDFTSize(rows)</span><br><span class="line">In [20]: ncols &#x3D; cv.getOptimalDFTSize(cols)</span><br><span class="line">In [21]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(nrows,ncols))</span><br><span class="line">360 576</span><br></pre></td></tr></table></figure><br>参见，将大小(342,548)修改为(360，576)。现在让我们用零填充（对于OpenCV），并找到其DFT计算性能。您可以通过创建一个新的零数组并将数据复制到其中来完成此操作，或者使用<strong>cv.copyMakeBorder</strong>()。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nimg &#x3D; np.zeros((nrows,ncols))</span><br><span class="line">nimg[:rows,:cols] &#x3D; img</span><br></pre></td></tr></table></figure><br>或者:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">right &#x3D; ncols - cols</span><br><span class="line">bottom &#x3D; nrows - rows</span><br><span class="line">bordertype &#x3D; cv.BORDER_CONSTANT ＃只是为了避免PDF文件中的行中断</span><br><span class="line">nimg &#x3D; cv.copyMakeBorder(img,0,bottom,0,right,bordertype, value &#x3D; 0)</span><br></pre></td></tr></table></figure><br>现在，我们计算Numpy函数的DFT性能比较：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [22]: %timeit fft1 &#x3D; np.fft.fft2(img)</span><br><span class="line">10 loops, best of 3: 40.9 ms per loop</span><br><span class="line">In [23]: %timeit fft2 &#x3D; np.fft.fft2(img,[nrows,ncols])</span><br><span class="line">100 loops, best of 3: 10.4 ms per loop</span><br></pre></td></tr></table></figure><br>它显示了4倍的加速。现在，我们将尝试使用OpenCV函数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [24]: %timeit dft1&#x3D; cv.dft(np.float32(img),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 13.5 ms per loop</span><br><span class="line">In [27]: %timeit dft2&#x3D; cv.dft(np.float32(nimg),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 3.11 ms per loop</span><br></pre></td></tr></table></figure><br>它还显示了4倍的加速。您还可以看到OpenCV函数比Numpy函数快3倍左右。也可以对逆FFT进行测试，这留给您练习。</p>
<h2 id="为什么拉普拉斯算子是高通滤波器？"><a href="#为什么拉普拉斯算子是高通滤波器？" class="headerlink" title="为什么拉普拉斯算子是高通滤波器？"></a>为什么拉普拉斯算子是高通滤波器？</h2><p>在一个论坛上也有人提出了类似的问题。问题是，为什么拉普拉斯变换是高通滤波器?</p>
<p>为什么Sobel是HPF?等。第一个答案是关于傅里叶变换的。对于更大的FFT只需要拉普拉斯变换。分析下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"># 没有缩放参数的简单均值滤波器</span><br><span class="line">mean_filter &#x3D; np.ones((3,3))</span><br><span class="line"># 创建高斯滤波器</span><br><span class="line">x &#x3D; cv.getGaussianKernel(5,10)</span><br><span class="line">gaussian &#x3D; x*x.T</span><br><span class="line"># 不同的边缘检测滤波器</span><br><span class="line"># x方向上的scharr</span><br><span class="line">scharr &#x3D; np.array([[-3, 0, 3],</span><br><span class="line">                   [-10,0,10],</span><br><span class="line">                   [-3, 0, 3]])</span><br><span class="line"># x方向上的sobel</span><br><span class="line">sobel_x&#x3D; np.array([[-1, 0, 1],</span><br><span class="line">                   [-2, 0, 2],</span><br><span class="line">                   [-1, 0, 1]])</span><br><span class="line"># y方向上的sobel</span><br><span class="line">sobel_y&#x3D; np.array([[-1,-2,-1],</span><br><span class="line">                   [0, 0, 0],</span><br><span class="line">                   [1, 2, 1]])</span><br><span class="line"># 拉普拉斯变换</span><br><span class="line">laplacian&#x3D;np.array([[0, 1, 0],</span><br><span class="line">                    [1,-4, 1],</span><br><span class="line">                    [0, 1, 0]])</span><br><span class="line">filters &#x3D; [mean_filter, gaussian, laplacian, sobel_x, sobel_y, scharr]</span><br><span class="line">filter_name &#x3D; [&#39;mean_filter&#39;, &#39;gaussian&#39;,&#39;laplacian&#39;, &#39;sobel_x&#39;, \</span><br><span class="line">                &#39;sobel_y&#39;, &#39;scharr_x&#39;]</span><br><span class="line">fft_filters &#x3D; [np.fft.fft2(x) for x in filters]</span><br><span class="line">fft_shift &#x3D; [np.fft.fftshift(y) for y in fft_filters]</span><br><span class="line">mag_spectrum &#x3D; [np.log(np.abs(z)+1) for z in fft_shift]</span><br><span class="line">for i in range(6):</span><br><span class="line">    plt.subplot(2,3,i+1),plt.imshow(mag_spectrum[i],cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(filter_name[i]), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>看看结果：<br><img src="http://qiniu.aihubs.net/fft5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="模板匹配"><a href="#模板匹配" class="headerlink" title="模板匹配"></a><span id="header2">模板匹配</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本章中，您将学习 - 使用模板匹配在图像中查找对象 - 你将看到以下功能：cv.matchTemplate()，cv.minMaxLoc()</p>
<h2 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h2><p>模板匹配是一种用于在较大图像中搜索和查找模板图像位置的方法。</p>
<p>为此，OpenCV带有一个函数<strong>cv.matchTemplate</strong>()。 </p>
<p>它只是将模板图​​像滑动到输入图像上（就像在2D卷积中一样），然后在模板图像下比较模板和输入图像的拼图。 </p>
<p>OpenCV中实现了几种比较方法。（您可以检查文档以了解更多详细信息）。它返回一个灰度图像，其中每个像素表示该像素的邻域与模板匹配的程度。</p>
<p>如果输入图像的大小为(WxH)，而模板图像的大小为(wxh)，则输出图像的大小将为(W-w + 1，H-h + 1)。得到结果后，可以使用<strong>cv.minMaxLoc</strong>()函数查找最大/最小值在哪。将其作为矩形的左上角，并以(w，h)作为矩形的宽度和高度。该矩形是您模板的区域。</p>
<p>注意 如果使用<strong>cv.TM_SQDIFF</strong>作为比较方法，则最小值提供最佳匹配。</p>
<h2 id="OpenCV中的模板匹配"><a href="#OpenCV中的模板匹配" class="headerlink" title="OpenCV中的模板匹配"></a>OpenCV中的模板匹配</h2><p>作为示例，我们将在梅西的照片中搜索他的脸。所以我创建了一个模板，如下所示： <img src="http://qiniu.aihubs.net/messi_face.jpg" alt> 我们将尝试所有比较方法，以便我们可以看到它们的结果如何：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">img2 &#x3D; img.copy()</span><br><span class="line">template &#x3D; cv.imread(&#39;template.jpg&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line"># 列表中所有的6种比较方法</span><br><span class="line">methods &#x3D; [&#39;cv.TM_CCOEFF&#39;, &#39;cv.TM_CCOEFF_NORMED&#39;, &#39;cv.TM_CCORR&#39;,</span><br><span class="line">            &#39;cv.TM_CCORR_NORMED&#39;, &#39;cv.TM_SQDIFF&#39;, &#39;cv.TM_SQDIFF_NORMED&#39;]</span><br><span class="line">for meth in methods:</span><br><span class="line">    img &#x3D; img2.copy()</span><br><span class="line">    method &#x3D; eval(meth)</span><br><span class="line">    # 应用模板匹配</span><br><span class="line">    res &#x3D; cv.matchTemplate(img,template,method)</span><br><span class="line">    min_val, max_val, min_loc, max_loc &#x3D; cv.minMaxLoc(res)</span><br><span class="line">    # 如果方法是TM_SQDIFF或TM_SQDIFF_NORMED，则取最小值</span><br><span class="line">    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:</span><br><span class="line">        top_left &#x3D; min_loc</span><br><span class="line">    else:</span><br><span class="line">        top_left &#x3D; max_loc</span><br><span class="line">    bottom_right &#x3D; (top_left[0] + w, top_left[1] + h)</span><br><span class="line">    cv.rectangle(img,top_left, bottom_right, 255, 2)</span><br><span class="line">    plt.subplot(121),plt.imshow(res,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Matching Result&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(122),plt.imshow(img,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Detected Point&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.suptitle(meth)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><br>查看以下结果：</p>
<ul>
<li>cv.TM_CCOEFF<img src="http://qiniu.aihubs.net/template_ccoeff_1.jpg" alt></li>
<li>cv.TM_CCOEFF_NORMED<img src="http://qiniu.aihubs.net/template_ccoeffn_2.jpg" alt></li>
<li>cv.TM_CCORR<img src="http://qiniu.aihubs.net/template_ccorr_3.jpg" alt></li>
<li>cv.TM_CCORR_NORMED<img src="http://qiniu.aihubs.net/template_ccorrn_4.jpg" alt></li>
<li>cv.TM_SQDIFF<img src="http://qiniu.aihubs.net/template_sqdiff_5.jpg" alt></li>
<li>cv.TM_SQDIFF_NORMED<img src="http://qiniu.aihubs.net/template_sqdiffn_6.jpg" alt><br>使用<strong>cv.TM_CCORR</strong>的结果并不理想。</li>
</ul>
<h2 id="多对象的模板匹配"><a href="#多对象的模板匹配" class="headerlink" title="多对象的模板匹配"></a>多对象的模板匹配</h2><p>在上一节中，我们在图像中搜索了梅西的脸，该脸在图像中仅出现一次。</p>
<p>假设您正在搜索具有多次出现的对象，则<strong>cv.minMaxLoc</strong>()不会为您提供所有位置。</p>
<p>在这种情况下，我们将使用阈值化。因此，在此示例中，我们将使用著名游戏<strong>Mario</strong>的屏幕截图，并在其中找到硬币。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img_rgb &#x3D; cv.imread(&#39;mario.png&#39;)</span><br><span class="line">img_gray &#x3D; cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)</span><br><span class="line">template &#x3D; cv.imread(&#39;mario_coin.png&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line">res &#x3D; cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)</span><br><span class="line">threshold &#x3D; 0.8</span><br><span class="line">loc &#x3D; np.where( res &gt;&#x3D; threshold)</span><br><span class="line">for pt in zip(*loc[::-1]):</span><br><span class="line">    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)</span><br><span class="line">cv.imwrite(&#39;res.png&#39;,img_rgb)</span><br></pre></td></tr></table></figure><br>结果:<br><img src="http://qiniu.aihubs.net/res_mario.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="霍夫线变换"><a href="#霍夫线变换" class="headerlink" title="霍夫线变换"></a><span id="header3">霍夫线变换</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在这一章当中， </p>
<ul>
<li>我们将了解霍夫变换的概念。 </li>
<li>我们将看到如何使用它来检测图像中的线条。 </li>
<li>我们将看到以下函数：cv.HoughLines()，cv.HoughLinesP()</li>
</ul>
<h2 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h2><p>如果可以用数学形式表示形状，则霍夫变换是一种检测任何形状的流行技术。</p>
<p>即使形状有些破损或变形，也可以检测出形状。我们将看到它如何作用于一条线。</p>
<p>一条线可以表示为y=mx+c或以参数形式表示为ρ=xcosθ+ysinθ，</p>
<p>其中ρ是从原点到该线的垂直距离，</p>
<p>而θ是由该垂直线和水平轴形成的角度以逆时针方向测量（该方向随您如何表示坐标系而变化。此表示形式在OpenCV中使用）。</p>
<p>查看下面的图片：<br><img src="http://qiniu.aihubs.net/1.png" alt></p>
<p>因此，如果线在原点下方通过，则它将具有正的ρ且角度小于180。如果线在原点上方，则将角度取为小于180，而不是大于180的角度。</p>
<p>ρ取负值。任何垂直线将具有0度，水平线将具有90度。</p>
<p>现在，让我们看一下霍夫变换如何处理线条。任何一条线都可以用(ρ，θ)这两个术语表示。</p>
<p>因此，首先创建2D数组或累加器（以保存两个参数的值），并将其初始设置为0。</p>
<p>让行表示ρ，列表示θ。阵列的大小取决于所需的精度。假设您希望角度的精度为1度，则需要180列。</p>
<p>对于ρ，最大距离可能是图像的对角线长度。因此，以一个像素精度为准，行数可以是图像的对角线长度。</p>
<p>考虑一个100x100的图像，中间有一条水平线。</p>
<p>取直线的第一点。您知道它的(x，y)值。</p>
<p>现在在线性方程式中，将值θ= 0,1,2，….. 180放进去，然后检查得到ρ。</p>
<p>对于每对(ρ，θ)，在累加器中对应的(ρ，θ)单元格将值增加1。所以现在在累加器中，单元格(50,90)= 1以及其他一些单元格。</p>
<p>现在，对行的第二个点。执行与上述相同的操作。递增(ρ，θ)对应的单元格中的值。</p>
<p>这次，单元格(50,90)=2。实际上，您正在对(ρ，θ)值进行投票。</p>
<p>您对线路上的每个点都继续执行此过程。</p>
<p>在每个点上，单元格(50,90)都会增加或投票，而其他单元格可能会或可能不会投票。</p>
<p>这样一来，最后，单元格(50,90)的投票数将最高。</p>
<p>因此，如果您在累加器中搜索最大票数，则将获得(50,90)值，该值表示该图像中的一条线与原点的距离为50，角度为90度。</p>
<p>在下面的动画中很好地显示了该图片(图片提供：Amos Storkey)<br><img src="http://qiniu.aihubs.net/houghlinesdemo.gif" alt></p>
<p>这就是霍夫变换对线条的工作方式。它很简单，也许您可​​以自己使用Numpy来实现它。</p>
<p>下图显示了累加器。某些位置的亮点表示它们是图像中可能的线条的参数。</p>
<p><img src="http://qiniu.aihubs.net/houghlines2.jpg" alt></p>
<h2 id="OpenCV中的霍夫曼变换"><a href="#OpenCV中的霍夫曼变换" class="headerlink" title="OpenCV中的霍夫曼变换"></a>OpenCV中的霍夫曼变换</h2><p>上面说明的所有内容都封装在OpenCV函数<strong>cv.HoughLines</strong>()中。</p>
<p>它只是返回一个：math:(rho，theta)值的数组。ρ以像素为单位，θ以弧度为单位。</p>
<p>第一个参数，输入图像应该是二进制图像，因此在应用霍夫变换之前，请应用阈值或使用Canny边缘检测。</p>
<p>第二和第三参数分别是ρ和θ精度。</p>
<p>第四个参数是阈值，这意味着应该将其视为行的最低投票。</p>
<p>请记住，票数取决于线上的点数。因此，它表示应检测到的最小线长。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLines(edges,1,np.pi&#x2F;180,200)</span><br><span class="line">for line in lines:</span><br><span class="line">    rho,theta &#x3D; line[0]</span><br><span class="line">    a &#x3D; np.cos(theta)</span><br><span class="line">    b &#x3D; np.sin(theta)</span><br><span class="line">    x0 &#x3D; a*rho</span><br><span class="line">    y0 &#x3D; b*rho</span><br><span class="line">    x1 &#x3D; int(x0 + 1000*(-b))</span><br><span class="line">    y1 &#x3D; int(y0 + 1000*(a))</span><br><span class="line">    x2 &#x3D; int(x0 - 1000*(-b))</span><br><span class="line">    y2 &#x3D; int(y0 - 1000*(a))</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,0,255),2)</span><br><span class="line">cv.imwrite(&#39;houghlines3.jpg&#39;,img)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://qiniu.aihubs.net/houghlines3.jpg" alt></p>
<h2 id="概率霍夫变换"><a href="#概率霍夫变换" class="headerlink" title="概率霍夫变换"></a>概率霍夫变换</h2><p>在霍夫变换中，您可以看到，即使对于带有两个参数的行，也需要大量计算。</p>
<p>概率霍夫变换是我们看到的霍夫变换的优化。它没有考虑所有要点。</p>
<p>取而代之的是，它仅采用随机的点子集，足以进行线检测。</p>
<p>只是我们必须降低阈值。参见下图，比较了霍夫空间中的霍夫变换和概率霍夫变换。<br><img src="http://qiniu.aihubs.net/houghlines4.png" alt></p>
<p>OpenCV的实现基于Matas,J.和Galambos,C.和Kittler, J.V.使用渐进概率霍夫变换对行进行的稳健检测[145]。</p>
<p>使用的函数是<strong>cv.HoughLinesP</strong>()。它有两个新的论点。 </p>
<ul>
<li>minLineLength - 最小行长。小于此长度的线段将被拒绝。 </li>
<li>maxLineGap - 线段之间允许将它们视为一条线的最大间隙。</li>
</ul>
<p>最好的是，它直接返回行的两个端点。在以前的情况下，您仅获得线的参数，并且必须找到所有点。在这里，一切都是直接而简单的。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLinesP(edges,1,np.pi&#x2F;180,100,minLineLength&#x3D;100,maxLineGap&#x3D;10)</span><br><span class="line">for line in lines:</span><br><span class="line">    x1,y1,x2,y2 &#x3D; line[0]</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)</span><br><span class="line">cv.imwrite(&#39;houghlines5.jpg&#39;,img)</span><br></pre></td></tr></table></figure><br>看到如下结果：<br><img src="http://qiniu.aihubs.net/houghlines5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="霍夫圈变换"><a href="#霍夫圈变换" class="headerlink" title="霍夫圈变换"></a><span id="header4">霍夫圈变换</span></h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><p>在本章中， </p>
<ul>
<li>我们将学习使用霍夫变换来查找图像中的圆。 </li>
<li>我们将看到以下函数：cv.HoughCircles()</li>
</ul>
<p>理论<br>圆在数学上表示为$(x−x_{center})^2+(y−y_{center})^2=r^2$，其中$(x_{center},y_{center})$是圆的中心，r是圆的半径。从等式中，我们可以看到我们有3个参数，因此我们需要3D累加器进行霍夫变换，这将非常低效。因此，OpenCV使用更加技巧性的方法，即使用边缘的梯度信息的<strong>Hough梯度方法</strong>。</p>
<p>我们在这里使用的函数是<strong>cv.HoughCircles</strong>()。它有很多参数，这些参数在文档中有很好的解释。因此，我们直接转到代码。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;opencv-logo-white.png&#39;,0)</span><br><span class="line">img &#x3D; cv.medianBlur(img,5)</span><br><span class="line">cimg &#x3D; cv.cvtColor(img,cv.COLOR_GRAY2BGR)</span><br><span class="line">circles &#x3D; cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20,</span><br><span class="line">                            param1&#x3D;50,param2&#x3D;30,minRadius&#x3D;0,maxRadius&#x3D;0)</span><br><span class="line">circles &#x3D; np.uint16(np.around(circles))</span><br><span class="line">for i in circles[0,:]:</span><br><span class="line">    # 绘制外圆</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2)</span><br><span class="line">    # 绘制圆心</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),2,(0,0,255),3)</span><br><span class="line">cv.imshow(&#39;detected circles&#39;,cimg)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/houghcircles2.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="图像分割与Watershed算法"><a href="#图像分割与Watershed算法" class="headerlink" title="图像分割与Watershed算法"></a><span id="header5">图像分割与Watershed算法</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>在本章中， - 我们将学习使用分水岭算法实现基于标记的图像分割 - 我们将看到：cv.watershed()</p>
<h2 id="理论-3"><a href="#理论-3" class="headerlink" title="理论"></a>理论</h2><p>任何灰度图像都可以看作是一个地形表面，其中高强度表示山峰，低强度表示山谷。</p>
<p>你开始用不同颜色的水(标签)填充每个孤立的山谷(局部最小值)。</p>
<p>随着水位的上升，根据附近的山峰(坡度)，来自不同山谷的水明显会开始合并，颜色也不同。</p>
<p>为了避免这种情况，你要在水融合的地方建造屏障。你继续填满水，建造障碍，直到所有的山峰都在水下。</p>
<p>然后你创建的屏障将返回你的分割结果。这就是Watershed背后的“思想”。</p>
<p>你可以访问Watershed的CMM网页，了解它与一些动画的帮助。</p>
<p>但是这种方法会由于图像中的噪声或其他不规则性而产生过度分割的结果。</p>
<p>因此OpenCV实现了一个基于标记的分水岭算法，你可以指定哪些是要合并的山谷点，哪些不是。</p>
<p>这是一个交互式的图像分割。我们所做的是给我们知道的对象赋予不同的标签。</p>
<p>用一种颜色(或强度)标记我们确定为前景或对象的区域，用另一种颜色标记我们确定为背景或非对象的区域，最后用0标记我们不确定的区域。</p>
<p>这是我们的标记。然后应用分水岭算法。然后我们的标记将使用我们给出的标签进行更新，对象的边界值将为-1。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>下面我们将看到一个有关如何使用距离变换和分水岭来分割相互接触的对象的示例。</p>
<p>考虑下面的硬币图像，硬币彼此接触。即使你设置阈值，它也会彼此接触。<br><img src="http://qiniu.aihubs.net/water_coins.jpg" alt></p>
<p>我们先从寻找硬币的近似估计开始。因此，我们可以使用Otsu的二值化。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;coins.png&#39;)</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh &#x3D; cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/water_thresh.jpg" alt></p>
<p>现在我们需要去除图像中的任何白点噪声。为此，我们可以使用形态学扩张。</p>
<p>要去除对象中的任何小孔，我们可以使用形态学侵蚀。因此，现在我们可以确定，靠近对象中心的区域是前景，而离对象中心很远的区域是背景。</p>
<p>我们不确定的唯一区域是硬币的边界区域。</p>
<p>因此，我们需要提取我们可确定为硬币的区域。侵蚀会去除边界像素。</p>
<p>因此，无论剩余多少，我们都可以肯定它是硬币。如果物体彼此不接触，那将起作用。</p>
<p>但是，由于它们彼此接触，因此另一个好选择是找到距离变换并应用适当的阈值。</p>
<p>接下来，我们需要找到我们确定它们不是硬币的区域。</p>
<p>为此，我们扩张了结果。膨胀将对象边界增加到背景。</p>
<p>这样，由于边界区域已删除，因此我们可以确保结果中背景中的任何区域实际上都是背景。参见下图。<br><img src="http://qiniu.aihubs.net/water_fgbg.jpg" alt></p>
<p>剩下的区域是我们不知道的区域，无论是硬币还是背景。分水岭算法应该找到它。</p>
<p>这些区域通常位于前景和背景相遇（甚至两个不同的硬币相遇）的硬币边界附近。我们称之为边界。可以通过从sure_bg区域中减去sure_fg区域来获得。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 噪声去除</span><br><span class="line">kernel &#x3D; np.ones((3,3),np.uint8)</span><br><span class="line">opening &#x3D; cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations &#x3D; 2)</span><br><span class="line"># 确定背景区域</span><br><span class="line">sure_bg &#x3D; cv.dilate(opening,kernel,iterations&#x3D;3)</span><br><span class="line"># 寻找前景区域</span><br><span class="line">dist_transform &#x3D; cv.distanceTransform(opening,cv.DIST_L2,5)</span><br><span class="line">ret, sure_fg &#x3D; cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)</span><br><span class="line"># 找到未知区域</span><br><span class="line">sure_fg &#x3D; np.uint8(sure_fg)</span><br><span class="line">unknown &#x3D; cv.subtract(sure_bg,sure_fg)</span><br></pre></td></tr></table></figure><br>查看结果。在阈值图像中，我们得到了一些硬币区域，我们确定它们是硬币，并且现在已分离它们。（在某些情况下，你可能只对前景分割感兴趣，而不对分离相互接触的对象感兴趣。在那种情况下，你无需使用距离变换，只需侵蚀就足够了。侵蚀只是提取确定前景区域的另一种方法。）</p>
<p><img src="http://qiniu.aihubs.net/water_dt.jpg" alt></p>
<p>现在我们可以确定哪些是硬币的区域，哪些是背景。</p>
<p>因此，我们创建了标记（它的大小与原始图像的大小相同，但具有int32数据类型），并标记其中的区域。</p>
<p>我们肯定知道的区域（无论是前景还是背景）都标有任何正整数，但是带有不同的整数，而我们不确定的区域则保留为零。</p>
<p>为此，我们使用<strong>cv.connectedComponents</strong>()。它用0标记图像的背景，然后其他对象用从1开始的整数标记。</p>
<p>但是我们知道，如果背景标记为0，则分水岭会将其视为未知区域。所以我们想用不同的整数来标记它。相反，我们将未知定义的未知区域标记为0。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 类别标记</span><br><span class="line">ret, markers &#x3D; cv.connectedComponents(sure_fg)</span><br><span class="line"># 为所有的标记加1，保证背景是0而不是1</span><br><span class="line">markers &#x3D; markers+1</span><br><span class="line"># 现在让所有的未知区域为0</span><br><span class="line">markers[unknown&#x3D;&#x3D;255] &#x3D; 0</span><br></pre></td></tr></table></figure><br>参见JET colormap中显示的结果。深蓝色区域显示未知区域。当然,硬币的颜色不同。剩下,肯定为背景的区域显示在较浅的蓝色，跟未知区域相比。<br><img src="http://qiniu.aihubs.net/water_marker.jpg" alt></p>
<p>现在我们的标记已准备就绪。现在是最后一步的时候了，使用分水岭算法。然后标记图像将被修改。边界区域将标记为-1。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">markers &#x3D; cv.watershed(img,markers) </span><br><span class="line">img[markers &#x3D;&#x3D; -1] &#x3D; [255,0,0]</span><br></pre></td></tr></table></figure><br>请参阅下面的结果。对某些硬币，它们接触的区域被正确地分割，而对于某些硬币，却不是。</p>
<p><img src="http://qiniu.aihubs.net/water_result.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="交互式前景提取使用GrabCut算法"><a href="#交互式前景提取使用GrabCut算法" class="headerlink" title="交互式前景提取使用GrabCut算法"></a><span id="header6">交互式前景提取使用GrabCut算法</span></h1><h2 id="目标-4"><a href="#目标-4" class="headerlink" title="目标"></a>目标</h2><p>在本章中， - 我们将看到GrabCut算法来提取图像中的前景 - 我们将为此创建一个交互式应用程序。</p>
<h2 id="理论-4"><a href="#理论-4" class="headerlink" title="理论"></a>理论</h2><p>GrabCut算法由英国微软研究院的Carsten Rother，Vladimir Kolmogorov和Andrew Blake设计。</p>
<p>在他们的论文“GrabCut”中：使用迭代图割的交互式前景提取。需要用最少的用户交互进行前景提取的算法，结果是GrabCut。</p>
<p>从用户角度来看，它是如何工作的？</p>
<p>最初，用户在前景区域周围绘制一个矩形（前景区域应完全位于矩形内部）。</p>
<p>然后，算法会对其进行迭代分割，以获得最佳结果。</p>
<p>做完了但在某些情况下，分割可能不会很好，例如，可能已将某些前景区域标记为背景，反之亦然。在这种情况下，需要用户进行精修。</p>
<p>只需在图像错误分割区域上画些笔画。笔画基本上说 “嘿，该区域应该是前景，你将其标记为背景，在下一次迭代中对其进行校正”或与背景相反。</p>
<p>然后在下一次迭代中，你将获得更好的结果。</p>
<p>参见下图。</p>
<p>第一名球员和橄榄球被封闭在一个蓝色矩形中。然后用白色笔划（表示前景）和黑色笔划（表示背景）进行最后的修饰。而且我们得到了不错的结果。</p>
<p><img src="http://qiniu.aihubs.net/grabcut_output1.jpg" alt></p>
<p>那么背景发生了什么呢？ </p>
<ul>
<li>用户输入矩形。此矩形外部的所有内容都将作为背景（这是在矩形应包含所有对象之前提到的原因）。矩形内的所有内容都是未知的。同样，任何指定前景和背景的用户输入都被视为硬标签，这意味着它们在此过程中不会更改。 </li>
<li>计算机根据我们提供的数据进行初始标记。它标记前景和背景像素（或对其进行硬标记），现在使用高斯混合模型(GMM)对前景和背景进行建模。 </li>
<li>根据我们提供的数据，GMM可以学习并创建新的像素分布。也就是说，未知像素根据颜色统计上与其他硬标记像素的关系而被标记为可能的前景或可能的背景（就像聚类一样）。 </li>
<li>根据此像素分布构建图形。图中的节点为像素。添加了另外两个节点，即“源”节点和“接收器”节点。每个前景像素都连接到源节点，每个背景像素都连接到接收器节点。 </li>
<li>通过像素是前景/背景的概率来定义将像素连接到源节点/末端节点的边缘的权重。像素之间的权重由边缘信息或像素相似度定义。如果像素颜色差异很大，则它们之间的边缘将变低。 </li>
<li>然后使用mincut算法对图进行分割。它将图切成具有最小成本函数的两个分离的源节点和宿节点。成本函数是被切割边缘的所有权重的总和。剪切后，连接到“源”节点的所有像素都变为前景，而连接到“接收器”节点的像素都变为背景。 </li>
<li>继续该过程，直到分类收敛为止。</li>
</ul>
<p>如下图所示（图片提供：<a href="http://www.cs.ru.ac.za/research/g02m1682/）" target="_blank" rel="noopener">http://www.cs.ru.ac.za/research/g02m1682/）</a><br><img src="http://qiniu.aihubs.net/grabcut_scheme.jpg" alt></p>
<p>示例<br>现在我们使用OpenCV进行抓取算法。OpenCV为此具有功能<strong>cv.grabCut</strong>()，我们将首先看到其参数： </p>
<ul>
<li>img - 输入图像 </li>
<li>mask - 这是一个掩码图像，在其中我们指定哪些区域是背景，前景或可能的背景/前景等。这是通过以下标志完成的：cv.GC_BGD,cv.GC_FGD, cv.GC_PR_BGD,cv.GC_PR_FGD，或直接将0,1,2,3传递给图像。 </li>
<li>rect - 它是矩形的坐标，其中包括前景对象，格式为(x,y,w,h) - bdgModel, fgdModel - 这些是算法内部使用的数组。你只需创建两个大小为(1,65)的np.float64类型零数组。 </li>
<li>iterCount - 算法应运行的迭代次数。 </li>
<li>model - 应该是<strong>cv.GC_INIT_WITH_RECT</strong>或<strong>cv.GC_INIT_WITH_MASK</strong>或两者结合，决定我们要绘制矩形还是最终的修饰笔触。</li>
</ul>
<p>首先让我们看看矩形模式。我们加载图像，创建类似的mask图像。 </p>
<p>我们创建<em>fgdModel</em>和<em>bgdModel</em>。我们给出矩形参数。一切都是直截了当的。</p>
<p>让算法运行5次迭代。模式应为<strong>cv.GC_INIT_WITH_RECT</strong>, 因为我们使用的是矩形。 </p>
<p>然后运行grabcut。修改mask图像。在新的mask图像中，像素将被标记有四个标记，分别表示上面指定的背景/前景。</p>
<p>因此，我们修改mask，使所有0像素和2像素都置为0（即背景），而所有1像素和3像素均置为1（即前景像素）。</p>
<p>现在，我们的最终mask已经准备就绪。只需将其与输入图像相乘即可得到分割的图像。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;)</span><br><span class="line">mask &#x3D; np.zeros(img.shape[:2],np.uint8)</span><br><span class="line">bgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">fgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">rect &#x3D; (50,50,450,290)</span><br><span class="line">cv.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv.GC_INIT_WITH_RECT)</span><br><span class="line">mask2 &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask2[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/grabcut_rect.jpg" alt></p>
<p>糟糕，梅西的头发不见了。谁会喜欢没有头发的梅西？我们需要把它找回来。</p>
<p>因此，我们将使用1像素（确保前景）进行精细修饰。</p>
<p>同时，一些不需要的地面也出现在图片里。我们需要删除它们。</p>
<p>在那里，我们给出了一些0像素的修饰（确保背景）。</p>
<p>因此，如现在所说，我们在以前的情况下修改生成的mask。</p>
<p>我实际上所做的是，我在paint应用程序中打开了输入图像，并在图像中添加了另一层。</p>
<p>使用画笔中的画笔工具，我在新图层上用白色标记了错过的前景（头发，鞋子，球等），而用白色标记了不需要的背景（例如logo，地面等）。</p>
<p>然后用灰色填充剩余的背景。</p>
<p>然后将该mask图像加载到OpenCV中，编辑我们在新添加的mask图像中具有相应值的原始mask图像。</p>
<p>检查以下代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">＃newmask是我手动标记过的mask图像</span><br><span class="line">newmask &#x3D; cv.imread(&#39;newmask.png&#39;,0)</span><br><span class="line"># 标记为白色（确保前景）的地方，更改mask &#x3D; 1</span><br><span class="line"># 标记为黑色（确保背景）的地方，更改mask &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 0] &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 255] &#x3D; 1</span><br><span class="line">mask, bgdModel, fgdModel &#x3D; cv.grabCut(img,mask,None,bgdModel,fgdModel,5,cv.GC_INIT_WITH_MASK)</span><br><span class="line">mask &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure><br>就是这样了。在这里，你无需直接在rect模式下初始化，而可以直接进入mask模式。</p>
<p>只需用2像素或3像素（可能的背景/前景）标记mask图像中的矩形区域。</p>
<p>然后像在第二个示例中一样，将我们的sure_foreground标记为1像素。然后直接在mask模式下应用grabCut功能。<br><img src="http://qiniu.aihubs.net/grabcut_mask.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>opencv中的图像处理4-直方图</title>
    <url>/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%864-%E7%9B%B4%E6%96%B9%E5%9B%BE/</url>
    <content><![CDATA[<ul>
<li>10.1.<a href="#header1">直方图-1：查找、绘制和分析</a></li>
<li>10.2.<a href="#header2">直方图-2：直方图均衡</a></li>
<li>10.3.<a href="#header3">直方图-3：二维直方图</a></li>
<li>10.4.<a href="#header4">直方图4：直方图反投影</a><a id="more"></a>
</li>
</ul>
<h1 id="直方图-1：查找、绘制和分析"><a href="#直方图-1：查找、绘制和分析" class="headerlink" title="直方图-1：查找、绘制和分析"></a><span id="header1">直方图-1：查找、绘制和分析</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>学会 </p>
<ul>
<li>使用OpenCV和Numpy函数查找直方图 </li>
<li>使用OpenCV和Matplotlib函数绘制直方图 </li>
<li>你将看到以下函数：cv.calcHist()，np.histogram()等。</li>
</ul>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>那么直方图是什么？您可以将直方图视为图形或绘图，从而可以总体了解图像的强度分布。</p>
<p>它是在X轴上具有像素值（不总是从0到255的范围），在Y轴上具有图像中相应像素数的图。</p>
<p>这只是理解图像的另一种方式。通过查看图像的直方图，您可以直观地了解该图像的对比度，亮度，强度分布等。</p>
<p>当今几乎所有图像处理工具都提供直方图功能。以下是剑桥彩色网站的图片，我建议您访问该网站以获取更多详细信息。</p>
<p><img src="http://qiniu.aihubs.net/histogram_sample.jpg" alt></p>
<p>您可以看到图像及其直方图。（请记住，此直方图是针对灰度图像而非彩色图像绘制的）。</p>
<p>直方图的左侧区域显示图像中较暗像素的数量，而右侧区域则显示明亮像素的数量。</p>
<p>从直方图中，您可以看到暗区域多于亮区域，而中间调的数量（中间值的像素值，例如127附近）则非常少。</p>
<h2 id="寻找直方图"><a href="#寻找直方图" class="headerlink" title="寻找直方图"></a>寻找直方图</h2><p>现在我们有了一个关于直方图的想法，我们可以研究如何找到它。OpenCV和Numpy都为此内置了功能。</p>
<p>在使用这些功能之前，我们需要了解一些与直方图有关的术语。</p>
<p>BINS：上面的直方图显示每个像素值的像素数，即从0到255。即，您需要256个值来显示上面的直方图。但是考虑一下，如果您不需要分别找到所有像素值的像素数，而是找到像素值间隔中的像素数怎么办？ 例如，您需要找到介于0到15之间的像素数，然后找到16到31之间，…，240到255之间的像素数。只需要16个值即可表示直方图。这就是在OpenCV教程中有关直方图的示例中显示的内容。</p>
<p>因此，您要做的就是将整个直方图分成16个子部分，每个子部分的值就是其中所有像素数的总和。 每个子部分都称为“ BIN”。在第一种情况下，bin的数量为256个（每个像素一个），而在第二种情况下，bin的数量仅为16个。BINS由OpenCV文档中的<strong>histSize</strong>术语表示。</p>
<p>DIMS：这是我们为其收集数据的参数的数量。在这种情况下，我们仅收集关于强度值的一件事的数据。所以这里是1。</p>
<p>RANGE：这是您要测量的强度值的范围。通常，它是[0,256]，即所有强度值。</p>
<h2 id="1-OpenCV中的直方图计算"><a href="#1-OpenCV中的直方图计算" class="headerlink" title="1. OpenCV中的直方图计算"></a>1. OpenCV中的直方图计算</h2><p>因此，现在我们使用<strong>cv.calcHist</strong>()函数查找直方图。让我们熟悉一下该函数及其参数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.calcHist（images，channels，mask，histSize，ranges [，hist [，accumulate]]）</span><br><span class="line">images：它是uint8或float32类型的源图像。它应该放在方括号中，即“ [img]”。</span><br><span class="line">channels：也以方括号给出。它是我们计算直方图的通道的索引。例如，如果输入为灰度图像，则其值为[0]。对于彩色图像，您可以传递[0]，[1]或[2]分别计算蓝色，绿色或红色通道的直方图。</span><br><span class="line">mask：图像掩码。为了找到完整图像的直方图，将其指定为“无”。但是，如果要查找图像特定区域的直方图，则必须为此创建一个掩码图像并将其作为掩码。（我将在后面显示一个示例。）</span><br><span class="line">histSize：这表示我们的BIN计数。需要放在方括号中。对于全尺寸，我们通过[256]。</span><br><span class="line">ranges：这是我们的RANGE。通常为[0,256]。</span><br></pre></td></tr></table></figure><br>因此，让我们从示例图像开始。只需以灰度模式加载图像并找到其完整直方图即可。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line">hist &#x3D; cv.calcHist([img],[0],None,[256],[0,256])</span><br></pre></td></tr></table></figure><br>hist是256x1的数组，每个值对应于该图像中具有相应像素值的像素数。</p>
<h2 id="2-numpy的直方图计算"><a href="#2-numpy的直方图计算" class="headerlink" title="2. numpy的直方图计算"></a>2. numpy的直方图计算</h2><p>Numpy还为您提供了一个函数<strong>np.histogram</strong>()。因此，除了<strong>calcHist</strong>()函数外，您可以尝试下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hist,bins &#x3D; np.histogram(img.ravel(),256,[0,256])</span><br></pre></td></tr></table></figure><br>hist与我们之前计算的相同。但是bin将具有257个元素，因为Numpy计算出bin的范围为0-0.99、1-1.99、2-2.99等。因此最终范围为255-255.99。为了表示这一点，他们还在最后添加了256。但是我们不需要256。最多255就足够了。</p>
<p>另外 Numpy还有另一个函数<strong>np.bincount</strong>()，它比np.histogram()快10倍左右。因此，对于一维直方图，您可以更好地尝试一下。不要忘记在np.bincount中设置minlength = 256。例如，hist = np.bincount(img.ravel()，minlength = 256)<br>注意 OpenCV函数比np.histogram()快大约40倍。因此，尽可能使用OpenCV函数。</p>
<p>现在我们应该绘制直方图，但是怎么绘制？</p>
<h2 id="绘制直方图"><a href="#绘制直方图" class="headerlink" title="绘制直方图"></a>绘制直方图</h2><p>有两种方法， 1. 简短的方法：使用Matplotlib绘图功能 2. 稍长的方法：使用OpenCV绘图功能</p>
<h2 id="1-使用Matplotlib"><a href="#1-使用Matplotlib" class="headerlink" title="1. 使用Matplotlib"></a>1. 使用Matplotlib</h2><p>Matplotlib带有直方图绘图功能：matplotlib.pyplot.hist() 它直接找到直方图并将其绘制。</p>
<p>您无需使用<strong>calcHist</strong>()或np.histogram()函数来查找直方图。请参见下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line">plt.hist(img.ravel(),256,[0,256]); plt.show()</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/histogram_matplotlib.jpg" alt></p>
<p>或者，您可以使用matplotlib的法线图，这对于BGR图是很好的。为此，您需要首先找到直方图数据。试试下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">color &#x3D; (&#39;b&#39;,&#39;g&#39;,&#39;r&#39;)</span><br><span class="line">for i,col in enumerate(color):</span><br><span class="line">    histr &#x3D; cv.calcHist([img],[i],None,[256],[0,256])</span><br><span class="line">    plt.plot(histr,color &#x3D; col)</span><br><span class="line">    plt.xlim([0,256])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/histogram_rgb_plot.jpg" alt><br>您可以从上图中得出，蓝色在图像中具有一些高值域（显然这应该是由于天空）</p>
<h2 id="2-使用-OpenCV"><a href="#2-使用-OpenCV" class="headerlink" title="2. 使用 OpenCV"></a>2. 使用 OpenCV</h2><p>好吧，在这里您可以调整直方图的值及其bin值，使其看起来像x，y坐标，</p>
<p>以便您可以使用<strong>cv.line</strong>()或cv.polyline()函数绘制它以生成与上述相同的图像。</p>
<p>OpenCV-Python2官方示例已经提供了此功能。检查示例/python/hist.py中的代码。</p>
<p>掩码的应用<br>我们使用了cv.calcHist()来查找整个图像的直方图。如果你想找到图像某些区域的直方图呢?只需创建一个掩码图像，在你要找到直方图为白色，否则黑色。然后把这个作为掩码传递。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line"># create a mask</span><br><span class="line">mask &#x3D; np.zeros(img.shape[:2], np.uint8)</span><br><span class="line">mask[100:300, 100:400] &#x3D; 255</span><br><span class="line">masked_img &#x3D; cv.bitwise_and(img,img,mask &#x3D; mask)</span><br><span class="line"># 计算掩码区域和非掩码区域的直方图</span><br><span class="line"># 检查作为掩码的第三个参数</span><br><span class="line">hist_full &#x3D; cv.calcHist([img],[0],None,[256],[0,256])</span><br><span class="line">hist_mask &#x3D; cv.calcHist([img],[0],mask,[256],[0,256])</span><br><span class="line">plt.subplot(221), plt.imshow(img, &#39;gray&#39;)</span><br><span class="line">plt.subplot(222), plt.imshow(mask,&#39;gray&#39;)</span><br><span class="line">plt.subplot(223), plt.imshow(masked_img, &#39;gray&#39;)</span><br><span class="line">plt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)</span><br><span class="line">plt.xlim([0,256])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>查看结果。在直方图中，蓝线表示完整图像的直方图，绿线表示掩码区域的直方图。<br><img src="http://qiniu.aihubs.net/histogram_masking.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="直方图-2：直方图均衡"><a href="#直方图-2：直方图均衡" class="headerlink" title="直方图-2：直方图均衡"></a><span id="header2">直方图-2：直方图均衡</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本节中, - 我们将学习直方图均衡化的概念,并利用它来提高图像的对比度。</p>
<h2 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h2><p>考虑这样一个图像，它的像素值仅局限于某个特定的值范围。</p>
<p>例如，较亮的图像将把所有像素限制在高值上。但是一幅好的图像会有来自图像所有区域的像素。</p>
<p>因此，您需要将这个直方图拉伸到两端(如下图所示，来自wikipedia)，这就是直方图均衡化的作用(简单来说)。这通常会提高图像的对比度。<br><img src="http://qiniu.aihubs.net/histogram_equalization.png" alt><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;wiki.jpg&#39;,0)</span><br><span class="line">hist,bins &#x3D; np.histogram(img.flatten(),256,[0,256])</span><br><span class="line">cdf &#x3D; hist.cumsum()</span><br><span class="line">cdf_normalized &#x3D; cdf * float(hist.max()) &#x2F; cdf.max()</span><br><span class="line">plt.plot(cdf_normalized, color &#x3D; &#39;b&#39;)</span><br><span class="line">plt.hist(img.flatten(),256,[0,256], color &#x3D; &#39;r&#39;)</span><br><span class="line">plt.xlim([0,256])</span><br><span class="line">plt.legend((&#39;cdf&#39;,&#39;histogram&#39;), loc &#x3D; &#39;upper left&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/histeq_numpy1.jpg" alt></p>
<p>你可以看到直方图位于较亮的区域。我们需要全频谱。</p>
<p>为此，我们需要一个转换函数，将亮区域的输入像素映射到整个区域的输出像素。这就是直方图均衡化的作用。</p>
<p>现在我们找到最小的直方图值(不包括0)，并应用wiki页面中给出的直方图均衡化方程。</p>
<p>但我在这里用过，来自Numpy的掩码数组概念数组。对于掩码数组，所有操作都在非掩码元素上执行。您可以从Numpy文档中了解更多关于掩码数组的信息。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cdf_m &#x3D; np.ma.masked_equal(cdf,0)</span><br><span class="line">cdf_m &#x3D; (cdf_m - cdf_m.min())*255&#x2F;(cdf_m.max()-cdf_m.min())</span><br><span class="line">cdf &#x3D; np.ma.filled(cdf_m,0).astype(&#39;uint8&#39;)</span><br></pre></td></tr></table></figure><br>现在我们有了查找表，该表为我们提供了有关每个输入像素值的输出像素值是什么的信息。因此，我们仅应用变换。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img2 &#x3D; cdf[img]</span><br></pre></td></tr></table></figure><br>现在，我们像以前一样计算其直方图和cdf（您这样做），结果如下所示：<br><img src="http://qiniu.aihubs.net/histeq_numpy2.jpg" alt><br>另一个重要的特征是，即使图像是一个较暗的图像(而不是我们使用的一个较亮的图像)，经过均衡后，我们将得到几乎相同的图像。因此，这是作为一个“参考工具”，使所有的图像具有相同的照明条件。这在很多情况下都很有用。</p>
<p>例如，在人脸识别中，在对人脸数据进行训练之前，对人脸图像进行直方图均衡化处理，使其具有相同的光照条件。</p>
<h2 id="OpenCV中的直方图均衡"><a href="#OpenCV中的直方图均衡" class="headerlink" title="OpenCV中的直方图均衡"></a>OpenCV中的直方图均衡</h2><p>OpenCV具有执行此操作的功能cv.equalizeHist（）。</p>
<p>它的输入只是灰度图像，输出是我们的直方图均衡图像。 下面是一个简单的代码片段，显示了它与我们使用的同一图像的用法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;wiki.jpg&#39;,0)</span><br><span class="line">equ &#x3D; cv.equalizeHist(img)</span><br><span class="line">res &#x3D; np.hstack((img,equ)) #stacking images side-by-side</span><br><span class="line">cv.imwrite(&#39;res.png&#39;,res)</span><br></pre></td></tr></table></figure><br><img src="http://qiniu.aihubs.net/equalization_opencv.jpg" alt><br>因此，现在您可以在不同的光照条件下拍摄不同的图像，对其进行均衡并检查结果。</p>
<p>当图像的直方图限制在特定区域时，直方图均衡化效果很好。在直方图覆盖较大区域（即同时存在亮像素和暗像素）的强度变化较大的地方，效果不好。请检查其他资源中的SOF链接。</p>
<h2 id="CLAHE（对比度受限的自适应直方图均衡）"><a href="#CLAHE（对比度受限的自适应直方图均衡）" class="headerlink" title="CLAHE（对比度受限的自适应直方图均衡）"></a>CLAHE（对比度受限的自适应直方图均衡）</h2><p>我们刚刚看到的第一个直方图均衡化考虑了图像的整体对比度。</p>
<p>在许多情况下，这不是一个好主意。例如，下图显示了输入图像及其在全局直方图均衡后的结果。<br><img src="http://qiniu.aihubs.net/clahe_1.jpg" alt></p>
<p>直方图均衡后，背景对比度确实得到了改善。但是在两个图像中比较雕像的脸。</p>
<p>由于亮度过高，我们在那里丢失了大多数信息。</p>
<p>这是因为它的直方图不像我们在前面的案例中所看到的那样局限于特定区域（尝试绘制输入图像的直方图，您将获得更多的直觉）。</p>
<p>因此，为了解决这个问题，使用了<strong>自适应直方图均衡</strong>。</p>
<p>在这种情况下，图像被分成称为“tiles”的小块（在OpenCV中，tileSize默认为8x8）。</p>
<p>然后，像往常一样对这些块中的每一个进行直方图均衡。</p>
<p>因此，在较小的区域中，直方图将限制在一个较小的区域中（除非存在噪声）。</p>
<p>如果有噪音，它将被放大。为了避免这种情况，应用了对比度限制。</p>
<p>如果任何直方图bin超出指定的对比度限制（在OpenCV中默认为40），则在应用直方图均衡之前，将这些像素裁剪并均匀地分布到其他bin。</p>
<p>均衡后，要消除图块边界中的伪影，请应用双线性插值。</p>
<p>下面的代码片段显示了如何在OpenCV中应用CLAHE：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;tsukuba_l.png&#39;,0)</span><br><span class="line"># create a CLAHE object (Arguments are optional).</span><br><span class="line">clahe &#x3D; cv.createCLAHE(clipLimit&#x3D;2.0, tileGridSize&#x3D;(8,8))</span><br><span class="line">cl1 &#x3D; clahe.apply(img)</span><br><span class="line">cv.imwrite(&#39;clahe_2.jpg&#39;,cl1)</span><br></pre></td></tr></table></figure><br>查看下面的结果，并将其与上面的结果进行比较，尤其是雕像区域：<br><img src="http://qiniu.aihubs.net/clahe_2.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="直方图-3：二维直方图"><a href="#直方图-3：二维直方图" class="headerlink" title="直方图-3：二维直方图"></a><span id="header3">直方图-3：二维直方图</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习查找和绘制2D直方图。这将在以后的章节中有所帮助。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在第一篇文章中，我们计算并绘制了一维直方图。 之所以称为一维，是因为我们仅考虑一个特征，即像素的灰度强度值。 </p>
<p>但是在二维直方图中，您要考虑两个特征。 通常，它用于查找颜色直方图，其中两个特征是每个像素的色相和饱和度值。</p>
<p>我们将尝试了解如何创建这种颜色直方图，这对于理解诸如直方图反向投影之类的更多主题将很有用。</p>
<h2 id="OpenCV中的二维直方图"><a href="#OpenCV中的二维直方图" class="headerlink" title="OpenCV中的二维直方图"></a>OpenCV中的二维直方图</h2><p>它非常简单，并且使用相同的函数<strong>cv.calcHist</strong>()进行计算。 </p>
<p>对于颜色直方图，我们需要将图像从BGR转换为HSV。（请记住，对于一维直方图，我们从BGR转换为灰度）。对于二维直方图，其参数将进行如下修改：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">channel &#x3D; [0,1]，因为我们需要同时处理H和S平面。</span><br><span class="line">bins &#x3D; [180,256] 对于H平面为180，对于S平面为256。</span><br><span class="line">range &#x3D; [0,180,0,256] 色相值介于0和180之间，饱和度介于0和256之间。</span><br></pre></td></tr></table></figure><br>现在检查以下代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist &#x3D; cv.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])</span><br></pre></td></tr></table></figure><br>就是这样。</p>
<h2 id="Numpy中的二维直方图"><a href="#Numpy中的二维直方图" class="headerlink" title="Numpy中的二维直方图"></a>Numpy中的二维直方图</h2><p>Numpy还为此提供了一个特定的函数:np.histogram2d()。(记住，对于一维直方图我们使用了<strong>np.histogram</strong>())。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist, xbins, ybins &#x3D; np.histogram2d(h.ravel(),s.ravel(),[180,256],[[0,180],[0,256]])</span><br></pre></td></tr></table></figure><br>第一个参数是H平面，第二个是S平面，第三个是每个箱子的数量，第四个是它们的范围。</p>
<p>现在我们可以检查如何绘制这个颜色直方图。</p>
<h2 id="绘制二维直方图"><a href="#绘制二维直方图" class="headerlink" title="绘制二维直方图"></a>绘制二维直方图</h2><h2 id="方法1：使用-cv-imshow"><a href="#方法1：使用-cv-imshow" class="headerlink" title="方法1：使用 cv.imshow()"></a>方法1：使用 cv.imshow()</h2><p>我们得到的结果是尺寸为80x256的二维数组。因此，可以使用<strong>cv.imshow</strong>()函数像平常一样显示它们。</p>
<p>它将是一幅灰度图像，除非您知道不同颜色的色相值，否则不会对其中的颜色有太多了解。</p>
<h2 id="方法2：使用Matplotlib"><a href="#方法2：使用Matplotlib" class="headerlink" title="方法2：使用Matplotlib"></a>方法2：使用Matplotlib</h2><p>我们可以使用matplotlib.pyplot.imshow()函数绘制具有不同颜色图的2D直方图。</p>
<p>它使我们对不同的像素密度有了更好的了解。但是，除非您知道不同颜色的色相值，否则乍一看并不能使我们知道到底是什么颜色。</p>
<p>注意 使用此功能时，请记住，插值法应采用最近邻以获得更好的结果。</p>
<p>考虑下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist &#x3D; cv.calcHist( [hsv], [0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line">plt.imshow(hist,interpolation &#x3D; &#39;nearest&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>下面是输入图像及其颜色直方图。X轴显示S值，Y轴显示色相。<br><img src="http://qiniu.aihubs.net/2dhist_matplotlib.jpg" alt></p>
<p>在直方图中，您可以在H = 100和S = 200附近看到一些较高的值。</p>
<p>它对应于天空的蓝色。同样，在H = 25和S = 100附近可以看到另一个峰值。它对应于宫殿的黄色。您可以使用GIMP等任何图像编辑工具进行验证。</p>
<h2 id="方法3：OpenCV示例样式"><a href="#方法3：OpenCV示例样式" class="headerlink" title="方法3：OpenCV示例样式"></a>方法3：OpenCV示例样式</h2><p>OpenCV-Python2示例中有一个颜色直方图的示例代码(samples / python / color_histogram.py)。</p>
<p>如果运行代码，则可以看到直方图也显示了相应的颜色。或者简单地，它输出颜色编码的直方图。其结果非常好（尽管您需要添加额外的线束）。</p>
<p>在该代码中，作者在HSV中创建了一个颜色图。然后将其转换为BGR。将所得的直方图图像与此颜色图相乘。他还使用一些预处理步骤来删除小的孤立像素，从而获得良好的直方图。</p>
<p>我将其留给读者来运行代码，对其进行分析并拥有自己的解决方法。下面是与上面相同的图像的代码输出：<br><img src="http://qiniu.aihubs.net/2dhist_opencv.jpg" alt></p>
<p>您可以在直方图中清楚地看到存在什么颜色，那里是蓝色，那里是黄色，并且由于棋盘的存在而有些白色。很好！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="直方图4：直方图反投影"><a href="#直方图4：直方图反投影" class="headerlink" title="直方图4：直方图反投影"></a><span id="header4">直方图4：直方图反投影</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>在本章中，我们将学习直方图反投影。</p>
<h2 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h2><p>这是由<strong>Michael J. Swain</strong>和<strong>Dana H. Ballard</strong>在他们的论文《通过颜色直方图索引》中提出的。</p>
<p>用简单的话说是什么意思？它用于图像分割或在图像中查找感兴趣的对象。</p>
<p>简而言之，它创建的图像大小与输入图像相同（但只有一个通道），其中每个像素对应于该像素属于我们物体的概率。</p>
<p>用更简单的话来说，与其余部分相比，输出图像将在可能有对象的区域具有更多的白色值。</p>
<p>好吧，这是一个直观的解释。（我无法使其更简单）。直方图反投影与camshift算法等配合使用。</p>
<p>我们该怎么做呢？我们创建一个图像的直方图，其中包含我们感兴趣的对象（在我们的示例中是背景，离开播放器等）。</p>
<p>对象应尽可能填充图像以获得更好的效果。而且颜色直方图比灰度直方图更可取，因为对象的颜色对比灰度强度是定义对象的好方法。</p>
<p>然后，我们将该直方图“反投影”到需要找到对象的测试图像上，</p>
<p>换句话说，我们计算出属于背景的每个像素的概率并将其显示出来。在适当的阈值下产生的输出使我们仅获得背景。</p>
<h2 id="Numpy中的算法"><a href="#Numpy中的算法" class="headerlink" title="Numpy中的算法"></a>Numpy中的算法</h2><p>首先，我们需要计算我们要查找的对象（使其为“ M”）和要搜索的图像（使其为“ I”）的颜色直方图。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cvfrom matplotlib import pyplot as plt</span><br><span class="line">#roi是我们需要找到的对象或对象区域</span><br><span class="line">roi &#x3D; cv.imread(&#39;rose_red.png&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(roi,cv.COLOR_BGR2HSV)</span><br><span class="line">#目标是我们搜索的图像</span><br><span class="line">target &#x3D; cv.imread(&#39;rose.png&#39;)</span><br><span class="line">hsvt &#x3D; cv.cvtColor(target,cv.COLOR_BGR2HSV)</span><br><span class="line"># 使用calcHist查找直方图。也可以使用np.histogram2d完成</span><br><span class="line">M &#x3D; cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line">I &#x3D; cv.calcHist([hsvt],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br></pre></td></tr></table></figure></p>
<p>求出比值R=$\frac{M}{I}$。然后反向投影R，即使用R作为调色板，并以每个像素作为其对应的目标概率创建一个新图像。即B(x,y) = R[h(x,y),s(x,y)] 其中h是色调，s是像素在(x，y)的饱和度。之后，应用条件B(x,y)=min[B(x,y),1]。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">h,s,v &#x3D; cv.split(hsvt)</span><br><span class="line">B &#x3D; R[h.ravel(),s.ravel()]</span><br><span class="line">B &#x3D; np.minimum(B,1)</span><br><span class="line">B &#x3D; B.reshape(hsvt.shape[:2])</span><br></pre></td></tr></table></figure>
<p>现在对圆盘应用卷积，B=D∗B，其中D是圆盘内核。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">disc &#x3D; cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))</span><br><span class="line">cv.filter2D(B,-1,disc,B)</span><br><span class="line">B &#x3D; np.uint8(B)</span><br><span class="line">cv.normalize(B,B,0,255,cv.NORM_MINMAX)</span><br></pre></td></tr></table></figure><br>现在最大强度的位置给了我们物体的位置。如果我们期望图像中有一个区域，则对合适的值进行阈值处理将获得不错的结果。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ret,thresh &#x3D; cv.threshold(B,50,255,0)</span><br></pre></td></tr></table></figure><br>就是这样</p>
<h2 id="OpenCV的反投影"><a href="#OpenCV的反投影" class="headerlink" title="OpenCV的反投影"></a>OpenCV的反投影</h2><p>OpenCV提供了一个内建的函数<strong>cv.calcBackProject</strong>()。</p>
<p>它的参数几乎与<strong>cv.calchist</strong>()函数相同。它的一个参数是直方图，也就是物体的直方图，我们必须找到它。</p>
<p>另外，在传递给backproject函数之前，应该对对象直方图进行归一化。它返回概率图像。</p>
<p>然后我们用圆盘内核对图像进行卷积并应用阈值。下面是我的代码和结果:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">roi &#x3D; cv.imread(&#39;rose_red.png&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(roi,cv.COLOR_BGR2HSV)</span><br><span class="line">target &#x3D; cv.imread(&#39;rose.png&#39;)</span><br><span class="line">hsvt &#x3D; cv.cvtColor(target,cv.COLOR_BGR2HSV)</span><br><span class="line"># 计算对象的直方图</span><br><span class="line">roihist &#x3D; cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line"># 直方图归一化并利用反传算法</span><br><span class="line">cv.normalize(roihist,roihist,0,255,cv.NORM_MINMAX)</span><br><span class="line">dst &#x3D; cv.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)</span><br><span class="line"># 用圆盘进行卷积</span><br><span class="line">disc &#x3D; cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))</span><br><span class="line">cv.filter2D(dst,-1,disc,dst)</span><br><span class="line"># 应用阈值作与操作</span><br><span class="line">ret,thresh &#x3D; cv.threshold(dst,50,255,0)</span><br><span class="line">thresh &#x3D; cv.merge((thresh,thresh,thresh))</span><br><span class="line">res &#x3D; cv.bitwise_and(target,thresh)</span><br><span class="line">res &#x3D; np.vstack((target,thresh,res))</span><br><span class="line">cv.imwrite(&#39;res.jpg&#39;,res)</span><br></pre></td></tr></table></figure><br>以下是我处理过的一个示例。我将蓝色矩形内的区域用作示例对象，我想提取整个地面。<br><img src="http://qiniu.aihubs.net/backproject_opencv.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>单层感知器</title>
    <url>/2020/07/09/%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8/</url>
    <content><![CDATA[<p>单层感知器代码简单演示<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">-1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">-1</span>]])</span><br><span class="line">W = (np.random.random([<span class="number">3</span>,<span class="number">1</span>])<span class="number">-0.5</span>)*<span class="number">2</span></span><br><span class="line">print(W)</span><br><span class="line">lr = <span class="number">0.11</span></span><br><span class="line">O = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> X,Y,W,lr</span><br><span class="line">    O = np.sign(np.dot(X,W))</span><br><span class="line">    W_C = lr*(X.T.dot(Y-O))/int(X.shape[<span class="number">0</span>])</span><br><span class="line">    W = W + W_C</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.23015384]
 [-0.72847367]
 [ 0.52092108]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    update()</span><br><span class="line">    O = np.sign(np.dot(X,W))</span><br><span class="line">    <span class="keyword">if</span>(O==Y).all():</span><br><span class="line">        print(<span class="string">'Finished'</span>)</span><br><span class="line">        print(<span class="string">'epoch:'</span>,i)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">x1 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">y1 = [<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">x2 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">y2 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">k = -W[<span class="number">1</span>]/W[<span class="number">2</span>]</span><br><span class="line">d = -W[<span class="number">0</span>]/W[<span class="number">2</span>]</span><br><span class="line">print(<span class="string">'k='</span>,k)</span><br><span class="line">print(<span class="string">'d='</span>,d)</span><br><span class="line">xdata = (<span class="number">-2</span>,<span class="number">3</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(xdata,xdata*k+d,<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(x1,y1,c=<span class="string">'b'</span>)</span><br><span class="line">plt.scatter(x2,y2,c=<span class="string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>k= [0.16650386]
d= [-0.80515292]
</code></pre><p><img src="/2020/07/09/%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8/output_2_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(1)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/</url>
    <content><![CDATA[<p>机器学习是目前信息技术中最激动人心的方向之一，其应用已经深入到生活的各个层面且与普通人的日常生活密切相关。本文为清华大学最新出版的《机器学习》教材的Learning Notes，书作者是南京大学周志华教授，多个大陆首位彰显其学术奢华。本篇主要介绍了该教材前两个章节的知识点以及自己一点浅陋的理解。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<p><strong>1  绪论</strong></p>
<p>傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！哈哈，也希望自己这学期的machine learning课程取得一个好成绩！</p>
<p><strong>1.1 机器学习的定义</strong></p>
<p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p>
<p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p>
<ul>
<li>P：计算机程序在某任务类T上的性能。</li>
<li>T：计算机程序希望实现的任务类。</li>
<li>E：表示经验，即历史的数据集。</li>
</ul>
<p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p>
<p><strong>1.2 机器学习的一些基本术语</strong></p>
<p>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：     </p>
<ul>
<li>所有记录的集合为：数据集。</li>
<li>每一条记录为：一个实例（instance）或样本（sample）。</li>
<li>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</li>
<li>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</li>
<li><p>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p>
<p> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：    </p>
</li>
<li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p>
</li>
<li>所有测试样本的集合为：测试集（test set），[一般]。  </li>
<li><p>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p>
<p> 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：    </p>
</li>
<li><p>预测值为离散值的问题为：分类（classification）。</p>
</li>
<li><p>预测值为连续值的问题为：回归（regression）。</p>
<p> 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：    </p>
</li>
<li><p>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p>
</li>
<li>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</li>
</ul>
<p><strong>2  模型的评估与选择</strong></p>
<p><strong>2.1 误差与过拟合</strong></p>
<p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：    </p>
<ul>
<li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li>
<li>在测试集上的误差称为测试误差（test error）。</li>
<li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li>
</ul>
<p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p>
<ul>
<li>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</li>
<li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li>
</ul>
<p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7181172996.png" alt></p>
<p><strong>2.2 评估方法</strong></p>
<p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p>
<p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p>
<p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p>
<p><strong>2.3 训练集与测试集的划分方法</strong></p>
<p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p>
<p><strong>2.3.1 留出法</strong></p>
<p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p>
<p><strong>2.3.2 交叉验证法</strong></p>
<p>将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc718115d224.png" alt></p>
<p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p>
<p><strong>2.3.3 自助法</strong></p>
<p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p>
<p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71811246dd.png" alt></p>
<p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p>
<p><strong>2.4 调参</strong></p>
<p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p>
<p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5<em>5</em>5= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p>
<p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。用上面考试的例子来比喻：就像高中时大家每次考试完，要将考卷的题目消化掉（大多数题目都还是之前没有见过的吧？），这样即使考差了也能开心的玩耍了~。</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(11)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/</url>
    <content><![CDATA[<p>上篇主要介绍了一种机器学习的通用框架—集成学习方法，首先从准确性和差异性两个重要概念引出集成学习“<strong>好而不同</strong>”的四字真言，接着介绍了现阶段主流的三种集成学习方法：AdaBoost、Bagging及Random Forest，AdaBoost采用最小化指数损失函数迭代式更新样本分布权重和计算基学习器权重，Bagging通过自助采样引入样本扰动增加了基学习器之间的差异性，随机森林则进一步引入了属性扰动，最后简单概述了集成模型中的三类结合策略：平均法、投票法及学习法，其中Stacking是学习法的典型代表。本篇将讨论无监督学习中应用最为广泛的学习算法—聚类。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="10、聚类算法"><a href="#10、聚类算法" class="headerlink" title="10、聚类算法"></a><strong>10、聚类算法</strong></h1><p>聚类是一种经典的<strong>无监督学习</strong>方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p>
<p>聚类直观上来说是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。那首先的问题是如何来<strong>度量相似性</strong>（similarity measure）呢？这便是<strong>距离度量</strong>，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是<strong>性能度量</strong>，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p>
<h2 id="10-1-距离度量"><a href="#10-1-距离度量" class="headerlink" title="10.1 距离度量"></a><strong>10.1 距离度量</strong></h2><p>谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed4c0390.png" alt="1.png"></p>
<p>最常用的距离度量方法是<strong>“闵可夫斯基距离”（Minkowski distance)</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed49e31f.png" alt="2.png"></p>
<p>当p=1时，闵可夫斯基距离即<strong>曼哈顿距离（Manhattan distance）</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed49c31f.png" alt="3.png"></p>
<p>当p=2时，闵可夫斯基距离即<strong>欧氏距离（Euclidean distance）</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed497613.png" alt="4.png"></p>
<p>我们知道属性分为两种：<strong>连续属性</strong>和<strong>离散属性</strong>（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p>
<blockquote>
<p>若属性值之间<strong>存在序关系</strong>，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。<br>若属性值之间<strong>不存在序关系</strong>，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。</p>
</blockquote>
<p>在进行距离度量时，易知<strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，因为它们都可以反映一种程度，我们称其为“<strong>有序属性</strong>”；而对于不存在序关系的离散属性，我们称其为：“<strong>无序属性</strong>”，显然无序属性再使用闵可夫斯基距离就行不通了。</p>
<p><strong>对于无序属性，我们一般采用VDM进行距离的计算</strong>，例如：对于离散属性的两个取值a和b，定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed4e9560.png" alt="5.png"></p>
<p>于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed507bc7.png" alt="6.png"></p>
<p>若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：<strong>非度量距离（non-metric distance）</strong>。</p>
<h2 id="10-2-性能度量"><a href="#10-2-性能度量" class="headerlink" title="10.2 性能度量"></a><strong>10.2 性能度量</strong></h2><p>由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：<strong>外部指标</strong>和<strong>内部指标</strong>。</p>
<h3 id="10-2-1-外部指标"><a href="#10-2-1-外部指标" class="headerlink" title="10.2.1 外部指标"></a><strong>10.2.1 外部指标</strong></h3><p>即将聚类结果与某个参考模型的结果进行比较，<strong>以参考模型的输出作为标准，来评价聚类好坏</strong>。假设聚类给出的结果为λ，参考模型给出的结果是λ*，则我们将样本进行两两配对，定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed59160e.png" alt="7.png"></p>
<p>显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed587438.png" alt="8.png"></p>
<h3 id="10-2-2-内部指标"><a href="#10-2-2-内部指标" class="headerlink" title="10.2.2 内部指标"></a><strong>10.2.2 内部指标</strong></h3><p>内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：<strong>簇内高内聚紧紧抱团，簇间低耦合老死不相往来</strong>。定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed581852.png" alt="9.png"></p>
<p>基于上面的四个距离，可以导出下面这些常用的内部评价指标：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed582854.png" alt="10.png"></p>
<h2 id="10-3-原型聚类"><a href="#10-3-原型聚类" class="headerlink" title="10.3 原型聚类"></a><strong>10.3 原型聚类</strong></h2><p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</p>
<h3 id="10-3-1-K-Means"><a href="#10-3-1-K-Means" class="headerlink" title="10.3.1 K-Means"></a><strong>10.3.1 K-Means</strong></h3><p>K-Means的思想十分简单，<strong>首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛</strong>。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过<strong>EM算法</strong>的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb82b5d3.png" alt="11.png"></p>
<p>K-Means的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9c0817.png" alt="12.png"></p>
<h3 id="10-3-2-学习向量量化（LVQ）"><a href="#10-3-2-学习向量量化（LVQ）" class="headerlink" title="10.3.2 学习向量量化（LVQ）"></a><strong>10.3.2 学习向量量化（LVQ）</strong></h3><p>LVQ也是基于原型的聚类算法，与K-Means不同的是，<strong>LVQ使用样本真实类标记辅助聚类</strong>，首先LVQ根据样本的类标记，从各类中分别随机选出一个样本作为该类簇的原型，从而组成了一个<strong>原型特征向量组</strong>，接着从样本集中随机挑选一个样本，计算其与原型向量组中每个向量的距离，并选取距离最小的原型向量所在的类簇作为它的划分结果，再与真实类标比较。</p>
<blockquote>
<p><strong>若划分结果正确，则对应原型向量向这个样本靠近一些</strong><br><strong>若划分结果不正确，则对应原型向量向这个样本远离一些</strong></p>
</blockquote>
<p>LVQ算法的流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9d59f2.png" alt="13.png"></p>
<h3 id="10-3-3-高斯混合聚类"><a href="#10-3-3-高斯混合聚类" class="headerlink" title="10.3.3 高斯混合聚类"></a><strong>10.3.3 高斯混合聚类</strong></h3><p>现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设<strong>每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成</strong>。</p>
<p>对于多维高斯分布，其概率密度函数如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb870d98.png" alt="14.png"></p>
<p>其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb876794.png" alt="15.png"></p>
<p>α称为混合系数，这样空间中样本的采集过程则可以抽象为：<strong>（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样</strong>，这时候贝叶斯公式又能大展身手了：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9191d9.png" alt="16.png"></p>
<p>此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：<strong>这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法美好地计算出来</strong>，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb871d4a.png" alt="17.png"></p>
<p>可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。<strong>这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb8a6f32.png" alt="18.png"></p>
<p>高斯混合聚类的算法流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9c4fa4.png" alt="19.png"></p>
<h2 id="10-4-密度聚类"><a href="#10-4-密度聚类" class="headerlink" title="10.4 密度聚类"></a><strong>10.4 密度聚类</strong></h2><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是<strong>DBSCAN</strong>算法，首先定义以下概念：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9bd69c.png" alt="20.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509f8d619.png" alt="21.png"></p>
<p>简单来理解DBSCAN便是：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509feb587.png" alt="22.png"></p>
<h2 id="10-5-层次聚类"><a href="#10-5-层次聚类" class="headerlink" title="10.5 层次聚类"></a><strong>10.5 层次聚类</strong></h2><p>层次聚类是一种基于树形结构的聚类方法，常用的是<strong>自底向上</strong>的结合策略（<strong>AGNES算法</strong>）。假设有N个待聚类的样本，其基本步骤是：</p>
<blockquote>
<p>1.初始化—&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；<br>2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；<br>3.重新计算新生成的这个<strong>类与各个旧类之间的相似度</strong>；<br>4.重复2和3直到所有样本点都归为一类，结束。</p>
</blockquote>
<p>可以看出其中最关键的一步就是<strong>计算两个类簇的相似度</strong>，这里有多种度量方法：</p>
<pre><code>* 单链接（single-linkage）:取类间最小距离。
</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509ebb022.png" alt="23.png"></p>
<pre><code>* 全链接（complete-linkage）:取类间最大距离
</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509eb2b30.png" alt="24.png"></p>
<pre><code>* 均链接（average-linkage）:取类间两两的平均距离
</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509f089a7.png" alt="25.png"></p>
<p>很容易看出：<strong>单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局</strong>。层次聚类法的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509f9d4a0.png" alt="26.png"></p>
<blockquote>
<p>在此聚类算法就介绍完毕，分类/聚类都是机器学习中最常见的任务，我实验室的大Boss也是靠着聚类起家，从此走上人生事业钱途…之巅峰，在书最后的阅读材料还看见Boss的名字，所以这章也是必读不可了…</p>
</blockquote>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(10)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>上篇主要介绍了鼎鼎大名的EM算法，从算法思想到数学公式推导（边际似然引入隐变量，Jensen不等式简化求导），EM算法实际上可以理解为一种坐标下降法，首先固定一个变量，接着求另外变量的最优解，通过其优美的“两步走”策略能较好地估计隐变量的值。本篇将继续讨论下一类经典算法—集成学习。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="9、集成学习"><a href="#9、集成学习" class="headerlink" title="9、集成学习"></a><strong>9、集成学习</strong></h1><p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生~…~，即其泛化性能要能优于其中任何一个学习器。</p>
<h2 id="9-1-个体与集成"><a href="#9-1-个体与集成" class="headerlink" title="9.1 个体与集成"></a><strong>9.1 个体与集成</strong></h2><p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0c15683.png" alt="1.png"></p>
<p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p>
<blockquote>
<p><strong>同质集成</strong>：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。<br><strong>异质集成</strong>：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p>
</blockquote>
<p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d23e13.png" alt="2.png"></p>
<p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0cce0bb.png" alt="3.png"></p>
<p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A=1 | B=1）&gt; P（A=1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，<strong>个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量</strong>，准确性高意味着牺牲多样性，所以产生“<strong>好而不同</strong>”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p>
<h2 id="9-2-Boosting"><a href="#9-2-Boosting" class="headerlink" title="9.2 Boosting"></a><strong>9.2 Boosting</strong></h2><p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p>
<p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p>
<p>定义基学习器的集成为加权结合，则有：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ca2ca5.png" alt="4.png"></p>
<p>AdaBoost算法的指数损失函数定义为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d10461.png" alt="5.png"></p>
<p>具体说来，整个Adaboost 迭代算法分为3步：</p>
<ul>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>
</ul>
<p>整个AdaBoost的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d7c057.png" alt="6.png"></p>
<p>可以看出：<strong>AdaBoost的核心步骤就是计算基学习器权重和样本权重分布</strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为<strong>大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到</strong>，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开。</p>
<p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p>
<blockquote>
<p><strong>重赋权法</strong> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><strong>重采样法</strong> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p>
</blockquote>
<p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。在此，当选为数据挖掘十大算法之一的AdaBoost介绍到这里，能够当选正是说明这个算法十分婀娜多姿，背后的数学证明和推导充分证明了这一点，限于篇幅不再继续展开。</p>
<h2 id="9-3-Bagging与Random-Forest"><a href="#9-3-Bagging与Random-Forest" class="headerlink" title="9.3 Bagging与Random Forest"></a><strong>9.3 Bagging与Random Forest</strong></h2><p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p>
<h3 id="9-3-1-Bagging"><a href="#9-3-1-Bagging" class="headerlink" title="9.3.1 Bagging"></a><strong>9.3.1 Bagging</strong></h3><p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ce62fc.png" alt="7.png"></p>
<p>Bagging算法的流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d0e761.png" alt="8.png"></p>
<p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p>
<h3 id="9-3-2-随机森林"><a href="#9-3-2-随机森林" class="headerlink" title="9.3.2 随机森林"></a><strong>9.3.2 随机森林</strong></h3><p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K=log2（d）。</p>
<p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png" alt="9.png"></p>
<h2 id="9-4-结合策略"><a href="#9-4-结合策略" class="headerlink" title="9.4 结合策略"></a><strong>9.4 结合策略</strong></h2><p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p>
<h3 id="9-4-1-平均法（回归问题）"><a href="#9-4-1-平均法（回归问题）" class="headerlink" title="9.4.1 平均法（回归问题）"></a><strong>9.4.1 平均法（回归问题）</strong></h3><p><img src="https://i.loli.net/2018/10/18/5bc84d0d07983.png" alt="10.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1b74ff.png" alt="11.png"></p>
<p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，<strong>一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法</strong>。</p>
<h3 id="9-4-2-投票法（分类问题）"><a href="#9-4-2-投票法（分类问题）" class="headerlink" title="9.4.2 投票法（分类问题）"></a><strong>9.4.2 投票法（分类问题）</strong></h3><p><img src="https://i.loli.net/2018/10/18/5bc84de2629c4.png" alt="12.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25a74b.png" alt="13.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1bacc4.png" alt="14.png"></p>
<p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de2768c1.png" alt="15.png"></p>
<p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p>
<h3 id="9-4-3-学习法"><a href="#9-4-3-学习法" class="headerlink" title="9.4.3 学习法"></a><strong>9.4.3 学习法</strong></h3><p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m<em>T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：<em>*投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果</em></em>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25cbaf.png" alt="16.png"></p>
<h2 id="9-5-多样性（diversity）"><a href="#9-5-多样性（diversity）" class="headerlink" title="9.5 多样性（diversity）"></a><strong>9.5 多样性（diversity）</strong></h2><p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p>
<blockquote>
<p><strong>数据样本扰动</strong>，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br><strong>输入属性扰动</strong>，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br><strong>输出表示扰动</strong>，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br><strong>算法参数扰动</strong>，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p>
</blockquote>
<p>在此，集成学习就介绍完毕，看到这里，大家也会发现集成学习实质上是一种通用框架，可以使用任何一种基学习器，从而改进单个学习器的泛化性能。据说数据挖掘竞赛KDDCup历年的冠军几乎都使用了集成学习，看来的确是个好东西~</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(14)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<p>上篇主要介绍了常用的特征选择方法及稀疏学习。首先从相关/无关特征出发引出了特征选择的基本概念，接着分别介绍了子集搜索与评价、过滤式、包裹式以及嵌入式四种类型的特征选择方法。子集搜索与评价使用的是一种优中生优的贪婪算法，即每次从候选特征子集中选出最优子集；过滤式方法计算一个相关统计量来评判特征的重要程度；包裹式方法将学习器作为特征选择的评价准则；嵌入式方法则是通过L1正则项将特征选择融入到学习器参数优化的过程中。最后介绍了稀疏表示与压缩感知的核心思想：稀疏表示利用稀疏矩阵的优良性质，试图通过某种方法找到原始稠密矩阵的合适稀疏表示；压缩感知则试图利用可稀疏表示的欠采样信息来恢复全部信息。本篇将讨论一种为机器学习提供理论保证的学习方法—计算学习理论。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="13、计算学习理论"><a href="#13、计算学习理论" class="headerlink" title="13、计算学习理论"></a><strong>13、计算学习理论</strong></h1><p>计算学习理论（computational learning theory）是通过“计算”来研究机器学习的理论，简而言之，其目的是分析学习任务的本质，例如：<strong>在什么条件下可进行有效的学习，需要多少训练样本能获得较好的精度等，从而为机器学习算法提供理论保证</strong>。</p>
<p>首先我们回归初心，再来谈谈经验误差和泛化误差。假设给定训练集D，其中所有的训练样本都服从一个未知的分布T，且它们都是在总体分布T中独立采样得到，即<strong>独立同分布</strong>（independent and identically distributed，i.i.d.），在《贝叶斯分类器》中我们已经提到：独立同分布是很多统计学习算法的基础假设，例如最大似然法，贝叶斯分类器，高斯混合聚类等，简单来理解独立同分布：每个样本都是从总体分布中独立采样得到，而没有拖泥带水。例如现在要进行问卷调查，要从总体人群中随机采样，看到一个美女你高兴地走过去，结果她男票突然冒了出来，说道：you jump，i jump，于是你本来只想调查一个人结果被强行撒了一把狗粮得到两份问卷，这样这两份问卷就不能称为独立同分布了，因为它们的出现具有强相关性。</p>
<p>回归正题，<strong>泛化误差指的是学习器在总体上的预测误差，经验误差则是学习器在某个特定数据集D上的预测误差</strong>。在实际问题中，往往我们并不能得到总体且数据集D是通过独立同分布采样得到的，因此我们常常使用经验误差作为泛化误差的近似。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f38d4fe.png" alt="1.png"></p>
<h2 id="13-1-PAC学习"><a href="#13-1-PAC学习" class="headerlink" title="13.1 PAC学习"></a><strong>13.1 PAC学习</strong></h2><p>在高中课本中，我们将<strong>函数定义为：从自变量到因变量的一种映射；对于机器学习算法，学习器也正是为了寻找合适的映射规则</strong>，即如何从条件属性得到目标属性。从样本空间到标记空间存在着很多的映射，我们将每个映射称之为<strong>概念</strong>（concept），定义：</p>
<blockquote>
<p>若概念c对任何样本x满足c(x)=y，则称c为<strong>目标概念</strong>，即最理想的映射，所有的目标概念构成的集合称为<strong>“概念类”</strong>；<br>给定学习算法，它所有可能映射/概念的集合称为<strong>“假设空间”</strong>，其中单个的概念称为<strong>“假设”</strong>（hypothesis）；<br>若一个算法的假设空间包含目标概念，则称该数据集对该算法是<strong>可分</strong>（separable）的，亦称<strong>一致</strong>（consistent）的；<br>若一个算法的假设空间不包含目标概念，则称该数据集对该算法是<strong>不可分</strong>（non-separable）的，或称<strong>不一致</strong>（non-consistent）的。</p>
</blockquote>
<p>举个简单的例子：对于非线性分布的数据集，若使用一个线性分类器，则该线性分类器对应的假设空间就是空间中所有可能的超平面，显然假设空间不包含该数据集的目标概念，所以称数据集对该学习器是不可分的。给定一个数据集D，我们希望模型学得的假设h尽可能地与目标概念一致，这便是<strong>概率近似正确</strong> (Probably Approximately Correct，简称PAC)的来源，即以较大的概率学得模型满足误差的预设上限。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f446f97.png" alt="2.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f482d0b.png" alt="3.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f47d006.png" alt="4.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f46ad91.png" alt="5.png"></p>
<p>上述关于PAC的几个定义层层相扣：定义12.1表达的是对于某种学习算法，如果能以一个置信度学得假设满足泛化误差的预设上限，则称该算法能PAC辨识概念类，即该算法的输出假设已经十分地逼近目标概念。定义12.2则将样本数量考虑进来，当样本超过一定数量时，学习算法总是能PAC辨识概念类，则称概念类为PAC可学习的。定义12.3将学习器运行时间也考虑进来，若运行时间为多项式时间，则称PAC学习算法。</p>
<p>显然，PAC学习中的一个关键因素就是<strong>假设空间的复杂度</strong>，对于某个学习算法，<strong>若假设空间越大，则其中包含目标概念的可能性也越大，但同时找到某个具体概念的难度也越大</strong>，一般假设空间分为有限假设空间与无限假设空间。</p>
<h2 id="13-2-有限假设空间"><a href="#13-2-有限假设空间" class="headerlink" title="13.2 有限假设空间"></a><strong>13.2 有限假设空间</strong></h2><h3 id="13-2-1-可分情形"><a href="#13-2-1-可分情形" class="headerlink" title="13.2.1 可分情形"></a><strong>13.2.1 可分情形</strong></h3><p>可分或一致的情形指的是：<strong>目标概念包含在算法的假设空间中</strong>。对于目标概念，在训练集D中的经验误差一定为0，因此首先我们可以想到的是：不断地剔除那些出现预测错误的假设，直到找到经验误差为0的假设即为目标概念。但<strong>由于样本集有限，可能会出现多个假设在D上的经验误差都为0，因此问题转化为：需要多大规模的数据集D才能让学习算法以置信度的概率从这些经验误差都为0的假设中找到目标概念的有效近似</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f484f30.png" alt="6.png"></p>
<p>通过上式可以得知：<strong>对于可分情形的有限假设空间，目标概念都是PAC可学习的，即当样本数量满足上述条件之后，在与训练集一致的假设中总是可以在1-σ概率下找到目标概念的有效近似。</strong></p>
<h3 id="13-2-2-不可分情形"><a href="#13-2-2-不可分情形" class="headerlink" title="13.2.2 不可分情形"></a><strong>13.2.2 不可分情形</strong></h3><p>不可分或不一致的情形指的是：<strong>目标概念不存在于假设空间中</strong>，这时我们就不能像可分情形时那样从假设空间中寻找目标概念的近似。但<strong>当假设空间给定时，必然存一个假设的泛化误差最小，若能找出此假设的有效近似也不失为一个好的目标，这便是不可知学习(agnostic learning)的来源。</strong></p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f485f2e.png" alt="7.png"></p>
<p>这时候便要用到<strong>Hoeffding不等式</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f46970a.png" alt="8.png"></p>
<p>对于假设空间中的所有假设，出现泛化误差与经验误差之差大于e的概率和为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f4114fd.png" alt="9.png"></p>
<p>因此，可令不等式的右边小于（等于）σ，便可以求出满足泛化误差与经验误差相差小于e所需的最少样本数，同时也可以求出泛化误差界。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc854f440a02.png" alt="10.png"></p>
<h2 id="13-3-VC维"><a href="#13-3-VC维" class="headerlink" title="13.3 VC维"></a><strong>13.3 VC维</strong></h2><p>现实中的学习任务通常都是无限假设空间，例如d维实数域空间中所有的超平面等，因此要对此种情形进行可学习研究，需要度量<strong>假设空间的复杂度</strong>。这便是<strong>VC维</strong>（Vapnik-Chervonenkis dimension）的来源。在介绍VC维之前，需要引入两个概念：</p>
<blockquote>
<p><strong>增长函数</strong>：对于给定数据集D，假设空间中的每个假设都能对数据集的样本赋予标记，因此一个假设对应着一种打标结果，不同假设对D的打标结果可能是相同的，也可能是不同的。随着样本数量m的增大，假设空间对样本集D的打标结果也会增多，增长函数则表示假设空间对m个样本的数据集D打标的最大可能结果数，因此<strong>增长函数描述了假设空间的表示能力与复杂度。</strong></p>
<p><img src="https://i.loli.net/2018/10/18/5bc855ba970cd.png" alt="11.png"></p>
<p><strong>打散</strong>：例如对二分类问题来说，m个样本最多有2^m个可能结果，每种可能结果称为一种<strong>“对分”</strong>，若假设空间能实现数据集D的所有对分，则称数据集能被该假设空间打散。</p>
</blockquote>
<p><strong>因此尽管假设空间是无限的，但它对特定数据集打标的不同结果数是有限的，假设空间的VC维正是它能打散的最大数据集大小</strong>。通常这样来计算假设空间的VC维：若存在大小为d的数据集能被假设空间打散，但不存在任何大小为d+1的数据集能被假设空间打散，则其VC维为d。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc855bb20c1e.png" alt="12.png"></p>
<p>同时书中给出了假设空间VC维与增长函数的两个关系：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc855ba83eb8.png" alt="13.png"></p>
<p>直观来理解（1）式也十分容易： 首先假设空间的VC维是d，说明当m&lt;=d时，增长函数与2^m相等，例如：当m=d时，右边的组合数求和刚好等于2^d；而当m=d+1时，右边等于2^(d+1)-1，十分符合VC维的定义，同时也可以使用数学归纳法证明；（2）式则是由（1）式直接推导得出。</p>
<p>在有限假设空间中，根据Hoeffding不等式便可以推导得出学习算法的泛化误差界；但在无限假设空间中，由于假设空间的大小无法计算，只能通过增长函数来描述其复杂度，因此无限假设空间中的泛化误差界需要引入增长函数。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc855babc890.png" alt="14.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc855ba5b2c3.png" alt="15.png"></p>
<p>上式给出了基于VC维的泛化误差界，同时也可以计算出满足条件需要的样本数（样本复杂度）。若学习算法满足<strong>经验风险最小化原则（ERM）</strong>，即学习算法的输出假设h在数据集D上的经验误差最小，可证明：<strong>任何VC维有限的假设空间都是（不可知）PAC可学习的，换而言之：若假设空间的最小泛化误差为0即目标概念包含在假设空间中，则是PAC可学习，若最小泛化误差不为0，则称为不可知PAC可学习。</strong></p>
<h2 id="13-4-稳定性"><a href="#13-4-稳定性" class="headerlink" title="13.4 稳定性"></a><strong>13.4 稳定性</strong></h2><p>稳定性考察的是当算法的输入发生变化时，输出是否会随之发生较大的变化，输入的数据集D有以下两种变化：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc855badc5a8.png" alt="16.png"></p>
<p>若对数据集中的任何样本z，满足：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc855ba59b06.png" alt="17.png"></p>
<p>即原学习器和剔除一个样本后生成的学习器对z的损失之差保持β稳定，称学习器关于损失函数满足<strong>β-均匀稳定性</strong>。同时若损失函数有上界，即原学习器对任何样本的损失函数不超过M，则有如下定理：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc855babe7c3.png" alt="18.png"></p>
<p>事实上，<strong>若学习算法符合经验风险最小化原则（ERM）且满足β-均匀稳定性，则假设空间是可学习的</strong>。稳定性通过损失函数与假设空间的可学习联系在了一起，区别在于：假设空间关注的是经验误差与泛化误差，需要考虑到所有可能的假设；而稳定性只关注当前的输出假设。</p>
<p>在此，计算学习理论就介绍完毕，一看这个名字就知道这一章比较偏底层理论了，最终还是咬着牙看完了它，这里引用一段小文字来梳理一下现在的心情：“孤岂欲卿治经为博士邪？但当涉猎，见往事耳”，就当扩充知识体系吧~</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(13)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>上篇主要介绍了经典的降维方法与度量学习，首先从“维数灾难”导致的样本稀疏以及距离难计算两大难题出发，引出了降维的概念，即通过某种数学变换将原始高维空间转变到一个低维的子空间，接着分别介绍了kNN、MDS、PCA、KPCA以及两种经典的流形学习方法，k近邻算法的核心在于k值的选取以及距离的度量，MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持，主成分分析试图找到一个低维超平面来表出原空间样本点，核化主成分分析先将样本点映射到高维空间，再在高维空间中使用线性降维的方法，从而解决了原空间样本非线性分布的情形，基于流形学习的降维则是一种“邻域保持”的思想，最后度量学习试图去学习出一个距离度量来等效降维的效果。本篇将讨论另一种常用方法—特征选择与稀疏学习。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="12、特征选择与稀疏学习"><a href="#12、特征选择与稀疏学习" class="headerlink" title="12、特征选择与稀疏学习"></a><strong>12、特征选择与稀疏学习</strong></h1><p>最近在看论文的过程中，发现对于数据集行和列的叫法颇有不同，故在介绍本篇之前，决定先将最常用的术语罗列一二，以后再见到了不管它脚扑朔还是眼迷离就能一眼识破真身了~对于数据集中的一个对象及组成对象的零件元素：</p>
<blockquote>
<p>统计学家常称它们为<strong>观测</strong>（<strong>observation</strong>）和<strong>变量</strong>（<strong>variable</strong>）；<br>数据库分析师则称其为<strong>记录</strong>（<strong>record</strong>）和<strong>字段</strong>（<strong>field</strong>）；<br>数据挖掘/机器学习学科的研究者则习惯把它们叫做<strong>样本</strong>/<strong>示例</strong>（<strong>example</strong>/<strong>instance</strong>）和<strong>属性</strong>/<strong>特征</strong>（<strong>attribute</strong>/<strong>feature</strong>）。</p>
</blockquote>
<p>回归正题，在机器学习中特征选择是一个重要的“<strong>数据预处理</strong>”（<strong>data</strong> <strong>preprocessing</strong>）过程，即试图从数据集的所有特征中挑选出与当前学习任务相关的特征子集，接着再利用数据子集来训练学习器；稀疏学习则是围绕着稀疏矩阵的优良性质，来完成相应的学习任务。</p>
<h2 id="12-1-子集搜索与评价"><a href="#12-1-子集搜索与评价" class="headerlink" title="12.1 子集搜索与评价"></a><strong>12.1 子集搜索与评价</strong></h2><p>一般地，我们可以用很多属性/特征来描述一个示例，例如对于一个人可以用性别、身高、体重、年龄、学历、专业、是否吃货等属性来描述，那现在想要训练出一个学习器来预测人的收入。根据生活经验易知：并不是所有的特征都与学习任务相关，例如年龄/学历/专业可能很大程度上影响了收入，身高/体重这些外貌属性也有较小的可能性影响收入，但像是否是一个地地道道的吃货这种属性就八杆子打不着了。因此我们只需要那些与学习任务紧密相关的特征，<strong>特征选择便是从给定的特征集合中选出相关特征子集的过程</strong>。</p>
<p>与上篇中降维技术有着异曲同工之处的是，特征选择也可以有效地解决维数灾难的难题。具体而言：<strong>降维从一定程度起到了提炼优质低维属性和降噪的效果，特征选择则是直接剔除那些与学习任务无关的属性而选择出最佳特征子集</strong>。若直接遍历所有特征子集，显然当维数过多时遭遇指数爆炸就行不通了；若采取从候选特征子集中不断迭代生成更优候选子集的方法，则时间复杂度大大减小。这时就涉及到了两个关键环节：<strong>1.如何生成候选子集；2.如何评价候选子集的好坏</strong>，这便是早期特征选择的常用方法。书本上介绍了贪心算法，分为三种策略：</p>
<blockquote>
<p><strong>前向搜索</strong>：初始将每个特征当做一个候选特征子集，然后从当前所有的候选子集中选择出最佳的特征子集；接着在上一轮选出的特征子集中添加一个新的特征，同样地选出最佳特征子集；最后直至选不出比上一轮更好的特征子集。<br><strong>后向搜索</strong>：初始将所有特征作为一个候选特征子集；接着尝试去掉上一轮特征子集中的一个特征并选出当前最优的特征子集；最后直到选不出比上一轮更好的特征子集。<br><strong>双向搜索</strong>：将前向搜索与后向搜索结合起来，即在每一轮中既有添加操作也有剔除操作。</p>
</blockquote>
<p>对于特征子集的评价，书中给出了一些想法及基于信息熵的方法。假设数据集的属性皆为离散属性，这样给定一个特征子集，便可以通过这个特征子集的取值将数据集合划分为V个子集。例如：A1={男,女}，A2={本科,硕士}就可以将原数据集划分为2*2=4个子集，其中每个子集的取值完全相同。这时我们就可以像决策树选择划分属性那样，通过计算信息增益来评价该属性子集的好坏。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853eca1a43.png" alt="1.png"></p>
<p>此时，信息增益越大表示该属性子集包含有助于分类的特征越多，使用上述这种<strong>子集搜索与子集评价相结合的机制，便可以得到特征选择方法</strong>。值得一提的是若将前向搜索策略与信息增益结合在一起，与前面我们讲到的ID3决策树十分地相似。事实上，决策树也可以用于特征选择，树节点划分属性组成的集合便是选择出的特征子集。</p>
<h2 id="12-2-过滤式选择（Relief）"><a href="#12-2-过滤式选择（Relief）" class="headerlink" title="12.2 过滤式选择（Relief）"></a><strong>12.2 过滤式选择（Relief）</strong></h2><p>过滤式方法是一种将特征选择与学习器训练相分离的特征选择技术，即首先将相关特征挑选出来，再使用选择出的数据子集来训练学习器。Relief是其中著名的代表性算法，它使用一个“<strong>相关统计量</strong>”来度量特征的重要性，该统计量是一个向量，其中每个分量代表着相应特征的重要性，因此我们最终可以根据这个统计量各个分量的大小来选择出合适的特征子集。</p>
<p>易知Relief算法的核心在于如何计算出该相关统计量。对于数据集中的每个样例xi，Relief首先找出与xi同类别的最近邻与不同类别的最近邻，分别称为<strong>猜中近邻（near-hit）</strong>与<strong>猜错近邻（near-miss）</strong>，接着便可以分别计算出相关统计量中的每个分量。对于j分量：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ec70c88.png" alt="2.png"></p>
<p>直观上理解：对于猜中近邻，两者j属性的距离越小越好，对于猜错近邻，j属性距离越大越好。更一般地，若xi为离散属性，diff取海明距离，即相同取0，不同取1；若xi为连续属性，则diff为曼哈顿距离，即取差的绝对值。分别计算每个分量，最终取平均便得到了整个相关统计量。</p>
<p>标准的Relief算法只用于二分类问题，后续产生的拓展变体Relief-F则解决了多分类问题。对于j分量，新的计算公式如下：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ec93042.png" alt="3.png"></p>
<p>其中pl表示第l类样本在数据集中所占的比例，易知两者的不同之处在于：<strong>标准Relief 只有一个猜错近邻，而Relief-F有多个猜错近邻</strong>。</p>
<h2 id="12-3-包裹式选择（LVW）"><a href="#12-3-包裹式选择（LVW）" class="headerlink" title="12.3 包裹式选择（LVW）"></a><strong>12.3 包裹式选择（LVW）</strong></h2><p>与过滤式选择不同的是，包裹式选择将后续的学习器也考虑进来作为特征选择的评价准则。因此包裹式选择可以看作是为某种学习器<strong>量身定做</strong>的特征选择方法，由于在每一轮迭代中，包裹式选择都需要训练学习器，因此在获得较好性能的同时也产生了较大的开销。下面主要介绍一种经典的包裹式特征选择方法 —LVW（Las Vegas Wrapper），它在拉斯维加斯框架下使用随机策略来进行特征子集的搜索。拉斯维加斯？怎么听起来那么耳熟，不是那个声名显赫的赌场吗？歪果仁真会玩。怀着好奇科普一下，结果又顺带了一个赌场：</p>
<blockquote>
<p><strong>蒙特卡罗算法</strong>：采样越多，越近似最优解，一定会给出解，但给出的解不一定是正确解；<br><strong>拉斯维加斯算法</strong>：采样越多，越有机会找到最优解，不一定会给出解，且给出的解一定是正确解。</p>
</blockquote>
<p>举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找较好的，但不保证是最好的。</p>
<p>而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（正确解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。</p>
<p>LVW算法的具体流程如下所示，其中比较特别的是停止条件参数T的设置，即在每一轮寻找最优特征子集的过程中，若随机T次仍没找到，算法就会停止，从而保证了算法运行时间的可行性。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ed5e08e.png" alt="4.png"></p>
<h2 id="12-4-嵌入式选择与正则化"><a href="#12-4-嵌入式选择与正则化" class="headerlink" title="12.4 嵌入式选择与正则化"></a><strong>12.4 嵌入式选择与正则化</strong></h2><p>前面提到了的两种特征选择方法：<strong>过滤式中特征选择与后续学习器完全分离，包裹式则是使用学习器作为特征选择的评价准则；嵌入式是一种将特征选择与学习器训练完全融合的特征选择方法，即将特征选择融入学习器的优化过程中</strong>。在之前《经验风险与结构风险》中已经提到：经验风险指的是模型与训练数据的契合度，结构风险则是模型的复杂程度，机器学习的核心任务就是：<strong>在模型简单的基础上保证模型的契合度</strong>。例如：岭回归就是加上了L2范数的最小二乘法，有效地解决了奇异矩阵、过拟合等诸多问题，下面的嵌入式特征选择则是在损失函数后加上了L1范数。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ec8b203.png" alt="5.png"></p>
<p>L1范数美名又约<strong>Lasso Regularization</strong>，指的是向量中每个元素的绝对值之和，这样在优化目标函数的过程中，就会使得w尽可能地小，在一定程度上起到了防止过拟合的作用，同时与L2范数（Ridge Regularization ）不同的是，L1范数会使得部分w变为0， 从而达到了特征选择的效果。</p>
<p>总的来说：<strong>L1范数会趋向产生少量的特征，其他特征的权值都是0；L2会选择更多的特征，这些特征的权值都会接近于0</strong>。这样L1范数在特征选择上就十分有用，而L2范数则具备较强的控制过拟合能力。可以从下面两个方面来理解：</p>
<p>（1）<strong>下降速度</strong>：L1范数按照绝对值函数来下降，L2范数按照二次函数来下降。因此在0附近，L1范数的下降速度大于L2范数，故L1范数能很快地下降到0，而L2范数在0附近的下降速度非常慢，因此较大可能收敛在0的附近。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ed0aaf5.png" alt="6.png"></p>
<p>（2）<strong>空间限制</strong>：L1范数与L2范数都试图在最小化损失函数的同时，让权值W也尽可能地小。我们可以将原优化问题看做为下面的问题，即让后面的规则则都小于某个阈值。这样从图中可以看出：L1范数相比L2范数更容易得到稀疏解。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ecc223e.png" alt="7.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ed51aa1.png" alt="8.png"></p>
<h2 id="12-5-稀疏表示与字典学习"><a href="#12-5-稀疏表示与字典学习" class="headerlink" title="12.5 稀疏表示与字典学习"></a><strong>12.5 稀疏表示与字典学习</strong></h2><p>当样本数据是一个稀疏矩阵时，对学习任务来说会有不少的好处，例如很多问题变得线性可分，储存更为高效等。这便是稀疏表示与字典学习的基本出发点。稀疏矩阵即矩阵的每一行/列中都包含了大量的零元素，且这些零元素没有出现在同一行/列，对于一个给定的稠密矩阵，若我们能<strong>通过某种方法找到其合适的稀疏表示</strong>，则可以使得学习任务更加简单高效，我们称之为<strong>稀疏编码（sparse coding）</strong>或<strong>字典学习（dictionary learning）</strong>。</p>
<p>给定一个数据集，字典学习/稀疏编码指的便是通过一个字典将原数据转化为稀疏表示，因此最终的目标就是求得字典矩阵B及稀疏表示α，书中使用变量交替优化的策略能较好地求得解，深感陷进去短时间无法自拔，故先不进行深入…</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ed0ca43.png" alt="9.png"></p>
<h2 id="12-6-压缩感知"><a href="#12-6-压缩感知" class="headerlink" title="12.6 压缩感知"></a><strong>12.6 压缩感知</strong></h2><p>压缩感知在前些年也是风风火火，与特征选择、稀疏表示不同的是：它关注的是通过欠采样信息来恢复全部信息。在实际问题中，为了方便传输和存储，我们一般将数字信息进行压缩，这样就有可能损失部分信息，如何根据已有的信息来重构出全部信号，这便是压缩感知的来历，压缩感知的前提是已知的信息具有稀疏表示。下面是关于压缩感知的一些背景：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc853ed431c6.png" alt="10.png"></p>
<p>在此，特征选择与稀疏学习就介绍完毕。在很多实际情形中，选了好的特征比选了好的模型更为重要，这也是为什么厉害的大牛能够很快地得出一些结论的原因，谓：吾昨晚夜观天象，星象云是否吃货乃无用也~</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(15)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>上篇主要介绍了机器学习的理论基础，首先从独立同分布引入泛化误差与经验误差，接着介绍了PAC可学习的基本概念，即以较大的概率学习出与目标概念近似的假设（泛化误差满足预设上限），对于有限假设空间：（1）可分情形时，假设空间都是PAC可学习的，即当样本满足一定的数量之后，总是可以在与训练集一致的假设中找出目标概念的近似；（2）不可分情形时，假设空间都是不可知PAC可学习的，即以较大概率学习出与当前假设空间中泛化误差最小的假设的有效近似（Hoeffding不等式）。对于无限假设空间，通过增长函数与VC维来描述其复杂度，若学习算法满足经验风险最小化原则，则任何VC维有限的假设空间都是（不可知）PAC可学习的，同时也给出了泛化误差界与样本复杂度。稳定性则考察的是输入发生变化时输出的波动，稳定性通过损失函数与假设空间的可学习理论联系在了一起。本篇将讨论一种介于监督与非监督学习之间的学习算法—半监督学习。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="14、半监督学习"><a href="#14、半监督学习" class="headerlink" title="14、半监督学习"></a><strong>14、半监督学习</strong></h1><p>前面我们一直围绕的都是监督学习与无监督学习，监督学习指的是训练样本包含标记信息的学习任务，例如：常见的分类与回归算法；无监督学习则是训练样本不包含标记信息的学习任务，例如：聚类算法。在实际生活中，常常会出现一部分样本有标记和较多样本无标记的情形，例如：做网页推荐时需要让用户标记出感兴趣的网页，但是少有用户愿意花时间来提供标记。若直接丢弃掉无标记样本集，使用传统的监督学习方法，常常会由于训练样本的不充足，使得其刻画总体分布的能力减弱，从而影响了学习器泛化性能。那如何利用未标记的样本数据呢？</p>
<p>一种简单的做法是通过专家知识对这些未标记的样本进行打标，但随之而来的就是巨大的人力耗费。若我们先使用有标记的样本数据集训练出一个学习器，再基于该学习器对未标记的样本进行预测，从中<strong>挑选出不确定性高或分类置信度低的样本来咨询专家并进行打标</strong>，最后使用扩充后的训练集重新训练学习器，这样便能大幅度降低标记成本，这便是<strong>主动学习</strong>（active learning），其目标是<strong>使用尽量少的/有价值的咨询来获得更好的性能</strong>。</p>
<p>显然，<strong>主动学习需要与外界进行交互/查询/打标，其本质上仍然属于一种监督学习</strong>。事实上，无标记样本虽未包含标记信息，但它们与有标记样本一样都是从总体中独立同分布采样得到，因此<strong>它们所包含的数据分布信息对学习器的训练大有裨益</strong>。如何让学习过程不依赖外界的咨询交互，自动利用未标记样本所包含的分布信息的方法便是<strong>半监督学习</strong>（semi-supervised learning），<strong>即训练集同时包含有标记样本数据和未标记样本数据</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc856e39801d.png" alt="1.png"></p>
<p>此外，半监督学习还可以进一步划分为<strong>纯半监督学习</strong>和<strong>直推学习</strong>，两者的区别在于：前者假定训练数据集中的未标记数据并非待预测数据，而后者假定学习过程中的未标记数据就是待预测数据。主动学习、纯半监督学习以及直推学习三者的概念如下图所示：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwJFJS.png" alt="iwJFJS.png"></p>
<h2 id="14-1-生成式方法"><a href="#14-1-生成式方法" class="headerlink" title="14.1 生成式方法"></a><strong>14.1 生成式方法</strong></h2><p><strong>生成式方法</strong>（generative methods）是基于生成式模型的方法，即先对联合分布P（x,c）建模，从而进一步求解 P（c | x），<strong>此类方法假定样本数据服从一个潜在的分布，因此需要充分可靠的先验知识</strong>。例如：前面已经接触到的贝叶斯分类器与高斯混合聚类，都属于生成式模型。现假定总体是一个高斯混合分布，即由多个高斯分布组合形成，从而一个子高斯分布就代表一个类簇（类别）。高斯混合分布的概率密度函数如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc856e3b82dc.png" alt="3.png"></p>
<p>不失一般性，假设类簇与真实的类别按照顺序一一对应，即第i个类簇对应第i个高斯混合成分。与高斯混合聚类类似地，这里的主要任务也是估计出各个高斯混合成分的参数以及混合系数，不同的是：对于有标记样本，不再是可能属于每一个类簇，而是只能属于真实类标对应的特定类簇。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc856e431d30.png" alt="4.png"></p>
<p>直观上来看，<strong>基于半监督的高斯混合模型有机地整合了贝叶斯分类器与高斯混合聚类的核心思想</strong>，有效地利用了未标记样本数据隐含的分布信息，从而使得参数的估计更加准确。同样地，这里也要召唤出之前的EM大法进行求解，首先对各个高斯混合成分的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类，有标记样本则直接属于特定类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc856e43ff08.png" alt="5.png"></p>
<p>当参数迭代更新收敛后，对于待预测样本x，便可以像贝叶斯分类器那样计算出样本属于每个类簇的后验概率，接着找出概率最大的即可：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc856e3dfb1c.png" alt="6.png"></p>
<p>可以看出：基于生成式模型的方法十分依赖于对潜在数据分布的假设，即假设的分布要能和真实分布相吻合，否则利用未标记的样本数据反倒会在错误的道路上渐行渐远，从而降低学习器的泛化性能。因此，<strong>此类方法要求极强的领域知识和掐指观天的本领</strong>。</p>
<h2 id="14-2-半监督SVM"><a href="#14-2-半监督SVM" class="headerlink" title="14.2 半监督SVM"></a><strong>14.2 半监督SVM</strong></h2><p>监督学习中的SVM试图找到一个划分超平面，使得两侧支持向量之间的间隔最大，即“<strong>最大划分间隔</strong>”思想。对于半监督学习，S3VM则考虑超平面需穿过数据低密度的区域。TSVM是半监督支持向量机中的最著名代表，其核心思想是：尝试为未标记样本找到合适的标记指派，使得超平面划分后的间隔最大化。TSVM采用局部搜索的策略来进行迭代求解，即首先使用有标记样本集训练出一个初始SVM，接着使用该学习器对未标记样本进行打标，这样所有样本都有了标记，并基于这些有标记的样本重新训练SVM，之后再寻找易出错样本不断调整。整个算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc856e427830.png" alt="7.png"></p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwJZss.png" alt="iwJZss.png"></p>
<h2 id="14-3-基于分歧的方法"><a href="#14-3-基于分歧的方法" class="headerlink" title="14.3 基于分歧的方法"></a><strong>14.3 基于分歧的方法</strong></h2><p>基于分歧的方法通过多个学习器之间的<strong>分歧（disagreement）/多样性（diversity）</strong>来利用未标记样本数据，协同训练就是其中的一种经典方法。<strong>协同训练最初是针对于多视图（multi-view）数据而设计的，多视图数据指的是样本对象具有多个属性集，每个属性集则对应一个试图</strong>。例如：电影数据中就包含画面类属性和声音类属性，这样画面类属性的集合就对应着一个视图。首先引入两个关于视图的重要性质：</p>
<blockquote>
<p><strong>相容性</strong>：即使用单个视图数据训练出的学习器的输出空间是一致的。例如都是{好，坏}、{+1,-1}等。<br><strong>互补性</strong>：即不同视图所提供的信息是互补/相辅相成的，实质上这里体现的就是集成学习的思想。</p>
</blockquote>
<p>协同训练正是很好地利用了多视图数据的“<strong>相容互补性</strong>”，其基本的思想是：首先基于有标记样本数据在每个视图上都训练一个初始分类器，然后让每个分类器去挑选分类置信度最高的样本并赋予标记，并将带有伪标记的样本数据传给另一个分类器去学习，从而<strong>你依我侬/共同进步</strong>。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwJVMj.png" alt="iwJVMj.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwJeLn.png" alt="iwJeLn.png"></p>
<h2 id="14-4-半监督聚类"><a href="#14-4-半监督聚类" class="headerlink" title="14.4 半监督聚类"></a><strong>14.4 半监督聚类</strong></h2><p>前面提到的几种方法都是借助无标记样本数据来辅助监督学习的训练过程，从而使得学习更加充分/泛化性能得到提升；半监督聚类则是借助已有的监督信息来辅助聚类的过程。一般而言，监督信息大致有两种类型：</p>
<blockquote>
<p><strong>必连与勿连约束</strong>：必连指的是两个样本必须在同一个类簇，勿连则是必不在同一个类簇。<br><strong>标记信息</strong>：少量的样本带有真实的标记。</p>
</blockquote>
<p>下面主要介绍两种基于半监督的K-Means聚类算法：第一种是数据集包含一些必连与勿连关系，另外一种则是包含少量带有标记的样本。两种算法的基本思想都十分的简单：对于带有约束关系的k-均值算法，在迭代过程中对每个样本划分类簇时，需要<strong>检测当前划分是否满足约束关系</strong>，若不满足则会将该样本划分到距离次小对应的类簇中，再继续检测是否满足约束关系，直到完成所有样本的划分。算法流程如下图所示：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwJAzQ.png" alt="iwJAzQ.png"></p>
<p>对于带有少量标记样本的k-均值算法，则可以<strong>利用这些有标记样本进行类中心的指定，同时在对样本进行划分时，不需要改变这些有标记样本的簇隶属关系</strong>，直接将其划分到对应类簇即可。算法流程如下所示：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwJkRg.png" alt="iwJkRg.png"></p>
<p>在此，半监督学习就介绍完毕。十分有趣的是：半监督学习将前面许多知识模块联系在了一起，足以体现了作者编排的用心。结合本篇的新知识再来回想之前自己做过的一些研究，发现还是蹚了一些浑水，也许越是觉得过去的自己傻，越就是好的兆头吧~</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(16)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>上篇主要介绍了半监督学习，首先从如何利用未标记样本所蕴含的分布信息出发，引入了半监督学习的基本概念，即训练数据同时包含有标记样本和未标记样本的学习方法；接着分别介绍了几种常见的半监督学习方法：生成式方法基于对数据分布的假设，利用未标记样本隐含的分布信息，使得对模型参数的估计更加准确；TSVM给未标记样本赋予伪标记，并通过不断调整易出错样本的标记得到最终输出；基于分歧的方法结合了集成学习的思想，通过多个学习器在不同视图上的协作，有效利用了未标记样本数据 ；最后半监督聚类则是借助已有的监督信息来辅助聚类的过程，带约束k-均值算法需检测当前样本划分是否满足约束关系，带标记k-均值算法则利用有标记样本指定初始类中心。本篇将讨论一种基于图的学习算法—概率图模型。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="15、概率图模型"><a href="#15、概率图模型" class="headerlink" title="15、概率图模型"></a><strong>15、概率图模型</strong></h1><p>现在再来谈谈机器学习的核心价值观，可以更通俗地理解为：<strong>根据一些已观察到的证据来推断未知</strong>，更具哲学性地可以阐述为：未来的发展总是遵循着历史的规律。其中<strong>基于概率的模型将学习任务归结为计算变量的概率分布</strong>，正如之前已经提到的：生成式模型先对联合分布进行建模，从而再来求解后验概率，例如：贝叶斯分类器先对联合分布进行最大似然估计，从而便可以计算类条件概率；判别式模型则是直接对条件分布进行建模。</p>
<p><strong>概率图模型</strong>（probabilistic graphical model）是一类用<strong>图结构</strong>来表达各属性之间相关关系的概率模型，一般而言：<strong>图中的一个结点表示一个或一组随机变量，结点之间的边则表示变量间的相关关系</strong>，从而形成了一张“<strong>变量关系图</strong>”。若使用有向的边来表达变量之间的依赖关系，这样的有向关系图称为<strong>贝叶斯网</strong>（Bayesian nerwork）或有向图模型；若使用无向边，则称为<strong>马尔可夫网</strong>（Markov network）或无向图模型。</p>
<h2 id="15-1-隐马尔可夫模型-HMM"><a href="#15-1-隐马尔可夫模型-HMM" class="headerlink" title="15.1 隐马尔可夫模型(HMM)"></a><strong>15.1 隐马尔可夫模型(HMM)</strong></h2><p>隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的一种贝叶斯网，在语音识别与自然语言处理领域上有着广泛的应用。HMM中的变量分为两组：<strong>状态变量</strong>与<strong>观测变量</strong>，其中状态变量一般是未知的，因此又称为“<strong>隐变量</strong>”，观测变量则是已知的输出值。在隐马尔可夫模型中，变量之间的依赖关系遵循如下两个规则：</p>
<blockquote>
<p><strong>1. 观测变量的取值仅依赖于状态变量</strong>；<br><strong>2. 下一个状态的取值仅依赖于当前状态</strong>，通俗来讲：<strong>现在决定未来，未来与过去无关</strong>，这就是著名的<strong>马尔可夫性</strong>。</p>
</blockquote>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYPmR.png" alt="iwYPmR.png"></p>
<p>基于上述变量之间的依赖关系，我们很容易写出隐马尔可夫模型中所有变量的联合概率分布：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwY9X9.png" alt="iwY9X9.png"></p>
<p>易知：<strong>欲确定一个HMM模型需要以下三组参数</strong>：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYi01.png" alt="iwYi01.png"></p>
<p>当确定了一个HMM模型的三个参数后，便按照下面的规则来生成观测值序列：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYFTx.png" alt="iwYFTx.png"></p>
<p>在实际应用中，HMM模型的发力点主要体现在下述三个问题上：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYEtK.png" alt="iwYEtK.png"></p>
<h3 id="15-1-1-HMM评估问题"><a href="#15-1-1-HMM评估问题" class="headerlink" title="15.1.1 HMM评估问题"></a><strong>15.1.1 HMM评估问题</strong></h3><p>HMM评估问题指的是：<strong>给定了模型的三个参数与观测值序列，求该观测值序列出现的概率</strong>。例如：对于赌场问题，便可以依据骰子掷出的结果序列来计算该结果序列出现的可能性，若小概率的事件发生了则可认为赌场的骰子有作弊的可能。解决该问题使用的是<strong>前向算法</strong>，即步步为营，自底向上的方式逐步增加序列的长度，直到获得目标概率值。在前向算法中，定义了一个<strong>前向变量</strong>，即给定观察值序列且t时刻的状态为Si的概率：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYVfO.png" alt="iwYVfO.png"></p>
<p>基于前向变量，很容易得到该问题的递推关系及终止条件：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYAk6.png" alt="iwYAk6.png"></p>
<p>因此可使用动态规划法，从最小的子问题开始，通过填表格的形式一步一步计算出目标结果。</p>
<h3 id="15-1-2-HMM解码问题"><a href="#15-1-2-HMM解码问题" class="headerlink" title="15.1.2 HMM解码问题"></a><strong>15.1.2 HMM解码问题</strong></h3><p>HMM解码问题指的是：<strong>给定了模型的三个参数与观测值序列，求可能性最大的状态序列</strong>。例如：在语音识别问题中，人说话形成的数字信号对应着观测值序列，对应的具体文字则是状态序列，从数字信号转化为文字正是对应着根据观测值序列推断最有可能的状态值序列。解决该问题使用的是<strong>Viterbi算法</strong>，与前向算法十分类似地，Viterbi算法定义了一个<strong>Viterbi变量</strong>，也是采用动态规划的方法，自底向上逐步求解。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYepD.png" alt="iwYepD.png"></p>
<h3 id="15-1-3-HMM学习问题"><a href="#15-1-3-HMM学习问题" class="headerlink" title="15.1.3 HMM学习问题"></a><strong>15.1.3 HMM学习问题</strong></h3><p>HMM学习问题指的是：<strong>给定观测值序列，如何调整模型的参数使得该序列出现的概率最大</strong>。这便转化成了机器学习问题，即从给定的观测值序列中学习出一个HMM模型，<strong>该问题正是EM算法的经典案例之一</strong>。其思想也十分简单：对于给定的观测值序列，如果我们能够按照该序列潜在的规律来调整模型的三个参数，则可以使得该序列出现的可能性最大。假设状态值序列也已知，则很容易计算出与该序列最契合的模型参数：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYm1e.png" alt="iwYm1e.png"></p>
<p>但一般状态值序列都是不可观测的，且<strong>即使给定观测值序列与模型参数，状态序列仍然遭遇组合爆炸</strong>。因此上面这种简单的统计方法就行不通了，若将状态值序列看作为隐变量，这时便可以考虑使用EM算法来对该问题进行求解：</p>
<p>【1】首先对HMM模型的三个参数进行随机初始化；<br>【2】根据模型的参数与观测值序列，计算t时刻状态为i且t+1时刻状态为j的概率以及t时刻状态为i的概率。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYn6H.png" alt="iwYn6H.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYdns.png" alt="iwYdns.png"></p>
<p>【3】接着便可以对模型的三个参数进行重新估计：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYY9S.png" alt="iwYY9S.png"></p>
<p>【4】重复步骤2-3，直至三个参数值收敛，便得到了最终的HMM模型。</p>
<h2 id="15-2-马尔可夫随机场（MRF）"><a href="#15-2-马尔可夫随机场（MRF）" class="headerlink" title="15.2 马尔可夫随机场（MRF）"></a><strong>15.2 马尔可夫随机场（MRF）</strong></h2><p>马尔可夫随机场（Markov Random Field）是一种典型的马尔可夫网，即使用无向边来表达变量间的依赖关系。在马尔可夫随机场中，对于关系图中的一个子集，<strong>若任意两结点间都有边连接，则称该子集为一个团；若再加一个结点便不能形成团，则称该子集为极大团</strong>。MRF使用<strong>势函数</strong>来定义多个变量的概率分布函数，其中<strong>每个（极大）团对应一个势函数</strong>，一般团中的变量关系也体现在它所对应的极大团中，因此常常基于极大团来定义变量的联合概率分布函数。具体而言，若所有变量构成的极大团的集合为C，则MRF的联合概率函数可以定义为：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYGh8.png" alt="iwYGh8.png"></p>
<p>对于条件独立性，<strong>马尔可夫随机场通过分离集来实现条件独立</strong>，若A结点集必须经过C结点集才能到达B结点集，则称C为分离集。书上给出了一个简单情形下的条件独立证明过程，十分贴切易懂，此处不再展开。基于分离集的概念，得到了MRF的三个性质：</p>
<blockquote>
<p><strong>全局马尔可夫性</strong>：给定两个变量子集的分离集，则这两个变量子集条件独立。<br><strong>局部马尔可夫性</strong>：给定某变量的邻接变量，则该变量与其它变量条件独立。<br><strong>成对马尔可夫性</strong>：给定所有其他变量，两个非邻接变量条件独立。</p>
</blockquote>
<p><img src="https://s1.ax1x.com/2018/10/18/iwY07q.png" alt="iwY07q.png"></p>
<p>对于MRF中的势函数，势函数主要用于描述团中变量之间的相关关系，且要求为非负函数，直观来看：势函数需要在偏好的变量取值上函数值较大，例如：若x1与x2成正相关，则需要将这种关系反映在势函数的函数值中。一般我们常使用指数函数来定义势函数：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwY8tf.png" alt="iwY8tf.png"></p>
<h2 id="15-3-条件随机场（CRF）"><a href="#15-3-条件随机场（CRF）" class="headerlink" title="15.3 条件随机场（CRF）"></a><strong>15.3 条件随机场（CRF）</strong></h2><p>前面所讲到的<strong>隐马尔可夫模型和马尔可夫随机场都属于生成式模型，即对联合概率进行建模，条件随机场则是对条件分布进行建模</strong>。CRF试图在给定观测值序列后，对状态序列的概率分布进行建模，即P(y | x)。直观上看：CRF与HMM的解码问题十分类似，都是在给定观测值序列后，研究状态序列可能的取值。CRF可以有多种结构，只需保证状态序列满足马尔可夫性即可，一般我们常使用的是<strong>链式条件随机场</strong>：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYt1g.png" alt="iwYt1g.png"></p>
<p>与马尔可夫随机场定义联合概率类似地，CRF也通过团以及势函数的概念来定义条件概率P(y | x)。在给定观测值序列的条件下，链式条件随机场主要包含两种团结构：单个状态团及相邻状态团，通过引入两类特征函数便可以定义出目标条件概率：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYNcQ.png" alt="iwYNcQ.png"></p>
<p>以词性标注为例，如何判断给出的一个标注序列靠谱不靠谱呢？<strong>转移特征函数主要判定两个相邻的标注是否合理</strong>，例如：动词+动词显然语法不通；<strong>状态特征函数则判定观测值与对应的标注是否合理</strong>，例如： ly结尾的词—&gt;副词较合理。因此我们可以定义一个特征函数集合，用这个特征函数集合来为一个标注序列打分，并据此选出最靠谱的标注序列。也就是说，每一个特征函数（对应一种规则）都可以用来为一个标注序列评分，把集合中所有特征函数对同一个标注序列的评分综合起来，就是这个标注序列最终的评分值。可以看出：<strong>特征函数是一些经验的特性</strong>。</p>
<h2 id="15-4-学习与推断"><a href="#15-4-学习与推断" class="headerlink" title="15.4 学习与推断"></a><strong>15.4 学习与推断</strong></h2><p>对于生成式模型，通常我们都是先对变量的联合概率分布进行建模，接着再求出目标变量的<strong>边际分布</strong>（marginal distribution），那如何从联合概率得到边际分布呢？这便是学习与推断。下面主要介绍两种精确推断的方法：<strong>变量消去</strong>与<strong>信念传播</strong>。</p>
<h3 id="15-4-1-变量消去"><a href="#15-4-1-变量消去" class="headerlink" title="15.4.1 变量消去"></a><strong>15.4.1 变量消去</strong></h3><p>变量消去利用条件独立性来消减计算目标概率值所需的计算量，它通过运用<strong>乘法与加法的分配率</strong>，将对变量的积的求和问题转化为对部分变量交替进行求积与求和的问题，从而将每次的<strong>运算控制在局部</strong>，达到简化运算的目的。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYUXj.png" alt="iwYUXj.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYwBn.png" alt="iwYwBn.png"></p>
<h3 id="15-4-2-信念传播"><a href="#15-4-2-信念传播" class="headerlink" title="15.4.2 信念传播"></a><strong>15.4.2 信念传播</strong></h3><p>若将变量求和操作看作是一种消息的传递过程，信念传播可以理解成：<strong>一个节点在接收到所有其它节点的消息后才向另一个节点发送消息</strong>，同时当前节点的边际概率正比于他所接收的消息的乘积：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYDA0.png" alt="iwYDA0.png"></p>
<p>因此只需要经过下面两个步骤，便可以完成所有的消息传递过程。利用动态规划法的思想记录传递过程中的所有消息，当计算某个结点的边际概率分布时，只需直接取出传到该结点的消息即可，从而避免了计算多个边际分布时的冗余计算问题。</p>
<blockquote>
<p>1.指定一个根节点，从所有的叶节点开始向根节点传递消息，直到根节点收到所有邻接结点的消息<strong>（从叶到根）</strong>；<br>2.从根节点开始向叶节点传递消息，直到所有叶节点均收到消息<strong>（从根到叶）</strong>。</p>
</blockquote>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYgc4.png" alt="iwYgc4.png"></p>
<h2 id="15-5-LDA话题模型"><a href="#15-5-LDA话题模型" class="headerlink" title="15.5 LDA话题模型"></a><strong>15.5 LDA话题模型</strong></h2><p>话题模型主要用于处理文本类数据，其中<strong>隐狄利克雷分配模型</strong>（Latent Dirichlet Allocation，简称LDA）是话题模型的杰出代表。在话题模型中，有以下几个基本概念：词（word）、文档（document）、话题（topic）。</p>
<blockquote>
<p><strong>词</strong>：最基本的离散单元；<br><strong>文档</strong>：由一组词组成，词在文档中不计顺序；<br><strong>话题</strong>：由一组特定的词组成，这组词具有较强的相关关系。</p>
</blockquote>
<p>在现实任务中，一般我们可以得出一个文档的词频分布，但不知道该文档对应着哪些话题，LDA话题模型正是为了解决这个问题。具体来说：<strong>LDA认为每篇文档包含多个话题，且其中每一个词都对应着一个话题</strong>。因此可以假设文档是通过如下方式生成：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwY2jJ.png" alt="iwY2jJ.png"></p>
<p>这样一个文档中的所有词都可以认为是通过话题模型来生成的，当已知一个文档的词频分布后（即一个N维向量，N为词库大小），则可以认为：<strong>每一个词频元素都对应着一个话题，而话题对应的词频分布则影响着该词频元素的大小</strong>。因此很容易写出LDA模型对应的联合概率函数：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYc3F.png" alt="iwYc3F.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYWu9.png" alt="iwYWu9.png"></p>
<p>从上图可以看出，LDA的三个表示层被三种颜色表示出来：</p>
<blockquote>
<p><strong>corpus-level（红色）：</strong> α和β表示语料级别的参数，也就是每个文档都一样，因此生成过程只采样一次。<br><strong>document-level（橙色）：</strong> θ是文档级别的变量，每个文档对应一个θ。<br><strong>word-level（绿色）：</strong> z和w都是单词级别变量，z由θ生成，w由z和β共同生成，一个单词w对应一个主题z。</p>
</blockquote>
<p>通过上面对LDA生成模型的讨论，可以知道<strong>LDA模型主要是想从给定的输入语料中学习训练出两个控制参数α和β</strong>，当学习出了这两个控制参数就确定了模型，便可以用来生成文档。其中α和β分别对应以下各个信息：</p>
<blockquote>
<p><strong>α</strong>：分布p(θ)需要一个向量参数，即Dirichlet分布的参数，用于生成一个主题θ向量；<br><strong>β</strong>：各个主题对应的单词概率分布矩阵p(w|z)。</p>
</blockquote>
<p>把w当做观察变量，θ和z当做隐藏变量，就可以通过EM算法学习出α和β，求解过程中遇到后验概率p(θ,z|w)无法直接求解，需要找一个似然函数下界来近似求解，原作者使用基于分解（factorization）假设的变分法（varialtional inference）进行计算，用到了EM算法。每次E-step输入α和β，计算似然函数，M-step最大化这个似然函数，算出α和β，不断迭代直到收敛。</p>
<p>在此，概率图模型就介绍完毕。上周受到协同训练的启发，让实验的小伙伴做了一个HMM的slides，结果扩充了好多知识，所以完成这篇笔记还是花费了不少功夫，还刚好赶上实验室没空调回到解放前的日子，可谓汗流之作…</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(2)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/</url>
    <content><![CDATA[<p>本篇主要是对第二章剩余知识的理解，包括：性能度量、比较检验和偏差与方差。在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。写到后面发现冗长之后读起来十分没有快感，故本篇主要知识点为性能度量。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<p><strong>2.5 性能度量</strong></p>
<p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p>
<p><strong>2.5.1 最常见的性能度量</strong></p>
<p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf76276.png" alt="1.png"></p>
<p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf4c704.png" alt="2.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf6fb84.png" alt="3.png"></p>
<p><strong>2.5.2 查准率/查全率/F1</strong></p>
<p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf885a4.png" alt="4.png"></p>
<p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf871a6.png" alt="5.png"></p>
<p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p>
<p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71dafc4411.png" alt="6.png"></p>
<p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。</p>
<p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf40ff6.png" alt="7.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf75407.png" alt="8.png"></p>
<p>特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf20885.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf4b90a.png" alt="10.png"></p>
<p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed70230e.png" alt="11.png"></p>
<p><strong>2.5.3 ROC与AUC</strong></p>
<p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6bee91.png" alt="12.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed75cefe.png" alt="13.png"></p>
<p>简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p>
<p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed740a24.png" alt="14.png"></p>
<p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p>
<p><strong>2.5.4 代价敏感错误率与代价曲线</strong></p>
<p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病—&gt;有疾病只是增多了检查，但有疾病—&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p>
<p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p>
<p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p>
<p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p>
<p>在此模型的性能度量方法就介绍完了，以前一直以为均方误差和精准度就可以了，现在才发现天空如此广阔~</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(12)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>上篇主要介绍了几种常用的聚类算法，首先从距离度量与性能评估出发，列举了常见的距离计算公式与聚类评价指标，接着分别讨论了K-Means、LVQ、高斯混合聚类、密度聚类以及层次聚类算法。K-Means与LVQ都试图以类簇中心作为原型指导聚类，其中K-Means通过EM算法不断迭代直至收敛，LVQ使用真实类标辅助聚类；高斯混合聚类采用高斯分布来描述类簇原型；密度聚类则是将一个核心对象所有密度可达的样本形成类簇，直到所有核心对象都遍历完；最后层次聚类是一种自底向上的树形聚类方法，不断合并最相近的两个小类簇。本篇将讨论机器学习常用的方法—降维与度量学习。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="11、降维与度量学习"><a href="#11、降维与度量学习" class="headerlink" title="11、降维与度量学习"></a><strong>11、降维与度量学习</strong></h1><p>样本的特征数称为<strong>维数</strong>（dimensionality），当维数非常大时，也就是现在所说的“<strong>维数灾难</strong>”，具体表现在：在高维情形下，<strong>数据样本将变得十分稀疏</strong>，因为此时要满足训练样本为“<strong>密采样</strong>”的总体样本数目是一个触不可及的天文数字，谓可远观而不可亵玩焉…<strong>训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力</strong>；同时当维数很高时，<strong>计算距离也变得十分复杂</strong>，甚至连计算内积都不再容易，这也是为什么支持向量机（SVM）使用核函数<strong>“低维计算，高维表现”</strong>的原因。</p>
<p>缓解维数灾难的一个重要途径就是<strong>降维，即通过某种数学变换将原始高维空间转变到一个低维的子空间</strong>。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这时也许会有疑问，这样降维之后不是会丢失原始数据的一部分信息吗？这是因为在很多实际的问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个<strong>低维嵌入</strong>，例如：数据属性中存在噪声属性、相似属性或冗余属性等，<strong>对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果</strong>。</p>
<h2 id="11-1-K近邻学习"><a href="#11-1-K近邻学习" class="headerlink" title="11.1 K近邻学习"></a><strong>11.1 K近邻学习</strong></h2><p>k近邻算法简称<strong>kNN（k-Nearest Neighbor）</strong>，是一种经典的监督学习方法，同时也实力担当入选数据挖掘十大算法。其工作机制十分简单粗暴：给定某个测试样本，kNN基于某种<strong>距离度量</strong>在训练集中找出与其距离最近的k个带有真实标记的训练样本，然后给基于这k个邻居的真实标记来进行预测，类似于前面集成学习中所讲到的基学习器结合策略：分类任务采用投票法，回归任务则采用平均法。接下来本篇主要就kNN分类进行讨论。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a43873a.png" alt="1.png"></p>
<p>从上图【来自Wiki】中我们可以看到，图中有两种类型的样本，一类是蓝色正方形，另一类是红色三角形。而那个绿色圆形是我们待分类的样本。基于kNN算法的思路，我们很容易得到以下结论：</p>
<blockquote>
<p>如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。<br>如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。</p>
</blockquote>
<p>可以发现：<strong>kNN虽然是一种监督学习方法，但是它却没有显式的训练过程</strong>，而是当有新样本需要预测时，才来计算出最近的k个邻居，因此<strong>kNN是一种典型的懒惰学习方法</strong>，再来回想一下朴素贝叶斯的流程，训练的过程就是参数估计，因此朴素贝叶斯也可以懒惰式学习，此类技术在<strong>训练阶段开销为零</strong>，待收到测试样本后再进行计算。相应地我们称那些一有训练数据立马开工的算法为“<strong>急切学习</strong>”，可见前面我们学习的大部分算法都归属于急切学习。</p>
<p>很容易看出：<strong>kNN算法的核心在于k值的选取以及距离的度量</strong>。k值选取太小，模型很容易受到噪声数据的干扰，例如：极端地取k=1，若待分类样本正好与一个噪声数据距离最近，就导致了分类错误；若k值太大， 则在更大的邻域内进行投票，此时模型的预测能力大大减弱，例如：极端取k=训练样本数，就相当于模型根本没有学习，所有测试样本的预测结果都是一样的。<strong>一般地我们都通过交叉验证法来选取一个适当的k值</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a47db9a.png" alt="2.png"></p>
<p>对于距离度量，<strong>不同的度量方法得到的k个近邻不尽相同，从而对最终的投票结果产生了影响</strong>，因此选择一个合适的距离度量方法也十分重要。在上一篇聚类算法中，在度量样本相似性时介绍了常用的几种距离计算方法，包括<strong>闵可夫斯基距离，曼哈顿距离，VDM</strong>等。在实际应用中，<strong>kNN的距离度量函数一般根据样本的特性来选择合适的距离度量，同时应对数据进行去量纲/归一化处理来消除大量纲属性的强权政治影响</strong>。</p>
<h2 id="11-2-MDS算法"><a href="#11-2-MDS算法" class="headerlink" title="11.2 MDS算法"></a><strong>11.2 MDS算法</strong></h2><p>不管是使用核函数升维还是对数据降维，我们都希望<strong>原始空间样本点之间的距离在新空间中基本保持不变</strong>，这样才不会使得原始空间样本之间的关系及总体分布发生较大的改变。<strong>“多维缩放”（MDS）</strong>正是基于这样的思想，<strong>MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持</strong>。</p>
<p>假定m个样本在原始空间中任意两两样本之间的距离矩阵为D∈R(m<em>m)，我们的目标便是获得样本在低维空间中的表示Z∈R(d’</em>m , d’&lt; d)，且任意两个样本在低维空间中的欧式距离等于原始空间中的距离，即||zi-zj||=Dist(ij)。因此接下来我们要做的就是根据已有的距离矩阵D来求解出降维后的坐标矩阵Z。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a4b664e.png" alt="3.png"></p>
<p>令降维后的样本坐标矩阵Z被中心化，<strong>中心化是指将每个样本向量减去整个样本集的均值向量，故所有样本向量求和得到一个零向量</strong>。这样易知：矩阵B的每一列以及每一列求和均为0，因为提取公因子后都有一项为所有样本向量的和向量。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a4a4ee2.png" alt="4.png"></p>
<p>根据上面矩阵B的特征，我们很容易得到等式（2）、（3）以及（4）：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a4a777b.png" alt="5.png"></p>
<p>这时根据(1)—(4)式我们便可以计算出bij，即<strong>bij=(1)-(2)<em>(1/m)-(3)</em>(1/m)+(4)*(1/(m^2))</strong>，再逐一地计算每个b(ij)，就得到了降维后低维空间中的内积矩阵B(B=Z’*Z)，只需对B进行特征值分解便可以得到Z。MDS的算法流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a5340dd.png" alt="6.png"></p>
<h2 id="11-3-主成分分析（PCA）"><a href="#11-3-主成分分析（PCA）" class="headerlink" title="11.3 主成分分析（PCA）"></a><strong>11.3 主成分分析（PCA）</strong></h2><p>不同于MDS采用距离保持的方法，<strong>主成分分析（PCA）直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中</strong>。简单来理解这一过程便是：<strong>PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。</strong></p>
<p>假设使用d’个新基向量来表示原来样本，实质上是将样本投影到一个由d’个基向量确定的一个<strong>超平面</strong>上（<strong>即舍弃了一些维度</strong>），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：<strong>若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来</strong>。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：</p>
<blockquote>
<p><strong>最近重构性</strong>：样本点到超平面的距离足够近，即尽可能在超平面附近；<br><strong>最大可分性</strong>：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。</p>
</blockquote>
<p>这里十分神奇的是：<strong>最近重构性与最大可分性虽然从不同的出发点来定义优化问题中的目标函数，但最终这两种特性得到了完全相同的优化问题</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a5213c1.png" alt="7.png"></p>
<p>接着使用拉格朗日乘子法求解上面的优化问题，得到：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a4a102a.png" alt="8.png"></p>
<p>因此只需对协方差矩阵进行特征值分解即可求解出W，PCA算法的整个流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a540eb3.png" alt="9.png"></p>
<p>另一篇博客给出更通俗更详细的理解：<a href="http://blog.csdn.net/u011826404/article/details/57472730" target="_blank" rel="noopener">主成分分析解析（基于最大方差理论）</a></p>
<h2 id="11-4-核化线性降维"><a href="#11-4-核化线性降维" class="headerlink" title="11.4 核化线性降维"></a><strong>11.4 核化线性降维</strong></h2><p>说起机器学习你中有我/我中有你/水乳相融…在这里能够得到很好的体现。正如SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，<strong>即先将样本映射到高维空间，再在高维空间中使用线性降维的方法</strong>。下面主要介绍<strong>核化主成分分析（KPCA）</strong>的思想。</p>
<p>若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：<strong>即空间中的任一向量，都可以由该空间中的所有样本线性表示</strong>。证明过程也十分简单：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851a51bd2a.png" alt="10.png"></p>
<p>这样我们便可以将高维特征空间中的投影向量wi使用所有高维样本点线性表出，接着代入PCA的求解问题，得到：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b74b083.png" alt="11.png"></p>
<p>化简到最后一步，发现结果十分的美妙，只需对核矩阵K进行特征分解，便可以得出投影向量wi对应的系数向量α，因此选取特征值前d’大对应的特征向量便是d’个系数向量。这时对于需要降维的样本点，只需按照以下步骤便可以求出其降维后的坐标。可以看出：KPCA在计算降维后的坐标表示时，需要与所有样本点计算核函数值并求和，因此该算法的计算开销十分大。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b735754.png" alt="12.png"></p>
<h2 id="11-5-流形学习"><a href="#11-5-流形学习" class="headerlink" title="11.5 流形学习"></a><strong>11.5 流形学习</strong></h2><p><strong>流形学习（manifold learning）是一种借助拓扑流形概念的降维方法</strong>，<strong>流形是指在局部与欧式空间同胚的空间</strong>，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种<strong>“邻域保持”</strong>的思想。其中<strong>等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系</strong>，下面将分别对这两种著名的流行学习方法进行介绍。</p>
<h3 id="11-5-1-等度量映射（Isomap）"><a href="#11-5-1-等度量映射（Isomap）" class="headerlink" title="11.5.1 等度量映射（Isomap）"></a><strong>11.5.1 等度量映射（Isomap）</strong></h3><p>等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。<strong>因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离</strong>，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的<strong>Dijkstra算法</strong>或<strong>Floyd算法</strong>计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b731a1e.png" alt="13.png"></p>
<p>从MDS算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵B，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量w，因此对于需要降维的新样本无从下手，书中给出的权宜之计是利用已知高/低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。Isomap算法流程如下图：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b6c7e37.png" alt="14.png"></p>
<p>对于近邻图的构建，常用的有两种方法：<strong>一种是指定近邻点个数</strong>，像kNN一样选取k个最近的邻居；<strong>另一种是指定邻域半径</strong>，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：</p>
<blockquote>
<p>若<strong>邻域范围指定过大，则会造成“短路问题”</strong>，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。<br>若<strong>邻域范围指定过小，则会造成“断路问题”</strong>，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。</p>
</blockquote>
<h3 id="11-5-2-局部线性嵌入-LLE"><a href="#11-5-2-局部线性嵌入-LLE" class="headerlink" title="11.5.2 局部线性嵌入(LLE)"></a><strong>11.5.2 局部线性嵌入(LLE)</strong></h3><p>不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本xi的坐标可以通过它的邻域样本线性表出：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b64236f.png" alt="15.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b6a7b9a.png" alt="16.png"></p>
<p>LLE算法分为两步走，<strong>首先第一步根据近邻关系计算出所有样本的邻域重构系数w</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b662815.png" alt="17.png"></p>
<p><strong>接着根据邻域重构系数不变，去求解低维坐标</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b648b98.png" alt="18.png"></p>
<p>这样利用矩阵M，优化问题可以重写为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b6948d7.png" alt="19.png"></p>
<p>M特征值分解后最小的d’个特征值对应的特征向量组成Z，LLE算法的具体流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851b757d8c.png" alt="20.png"></p>
<h2 id="11-6-度量学习"><a href="#11-6-度量学习" class="headerlink" title="11.6 度量学习"></a><strong>11.6 度量学习</strong></h2><p>本篇一开始就提到维数灾难，即在高维空间进行机器学习任务遇到样本稀疏、距离难计算等诸多的问题，因此前面讨论的降维方法都试图将原空间投影到一个合适的低维空间中，接着在低维空间进行学习任务从而产生较好的性能。事实上，不管高维空间还是低维空间都潜在对应着一个距离度量，那可不可以直接学习出一个距离度量来等效降维呢？例如：<strong>咋们就按照降维后的方式来进行距离的计算，这便是度量学习的初衷</strong>。</p>
<p><strong>首先要学习出距离度量必须先定义一个合适的距离度量形式</strong>。对两个样本xi与xj，它们之间的平方欧式距离为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851d3ca3d5.png" alt="21.png"></p>
<p>若各个属性重要程度不一样即都有一个权重，则得到加权的平方欧式距离：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851d3d82c5.png" alt="22.png"></p>
<p>此时各个属性之间都是相互独立无关的，但现实中往往会存在属性之间有关联的情形，例如：身高和体重，一般人越高，体重也会重一些，他们之间存在较大的相关性。这样计算距离就不能分属性单独计算，于是就引入经典的<strong>马氏距离(Mahalanobis distance)</strong>:</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851d3dc303.png" alt="23.png"></p>
<p><strong>标准的马氏距离中M是协方差矩阵的逆，马氏距离是一种考虑属性之间相关性且尺度无关（即无须去量纲）的距离度量</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc851d3e17c0.png" alt="24.png"></p>
<p><strong>矩阵M也称为“度量矩阵”，为保证距离度量的非负性与对称性，M必须为(半)正定对称矩阵</strong>，这样就为度量学习定义好了距离度量的形式，换句话说：<strong>度量学习便是对度量矩阵进行学习</strong>。现在来回想一下前面我们接触的机器学习不难发现：<strong>机器学习算法几乎都是在优化目标函数，从而求解目标函数中的参数</strong>。同样对于度量学习，也需要设置一个优化目标，书中简要介绍了错误率和相似性两种优化目标，此处限于篇幅不进行展开。</p>
<p>在此，降维和度量学习就介绍完毕。<strong>降维是将原高维空间嵌入到一个合适的低维子空间中，接着在低维空间中进行学习任务；度量学习则是试图去学习出一个距离度量来等效降维的效果</strong>，两者都是为了解决维数灾难带来的诸多问题。也许大家最后心存疑惑，那kNN呢，为什么一开头就说了kNN算法，但是好像和后面没有半毛钱关系？正是因为在降维算法中，低维子空间的维数d’通常都由人为指定，因此我们需要使用一些低开销的学习器来选取合适的d’，<strong>kNN这家伙懒到家了根本无心学习，在训练阶段开销为零，测试阶段也只是遍历计算了距离，因此拿kNN来进行交叉验证就十分有优势了~同时降维后样本密度增大同时距离计算变易，更为kNN来展示它独特的十八般手艺提供了用武之地。</strong></p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(17)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>上篇主要介绍了概率图模型，首先从生成式模型与判别式模型的定义出发，引出了概率图模型的基本概念，即利用图结构来表达变量之间的依赖关系；接着分别介绍了隐马尔可夫模型、马尔可夫随机场、条件随机场、精确推断方法以及LDA话题模型：HMM主要围绕着评估/解码/学习这三个实际问题展开论述；MRF基于团和势函数的概念来定义联合概率分布；CRF引入两种特征函数对状态序列进行评价打分；变量消去与信念传播在给定联合概率分布后计算特定变量的边际分布；LDA话题模型则试图去推断给定文档所蕴含的话题分布。本篇将介绍最后一种学习算法—强化学习。<br><a id="more"></a><br>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:20px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="16、强化学习"><a href="#16、强化学习" class="headerlink" title="16、强化学习"></a><strong>16、强化学习</strong></h1><p><strong>强化学习</strong>（Reinforcement Learning，简称<strong>RL</strong>）是机器学习的一个重要分支，前段时间人机大战的主角AlphaGo正是以强化学习为核心技术。在强化学习中，包含两种基本的元素：<strong>状态</strong>与<strong>动作</strong>，<strong>在某个状态下执行某种动作，这便是一种策略</strong>，学习器要做的就是通过不断地探索学习，从而获得一个好的策略。例如：在围棋中，一种落棋的局面就是一种状态，若能知道每种局面下的最优落子动作，那就攻无不克/百战不殆了~</p>
<p>若将状态看作为属性，动作看作为标记，易知：<strong>监督学习和强化学习都是在试图寻找一个映射，从已知属性/状态推断出标记/动作</strong>，这样强化学习中的策略相当于监督学习中的分类/回归器。但在实际问题中，<strong>强化学习并没有监督学习那样的标记信息</strong>，通常都是在<strong>尝试动作后才能获得结果</strong>，因此强化学习是通过反馈的结果信息不断调整之前的策略，从而算法能够学习到：在什么样的状态下选择什么样的动作可以获得最好的结果。</p>
<h2 id="16-1-基本要素"><a href="#16-1-基本要素" class="headerlink" title="16.1 基本要素"></a><strong>16.1 基本要素</strong></h2><p>强化学习任务通常使用<strong>马尔可夫决策过程</strong>（Markov Decision Process，简称<strong>MDP</strong>）来描述，具体而言：机器处在一个环境中，每个状态为机器对当前环境的感知；机器只能通过动作来影响环境，当机器执行一个动作后，会使得环境按某种概率转移到另一个状态；同时，环境会根据潜在的奖赏函数反馈给机器一个奖赏。综合而言，强化学习主要包含四个要素：状态、动作、转移概率以及奖赏函数。</p>
<blockquote>
<p><strong>状态（X）</strong>：机器对环境的感知，所有可能的状态称为状态空间；<br><strong>动作（A）</strong>：机器所采取的动作，所有能采取的动作构成动作空间；<br><strong>转移概率（P）</strong>：当执行某个动作后，当前状态会以某种概率转移到另一个状态；<br><strong>奖赏函数（R）</strong>：在状态转移的同时，环境给反馈给机器一个奖赏。</p>
</blockquote>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYOud.png" alt="iwYOud.png"></p>
<p>因此，<strong>强化学习的主要任务就是通过在环境中不断地尝试，根据尝试获得的反馈信息调整策略，最终生成一个较好的策略π，机器根据这个策略便能知道在什么状态下应该执行什么动作</strong>。常见的策略表示方法有以下两种：</p>
<blockquote>
<p><strong>确定性策略</strong>：π（x）=a，即在状态x下执行a动作；<br><strong>随机性策略</strong>：P=π（x,a），即在状态x下执行a动作的概率。</p>
</blockquote>
<p><strong>一个策略的优劣取决于长期执行这一策略后的累积奖赏</strong>，换句话说：可以使用累积奖赏来评估策略的好坏，最优策略则表示在初始状态下一直执行该策略后，最后的累积奖赏值最高。长期累积奖赏通常使用下述两种计算方法：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYH3D.png" alt="iwYH3D.png"></p>
<h2 id="16-2-K摇摆赌博机"><a href="#16-2-K摇摆赌博机" class="headerlink" title="16.2 K摇摆赌博机"></a><strong>16.2 K摇摆赌博机</strong></h2><p>首先我们考虑强化学习最简单的情形：仅考虑一步操作，即在状态x下只需执行一次动作a便能观察到奖赏结果。易知：欲最大化单步奖赏，我们需要知道每个动作带来的期望奖赏值，这样便能选择奖赏值最大的动作来执行。若每个动作的奖赏值为确定值，则只需要将每个动作尝试一遍即可，但大多数情形下，一个动作的奖赏值来源于一个概率分布，因此需要进行多次的尝试。</p>
<p>单步强化学习实质上是<strong>K-摇臂赌博机</strong>（K-armed bandit）的原型，一般我们<strong>尝试动作的次数是有限的</strong>，那如何利用有限的次数进行有效地探索呢？这里有两种基本的想法：</p>
<blockquote>
<p><strong>仅探索法</strong>：将尝试的机会平均分给每一个动作，即轮流执行，最终将每个动作的平均奖赏作为期望奖赏的近似值。<br><strong>仅利用法</strong>：将尝试的机会分给当前平均奖赏值最大的动作，隐含着让一部分人先富起来的思想。</p>
</blockquote>
<p>可以看出：上述<strong>两种方法是相互矛盾的</strong>，仅探索法能较好地估算每个动作的期望奖赏，但是没能根据当前的反馈结果调整尝试策略；仅利用法在每次尝试之后都更新尝试策略，符合强化学习的思（tao）维（lu），但容易找不到最优动作。因此需要在这两者之间进行折中。</p>
<h3 id="16-2-1-ε-贪心"><a href="#16-2-1-ε-贪心" class="headerlink" title="16.2.1 ε-贪心"></a><strong>16.2.1 ε-贪心</strong></h3><p><strong>ε-贪心法基于一个概率来对探索和利用进行折中</strong>，具体而言：在每次尝试时，以ε的概率进行探索，即以均匀概率随机选择一个动作；以1-ε的概率进行利用，即选择当前最优的动作。ε-贪心法只需记录每个动作的当前平均奖赏值与被选中的次数，便可以增量式更新。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYzUP.png" alt="iwYzUP.png"></p>
<h3 id="16-2-2-Softmax"><a href="#16-2-2-Softmax" class="headerlink" title="16.2.2 Softmax"></a><strong>16.2.2 Softmax</strong></h3><p><strong>Softmax算法则基于当前每个动作的平均奖赏值来对探索和利用进行折中，Softmax函数将一组值转化为一组概率</strong>，值越大对应的概率也越高，因此当前平均奖赏值越高的动作被选中的几率也越大。Softmax函数如下所示：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYbge.png" alt="iwYbge.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYqjH.png" alt="iwYqjH.png"></p>
<h2 id="16-3-有模型学习"><a href="#16-3-有模型学习" class="headerlink" title="16.3 有模型学习"></a><strong>16.3 有模型学习</strong></h2><p>若学习任务中的四个要素都已知，即状态空间、动作空间、转移概率以及奖赏函数都已经给出，这样的情形称为“<strong>有模型学习</strong>”。假设状态空间和动作空间均为有限，即均为离散值，这样我们不用通过尝试便可以对某个策略进行评估。</p>
<h3 id="16-3-1-策略评估"><a href="#16-3-1-策略评估" class="headerlink" title="16.3.1 策略评估"></a><strong>16.3.1 策略评估</strong></h3><p>前面提到：<strong>在模型已知的前提下，我们可以对任意策略的进行评估</strong>（后续会给出演算过程）。一般常使用以下两种值函数来评估某个策略的优劣：</p>
<blockquote>
<p><strong>状态值函数（V）</strong>：V（x），即从状态x出发，使用π策略所带来的累积奖赏；<br><strong>状态-动作值函数（Q）</strong>：Q（x,a），即从状态x出发，执行动作a后再使用π策略所带来的累积奖赏。</p>
</blockquote>
<p>根据累积奖赏的定义，我们可以引入T步累积奖赏与r折扣累积奖赏：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYjHI.png" alt="iwYjHI.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYXDA.png" alt="iwYXDA.png"></p>
<p>由于MDP具有马尔可夫性，即现在决定未来，将来和过去无关，我们很容易找到值函数的递归关系：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwtS4f.png" alt="iwtS4f.png"></p>
<p>类似地，对于r折扣累积奖赏可以得到：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwYxEt.png" alt="iwYxEt.png"></p>
<p>易知：<strong>当模型已知时，策略的评估问题转化为一种动态规划问题</strong>，即以填表格的形式自底向上，先求解每个状态的单步累积奖赏，再求解每个状态的两步累积奖赏，一直迭代逐步求解出每个状态的T步累积奖赏。算法流程如下所示：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwt9C8.png" alt="iwt9C8.png"></p>
<p>对于状态-动作值函数，只需通过简单的转化便可得到：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwt3r9.png" alt="iwt3r9.png"></p>
<h3 id="16-3-2-策略改进"><a href="#16-3-2-策略改进" class="headerlink" title="16.3.2 策略改进"></a><strong>16.3.2 策略改进</strong></h3><p>理想的策略应能使得每个状态的累积奖赏之和最大，简单来理解就是：不管处于什么状态，只要通过该策略执行动作，总能得到较好的结果。因此对于给定的某个策略，我们需要对其进行改进，从而得到<strong>最优的值函数</strong>。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwtm5V.png" alt="iwtm5V.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwtZEq.png" alt="iwtZEq.png"></p>
<p>最优Bellman等式改进策略的方式为：<strong>将策略选择的动作改为当前最优的动作</strong>，而不是像之前那样对每种可能的动作进行求和。易知：选择当前最优动作相当于将所有的概率都赋给累积奖赏值最大的动作，因此每次改进都会使得值函数单调递增。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwtEbn.png" alt="iwtEbn.png"></p>
<p>将策略评估与策略改进结合起来，我们便得到了生成最优策略的方法：先给定一个随机策略，现对该策略进行评估，然后再改进，接着再评估/改进一直到策略收敛、不再发生改变。这便是策略迭代算法，算法流程如下所示：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwteU0.png" alt="iwteU0.png"></p>
<p>可以看出：策略迭代法在每次改进策略后都要对策略进行重新评估，因此比较耗时。若从最优化值函数的角度出发，即先迭代得到最优的值函数，再来计算如何改变策略，这便是值迭代算法，算法流程如下所示：</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwtuCT.png" alt="iwtuCT.png"></p>
<h2 id="16-4-蒙特卡罗强化学习"><a href="#16-4-蒙特卡罗强化学习" class="headerlink" title="16.4 蒙特卡罗强化学习"></a><strong>16.4 蒙特卡罗强化学习</strong></h2><p>在现实的强化学习任务中，<strong>环境的转移函数与奖赏函数往往很难得知</strong>，因此我们需要考虑在不依赖于环境参数的条件下建立强化学习模型，这便是<strong>免模型学习</strong>。蒙特卡罗强化学习便是其中的一种经典方法。</p>
<p>由于模型参数未知，状态值函数不能像之前那样进行全概率展开，从而运用动态规划法求解。一种直接的方法便是通过采样来对策略进行评估/估算其值函数，<strong>蒙特卡罗强化学习正是基于采样来估计状态-动作值函数</strong>：对采样轨迹中的每一对状态-动作，记录其后的奖赏值之和，作为该状态-动作的一次累积奖赏，通过多次采样后，使用累积奖赏的平均作为状态-动作值的估计，并<strong>引入ε-贪心策略保证采样的多样性</strong>。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwt1KJ.png" alt="iwt1KJ.png"></p>
<p>在上面的算法流程中，被评估和被改进的都是同一个策略，因此称为<strong>同策略蒙特卡罗强化学习算法</strong>。引入ε-贪心仅是为了便于采样评估，而在使用策略时并不需要ε-贪心，那能否仅在评估时使用ε-贪心策略，而在改进时使用原始策略呢？这便是<strong>异策略蒙特卡罗强化学习算法</strong>。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwtK8U.png" alt="iwtK8U.png"></p>
<h2 id="16-5-AlphaGo原理浅析"><a href="#16-5-AlphaGo原理浅析" class="headerlink" title="16.5 AlphaGo原理浅析"></a><strong>16.5 AlphaGo原理浅析</strong></h2><p>本篇一开始便提到强化学习是AlphaGo的核心技术之一，刚好借着这个东风将AlphaGo的工作原理了解一番。正如人类下棋那般“<strong>手下一步棋，心想三步棋</strong>”，Alphago也正是这个思想，<strong>当处于一个状态时，机器会暗地里进行多次的尝试/采样，并基于反馈回来的结果信息改进估值函数，从而最终通过增强版的估值函数来选择最优的落子动作。</strong></p>
<p>其中便涉及到了三个主要的问题：<strong>（1）如何确定估值函数（2）如何进行采样（3）如何基于反馈信息改进估值函数</strong>，这正对应着AlphaGo的三大核心模块：<strong>深度学习</strong>、<strong>蒙特卡罗搜索树</strong>、<strong>强化学习</strong>。</p>
<blockquote>
<p><strong>1.深度学习（拟合估值函数）</strong></p>
</blockquote>
<p>由于围棋的状态空间巨大，像蒙特卡罗强化学习那样通过采样来确定值函数就行不通了。在围棋中，<strong>状态值函数可以看作为一种局面函数，状态-动作值函数可以看作一种策略函数</strong>，若我们能获得这两个估值函数，便可以根据这两个函数来完成：(1)衡量当前局面的价值；(2)选择当前最优的动作。那如何精确地估计这两个估值函数呢？<strong>这就用到了深度学习，通过大量的对弈数据自动学习出特征，从而拟合出估值函数。</strong></p>
<blockquote>
<p><strong>2.蒙特卡罗搜索树（采样）</strong></p>
</blockquote>
<p>蒙特卡罗树是一种经典的搜索框架，它通过反复地采样模拟对局来探索状态空间。具体表现在：从当前状态开始，利用策略函数尽可能选择当前最优的动作，同时也引入随机性来减小估值错误带来的负面影响，从而模拟棋局运行，使得棋盘达到终局或一定步数后停止。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwtM2F.png" alt="iwtM2F.png"></p>
<blockquote>
<p><strong>3.强化学习（调整估值函数）</strong></p>
</blockquote>
<p>在使用蒙特卡罗搜索树进行多次采样后，每次采样都会反馈后续的局面信息（利用局面函数进行评价），根据反馈回来的结果信息自动调整两个估值函数的参数，这便是强化学习的核心思想，最后基于改进后的策略函数选择出当前最优的落子动作。</p>
<p><img src="https://s1.ax1x.com/2018/10/18/iwtQv4.png" alt="iwtQv4.png"></p>
<p>在此，强化学习就介绍完毕。同时也意味着大口小口地啃完了这个西瓜，十分记得去年双11之后立下这个Flag，现在回想起来，大半年的时间里在嚼瓜上还是花费了不少功夫。有人说：当你阐述的能让别人看懂才算是真的理解，有人说：在写的过程中能发现那些只看书发现不了的东西，自己最初的想法十分简单：当健忘症发作的时候，如果能看到之前按照自己思路写下的文字，回忆便会汹涌澎湃一些~</p>
<p>最后，感谢自己这大半年以来的坚持~Get busy living, or get busy dying!</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(3)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/</url>
    <content><![CDATA[<p>在上两篇中，我们介绍了多种常见的评估方法和性能度量标准，这样我们就可以根据数据集以及模型任务的特征，选择出最合适的评估和性能度量方法来计算出学习器的“测试误差“。但由于“测试误差”受到很多因素的影响，例如：算法随机性(例如常见的K-Means)或测试集本身的选择，使得同一模型每次得到的结果不尽相同，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能，那如何对单个或多个学习器在不同或相同测试集上的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。本篇延续上一篇的内容，主要讨论了比较检验、方差与偏差。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h2 id="2-6-比较检验"><a href="#2-6-比较检验" class="headerlink" title="2.6 比较检验"></a><strong>2.6 比较检验</strong></h2><p>在比较学习器泛化性能的过程中，统计假设检验（hypothesis test）为学习器性能比较提供了重要依据，即若A在某测试集上的性能优于B，那A学习器比B好的把握有多大。 为方便论述，本篇中都是以“错误率”作为性能度量的标准。</p>
<h3 id="2-6-1-假设检验"><a href="#2-6-1-假设检验" class="headerlink" title="2.6.1 假设检验"></a><strong>2.6.1 假设检验</strong></h3><p>“假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想，例如：假设总体服从泊松分布，或假设正态总体的期望u=u0。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，这就是一种假设检验。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211aed8e3.png" alt="1.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a5817d.png" alt="2.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a336b5.png" alt="3.png"></p>
<h3 id="2-6-2-交叉验证t检验"><a href="#2-6-2-交叉验证t检验" class="headerlink" title="2.6.2 交叉验证t检验"></a><strong>2.6.2 交叉验证t检验</strong></h3><p><img src="https://i.loli.net/2018/10/17/5bc7211a68ef9.png" alt="4.png"></p>
<h3 id="2-6-3-McNemar检验"><a href="#2-6-3-McNemar检验" class="headerlink" title="2.6.3 McNemar检验"></a><strong>2.6.3 McNemar检验</strong></h3><p>MaNemar主要用于二分类问题，与成对t检验一样也是用于比较两个学习器的性能大小。主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即e01=e10，且|e01-e10|服从N（1，e01+e10）分布。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a2c7f9.png" alt="5.png"></p>
<p>因此，如下所示的变量服从自由度为1的卡方分布，即服从标准正态分布N（0,1）的随机变量的平方和，下式只有一个变量，故自由度为1，检验的方法同上：做出假设—&gt;求出满足显著度的临界点—&gt;给出拒绝域—&gt;验证假设。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a34e96.png" alt="6.png"></p>
<h3 id="2-6-4-Friedman检验与Nemenyi后续检验"><a href="#2-6-4-Friedman检验与Nemenyi后续检验" class="headerlink" title="2.6.4 Friedman检验与Nemenyi后续检验"></a><strong>2.6.4 Friedman检验与Nemenyi后续检验</strong></h3><p>上述的三种检验都只能在一组数据集上，F检验则可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3…，相同则平分序值，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a2db45.png" alt="7.png"></p>
<p>若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值ri服从正态分布N（（k+1）/2，（k+1）(k-1)/12），则有：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a45349.png" alt="8.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a2684c.png" alt="9.png"></p>
<p>服从自由度为k-1和(k-1)(N-1)的F分布。下面是F检验常用的临界值：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a7e3f0.png" alt="10.png"></p>
<p>若“H0：所有算法的性能相同”这个假设被拒绝，则需要进行后续检验，来得到具体的算法之间的差异。常用的就是Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域，下表是常用的qa值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度1-α拒绝“两个算法性能相同”的假设。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722232932b.png" alt="11.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7222348519.png" alt="12.png"></p>
<h2 id="2-7-偏差与方差"><a href="#2-7-偏差与方差" class="headerlink" title="2.7 偏差与方差"></a><strong>2.7 偏差与方差</strong></h2><p>偏差-方差分解是解释学习器泛化性能的重要工具。在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值得期望之间的差均方。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。通过对泛化误差的进行分解，可以得到：</p>
<ul>
<li><strong>期望泛化误差=方差+偏差</strong>    </li>
<li><strong>偏差刻画学习器的拟合能力</strong></li>
<li><strong>方差体现学习器的稳定性</strong></li>
</ul>
<p>易知：方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722234b09f.png" alt="13.png"></p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(4)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>笔记的前一部分主要是对机器学习预备知识的概括，包括机器学习的定义/术语、学习器性能的评估/度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容—线性模型。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="3、线性模型"><a href="#3、线性模型" class="headerlink" title="3、线性模型"></a><strong>3、线性模型</strong></h1><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p>
<h2 id="3-1-线性回归"><a href="#3-1-线性回归" class="headerlink" title="3.1 线性回归"></a><strong>3.1 线性回归</strong></h2><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000—&gt;13亿…2016—&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）—&gt;15k。</p>
<p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p>
<ul>
<li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p>
</li>
<li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p>
</li>
</ul>
<p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p>
<p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p>
<p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p>
<p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p>
<p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p>
<p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p>
<h2 id="3-2-线性几率回归"><a href="#3-2-线性几率回归" class="headerlink" title="3.2 线性几率回归"></a><strong>3.2 线性几率回归</strong></h2><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p>
<p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p>
<h2 id="3-3-线性判别分析"><a href="#3-3-线性判别分析" class="headerlink" title="3.3 线性判别分析"></a><strong>3.3 线性判别分析</strong></h2><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p>
<p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p>
<ul>
<li>类内散度矩阵（within-class scatter matrix）</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p>
<ul>
<li>类间散度矩阵(between-class scaltter matrix)</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p>
<p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p>
<p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p>
<p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。<br>​             </p>
<h2 id="3-4-多分类学习"><a href="#3-4-多分类学习" class="headerlink" title="3.4 多分类学习"></a><strong>3.4 多分类学习</strong></h2><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p>
<ul>
<li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p>
</li>
<li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p>
</li>
<li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p>
<h2 id="3-5-类别不平衡问题"><a href="#3-5-类别不平衡问题" class="headerlink" title="3.5 类别不平衡问题"></a><strong>3.5 类别不平衡问题</strong></h2><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p>
<ol>
<li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li>
<li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li>
<li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li>
</ol>
<p>​<br>​      </p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(5)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>上篇主要介绍和讨论了线性模型。首先从最简单的最小二乘法开始，讨论输入属性有一个和多个的情形，接着通过广义线性模型延伸开来，将预测连续值的回归问题转化为分类问题，从而引入了对数几率回归，最后线性判别分析LDA将样本点进行投影，多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。本篇将讨论另一种被广泛使用的分类算法—决策树（Decision Tree）。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="4、决策树"><a href="#4、决策树" class="headerlink" title="4、决策树"></a><strong>4、决策树</strong></h1><h2 id="4-1-决策树基本概念"><a href="#4-1-决策树基本概念" class="headerlink" title="4.1 决策树基本概念"></a><strong>4.1 决策树基本概念</strong></h2><p>顾名思义，决策树是基于树结构来进行决策的，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p>
<hr>
<pre><code>  女儿：多大年纪了？
  母亲：26。
  女儿：长的帅不帅？
  母亲：挺帅的。
  女儿：收入高不？
  母亲：不算很高，中等情况。
  女儿：是公务员不？
  母亲：是，在税务局上班呢。
  女儿：那好，我去见见。
</code></pre><hr>
<p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec84a77.png" alt="1.png"></p>
<p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p>
<pre><code>* 每个非叶节点表示一个特征属性测试。
* 每个分支代表这个特征属性在某个值域上的输出。
* 每个叶子节点存放一个类别。
* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。
</code></pre><h2 id="4-2-决策树的构造"><a href="#4-2-决策树的构造" class="headerlink" title="4.2 决策树的构造"></a><strong>4.2 决策树的构造</strong></h2><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ecc27fe.png" alt="2.png"></p>
<p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。</p>
<h3 id="4-2-1-ID3算法"><a href="#4-2-1-ID3算法" class="headerlink" title="4.2.1 ID3算法"></a><strong>4.2.1 ID3算法</strong></h3><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec515a5.png" alt="3.png"></p>
<p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec3e067.png" alt="4.png"></p>
<p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p>
<h3 id="4-2-2-C4-5算法"><a href="#4-2-2-C4-5算法" class="headerlink" title="4.2.2 C4.5算法"></a><strong>4.2.2 C4.5算法</strong></h3><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec69647.png" alt="5.png"></p>
<h3 id="4-2-3-CART算法"><a href="#4-2-3-CART算法" class="headerlink" title="4.2.3 CART算法"></a><strong>4.2.3 CART算法</strong></h3><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，基尼指数定义如下：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec5a2ff.png" alt="6.png"></p>
<p>进而，使用属性α划分后的基尼指数为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec62eaf.png" alt="7.png"></p>
<h2 id="4-3-剪枝处理"><a href="#4-3-剪枝处理" class="headerlink" title="4.3 剪枝处理"></a><strong>4.3 剪枝处理</strong></h2><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p>
<pre><code>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。
* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。
</code></pre><p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec80d34.png" alt="8.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9e330.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9d497.png" alt="10.png"></p>
<p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p>
<h2 id="4-4-连续值与缺失值处理"><a href="#4-4-连续值与缺失值处理" class="headerlink" title="4.4 连续值与缺失值处理"></a><strong>4.4 连续值与缺失值处理</strong></h2><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p>
<pre><code>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。
* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。
* 选择最大信息增益的划分点作为最优划分点。
</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72a0968fad.png" alt="11.png"></p>
<p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a098f3be.png" alt="12.png"></p>
<p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a096ccc3.png" alt="13.png"></p>
<p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a093ed3c.png" alt="14.png"></p>
<p>​    </p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(7)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    <content><![CDATA[<p>上篇主要介绍了神经网络。首先从生物学神经元出发，引出了它的数学抽象模型—MP神经元以及由两层神经元组成的感知机模型，并基于梯度下降的方法描述了感知机模型的权值调整规则。由于简单的感知机不能处理线性不可分的情形，因此接着引入了含隐层的前馈型神经网络，BP神经网络则是其中最为成功的一种学习方法，它使用误差逆传播的方法来逐层调节连接权。最后简单介绍了局部/全局最小以及目前十分火热的深度学习的概念。本篇围绕的核心则是曾经一度取代过神经网络的另一种监督学习算法—<strong>支持向量机</strong>（Support Vector Machine），简称<strong>SVM</strong>。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="6、支持向量机"><a href="#6、支持向量机" class="headerlink" title="6、支持向量机"></a><strong>6、支持向量机</strong></h1><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p>
<h2 id="6-1-函数间隔与几何间隔"><a href="#6-1-函数间隔与几何间隔" class="headerlink" title="6.1 函数间隔与几何间隔"></a><strong>6.1 函数间隔与几何间隔</strong></h2><p>对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png" alt="1.png"></p>
<p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a06d5a.png" alt="2.png"></p>
<h3 id="6-1-1-函数间隔"><a href="#6-1-1-函数间隔" class="headerlink" title="6.1.1 函数间隔"></a><strong>6.1.1 函数间隔</strong></h3><p>在超平面w’x+b=0确定的情况下，|w’x<em>+b|能够代表点x</em>距离超平面的远近，易知：当w’x<em>+b&gt;0时，表示x</em>在超平面的一侧（正类，类标为1），而当w’x<em>+b&lt;0时，则表示x</em>在超平面的另外一侧（负类，类别为-1），因此（w’x<em>+b）y</em> 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了<em>*函数间隔</em></em>的定义（functional margin）:</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f690a14b.png" alt="3.png"></p>
<p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f690ac26.png" alt="4.png"></p>
<p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念—几何间隔（geometrical margin）。</p>
<h3 id="6-1-2-几何间隔"><a href="#6-1-2-几何间隔" class="headerlink" title="6.1.2 几何间隔"></a><strong>6.1.2 几何间隔</strong></h3><p><strong>几何间隔</strong>代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x<em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x</em>=x-r(w/||w||)，又x<em>在超平面上，即w’x</em>+b=0，代入即可得：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f697d499.png" alt="5.png"></p>
<p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f696fd10.png" alt="6.png"></p>
<p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p>
<h2 id="6-2-最大间隔与支持向量"><a href="#6-2-最大间隔与支持向量" class="headerlink" title="6.2 最大间隔与支持向量"></a><strong>6.2 最大间隔与支持向量</strong></h2><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f69af163.png" alt="7.png"></p>
<p>一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f697bb1d.png" alt="8.png"></p>
<p>对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为<strong>支持向量</strong>（support vector），易知：对于所有的支持向量，它们恰好满足y<em>(w’x</em>+b)=1，而所有不是支持向量的点，有y<em>(w’x</em>+b)&gt;1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a838c4.png" alt="9.png"></p>
<h2 id="6-3-从原始优化问题到对偶问题"><a href="#6-3-从原始优化问题到对偶问题" class="headerlink" title="6.3 从原始优化问题到对偶问题"></a><strong>6.3 从原始优化问题到对偶问题</strong></h2><p>对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6978cbb.png" alt="10.png"></p>
<p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的<strong>对偶问题</strong>，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p>
<pre><code>* 一是因为使用对偶问题更容易求解；
* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。
</code></pre><p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9332be7.png" alt="11.png"></p>
<p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f93321c5.png" alt="12.png"></p>
<p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9330967.png" alt="13.png"></p>
<p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p>
<p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9333e66.png" alt="14.png"></p>
<p>将上述结果代入L得到：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f935ae21.png" alt="15.png"></p>
<p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9338a9d.png" alt="16.png"></p>
<p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f93419ca.png" alt="17.png"></p>
<p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9353166.png" alt="18.png"></p>
<p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f933c947.png" alt="19.png">        </p>
<h2 id="6-4-核函数"><a href="#6-4-核函数" class="headerlink" title="6.4 核函数"></a><strong>6.4 核函数</strong></h2><p>由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用<strong>映射</strong>的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f934303e.png" alt="20.png"></p>
<p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p>
<p>（1）原对偶问题变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc68b3b.png" alt="21.png"></p>
<p>（2）原分类函数变为：<br>​    <img src="https://i.loli.net/2018/10/17/5bc730cc1b673.png" alt="22.png"></p>
<p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了<strong>核函数</strong>（Kernel）的概念。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc49adc.png" alt="23.png"></p>
<p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效<strong>（低维计算，高维表现）</strong>，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p>
<p>（1）对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc173b2.png" alt="24.png"></p>
<p>（2）分类函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc05959.png" alt="25.png"></p>
<p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccc468c.png" alt="26.png"></p>
<p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccc541a.png" alt="27.png"></p>
<h2 id="6-5-软间隔支持向量机"><a href="#6-5-软间隔支持向量机" class="headerlink" title="6.5 软间隔支持向量机"></a><strong>6.5 软间隔支持向量机</strong></h2><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有<strong>噪声</strong>的情形，噪声数据（<strong>outlier</strong>）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccce68e.png" alt="28.png"></p>
<p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了<strong>“软间隔”支持向量机</strong>的概念</p>
<pre><code>* 允许某些数据点不满足约束y(w&#39;x+b)≥1；
* 同时又使得不满足约束的样本尽可能少。
</code></pre><p>这样优化目标变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc6c9fe.png" alt="29.png"></p>
<p>如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc5e5a9.png" alt="30.png"></p>
<p>支持向量机中的损失函数为<strong>hinge损失</strong>，引入<strong>“松弛变量”</strong>，目标函数与约束条件可以写为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317aa3411.png" alt="31.png"></p>
<p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317a4c96e.png" alt="32.png"></p>
<p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317a6dff2.png" alt="33.png"></p>
<p>将w代入L化简，便得到其对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317ab6646.png" alt="34.png"></p>
<p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p>
<p>——在此SVM就介绍完毕。</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(6)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>上篇主要讨论了决策树算法。首先从决策树的基本概念出发，引出决策树基于树形结构进行决策，进一步介绍了构造决策树的递归流程以及其递归终止条件，在递归的过程中，划分属性的选择起到了关键作用，因此紧接着讨论了三种评估属性划分效果的经典算法，介绍了剪枝策略来解决原生决策树容易产生的过拟合问题，最后简述了属性连续值/缺失值的处理方法。本篇将讨论现阶段十分热门的另一个经典监督学习算法—神经网络（neural network）。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="5、神经网络"><a href="#5、神经网络" class="headerlink" title="5、神经网络"></a><strong>5、神经网络</strong></h1><p>在机器学习中，神经网络一般指的是“神经网络学习”，是机器学习与神经网络两个学科的交叉部分。所谓神经网络，目前用得最广泛的一个定义是“神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应”。</p>
<h2 id="5-1-神经元模型"><a href="#5-1-神经元模型" class="headerlink" title="5.1 神经元模型"></a><strong>5.1 神经元模型</strong></h2><p>神经网络中最基本的单元是神经元模型（neuron）。在生物神经网络的原始机制中，每个神经元通常都有多个树突（dendrite），一个轴突（axon）和一个细胞体（cell body），树突短而多分支，轴突长而只有一个；在功能上，树突用于传入其它神经元传递的神经冲动，而轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。神经元的生物学结构如下图所示，不得不说高中的生化知识大学忘得可是真干净…</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb6cc11.png" alt="1.png"></p>
<p>一直沿用至今的“M-P神经元模型”正是对这一结构进行了抽象，也称“阈值逻辑单元“，其中树突对应于输入部分，每个神经元收到n个其他神经元传递过来的输入信号，这些信号通过带权重的连接传递给细胞体，这些权重又称为连接权（connection weight）。细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb7be44.png" alt="2.png"></p>
<p>与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb40dc5.png" alt="3.png"></p>
<p>将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种包含多个参数的模型，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。</p>
<h2 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="5.2 感知机与多层网络"></a><strong>5.2 感知机与多层网络</strong></h2><p>感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。</p>
<p>给定训练集，则感知机的n+1个参数（n个权重+1个阈值）都可以通过学习得到。阈值Θ可以看作一个输入值固定为-1的哑结点的权重ωn+1，即假设有一个固定输入xn+1=-1的输入层神经元，其对应的权重为ωn+1，这样就把权重和阈值统一为权重的学习了。简单感知机的结构如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb3fdf0.png" alt="4.png"></p>
<p>感知机权重的学习规则如下：对于训练样本（x，y），当该样本进入感知机学习后，会产生一个输出值，若该输出值与样本的真实标记不一致，则感知机会对权重进行调整，若激活函数为阶跃函数，则调整的方法为（基于梯度下降法）：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb3ba63.png" alt="5.png"></p>
<p>其中 η∈（0，1）称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。</p>
<p>由于感知机模型只有一层功能神经元，因此其功能十分有限，只能处理线性可分的问题，对于这类问题，感知机的学习过程一定会收敛（converge），因此总是可以求出适当的权值。但是对于像书上提到的异或问题，只通过一层功能神经元往往不能解决，因此要解决非线性可分问题，需要考虑使用多层功能神经元，即神经网络。多层神经网络的拓扑结构如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb58ec6.png" alt="6.png"></p>
<p>在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点：</p>
<pre><code>* 每层神经元与下一层神经元之间完全互连
* 神经元之间不存在同层连接
* 神经元之间不存在跨层连接
</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72cbb47ff8.png" alt="7.png"></p>
<p>根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播（下节中的BP神经网络正是基于前馈神经网络而增加了反馈调节机制）。神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个神经元的阈值，换句话说：神经网络所学习到的东西都蕴含在网络的连接权与阈值中。</p>
<h2 id="5-3-BP神经网络算法"><a href="#5-3-BP神经网络算法" class="headerlink" title="5.3 BP神经网络算法"></a><strong>5.3 BP神经网络算法</strong></h2><p>由上面可以得知：神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP神经网络算法即误差逆传播算法（error BackPropagation）正是为学习多层前馈神经网络而设计，BP神经网络算法是迄今为止最成功的的神经网络学习算法。</p>
<p>一般而言，只需包含一个足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数[Hornik et al.,1989]，故下面以训练单隐层的前馈神经网络为例，介绍BP神经网络的算法思想。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb92ff5.png" alt="8.png"></p>
<p>上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节。可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb86229.png" alt="9.png"></p>
<p>学习率η∈（0，1）控制着沿反梯度方向下降的步长，若步长太大则下降太快容易产生震荡，若步长太小则收敛速度太慢，一般地常把η设置为0.1，有时更新权重时会将输出层与隐含层设置为不同的学习率。BP算法的基本流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72cbb59e99.png" alt="10.png"></p>
<p>BP算法的更新规则是基于每个样本的预测值与真实类标的均方误差来进行权值调节，即BP算法每次更新只针对于单个样例。需要注意的是：BP算法的最终目标是要最小化整个训练集D上的累积误差，即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72ce222a96.png" alt="11.png"></p>
<p>如果基于累积误差最小化的更新规则，则得到了累积误差逆传播算法（accumulated error backpropagation），即每次读取全部的数据集一遍，进行一轮学习，从而基于当前的累积误差进行权值调整，因此参数更新的频率相比标准BP算法低了很多，但在很多任务中，尤其是在数据量很大的时候，往往标准BP算法会获得较好的结果。另外对于如何设置隐层神经元个数的问题，至今仍然没有好的解决方案，常使用“试错法”进行调整。</p>
<p>前面提到，BP神经网络强大的学习能力常常容易造成过拟合问题，有以下两种策略来缓解BP网络的过拟合问题：</p>
<ul>
<li>早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。</li>
<li>引入正则化（regularization）：基本思想是在累积误差函数中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc72ce227ff1.png" alt="12.png"></p>
<h2 id="5-4-全局最小与局部最小"><a href="#5-4-全局最小与局部最小" class="headerlink" title="5.4 全局最小与局部最小"></a><strong>5.4 全局最小与局部最小</strong></h2><p>模型学习的过程实质上就是一个寻找最优参数的过程，例如BP算法试图通过最速下降来寻找使得累积经验误差最小的权值与阈值，在谈到最优时，一般会提到局部极小（local minimum）和全局最小（global minimum）。</p>
<pre><code>* 局部极小解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的误差函数值。
* 全局最小解：参数空间中的某个点，所有其他点的误差函数值均不小于该点的误差函数值。
</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72ce2803dc.png" alt="13.png"></p>
<p>要成为局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。</p>
<pre><code>* 以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。
* 使用“模拟退火”技术，这里不做具体介绍。
* 使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。
</code></pre><h2 id="5-5-深度学习"><a href="#5-5-深度学习" class="headerlink" title="5.5 深度学习"></a><strong>5.5 深度学习</strong></h2><p>理论上，参数越多，模型复杂度就越高，容量（capability）就越大，从而能完成更复杂的学习任务。深度学习（deep learning）正是一种极其复杂而强大的模型。</p>
<p>怎么增大模型复杂度呢？两个办法，一是增加隐层的数目，二是增加隐层神经元的数目。前者更有效一些，因为它不仅增加了功能神经元的数量，还增加了激活函数嵌套的层数。但是对于多隐层神经网络，经典算法如标准BP算法往往会在误差逆传播时发散（diverge），无法收敛达到稳定状态。</p>
<p>那要怎么有效地训练多隐层神经网络呢？一般来说有以下两种方法：</p>
<ul>
<li><p>无监督逐层训练（unsupervised layer-wise training）：每次训练一层隐节点，把上一层隐节点的输出当作输入来训练，本层隐结点训练好后，输出再作为下一层的输入来训练，这称为预训练（pre-training）。全部预训练完成后，再对整个网络进行微调（fine-tuning）训练。一个典型例子就是深度信念网络（deep belief network，简称DBN）。这种做法其实可以视为把大量的参数进行分组，先找出每组较好的设置，再基于这些局部最优的结果来训练全局最优。</p>
</li>
<li><p>权共享（weight sharing）：令同一层神经元使用完全相同的连接权，典型的例子是卷积神经网络（Convolutional Neural Network，简称CNN）。这样做可以大大减少需要训练的参数数目。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc72ce28d756.png" alt="14.png"></p>
<p>深度学习可以理解为一种特征学习（feature learning）或者表示学习（representation learning），无论是DBN还是CNN，都是通过多个隐层来把与输出目标联系不大的初始输入转化为与输出目标更加密切的表示，使原来只通过单层映射难以完成的任务变为可能。即通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示，从而使得最后可以用简单的模型来完成复杂的学习任务。</p>
<p>传统任务中，样本的特征需要人类专家来设计，这称为特征工程（feature engineering）。特征好坏对泛化性能有至关重要的影响。而深度学习为全自动数据分析带来了可能，可以自动产生更好的特征。</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(8)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    <content><![CDATA[<p>上篇主要介绍和讨论了支持向量机。从最初的分类函数，通过最大化分类间隔，max(1/||w||)，min(1/2||w||^2)，凸二次规划，朗格朗日函数，对偶问题，一直到最后的SMO算法求解，都为寻找一个最优解。接着引入核函数将低维空间映射到高维特征空间，解决了非线性可分的情形。最后介绍了软间隔支持向量机，解决了outlier挤歪超平面的问题。本篇将讨论一个经典的统计学习算法—<strong>贝叶斯分类器</strong>。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="7、贝叶斯分类器"><a href="#7、贝叶斯分类器" class="headerlink" title="7、贝叶斯分类器"></a><strong>7、贝叶斯分类器</strong></h1><p>贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委—贝叶斯公式。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd7a2575.png" alt="1.png"></p>
<h2 id="7-1-贝叶斯决策论"><a href="#7-1-贝叶斯决策论" class="headerlink" title="7.1 贝叶斯决策论"></a><strong>7.1 贝叶斯决策论</strong></h2><p>若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“<strong>条件风险</strong>”（conditional risk）。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd15db94.png" alt="2.png"></p>
<p>我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了<strong>贝叶斯判定准则</strong>（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd308600.png" alt="3.png"></p>
<p>若损失函数λ取0-1损失，则有：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd37c502.png" alt="4.png"></p>
<p>即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：</p>
<pre><code>* 判别式模型：直接对 P（c | x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。
* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。
</code></pre><p>贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换，巴拉拉能量…. P（c | x）变身：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd501ad3.png" alt="5.png"></p>
<p>对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，在看这章之前也是模糊了我好久，这里普及一下它们的基本概念。</p>
<pre><code>* 先验概率： 根据以往经验和分析得到的概率。
* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。
</code></pre><p>实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。下面直接上Wiki上的一个例子，简单粗暴快速完事…</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd799610.png" alt="6.png"></p>
<p>回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。</p>
<h2 id="7-2-极大似然法"><a href="#7-2-极大似然法" class="headerlink" title="7.2 极大似然法"></a><strong>7.2 极大似然法</strong></h2><p>极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd70fb73.png" alt="7.png"></p>
<p>所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p>
<pre><code>* 1.写出似然函数；
* 2.对似然函数取对数，并整理；
* 3.求导数，令偏导数为0，得到似然方程组；
* 4.解似然方程组，得到所有参数即为所求。
</code></pre><p>例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd705729.png" alt="8.png"></p>
<p>上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识，搞统计越来越值钱也是这个道理，大牛们掐指一算比我们搬砖几天更有效果。</p>
<h2 id="7-3-朴素贝叶斯分类器"><a href="#7-3-朴素贝叶斯分类器" class="headerlink" title="7.3 朴素贝叶斯分类器"></a><strong>7.3 朴素贝叶斯分类器</strong></h2><p>不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd55e102.png" alt="9.png"></p>
<p>这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd6678cd.png" alt="10.png"></p>
<p>相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fe54aaed.png" alt="11.png"></p>
<p>当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。</p>
<p>针对朴素贝叶斯，人们觉得它too sample，sometimes too naive！因此又提出了半朴素的贝叶斯分类器，具体有SPODE、TAN、贝叶斯网络等来刻画属性之间的依赖关系，此处不进行深入，等哪天和贝叶斯邂逅了再回来讨论。在此鼎鼎大名的贝叶斯介绍完毕，下一篇将介绍这一章剩下的内容—EM算法，朴素贝叶斯和EM算法同为数据挖掘的十大经典算法，想着还是单独介绍吧~</p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>岭回归</title>
    <url>/2020/07/11/%E5%B2%AD%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p><a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/longley.csv" target="_blank" rel="noopener">longley.csv</a><br><a id="more"></a><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%991.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%992.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%993.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%994.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%995.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%996.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%997.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%998.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%999.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9910.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9911.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9912.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9913.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9914.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = genfromtxt(<span class="string">'./data/longley.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<pre><code>[[     nan      nan      nan      nan      nan      nan      nan      nan]
 [     nan   83.     234.289  235.6    159.     107.608 1947.      60.323]
 [     nan   88.5    259.426  232.5    145.6    108.632 1948.      61.122]
 [     nan   88.2    258.054  368.2    161.6    109.773 1949.      60.171]
 [     nan   89.5    284.599  335.1    165.     110.929 1950.      61.187]
 [     nan   96.2    328.975  209.9    309.9    112.075 1951.      63.221]
 [     nan   98.1    346.999  193.2    359.4    113.27  1952.      63.639]
 [     nan   99.     365.385  187.     354.7    115.094 1953.      64.989]
 [     nan  100.     363.112  357.8    335.     116.219 1954.      63.761]
 [     nan  101.2    397.469  290.4    304.8    117.388 1955.      66.019]
 [     nan  104.6    419.18   282.2    285.7    118.734 1956.      67.857]
 [     nan  108.4    442.769  293.6    279.8    120.445 1957.      68.169]
 [     nan  110.8    444.546  468.1    263.7    121.95  1958.      66.513]
 [     nan  112.6    482.704  381.3    255.2    123.366 1959.      68.655]
 [     nan  114.2    502.601  393.1    251.4    125.368 1960.      69.564]
 [     nan  115.7    518.173  480.6    257.2    127.852 1961.      69.331]
 [     nan  116.9    554.894  400.7    282.7    130.081 1962.      70.551]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = data[<span class="number">1</span>:,<span class="number">2</span>:]</span><br><span class="line">y_data = data[<span class="number">1</span>:,<span class="number">1</span>,np.newaxis]</span><br><span class="line">print(x_data)</span><br><span class="line">print(y_data)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 234.289  235.6    159.     107.608 1947.      60.323]
 [ 259.426  232.5    145.6    108.632 1948.      61.122]
 [ 258.054  368.2    161.6    109.773 1949.      60.171]
 [ 284.599  335.1    165.     110.929 1950.      61.187]
 [ 328.975  209.9    309.9    112.075 1951.      63.221]
 [ 346.999  193.2    359.4    113.27  1952.      63.639]
 [ 365.385  187.     354.7    115.094 1953.      64.989]
 [ 363.112  357.8    335.     116.219 1954.      63.761]
 [ 397.469  290.4    304.8    117.388 1955.      66.019]
 [ 419.18   282.2    285.7    118.734 1956.      67.857]
 [ 442.769  293.6    279.8    120.445 1957.      68.169]
 [ 444.546  468.1    263.7    121.95  1958.      66.513]
 [ 482.704  381.3    255.2    123.366 1959.      68.655]
 [ 502.601  393.1    251.4    125.368 1960.      69.564]
 [ 518.173  480.6    257.2    127.852 1961.      69.331]
 [ 554.894  400.7    282.7    130.081 1962.      70.551]]
[[ 83. ]
 [ 88.5]
 [ 88.2]
 [ 89.5]
 [ 96.2]
 [ 98.1]
 [ 99. ]
 [100. ]
 [101.2]
 [104.6]
 [108.4]
 [110.8]
 [112.6]
 [114.2]
 [115.7]
 [116.9]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.mat(x_data).shape)</span><br><span class="line">print(np.mat(y_data).shape)</span><br><span class="line">X_data = np.concatenate((np.ones((<span class="number">16</span>,<span class="number">1</span>)),x_data),axis=<span class="number">1</span>)</span><br><span class="line">print(X_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(16, 6)
(16, 1)
(16, 7)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(X_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[[1.00000e+00 2.34289e+02 2.35600e+02 1.59000e+02 1.07608e+02 1.94700e+03
  6.03230e+01]
 [1.00000e+00 2.59426e+02 2.32500e+02 1.45600e+02 1.08632e+02 1.94800e+03
  6.11220e+01]
 [1.00000e+00 2.58054e+02 3.68200e+02 1.61600e+02 1.09773e+02 1.94900e+03
  6.01710e+01]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights</span><span class="params">(xArr,yArr,lam=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    rxTx = xTx +np.eye(xMat.shape[<span class="number">1</span>])*lam</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(rxTx) == <span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'This martix cannot do inverse'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = rxTx.I*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = weights(X_data,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 7.38107363e-04]
 [ 2.07703836e-01]
 [ 2.10076376e-02]
 [ 5.05385441e-03]
 [-1.59173066e+00]
 [ 1.10442920e-01]
 [-2.42280461e-01]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.mat(X_data)* np.mat(ws)</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[ 83.55075226],
        [ 86.92588689],
        [ 88.09720227],
        [ 90.95677622],
        [ 96.06951002],
        [ 97.81955375],
        [ 98.36444357],
        [ 99.99814266],
        [103.26832266],
        [105.03165135],
        [107.45224671],
        [109.52190685],
        [112.91863666],
        [113.98357055],
        [115.29845063],
        [117.64279933]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>周志华《Machine Learning》学习笔记(9)</title>
    <url>/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>上篇主要介绍了贝叶斯分类器，从贝叶斯公式到贝叶斯决策论，再到通过极大似然法估计类条件概率，贝叶斯分类器的训练就是参数估计的过程。朴素贝叶斯则是“属性条件独立性假设”下的特例，它避免了假设属性联合分布过于经验性和训练集不足引起参数估计较大偏差两个大问题，最后介绍的拉普拉斯修正将概率值进行平滑处理。本篇将介绍另一个当选为数据挖掘十大算法之一的<strong>EM算法</strong>。<br><a id="more"></a></p>
<p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p>
<div style="text-align:center;font-size:25px">目录</div>

<ul>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li>
<li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li>
</ul>
<h1 id="8、EM算法"><a href="#8、EM算法" class="headerlink" title="8、EM算法"></a><strong>8、EM算法</strong></h1><p>EM（Expectation-Maximization）算法是一种常用的估计参数隐变量的利器，也称为“期望最大算法”，是数据挖掘的十大经典算法之一。EM算法主要应用于训练集样本不完整即存在隐变量时的情形（例如某个属性值未知），通过其独特的“两步走”策略能较好地估计出隐变量的值。</p>
<h2 id="8-1-EM算法思想"><a href="#8-1-EM算法思想" class="headerlink" title="8.1 EM算法思想"></a><strong>8.1 EM算法思想</strong></h2><p>EM是一种迭代式的方法，它的基本思想就是：若样本服从的分布参数θ已知，则可以根据已观测到的训练样本推断出隐变量Z的期望值（E步），若Z的值已知则运用最大似然法估计出新的θ值（M步）。重复这个过程直到Z和θ值不再发生变化。</p>
<p>简单来讲：假设我们想估计A和B这两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc843bf53eb2.png" alt="1.png"></p>
<p>现在再来回想聚类的代表算法K-Means：【首先随机选择类中心=&gt;将样本点划分到类簇中=&gt;重新计算类中心=&gt;不断迭代直至收敛】，不难发现这个过程和EM迭代的方法极其相似，事实上，若将样本的类别看做为“隐变量”（latent variable）Z，类中心看作样本的分布参数θ，K-Means就是通过EM算法来进行迭代的，与我们这里不同的是，K-Means的目标是最小化样本点到其对应类中心的距离和，上述为极大化似然函数。</p>
<h2 id="8-2-EM算法数学推导"><a href="#8-2-EM算法数学推导" class="headerlink" title="8.2 EM算法数学推导"></a><strong>8.2 EM算法数学推导</strong></h2><p>在上篇极大似然法中，当样本属性值都已知时，我们很容易通过极大化对数似然，接着对每个参数求偏导计算出参数的值。但当存在隐变量时，就无法直接求解，此时我们通常最大化已观察数据的对数“边际似然”（marginal likelihood）。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc843bfd84d2.png" alt="2.png"></p>
<p>这时候，通过边缘似然将隐变量Z引入进来，对于参数估计，现在与最大似然不同的只是似然函数式中多了一个未知的变量Z，也就是说我们的目标是找到适合的θ和Z让L(θ)最大，这样我们也可以分别对未知的θ和Z求偏导，再令其等于0。</p>
<p>然而观察上式可以发现，和的对数（ln(x1+x2+x3)）求导十分复杂，那能否通过变换上式得到一种求导简单的新表达式呢？这时候 Jensen不等式就派上用场了，先回顾一下高等数学凸函数的内容：</p>
<p><strong>Jensen’s inequality</strong>：过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方。理解起来也十分简单，对于凸函数f(x)’’&gt;0，即曲线的变化率是越来越大单调递增的，所以函数越到后面增长越厉害，这样在一个区间下，函数的均值就会大一些了。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc843c064c72.png" alt="3.png"></p>
<p>因为ln(*)函数为凹函数，故可以将上式“和的对数”变为“对数的和”，这样就很容易求导了。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc843c3490ad.png" alt="4.png"></p>
<p>接着求解Qi和θ：首先固定θ（初始值），通过求解Qi使得J（θ，Q）在θ处与L（θ）相等，即求出L（θ）的下界；然后再固定Qi，调整θ，最大化下界J（θ，Q）。不断重复两个步骤直到稳定。通过jensen不等式的性质，Qi的计算公式实际上就是后验概率：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc843c21276c.png" alt="5.png"></p>
<p>通过数学公式的推导，简单来理解这一过程：固定θ计算Q的过程就是在建立L（θ）的下界，即通过jenson不等式得到的下界（E步）；固定Q计算θ则是使得下界极大化（M步），从而不断推高边缘似然L（θ）。从而循序渐进地计算出L（θ）取得极大值时隐变量Z的估计值。</p>
<p>EM算法也可以看作一种“坐标下降法”，首先固定一个值，对另外一个值求极值，不断重复直到收敛。这时候也许大家就有疑问，问什么不直接这两个家伙求偏导用梯度下降呢？这时候就是坐标下降的优势，有些特殊的函数，例如曲线函数z=y^2+x^2+x^2y+xy+…，无法直接求导，这时如果先固定其中的一个变量，再对另一个变量求极值，则变得可行。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc843c34e7ff.png" alt="6.png"></p>
<h2 id="8-3-EM算法流程"><a href="#8-3-EM算法流程" class="headerlink" title="8.3 EM算法流程"></a><strong>8.3 EM算法流程</strong></h2><p>看完数学推导，算法的流程也就十分简单了，这里有两个版本，版本一来自西瓜书，周天使的介绍十分简洁；版本二来自于大牛的博客。结合着数学推导，自认为版本二更具有逻辑性，两者唯一的区别就在于版本二多出了红框的部分，这里我也没得到答案，欢迎骚扰讨论~</p>
<p><strong>版本一：</strong></p>
<p><img src="https://i.loli.net/2018/10/18/5bc843c0e19db.png" alt="7.png"></p>
<p><strong>版本二：</strong></p>
<p><img src="https://i.loli.net/2018/10/18/5bc843c34775b.png" alt="8.png"></p>
]]></content>
      <categories>
        <category>周志华《Machine Learning》学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>周志华《Machine Learning》学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>标准方程法</title>
    <url>/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/</url>
    <content><![CDATA[<p>标准方程法求线性回归代码展示:<br>数据集下载<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/data.csv" target="_blank" rel="noopener">data</a><br><a id="more"></a></p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%951.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%952.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%953.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%954.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%955.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%956.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%957.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%958.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data =np.genfromtxt(<span class="string">'./data/data.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">x_data = data[:,<span class="number">0</span>,np.newaxis]</span><br><span class="line">y_data = data[:,<span class="number">1</span>,np.newaxis]</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.mat(x_data).shape)</span><br><span class="line">print(np.mat(y_data).shape)</span><br><span class="line">X_data = np.concatenate((np.ones((<span class="number">100</span>,<span class="number">1</span>)),x_data),axis=<span class="number">1</span>)</span><br><span class="line">print(X_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(100, 1)
(100, 1)
(100, 2)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(X_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.         32.50234527]
 [ 1.         53.42680403]
 [ 1.         61.53035803]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) ==<span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'This martix cannot do inverse'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = xTx.I*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = weights(X_data,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>
<pre><code>[[7.99102098]
 [1.32243102]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test = np.array([[<span class="number">20</span>],[<span class="number">80</span>]])</span><br><span class="line">y_test = ws[<span class="number">0</span>]+x_test*ws[<span class="number">1</span>]</span><br><span class="line">plt.plot(x_data,y_data,<span class="string">'b.'</span>)</span><br><span class="line">plt.plot(x_test,y_test,<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/output_7_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降法</title>
    <url>/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</url>
    <content><![CDATA[<p>线性回归梯度下降代码简单演示</p>
<p>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/data.csv" target="_blank" rel="noopener">data.csv</a><br><a id="more"></a><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%951.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%952.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%953.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%954.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%955.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%956.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%957.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%958.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%959.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9510.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9511.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9512.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9513.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9514.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9515.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9516.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9517.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/data.csv'</span>,delimiter=<span class="string">","</span>)</span><br><span class="line">x_data = data[:,<span class="number">0</span>]</span><br><span class="line">y_data = data[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">k = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span><span class="params">(b,k,x_data,y_data)</span>:</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        totalError += (y_data[i]-(k*x_data[i]+b))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/float(len(x_data))/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span><span class="params">(x_data,y_data,b,k,lr,epochs)</span>:</span></span><br><span class="line">    m = float(len(x_data))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        b_grad = <span class="number">0</span></span><br><span class="line">        k_grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,len(x_data)):</span><br><span class="line">            b_grad += (<span class="number">1</span>/m)*(((k*x_data[j])+b)-y_data[j])</span><br><span class="line">            k_grad += (<span class="number">1</span>/m)*x_data[j]*(((k*x_data[j])+b)-y_data[j])</span><br><span class="line">        b = b-(lr*b_grad)</span><br><span class="line">        k = k-(lr*k_grad)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epochs:"</span>,i)</span><br><span class="line">            plt.plot(x_data,y_data,<span class="string">'b.'</span>)</span><br><span class="line">            plt.plot(x_data, k*x_data+b, <span class="string">'r'</span>)</span><br><span class="line">            plt.show()</span><br><span class="line">    <span class="keyword">return</span> b,k</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Starting b = &#123;0&#125;, k = &#123;1&#125;, error = &#123;2&#125;"</span>.format(b, k, compute_error(b, k, x_data, y_data)))</span><br><span class="line">print(<span class="string">"Running..."</span>)</span><br><span class="line">b, k = gradient_descent_runner(x_data, y_data, b, k, lr, epochs)</span><br><span class="line">print(<span class="string">"After &#123;0&#125; iterations b = &#123;1&#125;, k = &#123;2&#125;, error = &#123;3&#125;"</span>.format(epochs, b, k, compute_error(b, k, x_data, y_data)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.plot(x_data, y_data, <span class="string">'b.'</span>)</span><br><span class="line">plt.plot(x_data, k*x_data + b, <span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Starting b = 0, k = 0, error = 2782.5539172416056
Running...
epochs: 0
</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_1.png" alt="png"></p>
<pre><code>epochs: 10
</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_3.png" alt="png"></p>
<pre><code>epochs: 20
</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_5.png" alt="png"></p>
<pre><code>epochs: 30
</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_7.png" alt="png"></p>
<pre><code>epochs: 40
</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_9.png" alt="png"></p>
<pre><code>After 50 iterations b = 0.030569950649287983, k = 1.4788903781318357, error = 56.32488184238028
</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_11.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x0,x1 = np.meshgrid([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x0</span><br></pre></td></tr></table></figure>
<pre><code>array([[1, 2, 3, 4],
       [1, 2, 3, 4]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1</span><br></pre></td></tr></table></figure>
<pre><code>array([[4, 4, 4, 4],
       [5, 5, 5, 5]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战-泰坦尼克号获救预测</title>
    <url>/2020/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B/</url>
    <content><![CDATA[<p>数据集:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/titanic_train.csv" target="_blank" rel="noopener">titanic_train.csv</a><br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">titianic = pandas.read_csv(<span class="string">'../data/titanic_train.csv'</span>)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[<span class="string">'Age'</span>]=titianic[<span class="string">'Age'</span>].fillna(titianic[<span class="string">'Age'</span>].median())</span><br><span class="line">print(titianic.describe())</span><br></pre></td></tr></table></figure>
<pre><code>       PassengerId    Survived      Pclass         Age       SibSp  \
count   891.000000  891.000000  891.000000  891.000000  891.000000   
mean    446.000000    0.383838    2.308642   29.361582    0.523008   
std     257.353842    0.486592    0.836071   13.019697    1.102743   
min       1.000000    0.000000    1.000000    0.420000    0.000000   
25%     223.500000    0.000000    2.000000   22.000000    0.000000   
50%     446.000000    0.000000    3.000000   28.000000    0.000000   
75%     668.500000    1.000000    3.000000   35.000000    1.000000   
max     891.000000    1.000000    3.000000   80.000000    8.000000   

            Parch        Fare  
count  891.000000  891.000000  
mean     0.381594   32.204208  
std      0.806057   49.693429  
min      0.000000    0.000000  
25%      0.000000    7.910400  
50%      0.000000   14.454200  
75%      0.000000   31.000000  
max      6.000000  512.329200  
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(titianic[<span class="string">'Sex'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Sex'</span>] = titianic[<span class="string">'Sex'</span>].map(&#123;<span class="string">'male'</span>:<span class="number">0</span>,<span class="string">'female'</span>:<span class="number">1</span>&#125;)</span><br><span class="line"><span class="comment"># 把male变成0，把female变成1</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "male", "Sex"] = 0</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "female", "Sex"] = 1</span></span><br></pre></td></tr></table></figure>
<pre><code>[&#39;male&#39; &#39;female&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(titianic[<span class="string">'Embarked'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>)</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].map(&#123;<span class="string">'S'</span>:<span class="number">0</span>,<span class="string">'C'</span>:<span class="number">1</span>,<span class="string">'Q'</span>:<span class="number">2</span>&#125;)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;S&#39; &#39;C&#39; &#39;Q&#39; nan]
</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">predictors = [<span class="string">'Pclass'</span>,<span class="string">'Sex'</span>,<span class="string">'Age'</span>,<span class="string">'SibSp'</span>,<span class="string">'Parch'</span>,<span class="string">'Fare'</span>,<span class="string">'Embarked'</span>]</span><br><span class="line">x_data = titianic[predictors]</span><br><span class="line">y_data = titianic[<span class="string">'Survived'</span>]</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">x_data = scaler.fit_transform(x_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">LR = LogisticRegression()</span><br><span class="line">scores = model_selection.cross_val_score(LR,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>0.7901234567901234
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">20</span>,<span class="number">10</span>),max_iter=<span class="number">1000</span>)</span><br><span class="line">scores = model_selection.cross_val_score(mlp,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.8002244668911335
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line">knn = neighbors.KNeighborsClassifier(<span class="number">21</span>)</span><br><span class="line">scores=model_selection.cross_val_score(knn,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>0.8125701459034792
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">dtree = tree.DecisionTreeClassifier(max_depth=<span class="number">5</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(dtree,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>0.8080808080808081
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">RF1 = RandomForestClassifier(random_state=<span class="number">1</span>,n_estimators=<span class="number">10</span>,min_samples_split=<span class="number">2</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF1,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>0.7991021324354657
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RF2 = RandomForestClassifier(n_estimators=<span class="number">100</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF2,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>0.8125701459034792
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line">bagging_clf = BaggingClassifier(RF2,n_estimators=<span class="number">20</span>)</span><br><span class="line">scores=model_selection.cross_val_score(bagging_clf,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>0.819304152637486
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">adaboost = AdaBoostClassifier(bagging_clf,n_estimators=<span class="number">10</span>)</span><br><span class="line">scores=model_selection.cross_val_score(adaboost,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>0.8181818181818182
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line">sclf = StackingClassifier(classifiers=[bagging_clf,mlp,LR],</span><br><span class="line">                         meta_classifier=LogisticRegression())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.819304152637486
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sclf2 = VotingClassifier([(<span class="string">'adaboost'</span>,adaboost),(<span class="string">'mlp'</span>,mlp),(<span class="string">'LR'</span>,LR),(<span class="string">'knn'</span>,knn),(<span class="string">'dtree'</span>,dtree)])</span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf2,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.8159371492704826
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降法———多元</title>
    <url>/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E2%80%94%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83/</url>
    <content><![CDATA[<p>线性回归多元梯度下降代码简单演示</p>
<p>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/Delivery.csv" target="_blank" rel="noopener">Delivery.csv</a><br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = genfromtxt(<span class="string">'./data/Delivery.csv'</span>, delimiter=<span class="string">","</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<pre><code>[[100.    4.    9.3]
 [ 50.    3.    4.8]
 [100.    4.    8.9]
 [100.    2.    6.5]
 [ 50.    2.    4.2]
 [ 80.    2.    6.2]
 [ 75.    3.    7.4]
 [ 65.    4.    6. ]
 [ 90.    3.    7.6]
 [ 90.    2.    6.1]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = data[:,:<span class="number">-1</span>]</span><br><span class="line">y_data = data[:,<span class="number">-1</span>]</span><br><span class="line">print(x_data)</span><br><span class="line">print(y_data)</span><br></pre></td></tr></table></figure>
<pre><code>[[100.   4.]
 [ 50.   3.]
 [100.   4.]
 [100.   2.]
 [ 50.   2.]
 [ 80.   2.]
 [ 75.   3.]
 [ 65.   4.]
 [ 90.   3.]
 [ 90.   2.]]
[9.3 4.8 8.9 6.5 4.2 6.2 7.4 6.  7.6 6.1]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">theta0 = <span class="number">0</span></span><br><span class="line">theta1 = <span class="number">0</span></span><br><span class="line">theta2 = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span><span class="params">(theta0,theta1,theta2,x_data,y_data)</span>:</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        totalError += (y_data[i]-(theta1*x_data[i,<span class="number">0</span>]+theta2*x_data[i,<span class="number">1</span>]+theta0))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/float(len(x_data))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span><span class="params">(x_data,y_data,theta0,theta1,theta2,lr,epochs)</span>:</span></span><br><span class="line">    m = float(len(x_data))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        theta0_grad = <span class="number">0</span></span><br><span class="line">        theta1_grad = <span class="number">0</span></span><br><span class="line">        theta2_grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,len(x_data)):</span><br><span class="line">            theta0_grad+=(<span class="number">1</span>/m)*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]))</span><br><span class="line">            theta1_grad+=(<span class="number">1</span>/m)*x_data[j,<span class="number">0</span>]*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]+theta0)-y_data[j])</span><br><span class="line">            theta2_grad+=(<span class="number">1</span>/m)*x_data[j,<span class="number">1</span>]*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]+theta0)-y_data[j])</span><br><span class="line">            theta0 = theta0-(lr*theta0_grad)</span><br><span class="line">            theta1 = theta1-(lr*theta1_grad)</span><br><span class="line">            theta2 = theta2-(lr*theta2_grad)</span><br><span class="line">    <span class="keyword">return</span> theta0,theta1,theta2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = plt.figure().add_subplot(<span class="number">111</span>,projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],y_data,c=<span class="string">'r'</span>,marker=<span class="string">'o'</span>,s=<span class="number">100</span>)</span><br><span class="line">x0=x_data[:,<span class="number">0</span>]</span><br><span class="line">x1=x_data[:,<span class="number">1</span>]</span><br><span class="line">x0,x1 = np.meshgrid(x0,x1)</span><br><span class="line">z = theta0+x0*theta1+x1*theta2</span><br><span class="line"></span><br><span class="line">ax.plot_surface(x0,x1,z)</span><br><span class="line">ax.set_xlabel(<span class="string">'Miles'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Num of Deliveries'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E2%80%94%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习bagging</title>
    <url>/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/</url>
    <content><![CDATA[<p>bagging代码简单演示<br><a id="more"></a></p>
<p>个体学习器之间不存在强依赖关系，装袋（bagging）<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data=iris.data[:,:<span class="number">2</span>]</span><br><span class="line">y_data=iris.target</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = neighbors.KNeighborsClassifier()</span><br><span class="line">knn.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>
<pre><code>KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights=&#39;uniform&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据值所在的范围</span></span><br><span class="line">    x_min, x_max = x_data[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = x_data[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成网格矩阵</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                         np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line">    </span><br><span class="line">    z=model.predict(np.c_[xx.ravel(),yy.ravel()])</span><br><span class="line">    z = z.reshape(xx.shape)</span><br><span class="line">    cs = plt.contourf(xx,yy,z)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot(knn)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">knn.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_5_0.png" alt="png"></p>
<pre><code>0.631578947368421
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dtree = tree.DecisionTreeClassifier()</span><br><span class="line">dtree.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>
<pre><code>DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;,
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;,
                       random_state=None, splitter=&#39;best&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot(dtree)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">dtree.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_7_0.png" alt="png"></p>
<pre><code>0.6052631578947368
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bagging_knn = BaggingClassifier(knn,n_estimators=<span class="number">100</span>)</span><br><span class="line">bagging_knn.fit(x_train,y_train)</span><br><span class="line">plot(bagging_knn)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">bagging_knn.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_8_0.png" alt="png"></p>
<pre><code>0.631578947368421
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bagging_tree = BaggingClassifier(dtree,n_estimators=<span class="number">100</span>)</span><br><span class="line">bagging_tree.fit(x_train,y_train)</span><br><span class="line">plot(bagging_tree)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">-0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">bagging_tree.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_9_0.png" alt="png"></p>
<pre><code>0.6578947368421053
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习stacking</title>
    <url>/2020/07/11/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0stacking/</url>
    <content><![CDATA[<p>stacking代码简单演示<br><a id="more"></a></p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data,y_data=iris.data[:,<span class="number">1</span>:<span class="number">3</span>],iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = DecisionTreeClassifier()</span><br><span class="line">clf3 = LogisticRegression()</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1,clf2,clf3],meta_classifier=lr)</span><br><span class="line"><span class="keyword">for</span> clf,label <span class="keyword">in</span> zip([clf1,clf2,clf3,sclf],</span><br><span class="line">                    [<span class="string">'KNN'</span>,<span class="string">'Decision Tree'</span>,<span class="string">'LogisticRegression'</span>,<span class="string">'StackingClassifier'</span>]):</span><br><span class="line">    scores = model_selection.cross_val_score(clf,x_data,y_data,cv=<span class="number">3</span>,scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">'Accuracy:%0.2f [%s]'</span> %(scores.mean(),label))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy:0.91 [KNN]
Accuracy:0.93 [Decision Tree]
Accuracy:0.95 [LogisticRegression]
Accuracy:0.93 [StackingClassifier]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯单词拼写器</title>
    <url>/2020/07/11/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8D%95%E8%AF%8D%E6%8B%BC%E5%86%99%E5%99%A8/</url>
    <content><![CDATA[<p>贝叶斯单词拼写器代码:<br>数据集下载:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/big.txt" target="_blank" rel="noopener">big.txt</a><br><a id="more"></a><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF7.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF9.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF10.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF11.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF12.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF13.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF14.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF15.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF16.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF17.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF18.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF19.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = open(<span class="string">'./data/big.txt'</span>).read()</span><br><span class="line">text = re.findall(<span class="string">'[a-z]+'</span>, text.lower())</span><br><span class="line">dic_words = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> text:</span><br><span class="line">    dic_words[t]=dic_words.get(t,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">dic_words</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;the&#39;: 80030,
 &#39;project&#39;: 288,
 &#39;gutenberg&#39;: 263,
 &#39;ebook&#39;: 87,
 &#39;of&#39;: 40025,
 &#39;adventures&#39;: 17,
 &#39;sherlock&#39;: 101,
 &#39;holmes&#39;: 467,
 &#39;by&#39;: 6738,
 &#39;sir&#39;: 177,
 &#39;arthur&#39;: 34,
 &#39;conan&#39;: 4,
 &#39;doyle&#39;: 5,
 &#39;in&#39;: 22047,
 &#39;our&#39;: 1066,
 &#39;series&#39;: 128,
 &#39;copyright&#39;: 69,
 &#39;laws&#39;: 233,
 &#39;are&#39;: 3630,
 &#39;changing&#39;: 44,
 &#39;all&#39;: 4144,
 &#39;over&#39;: 1282,
 &#39;world&#39;: 362,
 &#39;be&#39;: 6155,
 &#39;sure&#39;: 123,
 &#39;to&#39;: 28766,
 &#39;check&#39;: 38,
 &#39;for&#39;: 6939,
 &#39;your&#39;: 1279,
 &#39;country&#39;: 423,
 &#39;before&#39;: 1363,
 &#39;downloading&#39;: 5,
 &#39;or&#39;: 5352,
 &#39;redistributing&#39;: 7,
 &#39;this&#39;: 4063,
 &#39;any&#39;: 1204,
 &#39;other&#39;: 1502,
 &#39;header&#39;: 7,
 &#39;should&#39;: 1297,
 &#39;first&#39;: 1177,
 &#39;thing&#39;: 303,
 &#39;seen&#39;: 444,
 &#39;when&#39;: 2923,
 &#39;viewing&#39;: 7,
 &#39;file&#39;: 21,
 &#39;please&#39;: 172,
 &#39;do&#39;: 1503,
 &#39;not&#39;: 6625,
 &#39;remove&#39;: 53,
 &#39;it&#39;: 10681,
 &#39;change&#39;: 150,
 &#39;edit&#39;: 4,
 &#39;without&#39;: 1015,
 &#39;written&#39;: 117,
 &#39;permission&#39;: 52,
 &#39;read&#39;: 218,
 &#39;legal&#39;: 52,
 &#39;small&#39;: 527,
 &#39;print&#39;: 47,
 &#39;and&#39;: 38312,
 &#39;information&#39;: 73,
 &#39;about&#39;: 1497,
 &#39;at&#39;: 6791,
 &#39;bottom&#39;: 42,
 &#39;included&#39;: 43,
 &#39;is&#39;: 9774,
 &#39;important&#39;: 285,
 &#39;specific&#39;: 37,
 &#39;rights&#39;: 168,
 &#39;restrictions&#39;: 23,
 &#39;how&#39;: 1315,
 &#39;may&#39;: 2551,
 &#39;used&#39;: 276,
 &#39;you&#39;: 5622,
 &#39;can&#39;: 1095,
 &#39;also&#39;: 778,
 &#39;find&#39;: 294,
 &#39;out&#39;: 1987,
 &#39;make&#39;: 504,
 &#39;a&#39;: 21155,
 &#39;donation&#39;: 10,
 &#39;get&#39;: 468,
 &#39;involved&#39;: 107,
 &#39;welcome&#39;: 18,
 &#39;free&#39;: 421,
 &#39;plain&#39;: 108,
 &#39;vanilla&#39;: 6,
 &#39;electronic&#39;: 58,
 &#39;texts&#39;: 7,
 &#39;ebooks&#39;: 54,
 &#39;readable&#39;: 13,
 &#39;both&#39;: 529,
 &#39;humans&#39;: 2,
 &#39;computers&#39;: 7,
 &#39;since&#39;: 260,
 &#39;these&#39;: 1231,
 &#39;were&#39;: 4289,
 &#39;prepared&#39;: 138,
 &#39;thousands&#39;: 93,
 &#39;volunteers&#39;: 22,
 &#39;title&#39;: 39,
 &#39;author&#39;: 29,
 &#39;release&#39;: 28,
 &#39;date&#39;: 48,
 &#39;march&#39;: 135,
 &#39;most&#39;: 908,
 &#39;recently&#39;: 30,
 &#39;updated&#39;: 4,
 &#39;november&#39;: 41,
 &#39;edition&#39;: 21,
 &#39;language&#39;: 61,
 &#39;english&#39;: 211,
 &#39;character&#39;: 174,
 &#39;set&#39;: 324,
 &#39;encoding&#39;: 5,
 &#39;ascii&#39;: 11,
 &#39;start&#39;: 67,
 &#39;additional&#39;: 30,
 &#39;editing&#39;: 6,
 &#39;jose&#39;: 1,
 &#39;menendez&#39;: 1,
 &#39;contents&#39;: 50,
 &#39;i&#39;: 7682,
 &#39;scandal&#39;: 19,
 &#39;bohemia&#39;: 15,
 &#39;ii&#39;: 77,
 &#39;red&#39;: 288,
 &#39;headed&#39;: 37,
 &#39;league&#39;: 53,
 &#39;iii&#39;: 91,
 &#39;case&#39;: 438,
 &#39;identity&#39;: 11,
 &#39;iv&#39;: 55,
 &#39;boscombe&#39;: 16,
 &#39;valley&#39;: 78,
 &#39;mystery&#39;: 39,
 &#39;v&#39;: 51,
 &#39;five&#39;: 279,
 &#39;orange&#39;: 23,
 &#39;pips&#39;: 12,
 &#39;vi&#39;: 37,
 &#39;man&#39;: 1652,
 &#39;with&#39;: 9740,
 &#39;twisted&#39;: 21,
 &#39;lip&#39;: 56,
 &#39;vii&#39;: 34,
 &#39;adventure&#39;: 34,
 &#39;blue&#39;: 143,
 &#39;carbuncle&#39;: 17,
 &#39;viii&#39;: 39,
 &#39;speckled&#39;: 5,
 &#39;band&#39;: 54,
 &#39;ix&#39;: 28,
 &#39;engineer&#39;: 12,
 &#39;s&#39;: 5631,
 &#39;thumb&#39;: 51,
 &#39;x&#39;: 136,
 &#39;noble&#39;: 48,
 &#39;bachelor&#39;: 18,
 &#39;xi&#39;: 28,
 &#39;beryl&#39;: 4,
 &#39;coronet&#39;: 29,
 &#39;xii&#39;: 28,
 &#39;copper&#39;: 26,
 &#39;beeches&#39;: 12,
 &#39;she&#39;: 3946,
 &#39;always&#39;: 608,
 &#39;woman&#39;: 325,
 &#39;have&#39;: 3493,
 &#39;seldom&#39;: 76,
 &#39;heard&#39;: 636,
 &#39;him&#39;: 5230,
 &#39;mention&#39;: 46,
 &#39;her&#39;: 5284,
 &#39;under&#39;: 963,
 &#39;name&#39;: 262,
 &#39;his&#39;: 10034,
 &#39;eyes&#39;: 939,
 &#39;eclipses&#39;: 2,
 &#39;predominates&#39;: 3,
 &#39;whole&#39;: 744,
 &#39;sex&#39;: 11,
 &#39;was&#39;: 11410,
 &#39;that&#39;: 12512,
 &#39;he&#39;: 12401,
 &#39;felt&#39;: 697,
 &#39;emotion&#39;: 36,
 &#39;akin&#39;: 14,
 &#39;love&#39;: 484,
 &#39;irene&#39;: 18,
 &#39;adler&#39;: 16,
 &#39;emotions&#39;: 10,
 &#39;one&#39;: 3371,
 &#39;particularly&#39;: 174,
 &#39;abhorrent&#39;: 1,
 &#39;cold&#39;: 257,
 &#39;precise&#39;: 13,
 &#39;but&#39;: 5653,
 &#39;admirably&#39;: 7,
 &#39;balanced&#39;: 6,
 &#39;mind&#39;: 341,
 &#39;take&#39;: 616,
 &#39;perfect&#39;: 39,
 &#39;reasoning&#39;: 41,
 &#39;observing&#39;: 21,
 &#39;machine&#39;: 39,
 &#39;has&#39;: 1603,
 &#39;as&#39;: 8064,
 &#39;lover&#39;: 26,
 &#39;would&#39;: 1953,
 &#39;placed&#39;: 182,
 &#39;himself&#39;: 1158,
 &#39;false&#39;: 64,
 &#39;position&#39;: 432,
 &#39;never&#39;: 593,
 &#39;spoke&#39;: 218,
 &#39;softer&#39;: 10,
 &#39;passions&#39;: 29,
 &#39;save&#39;: 110,
 &#39;gibe&#39;: 2,
 &#39;sneer&#39;: 6,
 &#39;they&#39;: 3938,
 &#39;admirable&#39;: 14,
 &#39;things&#39;: 321,
 &#39;observer&#39;: 13,
 &#39;excellent&#39;: 62,
 &#39;drawing&#39;: 240,
 &#39;veil&#39;: 16,
 &#39;from&#39;: 5709,
 &#39;men&#39;: 1145,
 &#39;motives&#39;: 14,
 &#39;actions&#39;: 77,
 &#39;trained&#39;: 23,
 &#39;reasoner&#39;: 6,
 &#39;admit&#39;: 65,
 &#39;such&#39;: 1436,
 &#39;intrusions&#39;: 1,
 &#39;into&#39;: 2124,
 &#39;own&#39;: 785,
 &#39;delicate&#39;: 54,
 &#39;finely&#39;: 11,
 &#39;adjusted&#39;: 16,
 &#39;temperament&#39;: 5,
 &#39;introduce&#39;: 23,
 &#39;distracting&#39;: 1,
 &#39;factor&#39;: 41,
 &#39;which&#39;: 4842,
 &#39;might&#39;: 536,
 &#39;throw&#39;: 48,
 &#39;doubt&#39;: 152,
 &#39;upon&#39;: 1111,
 &#39;mental&#39;: 37,
 &#39;results&#39;: 229,
 &#39;grit&#39;: 1,
 &#39;sensitive&#39;: 35,
 &#39;instrument&#39;: 35,
 &#39;crack&#39;: 20,
 &#39;high&#39;: 290,
 &#39;power&#39;: 548,
 &#39;lenses&#39;: 1,
 &#39;more&#39;: 1997,
 &#39;disturbing&#39;: 9,
 &#39;than&#39;: 1206,
 &#39;strong&#39;: 168,
 &#39;nature&#39;: 170,
 &#39;yet&#39;: 488,
 &#39;there&#39;: 2972,
 &#39;late&#39;: 165,
 &#39;dubious&#39;: 1,
 &#39;questionable&#39;: 3,
 &#39;memory&#39;: 55,
 &#39;had&#39;: 7383,
 &#39;little&#39;: 1001,
 &#39;lately&#39;: 22,
 &#39;my&#39;: 2249,
 &#39;marriage&#39;: 96,
 &#39;drifted&#39;: 5,
 &#39;us&#39;: 684,
 &#39;away&#39;: 838,
 &#39;each&#39;: 411,
 &#39;complete&#39;: 145,
 &#39;happiness&#39;: 143,
 &#39;home&#39;: 295,
 &#39;centred&#39;: 2,
 &#39;interests&#39;: 118,
 &#39;rise&#39;: 240,
 &#39;up&#39;: 2284,
 &#39;around&#39;: 271,
 &#39;who&#39;: 3050,
 &#39;finds&#39;: 23,
 &#39;master&#39;: 141,
 &#39;establishment&#39;: 40,
 &#39;sufficient&#39;: 75,
 &#39;absorb&#39;: 4,
 &#39;attention&#39;: 191,
 &#39;while&#39;: 768,
 &#39;loathed&#39;: 1,
 &#39;every&#39;: 650,
 &#39;form&#39;: 507,
 &#39;society&#39;: 169,
 &#39;bohemian&#39;: 8,
 &#39;soul&#39;: 168,
 &#39;remained&#39;: 231,
 &#39;lodgings&#39;: 11,
 &#39;baker&#39;: 49,
 &#39;street&#39;: 180,
 &#39;buried&#39;: 21,
 &#39;among&#39;: 451,
 &#39;old&#39;: 1180,
 &#39;books&#39;: 59,
 &#39;alternating&#39;: 2,
 &#39;week&#39;: 95,
 &#39;between&#39;: 654,
 &#39;cocaine&#39;: 4,
 &#39;ambition&#39;: 13,
 &#39;drowsiness&#39;: 4,
 &#39;drug&#39;: 21,
 &#39;fierce&#39;: 12,
 &#39;energy&#39;: 45,
 &#39;keen&#39;: 32,
 &#39;still&#39;: 922,
 &#39;ever&#39;: 274,
 &#39;deeply&#39;: 77,
 &#39;attracted&#39;: 36,
 &#39;study&#39;: 144,
 &#39;crime&#39;: 61,
 &#39;occupied&#39;: 116,
 &#39;immense&#39;: 77,
 &#39;faculties&#39;: 8,
 &#39;extraordinary&#39;: 74,
 &#39;powers&#39;: 149,
 &#39;observation&#39;: 39,
 &#39;following&#39;: 208,
 &#39;those&#39;: 1201,
 &#39;clues&#39;: 3,
 &#39;clearing&#39;: 29,
 &#39;mysteries&#39;: 9,
 &#39;been&#39;: 2599,
 &#39;abandoned&#39;: 72,
 &#39;hopeless&#39;: 17,
 &#39;official&#39;: 91,
 &#39;police&#39;: 94,
 &#39;time&#39;: 1529,
 &#39;some&#39;: 1536,
 &#39;vague&#39;: 39,
 &#39;account&#39;: 177,
 &#39;doings&#39;: 11,
 &#39;summons&#39;: 11,
 &#39;odessa&#39;: 3,
 &#39;trepoff&#39;: 1,
 &#39;murder&#39;: 30,
 &#39;singular&#39;: 36,
 &#39;tragedy&#39;: 9,
 &#39;atkinson&#39;: 1,
 &#39;brothers&#39;: 50,
 &#39;trincomalee&#39;: 1,
 &#39;finally&#39;: 156,
 &#39;mission&#39;: 34,
 &#39;accomplished&#39;: 39,
 &#39;so&#39;: 3017,
 &#39;delicately&#39;: 3,
 &#39;successfully&#39;: 25,
 &#39;reigning&#39;: 3,
 &#39;family&#39;: 210,
 &#39;holland&#39;: 12,
 &#39;beyond&#39;: 225,
 &#39;signs&#39;: 98,
 &#39;activity&#39;: 131,
 &#39;however&#39;: 430,
 &#39;merely&#39;: 189,
 &#39;shared&#39;: 25,
 &#39;readers&#39;: 11,
 &#39;daily&#39;: 44,
 &#39;press&#39;: 81,
 &#39;knew&#39;: 496,
 &#39;former&#39;: 177,
 &#39;friend&#39;: 283,
 &#39;companion&#39;: 81,
 &#39;night&#39;: 385,
 &#39;on&#39;: 6643,
 &#39;twentieth&#39;: 19,
 &#39;returning&#39;: 68,
 &#39;journey&#39;: 69,
 &#39;patient&#39;: 383,
 &#39;now&#39;: 1697,
 &#39;returned&#39;: 194,
 &#39;civil&#39;: 177,
 &#39;practice&#39;: 95,
 &#39;way&#39;: 859,
 &#39;led&#39;: 196,
 &#39;me&#39;: 1920,
 &#39;through&#39;: 815,
 &#39;passed&#39;: 367,
 &#39;well&#39;: 1198,
 &#39;remembered&#39;: 120,
 &#39;door&#39;: 498,
 &#39;must&#39;: 955,
 &#39;associated&#39;: 196,
 &#39;wooing&#39;: 2,
 &#39;dark&#39;: 181,
 &#39;incidents&#39;: 14,
 &#39;scarlet&#39;: 22,
 &#39;seized&#39;: 114,
 &#39;desire&#39;: 96,
 &#39;see&#39;: 1101,
 &#39;again&#39;: 866,
 &#39;know&#39;: 1048,
 &#39;employing&#39;: 7,
 &#39;rooms&#39;: 86,
 &#39;brilliantly&#39;: 5,
 &#39;lit&#39;: 74,
 &#39;even&#39;: 946,
 &#39;looked&#39;: 760,
 &#39;saw&#39;: 599,
 &#39;tall&#39;: 74,
 &#39;spare&#39;: 27,
 &#39;figure&#39;: 103,
 &#39;pass&#39;: 154,
 &#39;twice&#39;: 84,
 &#39;silhouette&#39;: 1,
 &#39;against&#39;: 660,
 &#39;blind&#39;: 23,
 &#39;pacing&#39;: 26,
 &#39;room&#39;: 960,
 &#39;swiftly&#39;: 38,
 &#39;eagerly&#39;: 39,
 &#39;head&#39;: 725,
 &#39;sunk&#39;: 27,
 &#39;chest&#39;: 81,
 &#39;hands&#39;: 455,
 &#39;clasped&#39;: 11,
 &#39;behind&#39;: 401,
 &#39;mood&#39;: 51,
 &#39;habit&#39;: 55,
 &#39;attitude&#39;: 72,
 &#39;manner&#39;: 135,
 &#39;told&#39;: 490,
 &#39;their&#39;: 2955,
 &#39;story&#39;: 133,
 &#39;work&#39;: 382,
 &#39;risen&#39;: 30,
 &#39;created&#39;: 62,
 &#39;dreams&#39;: 16,
 &#39;hot&#39;: 119,
 &#39;scent&#39;: 17,
 &#39;new&#39;: 1211,
 &#39;problem&#39;: 76,
 &#39;rang&#39;: 29,
 &#39;bell&#39;: 65,
 &#39;shown&#39;: 113,
 &#39;chamber&#39;: 35,
 &#39;formerly&#39;: 77,
 &#39;part&#39;: 704,
 &#39;effusive&#39;: 2,
 &#39;glad&#39;: 150,
 &#39;think&#39;: 557,
 &#39;hardly&#39;: 173,
 &#39;word&#39;: 298,
 &#39;spoken&#39;: 92,
 &#39;kindly&#39;: 86,
 &#39;eye&#39;: 110,
 &#39;waved&#39;: 29,
 &#39;an&#39;: 3423,
 &#39;armchair&#39;: 49,
 &#39;threw&#39;: 96,
 &#39;across&#39;: 222,
 &#39;cigars&#39;: 7,
 &#39;indicated&#39;: 88,
 &#39;spirit&#39;: 167,
 &#39;gasogene&#39;: 1,
 &#39;corner&#39;: 128,
 &#39;then&#39;: 1558,
 &#39;stood&#39;: 383,
 &#39;fire&#39;: 274,
 &#39;introspective&#39;: 3,
 &#39;fashion&#39;: 49,
 &#39;wedlock&#39;: 1,
 &#39;suits&#39;: 8,
 &#39;remarked&#39;: 169,
 &#39;watson&#39;: 83,
 &#39;put&#39;: 435,
 &#39;seven&#39;: 132,
 &#39;half&#39;: 318,
 &#39;pounds&#39;: 26,
 &#39;answered&#39;: 226,
 &#39;indeed&#39;: 139,
 &#39;thought&#39;: 902,
 &#39;just&#39;: 767,
 &#39;trifle&#39;: 11,
 &#39;fancy&#39;: 50,
 &#39;observe&#39;: 37,
 &#39;did&#39;: 1875,
 &#39;tell&#39;: 492,
 &#39;intended&#39;: 58,
 &#39;go&#39;: 905,
 &#39;harness&#39;: 27,
 &#39;deduce&#39;: 14,
 &#39;getting&#39;: 92,
 &#39;yourself&#39;: 162,
 &#39;very&#39;: 1340,
 &#39;wet&#39;: 60,
 &#39;clumsy&#39;: 8,
 &#39;careless&#39;: 14,
 &#39;servant&#39;: 46,
 &#39;girl&#39;: 166,
 &#39;dear&#39;: 449,
 &#39;said&#39;: 3464,
 &#39;too&#39;: 548,
 &#39;much&#39;: 671,
 &#39;certainly&#39;: 119,
 &#39;burned&#39;: 77,
 &#39;lived&#39;: 113,
 &#39;few&#39;: 458,
 &#39;centuries&#39;: 12,
 &#39;ago&#39;: 108,
 &#39;true&#39;: 205,
 &#39;walk&#39;: 75,
 &#39;thursday&#39;: 7,
 &#39;came&#39;: 979,
 &#39;dreadful&#39;: 68,
 &#39;mess&#39;: 10,
 &#39;changed&#39;: 134,
 &#39;clothes&#39;: 62,
 &#39;t&#39;: 1318,
 &#39;imagine&#39;: 96,
 &#39;mary&#39;: 705,
 &#39;jane&#39;: 2,
 &#39;incorrigible&#39;: 2,
 &#39;wife&#39;: 367,
 &#39;given&#39;: 364,
 &#39;notice&#39;: 98,
 &#39;fail&#39;: 40,
 &#39;chuckled&#39;: 7,
 &#39;rubbed&#39;: 32,
 &#39;long&#39;: 991,
 &#39;nervous&#39;: 54,
 &#39;together&#39;: 260,
 &#39;simplicity&#39;: 30,
 &#39;itself&#39;: 273,
 &#39;inside&#39;: 43,
 &#39;left&#39;: 834,
 &#39;shoe&#39;: 11,
 &#39;where&#39;: 977,
 &#39;firelight&#39;: 2,
 &#39;strikes&#39;: 19,
 &#39;leather&#39;: 35,
 &#39;scored&#39;: 4,
 &#39;six&#39;: 176,
 &#39;almost&#39;: 325,
 &#39;parallel&#39;: 17,
 &#39;cuts&#39;: 5,
 &#39;obviously&#39;: 38,
 &#39;caused&#39;: 102,
 &#39;someone&#39;: 160,
 &#39;carelessly&#39;: 14,
 &#39;scraped&#39;: 21,
 &#39;round&#39;: 556,
 &#39;edges&#39;: 70,
 &#39;sole&#39;: 70,
 &#39;order&#39;: 404,
 &#39;crusted&#39;: 2,
 &#39;mud&#39;: 36,
 &#39;hence&#39;: 32,
 &#39;double&#39;: 49,
 &#39;deduction&#39;: 12,
 &#39;vile&#39;: 16,
 &#39;weather&#39;: 42,
 &#39;malignant&#39;: 88,
 &#39;boot&#39;: 22,
 &#39;slitting&#39;: 2,
 &#39;specimen&#39;: 14,
 &#39;london&#39;: 76,
 &#39;slavey&#39;: 1,
 &#39;if&#39;: 2372,
 &#39;gentleman&#39;: 99,
 &#39;walks&#39;: 10,
 &#39;smelling&#39;: 5,
 &#39;iodoform&#39;: 43,
 &#39;black&#39;: 235,
 &#39;mark&#39;: 38,
 &#39;nitrate&#39;: 7,
 &#39;silver&#39;: 128,
 &#39;right&#39;: 710,
 &#39;forefinger&#39;: 7,
 &#39;bulge&#39;: 2,
 &#39;side&#39;: 511,
 &#39;top&#39;: 42,
 &#39;hat&#39;: 105,
 &#39;show&#39;: 213,
 &#39;secreted&#39;: 2,
 &#39;stethoscope&#39;: 2,
 &#39;dull&#39;: 74,
 &#39;pronounce&#39;: 9,
 &#39;active&#39;: 96,
 &#39;member&#39;: 50,
 &#39;medical&#39;: 22,
 &#39;profession&#39;: 22,
 &#39;could&#39;: 1700,
 &#39;help&#39;: 230,
 &#39;laughing&#39;: 115,
 &#39;ease&#39;: 44,
 &#39;explained&#39;: 60,
 &#39;process&#39;: 219,
 &#39;hear&#39;: 183,
 &#39;give&#39;: 523,
 &#39;reasons&#39;: 64,
 &#39;appears&#39;: 108,
 &#39;ridiculously&#39;: 1,
 &#39;simple&#39;: 139,
 &#39;easily&#39;: 114,
 &#39;myself&#39;: 227,
 &#39;though&#39;: 650,
 &#39;successive&#39;: 17,
 &#39;instance&#39;: 50,
 &#39;am&#39;: 746,
 &#39;baffled&#39;: 8,
 &#39;until&#39;: 325,
 &#39;explain&#39;: 123,
 &#39;believe&#39;: 183,
 &#39;good&#39;: 744,
 &#39;yours&#39;: 46,
 &#39;quite&#39;: 502,
 &#39;lighting&#39;: 16,
 &#39;cigarette&#39;: 6,
 &#39;throwing&#39;: 46,
 &#39;down&#39;: 1128,
 &#39;distinction&#39;: 19,
 &#39;clear&#39;: 233,
 &#39;example&#39;: 286,
 &#39;frequently&#39;: 218,
 &#39;steps&#39;: 188,
 &#39;lead&#39;: 137,
 &#39;hall&#39;: 83,
 &#39;often&#39;: 443,
 &#39;hundreds&#39;: 48,
 &#39;times&#39;: 236,
 &#39;many&#39;: 609,
 &#39;don&#39;: 581,
 &#39;observed&#39;: 131,
 &#39;point&#39;: 223,
 &#39;seventeen&#39;: 10,
 &#39;because&#39;: 630,
 &#39;interested&#39;: 65,
 &#39;problems&#39;: 78,
 &#39;enough&#39;: 175,
 &#39;chronicle&#39;: 7,
 &#39;two&#39;: 1138,
 &#39;trifling&#39;: 12,
 &#39;experiences&#39;: 11,
 &#39;sheet&#39;: 29,
 &#39;thick&#39;: 77,
 &#39;pink&#39;: 27,
 &#39;tinted&#39;: 9,
 &#39;notepaper&#39;: 2,
 &#39;lying&#39;: 118,
 &#39;open&#39;: 325,
 &#39;table&#39;: 296,
 &#39;last&#39;: 565,
 &#39;post&#39;: 117,
 &#39;aloud&#39;: 28,
 &#39;note&#39;: 115,
 &#39;undated&#39;: 1,
 &#39;either&#39;: 293,
 &#39;signature&#39;: 9,
 &#39;address&#39;: 76,
 &#39;will&#39;: 1577,
 &#39;call&#39;: 197,
 &#39;quarter&#39;: 46,
 &#39;eight&#39;: 128,
 &#39;o&#39;: 257,
 &#39;clock&#39;: 120,
 &#39;desires&#39;: 22,
 &#39;consult&#39;: 19,
 &#39;matter&#39;: 365,
 &#39;deepest&#39;: 15,
 &#39;moment&#39;: 487,
 &#39;recent&#39;: 54,
 &#39;services&#39;: 38,
 &#39;royal&#39;: 111,
 &#39;houses&#39;: 117,
 &#39;europe&#39;: 153,
 &#39;safely&#39;: 11,
 &#39;trusted&#39;: 16,
 &#39;matters&#39;: 136,
 &#39;importance&#39;: 117,
 &#39;exaggerated&#39;: 28,
 &#39;we&#39;: 1906,
 &#39;quarters&#39;: 72,
 &#39;received&#39;: 280,
 &#39;hour&#39;: 157,
 &#39;amiss&#39;: 6,
 &#39;visitor&#39;: 74,
 &#39;wear&#39;: 30,
 &#39;mask&#39;: 12,
 &#39;what&#39;: 3011,
 &#39;means&#39;: 253,
 &#39;no&#39;: 2348,
 &#39;data&#39;: 17,
 &#39;capital&#39;: 144,
 &#39;mistake&#39;: 39,
 &#39;theorise&#39;: 1,
 &#39;insensibly&#39;: 2,
 &#39;begins&#39;: 47,
 &#39;twist&#39;: 14,
 &#39;facts&#39;: 72,
 &#39;suit&#39;: 25,
 &#39;theories&#39;: 21,
 &#39;instead&#39;: 137,
 &#39;carefully&#39;: 72,
 &#39;examined&#39;: 49,
 &#39;writing&#39;: 69,
 &#39;paper&#39;: 177,
 &#39;wrote&#39;: 149,
 &#39;presumably&#39;: 8,
 &#39;endeavouring&#39;: 8,
 &#39;imitate&#39;: 7,
 &#39;processes&#39;: 35,
 &#39;bought&#39;: 55,
 &#39;crown&#39;: 61,
 &#39;packet&#39;: 11,
 &#39;peculiarly&#39;: 14,
 &#39;stiff&#39;: 20,
 &#39;peculiar&#39;: 84,
 &#39;hold&#39;: 114,
 &#39;light&#39;: 278,
 &#39;large&#39;: 483,
 &#39;e&#39;: 136,
 &#39;g&#39;: 55,
 &#39;p&#39;: 66,
 &#39;woven&#39;: 5,
 &#39;texture&#39;: 6,
 &#39;asked&#39;: 777,
 &#39;maker&#39;: 4,
 &#39;monogram&#39;: 4,
 &#39;rather&#39;: 219,
 &#39;stands&#39;: 19,
 &#39;gesellschaft&#39;: 1,
 &#39;german&#39;: 196,
 &#39;company&#39;: 192,
 &#39;customary&#39;: 19,
 &#39;contraction&#39;: 61,
 &#39;like&#39;: 1080,
 &#39;co&#39;: 30,
 &#39;course&#39;: 389,
 &#39;papier&#39;: 1,
 &#39;eg&#39;: 1,
 &#39;let&#39;: 506,
 &#39;glance&#39;: 91,
 &#39;continental&#39;: 46,
 &#39;gazetteer&#39;: 1,
 &#39;took&#39;: 573,
 &#39;heavy&#39;: 139,
 &#39;brown&#39;: 71,
 &#39;volume&#39;: 30,
 &#39;shelves&#39;: 3,
 &#39;eglow&#39;: 1,
 &#39;eglonitz&#39;: 1,
 &#39;here&#39;: 691,
 &#39;egria&#39;: 1,
 &#39;speaking&#39;: 185,
 &#39;far&#39;: 408,
 &#39;carlsbad&#39;: 1,
 &#39;remarkable&#39;: 77,
 &#39;being&#39;: 918,
 &#39;scene&#39;: 49,
 &#39;death&#39;: 330,
 &#39;wallenstein&#39;: 1,
 &#39;its&#39;: 1635,
 &#39;numerous&#39;: 50,
 &#39;glass&#39;: 116,
 &#39;factories&#39;: 29,
 &#39;mills&#39;: 39,
 &#39;ha&#39;: 75,
 &#39;boy&#39;: 169,
 &#39;sparkled&#39;: 5,
 &#39;sent&#39;: 319,
 &#39;great&#39;: 792,
 &#39;triumphant&#39;: 16,
 &#39;cloud&#39;: 30,
 &#39;made&#39;: 1007,
 &#39;precisely&#39;: 24,
 &#39;construction&#39;: 25,
 &#39;sentence&#39;: 26,
 &#39;frenchman&#39;: 102,
 &#39;russian&#39;: 461,
 &#39;uncourteous&#39;: 1,
 &#39;verbs&#39;: 1,
 &#39;only&#39;: 1873,
 &#39;remains&#39;: 73,
 &#39;therefore&#39;: 186,
 &#39;discover&#39;: 28,
 &#39;wanted&#39;: 213,
 &#39;writes&#39;: 20,
 &#39;prefers&#39;: 2,
 &#39;wearing&#39;: 87,
 &#39;showing&#39;: 104,
 &#39;face&#39;: 1125,
 &#39;comes&#39;: 91,
 &#39;mistaken&#39;: 59,
 &#39;resolve&#39;: 14,
 &#39;doubts&#39;: 39,
 &#39;sharp&#39;: 83,
 &#39;sound&#39;: 219,
 &#39;horses&#39;: 262,
 &#39;hoofs&#39;: 24,
 &#39;grating&#39;: 10,
 &#39;wheels&#39;: 47,
 &#39;curb&#39;: 4,
 &#39;followed&#39;: 329,
 &#39;pull&#39;: 23,
 &#39;whistled&#39;: 13,
 &#39;pair&#39;: 40,
 &#39;yes&#39;: 688,
 &#39;continued&#39;: 291,
 &#39;glancing&#39;: 98,
 &#39;window&#39;: 186,
 &#39;nice&#39;: 53,
 &#39;brougham&#39;: 4,
 &#39;beauties&#39;: 2,
 &#39;hundred&#39;: 229,
 &#39;fifty&#39;: 94,
 &#39;guineas&#39;: 3,
 &#39;apiece&#39;: 7,
 &#39;money&#39;: 326,
 &#39;nothing&#39;: 646,
 &#39;else&#39;: 201,
 &#39;better&#39;: 266,
 &#39;bit&#39;: 63,
 &#39;doctor&#39;: 183,
 &#39;stay&#39;: 74,
 &#39;lost&#39;: 224,
 &#39;boswell&#39;: 1,
 &#39;promises&#39;: 15,
 &#39;interesting&#39;: 71,
 &#39;pity&#39;: 75,
 &#39;miss&#39;: 112,
 &#39;client&#39;: 33,
 &#39;want&#39;: 323,
 &#39;sit&#39;: 89,
 &#39;best&#39;: 268,
 &#39;slow&#39;: 65,
 &#39;step&#39;: 139,
 &#39;stairs&#39;: 31,
 &#39;passage&#39;: 110,
 &#39;paused&#39;: 79,
 &#39;immediately&#39;: 182,
 &#39;outside&#39;: 110,
 &#39;loud&#39;: 64,
 &#39;authoritative&#39;: 2,
 &#39;tap&#39;: 10,
 &#39;come&#39;: 934,
 &#39;entered&#39;: 282,
 &#39;less&#39;: 367,
 &#39;feet&#39;: 179,
 &#39;inches&#39;: 16,
 &#39;height&#39;: 36,
 &#39;limbs&#39;: 67,
 &#39;hercules&#39;: 4,
 &#39;dress&#39;: 138,
 &#39;rich&#39;: 92,
 &#39;richness&#39;: 2,
 &#39;england&#39;: 311,
 &#39;bad&#39;: 155,
 &#39;taste&#39;: 23,
 &#39;bands&#39;: 27,
 &#39;astrakhan&#39;: 1,
 &#39;slashed&#39;: 3,
 &#39;sleeves&#39;: 30,
 &#39;fronts&#39;: 1,
 &#39;breasted&#39;: 1,
 &#39;coat&#39;: 172,
 &#39;deep&#39;: 215,
 &#39;cloak&#39;: 62,
 &#39;thrown&#39;: 92,
 &#39;shoulders&#39;: 125,
 &#39;lined&#39;: 32,
 &#39;flame&#39;: 15,
 &#39;coloured&#39;: 21,
 &#39;silk&#39;: 50,
 &#39;secured&#39;: 48,
 &#39;neck&#39;: 203,
 &#39;brooch&#39;: 1,
 &#39;consisted&#39;: 38,
 &#39;single&#39;: 173,
 &#39;flaming&#39;: 8,
 &#39;boots&#39;: 91,
 &#39;extended&#39;: 75,
 &#39;halfway&#39;: 19,
 &#39;calves&#39;: 3,
 &#39;trimmed&#39;: 8,
 &#39;tops&#39;: 3,
 &#39;fur&#39;: 38,
 &#39;completed&#39;: 25,
 &#39;impression&#39;: 67,
 &#39;barbaric&#39;: 2,
 &#39;opulence&#39;: 3,
 &#39;suggested&#39;: 69,
 &#39;appearance&#39;: 135,
 &#39;carried&#39;: 282,
 &#39;broad&#39;: 92,
 &#39;brimmed&#39;: 4,
 &#39;hand&#39;: 834,
 &#39;wore&#39;: 58,
 &#39;upper&#39;: 130,
 &#39;extending&#39;: 35,
 &#39;past&#39;: 223,
 &#39;cheekbones&#39;: 4,
 &#39;vizard&#39;: 1,
 &#39;apparently&#39;: 68,
 &#39;raised&#39;: 212,
 &#39;lower&#39;: 196,
 &#39;appeared&#39;: 197,
 &#39;hanging&#39;: 42,
 &#39;straight&#39;: 124,
 &#39;chin&#39;: 30,
 &#39;suggestive&#39;: 11,
 &#39;resolution&#39;: 57,
 &#39;pushed&#39;: 81,
 &#39;length&#39;: 63,
 &#39;obstinacy&#39;: 7,
 &#39;harsh&#39;: 22,
 &#39;voice&#39;: 462,
 &#39;strongly&#39;: 41,
 &#39;marked&#39;: 138,
 &#39;accent&#39;: 18,
 &#39;uncertain&#39;: 30,
 &#39;pray&#39;: 79,
 &#39;seat&#39;: 170,
 &#39;colleague&#39;: 7,
 &#39;dr&#39;: 48,
 &#39;occasionally&#39;: 89,
 &#39;cases&#39;: 453,
 &#39;whom&#39;: 489,
 &#39;honour&#39;: 16,
 &#39;count&#39;: 748,
 &#39;von&#39;: 11,
 &#39;kramm&#39;: 2,
 &#39;nobleman&#39;: 11,
 &#39;understand&#39;: 412,
 &#39;discretion&#39;: 13,
 &#39;trust&#39;: 68,
 &#39;extreme&#39;: 72,
 &#39;prefer&#39;: 21,
 &#39;communicate&#39;: 15,
 &#39;alone&#39;: 337,
 &#39;rose&#39;: 243,
 &#39;caught&#39;: 90,
 &#39;wrist&#39;: 68,
 &#39;back&#39;: 746,
 &#39;chair&#39;: 135,
 &#39;none&#39;: 110,
 &#39;say&#39;: 755,
 &#39;anything&#39;: 379,
 &#39;shrugged&#39;: 35,
 &#39;begin&#39;: 97,
 &#39;binding&#39;: 18,
 &#39;absolute&#39;: 56,
 &#39;secrecy&#39;: 18,
 &#39;years&#39;: 571,
 &#39;end&#39;: 465,
 &#39;present&#39;: 329,
 &#39;weight&#39;: 70,
 &#39;influence&#39;: 138,
 &#39;european&#39;: 99,
 &#39;history&#39;: 439,
 &#39;promise&#39;: 67,
 &#39;excuse&#39;: 53,
 &#39;strange&#39;: 220,
 &#39;august&#39;: 70,
 &#39;person&#39;: 185,
 &#39;employs&#39;: 2,
 &#39;wishes&#39;: 42,
 &#39;agent&#39;: 25,
 &#39;unknown&#39;: 87,
 &#39;confess&#39;: 36,
 &#39;once&#39;: 569,
 &#39;called&#39;: 450,
 &#39;exactly&#39;: 47,
 &#39;aware&#39;: 52,
 &#39;dryly&#39;: 5,
 &#39;circumstances&#39;: 107,
 &#39;delicacy&#39;: 11,
 &#39;precaution&#39;: 9,
 &#39;taken&#39;: 438,
 &#39;quench&#39;: 3,
 &#39;grow&#39;: 74,
 &#39;seriously&#39;: 63,
 &#39;compromise&#39;: 71,
 &#39;families&#39;: 45,
 &#39;speak&#39;: 255,
 &#39;plainly&#39;: 39,
 &#39;implicates&#39;: 5,
 &#39;house&#39;: 661,
 &#39;ormstein&#39;: 2,
 &#39;hereditary&#39;: 14,
 &#39;kings&#39;: 27,
 &#39;murmured&#39;: 18,
 &#39;settling&#39;: 16,
 &#39;closing&#39;: 35,
 &#39;glanced&#39;: 176,
 ...}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alphabet = <span class="string">'abcdefghigklmnopqrstuvwxyz'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span><span class="params">(word)</span>:</span></span><br><span class="line">    n = len(word)</span><br><span class="line">    <span class="keyword">return</span> set([word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]+</span><br><span class="line">              [word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>]+word[i]+word[i+<span class="number">2</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>)]+</span><br><span class="line">              [word[<span class="number">0</span>:i]+c+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet] +</span><br><span class="line">              [word[<span class="number">0</span>:i]+c+word[i:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> set(e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">e1 = edits1(<span class="string">'hello'</span>)</span><br><span class="line">e2 = edits2(<span class="string">'hello'</span>)</span><br><span class="line">len(e1)+len(e2)</span><br></pre></td></tr></table></figure>
<pre><code>33328
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span><span class="params">(words)</span>:</span></span><br><span class="line">    w = set()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dic_words:</span><br><span class="line">            w.add(word)</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correct</span><span class="params">(word)</span>:</span></span><br><span class="line">    candidates = known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known(edits2(word)) <span class="keyword">or</span> word</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> word == candidates:</span><br><span class="line">        <span class="keyword">return</span> word</span><br><span class="line">    max_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> candidates:</span><br><span class="line">        <span class="keyword">if</span> dic_words[c]&gt;=max_num:</span><br><span class="line">            max_num=dic_words[c]</span><br><span class="line">            candidate = c</span><br><span class="line">    <span class="keyword">return</span> candidate</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'ehell'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;shell&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'hel'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;he&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'heade'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;head&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'haved'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;have&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'adevnues'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;avenues&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯算法</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习boosting</title>
    <url>/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/</url>
    <content><![CDATA[<p>boosting代码简单演示<br><a id="more"></a></p>
<p>个体学习器之间存在强依赖关系，提升（boosting）<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting7.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_gaussian_quantiles</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1,y1 = make_gaussian_quantiles(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,n_classes=<span class="number">2</span>)</span><br><span class="line">x2,y2 = make_gaussian_quantiles(mean=(<span class="number">3</span>,<span class="number">3</span>),n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,n_classes=<span class="number">2</span>)</span><br><span class="line">x_data = np.concatenate((x1,x2))</span><br><span class="line">y_data = np.concatenate((y1,<span class="number">1</span>-y2))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x_data[:,<span class="number">-0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_3_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tree.DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line">model.fit(x_data,y_data)</span><br><span class="line">x_min,x_max = x_data[:,<span class="number">0</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">0</span>].max()+<span class="number">1</span></span><br><span class="line">y_min,y_max = x_data[:,<span class="number">1</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">1</span>].max()+<span class="number">1</span></span><br><span class="line"><span class="comment"># 生成网格矩阵</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">z = model.predict(np.c_[xx.ravel(), yy.ravel()])<span class="comment"># ravel与flatten类似，多维数据转一维。flatten不会改变原始数据，ravel会改变原始数据</span></span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># 等高线图</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># 样本散点图</span></span><br><span class="line">plt.scatter(x_data[:, <span class="number">0</span>], x_data[:, <span class="number">1</span>], c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.score(x_data,y_data)</span><br></pre></td></tr></table></figure>
<pre><code>0.705
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=<span class="number">3</span>),n_estimators=<span class="number">10</span>)</span><br><span class="line">model.fit(x_data,y_data)</span><br><span class="line"><span class="comment"># 获取数据值所在的范围</span></span><br><span class="line">x_min, x_max = x_data[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = x_data[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成网格矩阵</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取预测值</span></span><br><span class="line">z = model.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># 等高线图</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># 样本散点图</span></span><br><span class="line">plt.scatter(x_data[:, <span class="number">0</span>], x_data[:, <span class="number">1</span>], c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.score(x_data,y_data)</span><br></pre></td></tr></table></figure>
<pre><code>0.988
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习voting</title>
    <url>/2020/07/11/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0voting/</url>
    <content><![CDATA[<p>voting代码简单演示<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data,y_data = iris.data[:,<span class="number">1</span>:<span class="number">3</span>],iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = DecisionTreeClassifier()</span><br><span class="line">clf3 = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = VotingClassifier([(<span class="string">'knn'</span>,clf1),(<span class="string">'dtree'</span>,clf2),(<span class="string">'lr'</span>,clf3)])</span><br><span class="line"><span class="keyword">for</span> clf,label <span class="keyword">in</span> zip([clf1,clf2,clf3,sclf],</span><br><span class="line">                    [<span class="string">'KNN'</span>,<span class="string">'Decistion Tree'</span>,<span class="string">'LogisticRegression'</span>,<span class="string">'VotingClassifier'</span>]):</span><br><span class="line">    scores = model_selection.cross_val_score(clf,x_data,y_data,cv=<span class="number">3</span>,scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">'Accuracy: %0.2f[%s]'</span> %(scores.mean(),label))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy: 0.91[KNN]
Accuracy: 0.90[Decistion Tree]
Accuracy: 0.91[LogisticRegression]
Accuracy: 0.92[VotingClassifier]


C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\张帅\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>非线性逻辑回归</title>
    <url>/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>非线性逻辑回归代码演示<br>数据下载<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/LR-testSet2.txt" target="_blank" rel="noopener">LR-testSet2.txt</a><br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">scale = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<ul>
<li>np.newaxis的功能——插入新维度</li>
</ul>
<hr>
<p>举个简单的例子介绍一下吧。</p>
<ul>
<li>栗子1：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">print(a.shape)</span><br><span class="line">print (a)</span><br></pre></td></tr></table></figure></li>
<li><p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(5,)</span><br><span class="line">[1 2 3 4 5]</span><br></pre></td></tr></table></figure>
</li>
<li><p>栗子2：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">aa&#x3D;a[:,np.newaxis]</span><br><span class="line">print(aa.shape)</span><br><span class="line">print (aa)</span><br></pre></td></tr></table></figure>
</li>
<li><p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(5, 1)</span><br><span class="line">[[1]</span><br><span class="line">[2]</span><br><span class="line">[3]</span><br><span class="line">[4]</span><br><span class="line">[5]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>栗子3：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">aa&#x3D;a[np.newaxis,:]</span><br><span class="line">print(aa.shape)</span><br><span class="line">print (aa)</span><br></pre></td></tr></table></figure></li>
<li>输出<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1, 5)</span><br><span class="line">[[1 2 3 4 5]]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<p>看明白了吧，原来np.newaxis的作用是增加一个维度。</p>
<p>对于[: , np.newaxis] 和 [np.newaxis，：]<br>是在np.newaxis这里增加1维。</p>
<p>这样改变维度的作用往往是将一维的数据转变成一个矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/LR-testSet2.txt'</span>,delimiter=<span class="string">','</span>)</span><br><span class="line">x_data = data[:,:<span class="number">-1</span>] <span class="comment">#shape(118, 2)</span></span><br><span class="line">y_data = data[:,<span class="number">-1</span>,np.newaxis] <span class="comment">#shape(118, 1)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></span><br><span class="line">    x0 = []</span><br><span class="line">    x1 = []</span><br><span class="line">    y0 = []</span><br><span class="line">    y1 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        <span class="keyword">if</span> y_data[i]==<span class="number">0</span>:</span><br><span class="line">            x0.append(x_data[i,<span class="number">0</span>])</span><br><span class="line">            y0.append(x_data[i,<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x1.append(x_data[i,<span class="number">0</span>])</span><br><span class="line">            y1.append(x_data[i,<span class="number">1</span>])</span><br><span class="line">    scatter0 = plt.scatter(x0,y0,c=<span class="string">'b'</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">    scatter1 = plt.scatter(x1,y1,c=<span class="string">'r'</span>,marker=<span class="string">'x'</span>)</span><br><span class="line">    plt.legend(handles=[scatter0,scatter1],labels=[<span class="string">'labell0'</span>,<span class="string">'label1'</span>],loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plot()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/output_2_0.png" alt="png"></p>
<p>使用sklearn.preprocessing.PolynomialFeatures来进行特征的构造。</p>
<p>它是使用多项式的方法来进行的，如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2），这个多项式的形式是使用poly的效果。</p>
<p>PolynomialFeatures有三个参数</p>
<ul>
<li>degree：控制多项式的度</li>
<li>interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，上面的二次项中没有a^2和b^2。</li>
<li>include_bias：默认为True。如果为True的话，那么就会有上面的 1那一项。</li>
</ul>
<p>例子1，interaction_only为默认的False时<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;[[5,10]]    #c&#x3D;[[a,b]],这里要注意a的shape，如果是list形式，则将a.shape&#x3D;-1,1</span><br><span class="line">pl&#x3D;PolynomialFeatures()</span><br><span class="line">b&#x3D;pl.fit_transform(c)</span><br><span class="line">b</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[  1.,   5.,  10.,  25.,  50., 100.]]) </span><br><span class="line">#符合（1,a,b,a^2,ab, b^2）</span><br></pre></td></tr></table></figure></p>
<p>例子2，interaction_only=True时<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;[[5,10]]</span><br><span class="line">pl&#x3D;PolynomialFeatures(interaction_only&#x3D;True)</span><br><span class="line">b&#x3D;pl.fit_transform(c)</span><br><span class="line">b</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 1.,  5., 10., 50.]])   </span><br><span class="line">#输出中不包含a^2和b^2项</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly_reg = PolynomialFeatures(degree=<span class="number">3</span>)</span><br><span class="line">x_poly = poly_reg.fit_transform(x_data)</span><br><span class="line">x_poly</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 1.00000000e+00,  5.12670000e-02,  6.99560000e-01, ...,
         1.83865725e-03,  2.50892595e-02,  3.42353606e-01],
       [ 1.00000000e+00, -9.27420000e-02,  6.84940000e-01, ...,
         5.89122275e-03, -4.35092419e-02,  3.21334672e-01],
       [ 1.00000000e+00, -2.13710000e-01,  6.92250000e-01, ...,
         3.16164171e-02, -1.02411982e-01,  3.31733166e-01],
       ...,
       [ 1.00000000e+00, -4.84450000e-01,  9.99270000e-01, ...,
         2.34520477e-01, -4.83742961e-01,  9.97811598e-01],
       [ 1.00000000e+00, -6.33640000e-03,  9.99270000e-01, ...,
         4.01206555e-05, -6.32715223e-03,  9.97811598e-01],
       [ 1.00000000e+00,  6.32650000e-01, -3.06120000e-02, ...,
        -1.22523312e-02,  5.92852863e-04, -2.86863382e-05]])
</code></pre><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%921.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%922.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%923.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%924.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(xMat,yMat,ws)</span>:</span></span><br><span class="line">    left = np.multiply(yMat,np.log(sigmoid(xMat*ws)))</span><br><span class="line">    right = np.multiply(<span class="number">1</span>-yMat,np.log(<span class="number">1</span>-sigmoid(xMat*ws)))</span><br><span class="line">    <span class="keyword">return</span> np.sum(left+right)/-(len(xMat))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scale == <span class="literal">True</span>:</span><br><span class="line">        xArr = preprocessing.scale(xArr)</span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    lr = <span class="number">0.03</span></span><br><span class="line">    epochs = <span class="number">50000</span></span><br><span class="line">    costList = []</span><br><span class="line">    m,n = np.shape(xMat)</span><br><span class="line">    ws = np.mat(np.ones((n,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs+<span class="number">1</span>):</span><br><span class="line">        h = sigmoid(xMat*ws)</span><br><span class="line">        ws_grad = xMat.T*(h-yMat)/m</span><br><span class="line">        ws = ws - lr*ws_grad</span><br><span class="line">        <span class="keyword">if</span> i %<span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            costList.append(cost(xMat,yMat,ws))</span><br><span class="line">    <span class="keyword">return</span> ws,costList</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws, costList = gradAscent(x_poly,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 4.16787292]
 [ 2.72213524]
 [ 4.55120018]
 [-9.76109006]
 [-5.34880198]
 [-8.51458023]
 [-0.55950401]
 [-1.55418165]
 [-0.75929829]
 [-2.88573877]]
</code></pre><p>np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; np.array([[1, 2, 3],[7,8,9]])</span><br><span class="line"> </span><br><span class="line">b&#x3D;np.array([[4,5,6],[1,2,3]])</span><br><span class="line"> </span><br><span class="line">a</span><br></pre></td></tr></table></figure><br>Out[4]:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1, 2, 3],</span><br><span class="line">       [7, 8, 9]])</span><br></pre></td></tr></table></figure></p>
<p>b<br>Out[5]:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[4, 5, 6],</span><br><span class="line">       [1, 2, 3]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;np.c_[a,b]</span><br><span class="line"> </span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<p>Out[7]:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1, 2, 3, 4, 5, 6],</span><br><span class="line">       [7, 8, 9, 1, 2, 3]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">d&#x3D; np.array([7,8,9])</span><br><span class="line"> </span><br><span class="line">e&#x3D;np.array([1, 2, 3])</span><br><span class="line"> </span><br><span class="line">f&#x3D;np.c_[d,e]</span><br><span class="line"> </span><br><span class="line">f</span><br></pre></td></tr></table></figure>
<p>Out[12]:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[7, 1],</span><br><span class="line">       [8, 2],</span><br><span class="line">       [9, 3]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_min,x_max = x_data[:,<span class="number">0</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">0</span>].max()+<span class="number">1</span></span><br><span class="line">y_min,y_max = x_data[:,<span class="number">1</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">1</span>].max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">xx,yy = np.meshgrid(np.arange(x_min,y_max,<span class="number">0.02</span>),</span><br><span class="line">                   np.arange(y_min,y_max,<span class="number">0.02</span>))</span><br><span class="line"><span class="comment"># raval多维转一维</span></span><br><span class="line"><span class="comment"># dot矩阵乘积</span></span><br><span class="line"><span class="comment"># np.c_左右拼接两个矩阵</span></span><br><span class="line">z = sigmoid(poly_reg.fit_transform(np.c_[xx.ravel(),yy.ravel()]).dot(np.array(ws)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">    <span class="keyword">if</span> z[i]&gt;<span class="number">0.5</span>:</span><br><span class="line">        z[i]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        z[i]=<span class="number">0</span></span><br><span class="line">z=z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">cs = plt.contourf(xx,yy,z)</span><br><span class="line">plot()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/output_9_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x_data,ws)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scale == <span class="literal">True</span>:</span><br><span class="line">        x_data = preprocessing.scale(x_data)</span><br><span class="line">    xMat = np.mat(x_data)</span><br><span class="line">    ws = np.mat(ws)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x&gt;=<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> sigmoid(xMat*ws)] </span><br><span class="line"></span><br><span class="line">predictions = predict(x_poly,ws)</span><br><span class="line">print(classification_report(y_data,predictions))</span><br></pre></td></tr></table></figure>
<pre><code>              precision    recall  f1-score   support

         0.0       0.86      0.83      0.85        60
         1.0       0.83      0.86      0.85        58

    accuracy                           0.85       118
   macro avg       0.85      0.85      0.85       118
weighted avg       0.85      0.85      0.85       118
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-微调TorchVision对象检测</title>
    <url>/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>Pytorch-Image-微调TorchVision对象检测:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px"><a href="https://githubzhangshuai.github.io/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">Pytorch1.5.1官网教程目录</a></div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">微调TorchVision对象检测</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">计算机视觉迁移学习</a></li>
<li>2.3[对抗样本生成](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGAN教程</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">使用字符级RNN对名称进行分类</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">用字符级RNN生成名称</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXT的文本分类</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXT的语言翻译</a></li>
</ul>
</li>
<li>5.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-ReinforcementLearning/" target="_blank" rel="noopener">ReinforcementLearning</a></li>
<li>5.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Reinforcement-Learning/" target="_blank" rel="noopener">Reinforcement-Learning</a></li>
</ul>
<h1 id="TorchVision-0-3-Object-Detection-finetuning-tutorial"><a href="#TorchVision-0-3-Object-Detection-finetuning-tutorial" class="headerlink" title="TorchVision 0.3 Object Detection finetuning tutorial"></a>TorchVision 0.3 Object Detection finetuning tutorial</h1><p>For this tutorial, we will be finetuning a pre-trained <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a> model in the <a href="https://www.cis.upenn.edu/jshi/ped_html/" target="_blank" rel="noopener"><em>Penn-Fudan Database for Pedestrian Detection and Segmentation</em></a>. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.</p>
<p>First, we need to install <code>pycocotools</code>. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line">pip install cython</span><br><span class="line"><span class="comment"># Install pycocotools, the version by default in Colab</span></span><br><span class="line"><span class="comment"># has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354</span></span><br><span class="line">pip install -U <span class="string">'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</span></span><br></pre></td></tr></table></figure>
<pre><code>Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)
Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI
  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-h3isg2r5
  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-h3isg2r5
Requirement already satisfied, skipping upgrade: setuptools&gt;=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.1.0)
Requirement already satisfied, skipping upgrade: cython&gt;=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)
Requirement already satisfied, skipping upgrade: matplotlib&gt;=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)
Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.4.7)
Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.8.1)
Requirement already satisfied, skipping upgrade: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.18.5)
Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (0.10.0)
Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.2.0)
Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.15.0)
Building wheels for collected packages: pycocotools
  Building wheel for pycocotools (setup.py) ... [?25l[?25hdone
  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266460 sha256=3292fbae19c3df30ceb54183f71e1e7288d447743b1dcb8f88257833cf2f23e1
  Stored in directory: /tmp/pip-ephem-wheel-cache-y2jf5d3p/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a
Successfully built pycocotools
Installing collected packages: pycocotools
  Found existing installation: pycocotools 2.0.1
    Uninstalling pycocotools-2.0.1:
      Successfully uninstalled pycocotools-2.0.1
Successfully installed pycocotools-2.0
</code></pre><h2 id="Defining-the-Dataset"><a href="#Defining-the-Dataset" class="headerlink" title="Defining the Dataset"></a>Defining the Dataset</h2><p>The <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision reference scripts for training object detection, instance segmentation and person keypoint detection</a> allows for easily supporting adding new custom datasets.<br>The dataset should inherit from the standard <code>torch.utils.data.Dataset</code> class, and implement <code>__len__</code> and <code>__getitem__</code>.</p>
<p>The only specificity that we require is that the dataset <code>__getitem__</code> should return:</p>
<ul>
<li>image: a PIL Image of size (H, W)</li>
<li>target: a dict containing the following fields<ul>
<li><code>boxes</code> (<code>FloatTensor[N, 4]</code>): the coordinates of the <code>N</code> bounding boxes in <code>[x0, y0, x1, y1]</code> format, ranging from <code>0</code> to <code>W</code> and <code>0</code> to <code>H</code></li>
<li><code>labels</code> (<code>Int64Tensor[N]</code>): the label for each bounding box</li>
<li><code>image_id</code> (<code>Int64Tensor[1]</code>): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation</li>
<li><code>area</code> (<code>Tensor[N]</code>): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.</li>
<li><code>iscrowd</code> (<code>UInt8Tensor[N]</code>): instances with <code>iscrowd=True</code> will be ignored during evaluation.</li>
<li>(optionally) <code>masks</code> (<code>UInt8Tensor[N, H, W]</code>): The segmentation masks for each one of the objects</li>
<li>(optionally) <code>keypoints</code> (<code>FloatTensor[N, K, 3]</code>): For each one of the <code>N</code> objects, it contains the <code>K</code> keypoints in <code>[x, y, visibility]</code> format, defining the object. <code>visibility=0</code> means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt <code>references/detection/transforms.py</code> for your new keypoint representation</li>
</ul>
</li>
</ul>
<p>If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.</p>
<p>Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a <code>get_height_and_width</code> method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via <code>__getitem__</code> , which loads the image in memory and is slower than if a custom method is provided.</p>
<h3 id="Writing-a-custom-dataset-for-Penn-Fudan"><a href="#Writing-a-custom-dataset-for-Penn-Fudan" class="headerlink" title="Writing a custom dataset for Penn-Fudan"></a>Writing a custom dataset for Penn-Fudan</h3><p>Let’s write a dataset for the Penn-Fudan dataset.</p>
<p>First, let’s download and extract the data, present in a zip file at <a href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip" target="_blank" rel="noopener">https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the Penn-Fudan dataset</span></span><br><span class="line">wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .</span><br><span class="line"><span class="comment"># extract it in the current folder</span></span><br><span class="line">unzip PennFudanPed.zip</span><br></pre></td></tr></table></figure>
<pre><code>--2020-07-23 13:46:08--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip
Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d
Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 53723336 (51M) [application/zip]
Saving to: ‘PennFudanPed.zip’

PennFudanPed.zip    100%[===================&gt;]  51.23M  1009KB/s    in 48s     

2020-07-23 13:46:58 (1.06 MB/s) - ‘PennFudanPed.zip’ saved [53723336/53723336]

--2020-07-23 13:46:58--  http://./
Resolving . (.)... failed: No address associated with hostname.
wget: unable to resolve host address ‘.’
FINISHED --2020-07-23 13:46:58--
Total wall clock time: 50s
Downloaded: 1 files, 51M in 48s (1.06 MB/s)
Archive:  PennFudanPed.zip
   creating: PennFudanPed/
  inflating: PennFudanPed/added-object-list.txt  
   creating: PennFudanPed/Annotation/
  inflating: PennFudanPed/Annotation/FudanPed00001.txt  
  inflating: PennFudanPed/Annotation/FudanPed00002.txt  
  inflating: PennFudanPed/Annotation/FudanPed00003.txt  
  inflating: PennFudanPed/Annotation/FudanPed00004.txt  
  inflating: PennFudanPed/Annotation/FudanPed00005.txt  
  inflating: PennFudanPed/Annotation/FudanPed00006.txt  
  inflating: PennFudanPed/Annotation/FudanPed00007.txt  
  inflating: PennFudanPed/Annotation/FudanPed00008.txt  
  inflating: PennFudanPed/Annotation/FudanPed00009.txt  
  inflating: PennFudanPed/Annotation/FudanPed00010.txt  
  inflating: PennFudanPed/Annotation/FudanPed00011.txt  
  inflating: PennFudanPed/Annotation/FudanPed00012.txt  
  inflating: PennFudanPed/Annotation/FudanPed00013.txt  
  inflating: PennFudanPed/Annotation/FudanPed00014.txt  
  inflating: PennFudanPed/Annotation/FudanPed00015.txt  
  inflating: PennFudanPed/Annotation/FudanPed00016.txt  
  inflating: PennFudanPed/Annotation/FudanPed00017.txt  
  inflating: PennFudanPed/Annotation/FudanPed00018.txt  
  inflating: PennFudanPed/Annotation/FudanPed00019.txt  
  inflating: PennFudanPed/Annotation/FudanPed00020.txt  
  inflating: PennFudanPed/Annotation/FudanPed00021.txt  
  inflating: PennFudanPed/Annotation/FudanPed00022.txt  
  inflating: PennFudanPed/Annotation/FudanPed00023.txt  
  inflating: PennFudanPed/Annotation/FudanPed00024.txt  
  inflating: PennFudanPed/Annotation/FudanPed00025.txt  
  inflating: PennFudanPed/Annotation/FudanPed00026.txt  
  inflating: PennFudanPed/Annotation/FudanPed00027.txt  
  inflating: PennFudanPed/Annotation/FudanPed00028.txt  
  inflating: PennFudanPed/Annotation/FudanPed00029.txt  
  inflating: PennFudanPed/Annotation/FudanPed00030.txt  
  inflating: PennFudanPed/Annotation/FudanPed00031.txt  
  inflating: PennFudanPed/Annotation/FudanPed00032.txt  
  inflating: PennFudanPed/Annotation/FudanPed00033.txt  
  inflating: PennFudanPed/Annotation/FudanPed00034.txt  
  inflating: PennFudanPed/Annotation/FudanPed00035.txt  
  inflating: PennFudanPed/Annotation/FudanPed00036.txt  
  inflating: PennFudanPed/Annotation/FudanPed00037.txt  
  inflating: PennFudanPed/Annotation/FudanPed00038.txt  
  inflating: PennFudanPed/Annotation/FudanPed00039.txt  
  inflating: PennFudanPed/Annotation/FudanPed00040.txt  
  inflating: PennFudanPed/Annotation/FudanPed00041.txt  
  inflating: PennFudanPed/Annotation/FudanPed00042.txt  
  inflating: PennFudanPed/Annotation/FudanPed00043.txt  
  inflating: PennFudanPed/Annotation/FudanPed00044.txt  
  inflating: PennFudanPed/Annotation/FudanPed00045.txt  
  inflating: PennFudanPed/Annotation/FudanPed00046.txt  
  inflating: PennFudanPed/Annotation/FudanPed00047.txt  
  inflating: PennFudanPed/Annotation/FudanPed00048.txt  
  inflating: PennFudanPed/Annotation/FudanPed00049.txt  
  inflating: PennFudanPed/Annotation/FudanPed00050.txt  
  inflating: PennFudanPed/Annotation/FudanPed00051.txt  
  inflating: PennFudanPed/Annotation/FudanPed00052.txt  
  inflating: PennFudanPed/Annotation/FudanPed00053.txt  
  inflating: PennFudanPed/Annotation/FudanPed00054.txt  
  inflating: PennFudanPed/Annotation/FudanPed00055.txt  
  inflating: PennFudanPed/Annotation/FudanPed00056.txt  
  inflating: PennFudanPed/Annotation/FudanPed00057.txt  
  inflating: PennFudanPed/Annotation/FudanPed00058.txt  
  inflating: PennFudanPed/Annotation/FudanPed00059.txt  
  inflating: PennFudanPed/Annotation/FudanPed00060.txt  
  inflating: PennFudanPed/Annotation/FudanPed00061.txt  
  inflating: PennFudanPed/Annotation/FudanPed00062.txt  
  inflating: PennFudanPed/Annotation/FudanPed00063.txt  
  inflating: PennFudanPed/Annotation/FudanPed00064.txt  
  inflating: PennFudanPed/Annotation/FudanPed00065.txt  
  inflating: PennFudanPed/Annotation/FudanPed00066.txt  
  inflating: PennFudanPed/Annotation/FudanPed00067.txt  
  inflating: PennFudanPed/Annotation/FudanPed00068.txt  
  inflating: PennFudanPed/Annotation/FudanPed00069.txt  
  inflating: PennFudanPed/Annotation/FudanPed00070.txt  
  inflating: PennFudanPed/Annotation/FudanPed00071.txt  
  inflating: PennFudanPed/Annotation/FudanPed00072.txt  
  inflating: PennFudanPed/Annotation/FudanPed00073.txt  
  inflating: PennFudanPed/Annotation/FudanPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00001.txt  
  inflating: PennFudanPed/Annotation/PennPed00002.txt  
  inflating: PennFudanPed/Annotation/PennPed00003.txt  
  inflating: PennFudanPed/Annotation/PennPed00004.txt  
  inflating: PennFudanPed/Annotation/PennPed00005.txt  
  inflating: PennFudanPed/Annotation/PennPed00006.txt  
  inflating: PennFudanPed/Annotation/PennPed00007.txt  
  inflating: PennFudanPed/Annotation/PennPed00008.txt  
  inflating: PennFudanPed/Annotation/PennPed00009.txt  
  inflating: PennFudanPed/Annotation/PennPed00010.txt  
  inflating: PennFudanPed/Annotation/PennPed00011.txt  
  inflating: PennFudanPed/Annotation/PennPed00012.txt  
  inflating: PennFudanPed/Annotation/PennPed00013.txt  
  inflating: PennFudanPed/Annotation/PennPed00014.txt  
  inflating: PennFudanPed/Annotation/PennPed00015.txt  
  inflating: PennFudanPed/Annotation/PennPed00016.txt  
  inflating: PennFudanPed/Annotation/PennPed00017.txt  
  inflating: PennFudanPed/Annotation/PennPed00018.txt  
  inflating: PennFudanPed/Annotation/PennPed00019.txt  
  inflating: PennFudanPed/Annotation/PennPed00020.txt  
  inflating: PennFudanPed/Annotation/PennPed00021.txt  
  inflating: PennFudanPed/Annotation/PennPed00022.txt  
  inflating: PennFudanPed/Annotation/PennPed00023.txt  
  inflating: PennFudanPed/Annotation/PennPed00024.txt  
  inflating: PennFudanPed/Annotation/PennPed00025.txt  
  inflating: PennFudanPed/Annotation/PennPed00026.txt  
  inflating: PennFudanPed/Annotation/PennPed00027.txt  
  inflating: PennFudanPed/Annotation/PennPed00028.txt  
  inflating: PennFudanPed/Annotation/PennPed00029.txt  
  inflating: PennFudanPed/Annotation/PennPed00030.txt  
  inflating: PennFudanPed/Annotation/PennPed00031.txt  
  inflating: PennFudanPed/Annotation/PennPed00032.txt  
  inflating: PennFudanPed/Annotation/PennPed00033.txt  
  inflating: PennFudanPed/Annotation/PennPed00034.txt  
  inflating: PennFudanPed/Annotation/PennPed00035.txt  
  inflating: PennFudanPed/Annotation/PennPed00036.txt  
  inflating: PennFudanPed/Annotation/PennPed00037.txt  
  inflating: PennFudanPed/Annotation/PennPed00038.txt  
  inflating: PennFudanPed/Annotation/PennPed00039.txt  
  inflating: PennFudanPed/Annotation/PennPed00040.txt  
  inflating: PennFudanPed/Annotation/PennPed00041.txt  
  inflating: PennFudanPed/Annotation/PennPed00042.txt  
  inflating: PennFudanPed/Annotation/PennPed00043.txt  
  inflating: PennFudanPed/Annotation/PennPed00044.txt  
  inflating: PennFudanPed/Annotation/PennPed00045.txt  
  inflating: PennFudanPed/Annotation/PennPed00046.txt  
  inflating: PennFudanPed/Annotation/PennPed00047.txt  
  inflating: PennFudanPed/Annotation/PennPed00048.txt  
  inflating: PennFudanPed/Annotation/PennPed00049.txt  
  inflating: PennFudanPed/Annotation/PennPed00050.txt  
  inflating: PennFudanPed/Annotation/PennPed00051.txt  
  inflating: PennFudanPed/Annotation/PennPed00052.txt  
  inflating: PennFudanPed/Annotation/PennPed00053.txt  
  inflating: PennFudanPed/Annotation/PennPed00054.txt  
  inflating: PennFudanPed/Annotation/PennPed00055.txt  
  inflating: PennFudanPed/Annotation/PennPed00056.txt  
  inflating: PennFudanPed/Annotation/PennPed00057.txt  
  inflating: PennFudanPed/Annotation/PennPed00058.txt  
  inflating: PennFudanPed/Annotation/PennPed00059.txt  
  inflating: PennFudanPed/Annotation/PennPed00060.txt  
  inflating: PennFudanPed/Annotation/PennPed00061.txt  
  inflating: PennFudanPed/Annotation/PennPed00062.txt  
  inflating: PennFudanPed/Annotation/PennPed00063.txt  
  inflating: PennFudanPed/Annotation/PennPed00064.txt  
  inflating: PennFudanPed/Annotation/PennPed00065.txt  
  inflating: PennFudanPed/Annotation/PennPed00066.txt  
  inflating: PennFudanPed/Annotation/PennPed00067.txt  
  inflating: PennFudanPed/Annotation/PennPed00068.txt  
  inflating: PennFudanPed/Annotation/PennPed00069.txt  
  inflating: PennFudanPed/Annotation/PennPed00070.txt  
  inflating: PennFudanPed/Annotation/PennPed00071.txt  
  inflating: PennFudanPed/Annotation/PennPed00072.txt  
  inflating: PennFudanPed/Annotation/PennPed00073.txt  
  inflating: PennFudanPed/Annotation/PennPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00075.txt  
  inflating: PennFudanPed/Annotation/PennPed00076.txt  
  inflating: PennFudanPed/Annotation/PennPed00077.txt  
  inflating: PennFudanPed/Annotation/PennPed00078.txt  
  inflating: PennFudanPed/Annotation/PennPed00079.txt  
  inflating: PennFudanPed/Annotation/PennPed00080.txt  
  inflating: PennFudanPed/Annotation/PennPed00081.txt  
  inflating: PennFudanPed/Annotation/PennPed00082.txt  
  inflating: PennFudanPed/Annotation/PennPed00083.txt  
  inflating: PennFudanPed/Annotation/PennPed00084.txt  
  inflating: PennFudanPed/Annotation/PennPed00085.txt  
  inflating: PennFudanPed/Annotation/PennPed00086.txt  
  inflating: PennFudanPed/Annotation/PennPed00087.txt  
  inflating: PennFudanPed/Annotation/PennPed00088.txt  
  inflating: PennFudanPed/Annotation/PennPed00089.txt  
  inflating: PennFudanPed/Annotation/PennPed00090.txt  
  inflating: PennFudanPed/Annotation/PennPed00091.txt  
  inflating: PennFudanPed/Annotation/PennPed00092.txt  
  inflating: PennFudanPed/Annotation/PennPed00093.txt  
  inflating: PennFudanPed/Annotation/PennPed00094.txt  
  inflating: PennFudanPed/Annotation/PennPed00095.txt  
  inflating: PennFudanPed/Annotation/PennPed00096.txt  
   creating: PennFudanPed/PedMasks/
  inflating: PennFudanPed/PedMasks/FudanPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00014_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00017_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00027_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00052_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00061_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00065_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00068_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00069_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00071_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00073_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00014_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00017_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00027_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00052_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00061_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00065_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00068_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00069_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00071_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00073_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00075_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00076_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00077_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00078_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00079_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00080_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00081_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00082_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00083_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00084_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00085_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00086_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00087_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00088_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00089_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00090_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00091_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00092_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00093_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00094_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00095_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00096_mask.png  
   creating: PennFudanPed/PNGImages/
  inflating: PennFudanPed/PNGImages/FudanPed00001.png  
  inflating: PennFudanPed/PNGImages/FudanPed00002.png  
  inflating: PennFudanPed/PNGImages/FudanPed00003.png  
  inflating: PennFudanPed/PNGImages/FudanPed00004.png  
  inflating: PennFudanPed/PNGImages/FudanPed00005.png  
  inflating: PennFudanPed/PNGImages/FudanPed00006.png  
  inflating: PennFudanPed/PNGImages/FudanPed00007.png  
  inflating: PennFudanPed/PNGImages/FudanPed00008.png  
  inflating: PennFudanPed/PNGImages/FudanPed00009.png  
  inflating: PennFudanPed/PNGImages/FudanPed00010.png  
  inflating: PennFudanPed/PNGImages/FudanPed00011.png  
  inflating: PennFudanPed/PNGImages/FudanPed00012.png  
  inflating: PennFudanPed/PNGImages/FudanPed00013.png  
  inflating: PennFudanPed/PNGImages/FudanPed00014.png  
  inflating: PennFudanPed/PNGImages/FudanPed00015.png  
  inflating: PennFudanPed/PNGImages/FudanPed00016.png  
  inflating: PennFudanPed/PNGImages/FudanPed00017.png  
  inflating: PennFudanPed/PNGImages/FudanPed00018.png  
  inflating: PennFudanPed/PNGImages/FudanPed00019.png  
  inflating: PennFudanPed/PNGImages/FudanPed00020.png  
  inflating: PennFudanPed/PNGImages/FudanPed00021.png  
  inflating: PennFudanPed/PNGImages/FudanPed00022.png  
  inflating: PennFudanPed/PNGImages/FudanPed00023.png  
  inflating: PennFudanPed/PNGImages/FudanPed00024.png  
  inflating: PennFudanPed/PNGImages/FudanPed00025.png  
  inflating: PennFudanPed/PNGImages/FudanPed00026.png  
  inflating: PennFudanPed/PNGImages/FudanPed00027.png  
  inflating: PennFudanPed/PNGImages/FudanPed00028.png  
  inflating: PennFudanPed/PNGImages/FudanPed00029.png  
  inflating: PennFudanPed/PNGImages/FudanPed00030.png  
  inflating: PennFudanPed/PNGImages/FudanPed00031.png  
  inflating: PennFudanPed/PNGImages/FudanPed00032.png  
  inflating: PennFudanPed/PNGImages/FudanPed00033.png  
  inflating: PennFudanPed/PNGImages/FudanPed00034.png  
  inflating: PennFudanPed/PNGImages/FudanPed00035.png  
  inflating: PennFudanPed/PNGImages/FudanPed00036.png  
  inflating: PennFudanPed/PNGImages/FudanPed00037.png  
  inflating: PennFudanPed/PNGImages/FudanPed00038.png  
  inflating: PennFudanPed/PNGImages/FudanPed00039.png  
  inflating: PennFudanPed/PNGImages/FudanPed00040.png  
  inflating: PennFudanPed/PNGImages/FudanPed00041.png  
  inflating: PennFudanPed/PNGImages/FudanPed00042.png  
  inflating: PennFudanPed/PNGImages/FudanPed00043.png  
  inflating: PennFudanPed/PNGImages/FudanPed00044.png  
  inflating: PennFudanPed/PNGImages/FudanPed00045.png  
  inflating: PennFudanPed/PNGImages/FudanPed00046.png  
  inflating: PennFudanPed/PNGImages/FudanPed00047.png  
  inflating: PennFudanPed/PNGImages/FudanPed00048.png  
  inflating: PennFudanPed/PNGImages/FudanPed00049.png  
  inflating: PennFudanPed/PNGImages/FudanPed00050.png  
  inflating: PennFudanPed/PNGImages/FudanPed00051.png  
  inflating: PennFudanPed/PNGImages/FudanPed00052.png  
  inflating: PennFudanPed/PNGImages/FudanPed00053.png  
  inflating: PennFudanPed/PNGImages/FudanPed00054.png  
  inflating: PennFudanPed/PNGImages/FudanPed00055.png  
  inflating: PennFudanPed/PNGImages/FudanPed00056.png  
  inflating: PennFudanPed/PNGImages/FudanPed00057.png  
  inflating: PennFudanPed/PNGImages/FudanPed00058.png  
  inflating: PennFudanPed/PNGImages/FudanPed00059.png  
  inflating: PennFudanPed/PNGImages/FudanPed00060.png  
  inflating: PennFudanPed/PNGImages/FudanPed00061.png  
  inflating: PennFudanPed/PNGImages/FudanPed00062.png  
  inflating: PennFudanPed/PNGImages/FudanPed00063.png  
  inflating: PennFudanPed/PNGImages/FudanPed00064.png  
  inflating: PennFudanPed/PNGImages/FudanPed00065.png  
  inflating: PennFudanPed/PNGImages/FudanPed00066.png  
  inflating: PennFudanPed/PNGImages/FudanPed00067.png  
  inflating: PennFudanPed/PNGImages/FudanPed00068.png  
  inflating: PennFudanPed/PNGImages/FudanPed00069.png  
  inflating: PennFudanPed/PNGImages/FudanPed00070.png  
  inflating: PennFudanPed/PNGImages/FudanPed00071.png  
  inflating: PennFudanPed/PNGImages/FudanPed00072.png  
  inflating: PennFudanPed/PNGImages/FudanPed00073.png  
  inflating: PennFudanPed/PNGImages/FudanPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00001.png  
  inflating: PennFudanPed/PNGImages/PennPed00002.png  
  inflating: PennFudanPed/PNGImages/PennPed00003.png  
  inflating: PennFudanPed/PNGImages/PennPed00004.png  
  inflating: PennFudanPed/PNGImages/PennPed00005.png  
  inflating: PennFudanPed/PNGImages/PennPed00006.png  
  inflating: PennFudanPed/PNGImages/PennPed00007.png  
  inflating: PennFudanPed/PNGImages/PennPed00008.png  
  inflating: PennFudanPed/PNGImages/PennPed00009.png  
  inflating: PennFudanPed/PNGImages/PennPed00010.png  
  inflating: PennFudanPed/PNGImages/PennPed00011.png  
  inflating: PennFudanPed/PNGImages/PennPed00012.png  
  inflating: PennFudanPed/PNGImages/PennPed00013.png  
  inflating: PennFudanPed/PNGImages/PennPed00014.png  
  inflating: PennFudanPed/PNGImages/PennPed00015.png  
  inflating: PennFudanPed/PNGImages/PennPed00016.png  
  inflating: PennFudanPed/PNGImages/PennPed00017.png  
  inflating: PennFudanPed/PNGImages/PennPed00018.png  
  inflating: PennFudanPed/PNGImages/PennPed00019.png  
  inflating: PennFudanPed/PNGImages/PennPed00020.png  
  inflating: PennFudanPed/PNGImages/PennPed00021.png  
  inflating: PennFudanPed/PNGImages/PennPed00022.png  
  inflating: PennFudanPed/PNGImages/PennPed00023.png  
  inflating: PennFudanPed/PNGImages/PennPed00024.png  
  inflating: PennFudanPed/PNGImages/PennPed00025.png  
  inflating: PennFudanPed/PNGImages/PennPed00026.png  
  inflating: PennFudanPed/PNGImages/PennPed00027.png  
  inflating: PennFudanPed/PNGImages/PennPed00028.png  
  inflating: PennFudanPed/PNGImages/PennPed00029.png  
  inflating: PennFudanPed/PNGImages/PennPed00030.png  
  inflating: PennFudanPed/PNGImages/PennPed00031.png  
  inflating: PennFudanPed/PNGImages/PennPed00032.png  
  inflating: PennFudanPed/PNGImages/PennPed00033.png  
  inflating: PennFudanPed/PNGImages/PennPed00034.png  
  inflating: PennFudanPed/PNGImages/PennPed00035.png  
  inflating: PennFudanPed/PNGImages/PennPed00036.png  
  inflating: PennFudanPed/PNGImages/PennPed00037.png  
  inflating: PennFudanPed/PNGImages/PennPed00038.png  
  inflating: PennFudanPed/PNGImages/PennPed00039.png  
  inflating: PennFudanPed/PNGImages/PennPed00040.png  
  inflating: PennFudanPed/PNGImages/PennPed00041.png  
  inflating: PennFudanPed/PNGImages/PennPed00042.png  
  inflating: PennFudanPed/PNGImages/PennPed00043.png  
  inflating: PennFudanPed/PNGImages/PennPed00044.png  
  inflating: PennFudanPed/PNGImages/PennPed00045.png  
  inflating: PennFudanPed/PNGImages/PennPed00046.png  
  inflating: PennFudanPed/PNGImages/PennPed00047.png  
  inflating: PennFudanPed/PNGImages/PennPed00048.png  
  inflating: PennFudanPed/PNGImages/PennPed00049.png  
  inflating: PennFudanPed/PNGImages/PennPed00050.png  
  inflating: PennFudanPed/PNGImages/PennPed00051.png  
  inflating: PennFudanPed/PNGImages/PennPed00052.png  
  inflating: PennFudanPed/PNGImages/PennPed00053.png  
  inflating: PennFudanPed/PNGImages/PennPed00054.png  
  inflating: PennFudanPed/PNGImages/PennPed00055.png  
  inflating: PennFudanPed/PNGImages/PennPed00056.png  
  inflating: PennFudanPed/PNGImages/PennPed00057.png  
  inflating: PennFudanPed/PNGImages/PennPed00058.png  
  inflating: PennFudanPed/PNGImages/PennPed00059.png  
  inflating: PennFudanPed/PNGImages/PennPed00060.png  
  inflating: PennFudanPed/PNGImages/PennPed00061.png  
  inflating: PennFudanPed/PNGImages/PennPed00062.png  
  inflating: PennFudanPed/PNGImages/PennPed00063.png  
  inflating: PennFudanPed/PNGImages/PennPed00064.png  
  inflating: PennFudanPed/PNGImages/PennPed00065.png  
  inflating: PennFudanPed/PNGImages/PennPed00066.png  
  inflating: PennFudanPed/PNGImages/PennPed00067.png  
  inflating: PennFudanPed/PNGImages/PennPed00068.png  
  inflating: PennFudanPed/PNGImages/PennPed00069.png  
  inflating: PennFudanPed/PNGImages/PennPed00070.png  
  inflating: PennFudanPed/PNGImages/PennPed00071.png  
  inflating: PennFudanPed/PNGImages/PennPed00072.png  
  inflating: PennFudanPed/PNGImages/PennPed00073.png  
  inflating: PennFudanPed/PNGImages/PennPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00075.png  
  inflating: PennFudanPed/PNGImages/PennPed00076.png  
  inflating: PennFudanPed/PNGImages/PennPed00077.png  
  inflating: PennFudanPed/PNGImages/PennPed00078.png  
  inflating: PennFudanPed/PNGImages/PennPed00079.png  
  inflating: PennFudanPed/PNGImages/PennPed00080.png  
  inflating: PennFudanPed/PNGImages/PennPed00081.png  
  inflating: PennFudanPed/PNGImages/PennPed00082.png  
  inflating: PennFudanPed/PNGImages/PennPed00083.png  
  inflating: PennFudanPed/PNGImages/PennPed00084.png  
  inflating: PennFudanPed/PNGImages/PennPed00085.png  
  inflating: PennFudanPed/PNGImages/PennPed00086.png  
  inflating: PennFudanPed/PNGImages/PennPed00087.png  
  inflating: PennFudanPed/PNGImages/PennPed00088.png  
  inflating: PennFudanPed/PNGImages/PennPed00089.png  
  inflating: PennFudanPed/PNGImages/PennPed00090.png  
  inflating: PennFudanPed/PNGImages/PennPed00091.png  
  inflating: PennFudanPed/PNGImages/PennPed00092.png  
  inflating: PennFudanPed/PNGImages/PennPed00093.png  
  inflating: PennFudanPed/PNGImages/PennPed00094.png  
  inflating: PennFudanPed/PNGImages/PennPed00095.png  
  inflating: PennFudanPed/PNGImages/PennPed00096.png  
  inflating: PennFudanPed/readme.txt  
</code></pre><p>Let’s have a look at the dataset and how it is layed down.</p>
<p>The data is structured as follows<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PennFudanPed&#x2F;</span><br><span class="line">  PedMasks&#x2F;</span><br><span class="line">    FudanPed00001_mask.png</span><br><span class="line">    FudanPed00002_mask.png</span><br><span class="line">    FudanPed00003_mask.png</span><br><span class="line">    FudanPed00004_mask.png</span><br><span class="line">    ...</span><br><span class="line">  PNGImages&#x2F;</span><br><span class="line">    FudanPed00001.png</span><br><span class="line">    FudanPed00002.png</span><br><span class="line">    FudanPed00003.png</span><br><span class="line">    FudanPed00004.png</span><br></pre></td></tr></table></figure></p>
<p>Here is one example of an image in the dataset, with its corresponding instance segmentation mask</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">Image.open(<span class="string">'PennFudanPed/PNGImages/FudanPed00001.png'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask = Image.open(<span class="string">'PennFudanPed/PedMasks/FudanPed00001_mask.png'</span>)</span><br><span class="line"><span class="comment"># each mask instance has a different color, from zero to N, where</span></span><br><span class="line"><span class="comment"># N is the number of instances. In order to make visualization easier,</span></span><br><span class="line"><span class="comment"># let's adda color palette to the mask.</span></span><br><span class="line">mask.putpalette([</span><br><span class="line">    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># black background</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># index 1 is red</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>, <span class="comment"># index 2 is yellow</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">153</span>, <span class="number">0</span>, <span class="comment"># index 3 is orange</span></span><br><span class="line">])</span><br><span class="line">mask</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_7_0.png" alt="png"></p>
<p>So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a <code>torch.utils.data.Dataset</code> class for this dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PennFudanDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms=None)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># load all image files, sorting them to</span></span><br><span class="line">        <span class="comment"># ensure that they are aligned</span></span><br><span class="line">        self.imgs = list(sorted(os.listdir(os.path.join(root, <span class="string">"PNGImages"</span>))))</span><br><span class="line">        self.masks = list(sorted(os.listdir(os.path.join(root, <span class="string">"PedMasks"</span>))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="comment"># load images ad masks</span></span><br><span class="line">        img_path = os.path.join(self.root, <span class="string">"PNGImages"</span>, self.imgs[idx])</span><br><span class="line">        mask_path = os.path.join(self.root, <span class="string">"PedMasks"</span>, self.masks[idx])</span><br><span class="line">        img = Image.open(img_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        <span class="comment"># note that we haven't converted the mask to RGB,</span></span><br><span class="line">        <span class="comment"># because each color corresponds to a different instance</span></span><br><span class="line">        <span class="comment"># with 0 being background</span></span><br><span class="line">        mask = Image.open(mask_path)</span><br><span class="line"></span><br><span class="line">        mask = np.array(mask)</span><br><span class="line">        <span class="comment"># instances are encoded as different colors</span></span><br><span class="line">        obj_ids = np.unique(mask)</span><br><span class="line">        <span class="comment"># first id is the background, so remove it</span></span><br><span class="line">        obj_ids = obj_ids[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split the color-encoded mask into a set</span></span><br><span class="line">        <span class="comment"># of binary masks</span></span><br><span class="line">        masks = mask == obj_ids[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get bounding box coordinates for each mask</span></span><br><span class="line">        num_objs = len(obj_ids)</span><br><span class="line">        boxes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_objs):</span><br><span class="line">            pos = np.where(masks[i])</span><br><span class="line">            xmin = np.min(pos[<span class="number">1</span>])</span><br><span class="line">            xmax = np.max(pos[<span class="number">1</span>])</span><br><span class="line">            ymin = np.min(pos[<span class="number">0</span>])</span><br><span class="line">            ymax = np.max(pos[<span class="number">0</span>])</span><br><span class="line">            boxes.append([xmin, ymin, xmax, ymax])</span><br><span class="line"></span><br><span class="line">        boxes = torch.as_tensor(boxes, dtype=torch.float32)</span><br><span class="line">        <span class="comment"># there is only one class</span></span><br><span class="line">        labels = torch.ones((num_objs,), dtype=torch.int64)</span><br><span class="line">        masks = torch.as_tensor(masks, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        image_id = torch.tensor([idx])</span><br><span class="line">        area = (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]) * (boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>])</span><br><span class="line">        <span class="comment"># suppose all instances are not crowd</span></span><br><span class="line">        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">        target = &#123;&#125;</span><br><span class="line">        target[<span class="string">"boxes"</span>] = boxes</span><br><span class="line">        target[<span class="string">"labels"</span>] = labels</span><br><span class="line">        target[<span class="string">"masks"</span>] = masks</span><br><span class="line">        target[<span class="string">"image_id"</span>] = image_id</span><br><span class="line">        target[<span class="string">"area"</span>] = area</span><br><span class="line">        target[<span class="string">"iscrowd"</span>] = iscrowd</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img, target = self.transforms(img, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure>
<p>That’s all for the dataset. Let’s see how the outputs are structured for this dataset</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed/'</span>)</span><br><span class="line">dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(&lt;PIL.Image.Image image mode=RGB size=559x536 at 0x7FEBFE8767F0&gt;,
 {&#39;area&#39;: tensor([35358., 36225.]), &#39;boxes&#39;: tensor([[159., 181., 301., 430.],
          [419., 170., 534., 485.]]), &#39;image_id&#39;: tensor([0]), &#39;iscrowd&#39;: tensor([0, 0]), &#39;labels&#39;: tensor([1, 1]), &#39;masks&#39;: tensor([[[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]],

          [[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})
</code></pre><p>So we can see that by default, the dataset returns a <code>PIL.Image</code> and a dictionary<br>containing several fields, including <code>boxes</code>, <code>labels</code> and <code>masks</code>.</p>
<h2 id="Defining-your-model"><a href="#Defining-your-model" class="headerlink" title="Defining your model"></a>Defining your model</h2><p>In this tutorial, we will be using <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>, which is based on top of <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image03.png" alt="Faster R-CNN"></p>
<p>Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image04.png" alt="Mask R-CNN"></p>
<p>There are two common situations where one might want to modify one of the available models in torchvision modelzoo.<br>The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).</p>
<p>Let’s go see how we would do one or another in the following sections.</p>
<h3 id="1-Finetuning-from-a-pretrained-model"><a href="#1-Finetuning-from-a-pretrained-model" class="headerlink" title="1 - Finetuning from a pretrained model"></a>1 - Finetuning from a pretrained model</h3><p>Let’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection.faster_rcnn import FastRCNNPredictor</span><br><span class="line"></span><br><span class="line"># load a model pre-trained pre-trained on COCO</span><br><span class="line">model &#x3D; torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained&#x3D;True)</span><br><span class="line"></span><br><span class="line"># replace the classifier with a new one, that has</span><br><span class="line"># num_classes which is user-defined</span><br><span class="line">num_classes &#x3D; 2  # 1 class (person) + background</span><br><span class="line"># get number of input features for the classifier</span><br><span class="line">in_features &#x3D; model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line"># replace the pre-trained head with a new one</span><br><span class="line">model.roi_heads.box_predictor &#x3D; FastRCNNPredictor(in_features, num_classes)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-Modifying-the-model-to-add-a-different-backbone"><a href="#2-Modifying-the-model-to-add-a-different-backbone" class="headerlink" title="2 - Modifying the model to add a different backbone"></a>2 - Modifying the model to add a different backbone</h3><p>Another common situation arises when the user wants to replace the backbone of a detection<br>model with a different one. For example, the current default backbone (ResNet-50) might be too big for some applications, and smaller models might be necessary.</p>
<p>Here is how we would go into leveraging the functions provided by torchvision to modify a backbone.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection import FasterRCNN</span><br><span class="line">from torchvision.models.detection.rpn import AnchorGenerator</span><br><span class="line"></span><br><span class="line"># load a pre-trained model for classification and return</span><br><span class="line"># only the features</span><br><span class="line">backbone &#x3D; torchvision.models.mobilenet_v2(pretrained&#x3D;True).features</span><br><span class="line"># FasterRCNN needs to know the number of</span><br><span class="line"># output channels in a backbone. For mobilenet_v2, it&#39;s 1280</span><br><span class="line"># so we need to add it here</span><br><span class="line">backbone.out_channels &#x3D; 1280</span><br><span class="line"></span><br><span class="line"># let&#39;s make the RPN generate 5 x 3 anchors per spatial</span><br><span class="line"># location, with 5 different sizes and 3 different aspect</span><br><span class="line"># ratios. We have a Tuple[Tuple[int]] because each feature</span><br><span class="line"># map could potentially have different sizes and</span><br><span class="line"># aspect ratios </span><br><span class="line">anchor_generator &#x3D; AnchorGenerator(sizes&#x3D;((32, 64, 128, 256, 512),),</span><br><span class="line">                                   aspect_ratios&#x3D;((0.5, 1.0, 2.0),))</span><br><span class="line"></span><br><span class="line"># let&#39;s define what are the feature maps that we will</span><br><span class="line"># use to perform the region of interest cropping, as well as</span><br><span class="line"># the size of the crop after rescaling.</span><br><span class="line"># if your backbone returns a Tensor, featmap_names is expected to</span><br><span class="line"># be [0]. More generally, the backbone should return an</span><br><span class="line"># OrderedDict[Tensor], and in featmap_names you can choose which</span><br><span class="line"># feature maps to use.</span><br><span class="line">roi_pooler &#x3D; torchvision.ops.MultiScaleRoIAlign(featmap_names&#x3D;[0],</span><br><span class="line">                                                output_size&#x3D;7,</span><br><span class="line">                                                sampling_ratio&#x3D;2)</span><br><span class="line"></span><br><span class="line"># put the pieces together inside a FasterRCNN model</span><br><span class="line">model &#x3D; FasterRCNN(backbone,</span><br><span class="line">                   num_classes&#x3D;2,</span><br><span class="line">                   rpn_anchor_generator&#x3D;anchor_generator,</span><br><span class="line">                   box_roi_pool&#x3D;roi_pooler)</span><br></pre></td></tr></table></figure>
<h3 id="An-Instance-segmentation-model-for-PennFudan-Dataset"><a href="#An-Instance-segmentation-model-for-PennFudan-Dataset" class="headerlink" title="An Instance segmentation model for PennFudan Dataset"></a>An Instance segmentation model for PennFudan Dataset</h3><p>In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.</p>
<p>Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> FastRCNNPredictor</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.mask_rcnn <span class="keyword">import</span> MaskRCNNPredictor</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_instance_segmentation_model</span><span class="params">(num_classes)</span>:</span></span><br><span class="line">    <span class="comment"># load an instance segmentation model pre-trained on COCO</span></span><br><span class="line">    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the number of input features for the classifier</span></span><br><span class="line">    in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    <span class="comment"># replace the pre-trained head with a new one</span></span><br><span class="line">    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now get the number of input features for the mask classifier</span></span><br><span class="line">    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels</span><br><span class="line">    hidden_layer = <span class="number">256</span></span><br><span class="line">    <span class="comment"># and replace the mask predictor with a new one</span></span><br><span class="line">    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,</span><br><span class="line">                                                       hidden_layer,</span><br><span class="line">                                                       num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>That’s it, this will make model be ready to be trained and evaluated on our custom dataset.</p>
<h2 id="Training-and-evaluation-functions"><a href="#Training-and-evaluation-functions" class="headerlink" title="Training and evaluation functions"></a>Training and evaluation functions</h2><p>In <code>references/detection/,</code> we have a number of helper functions to simplify training and evaluating detection models.<br>Here, we will use <code>references/detection/engine.py</code>, <code>references/detection/utils.py</code> and <code>references/detection/transforms.py</code>.</p>
<p>Let’s copy those files (and their dependencies) in here so that they are available in the notebook</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download TorchVision repo to use some files from</span></span><br><span class="line"><span class="comment"># references/detection</span></span><br><span class="line">git clone https://github.com/pytorch/vision.git</span><br><span class="line">cd vision</span><br><span class="line">git checkout v0<span class="number">.3</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">cp references/detection/utils.py ../</span><br><span class="line">cp references/detection/transforms.py ../</span><br><span class="line">cp references/detection/coco_eval.py ../</span><br><span class="line">cp references/detection/engine.py ../</span><br><span class="line">cp references/detection/coco_utils.py ../</span><br></pre></td></tr></table></figure>
<pre><code>Cloning into &#39;vision&#39;...
remote: Enumerating objects: 20, done.[K
remote: Counting objects: 100% (20/20), done.[K
remote: Compressing objects: 100% (20/20), done.[K
remote: Total 9278 (delta 7), reused 3 (delta 0), pack-reused 9258[K
Receiving objects: 100% (9278/9278), 11.24 MiB | 9.51 MiB/s, done.
Resolving deltas: 100% (6426/6426), done.
Note: checking out &#39;v0.3.0&#39;.

You are in &#39;detached HEAD&#39; state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b &lt;new-branch-name&gt;

HEAD is now at be37608 version check against PyTorch&#39;s CUDA version
</code></pre><p>Let’s write some helper functions for data augmentation / transformation, which leverages the functions in <code>refereces/detection</code> that we have just copied:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> engine <span class="keyword">import</span> train_one_epoch, evaluate</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(train)</span>:</span></span><br><span class="line">    transforms = []</span><br><span class="line">    <span class="comment"># converts the image, a PIL image, into a PyTorch Tensor</span></span><br><span class="line">    transforms.append(T.ToTensor())</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        <span class="comment"># during training, randomly flip the training images</span></span><br><span class="line">        <span class="comment"># and ground-truth for data augmentation</span></span><br><span class="line">        transforms.append(T.RandomHorizontalFlip(<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> T.Compose(transforms)</span><br></pre></td></tr></table></figure>
<h4 id="Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model"><a href="#Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model" class="headerlink" title="Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."></a>Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.</h4><h3 id="Putting-everything-together"><a href="#Putting-everything-together" class="headerlink" title="Putting everything together"></a>Putting everything together</h3><p>We now have the dataset class, the models and the data transforms. Let’s instantiate them</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use our dataset and defined transformations</span></span><br><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">True</span>))</span><br><span class="line">dataset_test = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split the dataset in train and test set</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">indices = torch.randperm(len(dataset)).tolist()</span><br><span class="line">dataset = torch.utils.data.Subset(dataset, indices[:<span class="number">-50</span>])</span><br><span class="line">dataset_test = torch.utils.data.Subset(dataset_test, indices[<span class="number">-50</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># define training and validation data loaders</span></span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br><span class="line"></span><br><span class="line">data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_test, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br></pre></td></tr></table></figure>
<p>Now let’s instantiate the model and the optimizer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># our dataset has two classes only - background and person</span></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get the model using our helper function</span></span><br><span class="line">model = get_instance_segmentation_model(num_classes)</span><br><span class="line"><span class="comment"># move model to the right device</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct an optimizer</span></span><br><span class="line">params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=<span class="number">0.005</span>,</span><br><span class="line">                            momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and a learning rate scheduler which decreases the learning rate by</span></span><br><span class="line"><span class="comment"># 10x every 3 epochs</span></span><br><span class="line">lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,</span><br><span class="line">                                               step_size=<span class="number">3</span>,</span><br><span class="line">                                               gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading: &quot;https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth&quot; to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth



HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value=&#39;&#39;)))
</code></pre><p>And now let’s train the model for 10 epochs, evaluating at the end of every epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># let's train it for 10 epochs</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># train for one epoch, printing every 10 iterations</span></span><br><span class="line">    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># update the learning rate</span></span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    <span class="comment"># evaluate on the test dataset</span></span><br><span class="line">    evaluate(model, data_loader_test, device=device)</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:
    nonzero(Tensor input, *, Tensor out)
Consider using one of the following signatures instead:
    nonzero(Tensor input, *, bool as_tuple)


Epoch: [0]  [ 0/60]  eta: 0:02:18  lr: 0.000090  loss: 3.5827 (3.5827)  loss_classifier: 0.7385 (0.7385)  loss_box_reg: 0.1523 (0.1523)  loss_mask: 2.6620 (2.6620)  loss_objectness: 0.0224 (0.0224)  loss_rpn_box_reg: 0.0076 (0.0076)  time: 2.3152  data: 0.2933  max mem: 2303
Epoch: [0]  [10/60]  eta: 0:01:14  lr: 0.000936  loss: 1.5605 (2.1212)  loss_classifier: 0.4479 (0.4976)  loss_box_reg: 0.1826 (0.1906)  loss_mask: 0.9259 (1.4017)  loss_objectness: 0.0224 (0.0208)  loss_rpn_box_reg: 0.0090 (0.0105)  time: 1.4865  data: 0.0356  max mem: 2860
Epoch: [0]  [20/60]  eta: 0:00:57  lr: 0.001783  loss: 0.8700 (1.4312)  loss_classifier: 0.2338 (0.3409)  loss_box_reg: 0.1579 (0.1731)  loss_mask: 0.4010 (0.8836)  loss_objectness: 0.0191 (0.0216)  loss_rpn_box_reg: 0.0099 (0.0120)  time: 1.3888  data: 0.0096  max mem: 2861
Epoch: [0]  [30/60]  eta: 0:00:43  lr: 0.002629  loss: 0.5382 (1.1211)  loss_classifier: 0.0968 (0.2569)  loss_box_reg: 0.1155 (0.1598)  loss_mask: 0.2489 (0.6751)  loss_objectness: 0.0105 (0.0176)  loss_rpn_box_reg: 0.0099 (0.0117)  time: 1.4144  data: 0.0095  max mem: 3596
Epoch: [0]  [40/60]  eta: 0:00:28  lr: 0.003476  loss: 0.4041 (0.9495)  loss_classifier: 0.0690 (0.2099)  loss_box_reg: 0.1090 (0.1521)  loss_mask: 0.2121 (0.5609)  loss_objectness: 0.0038 (0.0142)  loss_rpn_box_reg: 0.0118 (0.0124)  time: 1.4593  data: 0.0098  max mem: 3596
Epoch: [0]  [50/60]  eta: 0:00:14  lr: 0.004323  loss: 0.3387 (0.8263)  loss_classifier: 0.0496 (0.1785)  loss_box_reg: 0.0833 (0.1393)  loss_mask: 0.1797 (0.4837)  loss_objectness: 0.0035 (0.0122)  loss_rpn_box_reg: 0.0118 (0.0128)  time: 1.4368  data: 0.0101  max mem: 3596
Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.2567 (0.7367)  loss_classifier: 0.0392 (0.1566)  loss_box_reg: 0.0545 (0.1240)  loss_mask: 0.1464 (0.4332)  loss_objectness: 0.0020 (0.0106)  loss_rpn_box_reg: 0.0109 (0.0122)  time: 1.4374  data: 0.0101  max mem: 3596
Epoch: [0] Total time: 0:01:26 (1.4425 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3444 (0.3444)  evaluator_time: 0.0059 (0.0059)  time: 0.4881  data: 0.1360  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)  time: 0.3262  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3309 s / it)
Averaged stats: model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.698
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.901
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.759
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.700
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.886
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.716
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.741
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.745
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.751
Epoch: [1]  [ 0/60]  eta: 0:01:39  lr: 0.005000  loss: 0.1716 (0.1716)  loss_classifier: 0.0231 (0.0231)  loss_box_reg: 0.0309 (0.0309)  loss_mask: 0.1041 (0.1041)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0124 (0.0124)  time: 1.6632  data: 0.3040  max mem: 3596
Epoch: [1]  [10/60]  eta: 0:01:14  lr: 0.005000  loss: 0.2137 (0.2460)  loss_classifier: 0.0314 (0.0385)  loss_box_reg: 0.0309 (0.0406)  loss_mask: 0.1438 (0.1540)  loss_objectness: 0.0011 (0.0017)  loss_rpn_box_reg: 0.0113 (0.0111)  time: 1.4996  data: 0.0356  max mem: 3596
Epoch: [1]  [20/60]  eta: 0:00:59  lr: 0.005000  loss: 0.2565 (0.2636)  loss_classifier: 0.0484 (0.0464)  loss_box_reg: 0.0338 (0.0442)  loss_mask: 0.1639 (0.1582)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0123 (0.0131)  time: 1.4682  data: 0.0102  max mem: 3596
Epoch: [1]  [30/60]  eta: 0:00:43  lr: 0.005000  loss: 0.2174 (0.2409)  loss_classifier: 0.0349 (0.0410)  loss_box_reg: 0.0266 (0.0365)  loss_mask: 0.1426 (0.1502)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0080 (0.0115)  time: 1.4462  data: 0.0105  max mem: 3596
Epoch: [1]  [40/60]  eta: 0:00:29  lr: 0.005000  loss: 0.1930 (0.2327)  loss_classifier: 0.0274 (0.0406)  loss_box_reg: 0.0189 (0.0334)  loss_mask: 0.1380 (0.1463)  loss_objectness: 0.0007 (0.0015)  loss_rpn_box_reg: 0.0075 (0.0109)  time: 1.4625  data: 0.0096  max mem: 3596
Epoch: [1]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.2011 (0.2291)  loss_classifier: 0.0344 (0.0409)  loss_box_reg: 0.0253 (0.0325)  loss_mask: 0.1287 (0.1427)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0079 (0.0115)  time: 1.5020  data: 0.0099  max mem: 3596
Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1680 (0.2220)  loss_classifier: 0.0294 (0.0398)  loss_box_reg: 0.0148 (0.0302)  loss_mask: 0.1265 (0.1394)  loss_objectness: 0.0011 (0.0016)  loss_rpn_box_reg: 0.0075 (0.0110)  time: 1.4470  data: 0.0098  max mem: 3596
Epoch: [1] Total time: 0:01:27 (1.4662 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3606 (0.3606)  evaluator_time: 0.0046 (0.0046)  time: 0.5041  data: 0.1374  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)  time: 0.3234  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3253 s / it)
Averaged stats: model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.772
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.747
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.891
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794
Epoch: [2]  [ 0/60]  eta: 0:01:38  lr: 0.005000  loss: 0.1239 (0.1239)  loss_classifier: 0.0123 (0.0123)  loss_box_reg: 0.0068 (0.0068)  loss_mask: 0.0971 (0.0971)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.6336  data: 0.2465  max mem: 3596
Epoch: [2]  [10/60]  eta: 0:01:12  lr: 0.005000  loss: 0.1866 (0.1751)  loss_classifier: 0.0283 (0.0308)  loss_box_reg: 0.0135 (0.0162)  loss_mask: 0.1129 (0.1189)  loss_objectness: 0.0007 (0.0011)  loss_rpn_box_reg: 0.0072 (0.0082)  time: 1.4478  data: 0.0310  max mem: 3596
Epoch: [2]  [20/60]  eta: 0:00:55  lr: 0.005000  loss: 0.1433 (0.1623)  loss_classifier: 0.0203 (0.0253)  loss_box_reg: 0.0094 (0.0129)  loss_mask: 0.1074 (0.1162)  loss_objectness: 0.0003 (0.0008)  loss_rpn_box_reg: 0.0046 (0.0071)  time: 1.3800  data: 0.0095  max mem: 3596
Epoch: [2]  [30/60]  eta: 0:00:42  lr: 0.005000  loss: 0.1621 (0.1821)  loss_classifier: 0.0218 (0.0294)  loss_box_reg: 0.0101 (0.0170)  loss_mask: 0.1160 (0.1257)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0077 (0.0088)  time: 1.4109  data: 0.0095  max mem: 3596
Epoch: [2]  [40/60]  eta: 0:00:28  lr: 0.005000  loss: 0.1841 (0.1834)  loss_classifier: 0.0286 (0.0291)  loss_box_reg: 0.0157 (0.0164)  loss_mask: 0.1288 (0.1278)  loss_objectness: 0.0005 (0.0012)  loss_rpn_box_reg: 0.0081 (0.0088)  time: 1.4780  data: 0.0099  max mem: 3596
Epoch: [2]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.1970 (0.1878)  loss_classifier: 0.0279 (0.0295)  loss_box_reg: 0.0173 (0.0175)  loss_mask: 0.1317 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0083 (0.0092)  time: 1.4749  data: 0.0099  max mem: 3596
Epoch: [2]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1872 (0.1894)  loss_classifier: 0.0279 (0.0307)  loss_box_reg: 0.0173 (0.0177)  loss_mask: 0.1296 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0094 (0.0095)  time: 1.5513  data: 0.0099  max mem: 3596
Epoch: [2] Total time: 0:01:28 (1.4738 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:21  model_time: 0.3020 (0.3020)  evaluator_time: 0.0047 (0.0047)  time: 0.4358  data: 0.1272  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)  time: 0.3183  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3203 s / it)
Averaged stats: model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.921
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798
Epoch: [3]  [ 0/60]  eta: 0:01:55  lr: 0.000500  loss: 0.1690 (0.1690)  loss_classifier: 0.0193 (0.0193)  loss_box_reg: 0.0098 (0.0098)  loss_mask: 0.1339 (0.1339)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0058 (0.0058)  time: 1.9331  data: 0.4201  max mem: 3596
Epoch: [3]  [10/60]  eta: 0:01:19  lr: 0.000500  loss: 0.1668 (0.1790)  loss_classifier: 0.0294 (0.0273)  loss_box_reg: 0.0102 (0.0148)  loss_mask: 0.1203 (0.1283)  loss_objectness: 0.0005 (0.0010)  loss_rpn_box_reg: 0.0058 (0.0076)  time: 1.5992  data: 0.0464  max mem: 3596
Epoch: [3]  [20/60]  eta: 0:01:01  lr: 0.000500  loss: 0.1635 (0.1723)  loss_classifier: 0.0225 (0.0257)  loss_box_reg: 0.0088 (0.0133)  loss_mask: 0.1203 (0.1243)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0061 (0.0079)  time: 1.5232  data: 0.0096  max mem: 3596
Epoch: [3]  [30/60]  eta: 0:00:44  lr: 0.000500  loss: 0.1603 (0.1683)  loss_classifier: 0.0212 (0.0251)  loss_box_reg: 0.0083 (0.0121)  loss_mask: 0.1198 (0.1228)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0060 (0.0073)  time: 1.4131  data: 0.0097  max mem: 3596
Epoch: [3]  [40/60]  eta: 0:00:29  lr: 0.000500  loss: 0.1603 (0.1725)  loss_classifier: 0.0266 (0.0268)  loss_box_reg: 0.0093 (0.0127)  loss_mask: 0.1150 (0.1239)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0069 (0.0082)  time: 1.4049  data: 0.0097  max mem: 3596
Epoch: [3]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1715 (0.1754)  loss_classifier: 0.0266 (0.0267)  loss_box_reg: 0.0109 (0.0134)  loss_mask: 0.1232 (0.1261)  loss_objectness: 0.0005 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0083)  time: 1.4872  data: 0.0099  max mem: 3596
Epoch: [3]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1509 (0.1709)  loss_classifier: 0.0256 (0.0263)  loss_box_reg: 0.0093 (0.0126)  loss_mask: 0.1055 (0.1231)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0081)  time: 1.4687  data: 0.0096  max mem: 3596
Epoch: [3] Total time: 0:01:28 (1.4791 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3690 (0.3690)  evaluator_time: 0.0046 (0.0046)  time: 0.5078  data: 0.1324  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)  time: 0.3199  data: 0.0051  max mem: 3596
Test: Total time: 0:00:16 (0.3224 s / it)
Averaged stats: model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.509
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.377
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.869
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.755
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.917
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.805
Epoch: [4]  [ 0/60]  eta: 0:01:27  lr: 0.000500  loss: 0.1045 (0.1045)  loss_classifier: 0.0070 (0.0070)  loss_box_reg: 0.0029 (0.0029)  loss_mask: 0.0902 (0.0902)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0043 (0.0043)  time: 1.4538  data: 0.2039  max mem: 3596
Epoch: [4]  [10/60]  eta: 0:01:12  lr: 0.000500  loss: 0.1510 (0.1583)  loss_classifier: 0.0209 (0.0197)  loss_box_reg: 0.0101 (0.0126)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0007)  loss_rpn_box_reg: 0.0071 (0.0075)  time: 1.4482  data: 0.0278  max mem: 3596
Epoch: [4]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1510 (0.1582)  loss_classifier: 0.0209 (0.0215)  loss_box_reg: 0.0073 (0.0110)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0060 (0.0066)  time: 1.3827  data: 0.0100  max mem: 3596
Epoch: [4]  [30/60]  eta: 0:00:41  lr: 0.000500  loss: 0.1427 (0.1624)  loss_classifier: 0.0227 (0.0243)  loss_box_reg: 0.0077 (0.0117)  loss_mask: 0.1005 (0.1176)  loss_objectness: 0.0005 (0.0014)  loss_rpn_box_reg: 0.0058 (0.0074)  time: 1.3627  data: 0.0097  max mem: 3596
Epoch: [4]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1472 (0.1611)  loss_classifier: 0.0255 (0.0253)  loss_box_reg: 0.0085 (0.0114)  loss_mask: 0.1079 (0.1161)  loss_objectness: 0.0004 (0.0013)  loss_rpn_box_reg: 0.0063 (0.0071)  time: 1.4537  data: 0.0094  max mem: 3596
Epoch: [4]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1548 (0.1612)  loss_classifier: 0.0250 (0.0249)  loss_box_reg: 0.0079 (0.0113)  loss_mask: 0.1106 (0.1161)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0068 (0.0077)  time: 1.4913  data: 0.0094  max mem: 3596
Epoch: [4]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1548 (0.1647)  loss_classifier: 0.0250 (0.0256)  loss_box_reg: 0.0079 (0.0121)  loss_mask: 0.1106 (0.1179)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0079 (0.0080)  time: 1.4724  data: 0.0097  max mem: 3596
Epoch: [4] Total time: 0:01:26 (1.4357 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3692 (0.3692)  evaluator_time: 0.0045 (0.0045)  time: 0.5054  data: 0.1301  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)  time: 0.3202  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3220 s / it)
Averaged stats: model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.813
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.944
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.520
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.825
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.868
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.920
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815
Epoch: [5]  [ 0/60]  eta: 0:02:13  lr: 0.000500  loss: 0.1545 (0.1545)  loss_classifier: 0.0223 (0.0223)  loss_box_reg: 0.0059 (0.0059)  loss_mask: 0.1200 (0.1200)  loss_objectness: 0.0004 (0.0004)  loss_rpn_box_reg: 0.0059 (0.0059)  time: 2.2323  data: 0.5519  max mem: 3596
Epoch: [5]  [10/60]  eta: 0:01:10  lr: 0.000500  loss: 0.1409 (0.1489)  loss_classifier: 0.0178 (0.0211)  loss_box_reg: 0.0076 (0.0094)  loss_mask: 0.1140 (0.1118)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0057 (0.0061)  time: 1.4098  data: 0.0540  max mem: 3596
Epoch: [5]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1379 (0.1454)  loss_classifier: 0.0189 (0.0208)  loss_box_reg: 0.0076 (0.0088)  loss_mask: 0.1032 (0.1091)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0054 (0.0061)  time: 1.3437  data: 0.0070  max mem: 3596
Epoch: [5]  [30/60]  eta: 0:00:42  lr: 0.000500  loss: 0.1430 (0.1597)  loss_classifier: 0.0240 (0.0246)  loss_box_reg: 0.0092 (0.0114)  loss_mask: 0.1032 (0.1154)  loss_objectness: 0.0005 (0.0008)  loss_rpn_box_reg: 0.0069 (0.0075)  time: 1.4147  data: 0.0100  max mem: 3596
Epoch: [5]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1503 (0.1609)  loss_classifier: 0.0242 (0.0243)  loss_box_reg: 0.0102 (0.0117)  loss_mask: 0.1148 (0.1163)  loss_objectness: 0.0004 (0.0008)  loss_rpn_box_reg: 0.0083 (0.0078)  time: 1.4826  data: 0.0101  max mem: 3596
Epoch: [5]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1397 (0.1571)  loss_classifier: 0.0198 (0.0237)  loss_box_reg: 0.0075 (0.0107)  loss_mask: 0.1017 (0.1144)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0066 (0.0075)  time: 1.4890  data: 0.0096  max mem: 3596
Epoch: [5]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1422 (0.1581)  loss_classifier: 0.0197 (0.0241)  loss_box_reg: 0.0066 (0.0107)  loss_mask: 0.1042 (0.1149)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0064 (0.0076)  time: 1.5030  data: 0.0094  max mem: 3596
Epoch: [5] Total time: 0:01:27 (1.4584 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3761 (0.3761)  evaluator_time: 0.0041 (0.0041)  time: 0.5475  data: 0.1655  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)  time: 0.3195  data: 0.0049  max mem: 3596
Test: Total time: 0:00:16 (0.3235 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [6]  [ 0/60]  eta: 0:01:53  lr: 0.000050  loss: 0.1645 (0.1645)  loss_classifier: 0.0255 (0.0255)  loss_box_reg: 0.0121 (0.0121)  loss_mask: 0.1195 (0.1195)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.8885  data: 0.2840  max mem: 3596
Epoch: [6]  [10/60]  eta: 0:01:17  lr: 0.000050  loss: 0.1414 (0.1454)  loss_classifier: 0.0206 (0.0221)  loss_box_reg: 0.0056 (0.0076)  loss_mask: 0.1039 (0.1099)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0058 (0.0053)  time: 1.5460  data: 0.0336  max mem: 3596
Epoch: [6]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1414 (0.1516)  loss_classifier: 0.0206 (0.0241)  loss_box_reg: 0.0065 (0.0101)  loss_mask: 0.1030 (0.1104)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0059 (0.0066)  time: 1.5057  data: 0.0092  max mem: 3596
Epoch: [6]  [30/60]  eta: 0:00:45  lr: 0.000050  loss: 0.1479 (0.1531)  loss_classifier: 0.0255 (0.0261)  loss_box_reg: 0.0087 (0.0099)  loss_mask: 0.1030 (0.1098)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0072 (0.0068)  time: 1.4797  data: 0.0104  max mem: 3596
Epoch: [6]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1493 (0.1593)  loss_classifier: 0.0255 (0.0267)  loss_box_reg: 0.0087 (0.0111)  loss_mask: 0.1043 (0.1137)  loss_objectness: 0.0005 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0072)  time: 1.4498  data: 0.0104  max mem: 3596
Epoch: [6]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1440 (0.1584)  loss_classifier: 0.0234 (0.0261)  loss_box_reg: 0.0088 (0.0110)  loss_mask: 0.1129 (0.1135)  loss_objectness: 0.0004 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0073)  time: 1.4308  data: 0.0097  max mem: 3596
Epoch: [6]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1440 (0.1588)  loss_classifier: 0.0216 (0.0260)  loss_box_reg: 0.0080 (0.0110)  loss_mask: 0.1118 (0.1140)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0070 (0.0073)  time: 1.4508  data: 0.0095  max mem: 3596
Epoch: [6] Total time: 0:01:28 (1.4739 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3615 (0.3615)  evaluator_time: 0.0038 (0.0038)  time: 0.5174  data: 0.1505  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)  time: 0.3199  data: 0.0057  max mem: 3596
Test: Total time: 0:00:16 (0.3225 s / it)
Averaged stats: model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.351
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [7]  [ 0/60]  eta: 0:01:47  lr: 0.000050  loss: 0.1122 (0.1122)  loss_classifier: 0.0151 (0.0151)  loss_box_reg: 0.0039 (0.0039)  loss_mask: 0.0920 (0.0920)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.7859  data: 0.4771  max mem: 3596
Epoch: [7]  [10/60]  eta: 0:01:11  lr: 0.000050  loss: 0.1316 (0.1467)  loss_classifier: 0.0157 (0.0221)  loss_box_reg: 0.0052 (0.0086)  loss_mask: 0.1004 (0.1097)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0037 (0.0053)  time: 1.4340  data: 0.0504  max mem: 3596
Epoch: [7]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1570 (0.1557)  loss_classifier: 0.0288 (0.0274)  loss_box_reg: 0.0085 (0.0100)  loss_mask: 0.1075 (0.1104)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0066 (0.0068)  time: 1.4943  data: 0.0092  max mem: 3596
Epoch: [7]  [30/60]  eta: 0:00:44  lr: 0.000050  loss: 0.1447 (0.1519)  loss_classifier: 0.0257 (0.0255)  loss_box_reg: 0.0076 (0.0093)  loss_mask: 0.1062 (0.1092)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0071 (0.0068)  time: 1.4985  data: 0.0104  max mem: 3596
Epoch: [7]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1418 (0.1546)  loss_classifier: 0.0222 (0.0251)  loss_box_reg: 0.0068 (0.0098)  loss_mask: 0.1095 (0.1120)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0054 (0.0069)  time: 1.4120  data: 0.0099  max mem: 3596
Epoch: [7]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1616 (0.1590)  loss_classifier: 0.0232 (0.0253)  loss_box_reg: 0.0082 (0.0107)  loss_mask: 0.1132 (0.1150)  loss_objectness: 0.0002 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0072)  time: 1.4197  data: 0.0096  max mem: 3596
Epoch: [7]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1474 (0.1592)  loss_classifier: 0.0230 (0.0256)  loss_box_reg: 0.0060 (0.0106)  loss_mask: 0.1101 (0.1150)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0057 (0.0072)  time: 1.4280  data: 0.0095  max mem: 3596
Epoch: [7] Total time: 0:01:27 (1.4505 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3761 (0.3761)  evaluator_time: 0.0044 (0.0044)  time: 0.5139  data: 0.1315  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)  time: 0.3181  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3223 s / it)
Averaged stats: model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.946
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.923
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.382
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.352
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [8]  [ 0/60]  eta: 0:01:59  lr: 0.000050  loss: 0.1533 (0.1533)  loss_classifier: 0.0187 (0.0187)  loss_box_reg: 0.0076 (0.0076)  loss_mask: 0.1242 (0.1242)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0027 (0.0027)  time: 1.9847  data: 0.7161  max mem: 3596
Epoch: [8]  [10/60]  eta: 0:01:14  lr: 0.000050  loss: 0.1533 (0.1537)  loss_classifier: 0.0227 (0.0245)  loss_box_reg: 0.0076 (0.0104)  loss_mask: 0.1094 (0.1121)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0047 (0.0061)  time: 1.4974  data: 0.0700  max mem: 3596
Epoch: [8]  [20/60]  eta: 0:00:58  lr: 0.000050  loss: 0.1512 (0.1592)  loss_classifier: 0.0217 (0.0230)  loss_box_reg: 0.0084 (0.0112)  loss_mask: 0.1071 (0.1168)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0070 (0.0072)  time: 1.4405  data: 0.0081  max mem: 3596
Epoch: [8]  [30/60]  eta: 0:00:43  lr: 0.000050  loss: 0.1390 (0.1557)  loss_classifier: 0.0217 (0.0237)  loss_box_reg: 0.0084 (0.0111)  loss_mask: 0.1021 (0.1130)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0071)  time: 1.4221  data: 0.0102  max mem: 3596
Epoch: [8]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1438 (0.1632)  loss_classifier: 0.0257 (0.0257)  loss_box_reg: 0.0086 (0.0117)  loss_mask: 0.1112 (0.1172)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0076 (0.0075)  time: 1.4616  data: 0.0096  max mem: 3596
Epoch: [8]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1613 (0.1628)  loss_classifier: 0.0265 (0.0265)  loss_box_reg: 0.0104 (0.0116)  loss_mask: 0.1130 (0.1157)  loss_objectness: 0.0005 (0.0011)  loss_rpn_box_reg: 0.0076 (0.0080)  time: 1.5084  data: 0.0096  max mem: 3596
Epoch: [8]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1426 (0.1593)  loss_classifier: 0.0209 (0.0254)  loss_box_reg: 0.0060 (0.0107)  loss_mask: 0.1046 (0.1148)  loss_objectness: 0.0003 (0.0011)  loss_rpn_box_reg: 0.0056 (0.0073)  time: 1.4303  data: 0.0097  max mem: 3596
Epoch: [8] Total time: 0:01:27 (1.4531 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3871 (0.3871)  evaluator_time: 0.0041 (0.0041)  time: 0.5413  data: 0.1481  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)  time: 0.3191  data: 0.0052  max mem: 3596
Test: Total time: 0:00:16 (0.3230 s / it)
Averaged stats: model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [9]  [ 0/60]  eta: 0:01:35  lr: 0.000005  loss: 0.1384 (0.1384)  loss_classifier: 0.0122 (0.0122)  loss_box_reg: 0.0035 (0.0035)  loss_mask: 0.1193 (0.1193)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0031 (0.0031)  time: 1.5976  data: 0.1940  max mem: 3596
Epoch: [9]  [10/60]  eta: 0:01:12  lr: 0.000005  loss: 0.1391 (0.1678)  loss_classifier: 0.0229 (0.0239)  loss_box_reg: 0.0087 (0.0123)  loss_mask: 0.1188 (0.1247)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0055 (0.0064)  time: 1.4416  data: 0.0261  max mem: 3596
Epoch: [9]  [20/60]  eta: 0:00:57  lr: 0.000005  loss: 0.1595 (0.1658)  loss_classifier: 0.0253 (0.0262)  loss_box_reg: 0.0106 (0.0121)  loss_mask: 0.1154 (0.1203)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0056 (0.0066)  time: 1.4367  data: 0.0094  max mem: 3596
Epoch: [9]  [30/60]  eta: 0:00:43  lr: 0.000005  loss: 0.1595 (0.1680)  loss_classifier: 0.0256 (0.0275)  loss_box_reg: 0.0088 (0.0125)  loss_mask: 0.1150 (0.1199)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0060 (0.0074)  time: 1.4554  data: 0.0095  max mem: 3596
Epoch: [9]  [40/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1449 (0.1605)  loss_classifier: 0.0212 (0.0258)  loss_box_reg: 0.0070 (0.0111)  loss_mask: 0.1049 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0064 (0.0069)  time: 1.4446  data: 0.0094  max mem: 3596
Epoch: [9]  [50/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1504 (0.1591)  loss_classifier: 0.0195 (0.0256)  loss_box_reg: 0.0083 (0.0109)  loss_mask: 0.1037 (0.1149)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0071)  time: 1.4887  data: 0.0095  max mem: 3596
Epoch: [9]  [59/60]  eta: 0:00:01  lr: 0.000005  loss: 0.1527 (0.1602)  loss_classifier: 0.0224 (0.0256)  loss_box_reg: 0.0083 (0.0108)  loss_mask: 0.1102 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0072)  time: 1.4984  data: 0.0097  max mem: 3596
Epoch: [9] Total time: 0:01:27 (1.4592 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3668 (0.3668)  evaluator_time: 0.0042 (0.0042)  time: 0.5024  data: 0.1296  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)  time: 0.3215  data: 0.0061  max mem: 3596
Test: Total time: 0:00:16 (0.3233 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.822
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.832
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.874
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.774
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
</code></pre><p>Now that training has finished, let’s have a look at what it actually predicts in a test image</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pick one image from the test set</span></span><br><span class="line">img, _ = dataset_test[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># put the model in evaluation mode</span></span><br><span class="line">model.eval()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    prediction = model([img.to(device)])</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
</code></pre><p>Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.<br>The dictionary contains the predictions for the image we passed. In this case, we can see that it contains <code>boxes</code>, <code>labels</code>, <code>masks</code> and <code>scores</code> as fields.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prediction</span><br></pre></td></tr></table></figure>
<pre><code>[{&#39;boxes&#39;: tensor([[ 59.6432,  41.9334, 195.6993, 327.8640],
          [276.4631,  22.6867, 290.8581,  73.6079]], device=&#39;cuda:0&#39;),
  &#39;labels&#39;: tensor([1, 1], device=&#39;cuda:0&#39;),
  &#39;masks&#39;: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]],


          [[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]]], device=&#39;cuda:0&#39;),
  &#39;scores&#39;: tensor([0.9991, 0.8170], device=&#39;cuda:0&#39;)}]
</code></pre><p>Let’s inspect the image and the predicted segmentation masks.</p>
<p>For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in <code>[C, H, W]</code> format.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Image.fromarray(img.mul(<span class="number">255</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).byte().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_31_0.png" alt="png"></p>
<p>And let’s now visualize the top predicted segmentation mask. The masks are predicted as <code>[N, 1, H, W]</code>, where <code>N</code> is the number of predictions, and are probability maps between 0-1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Image.fromarray(prediction[<span class="number">0</span>][<span class="string">'masks'</span>][<span class="number">0</span>, <span class="number">0</span>].mul(<span class="number">255</span>).byte().cpu().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_33_0.png" alt="png"></p>
<p>Looks pretty good!</p>
<h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset.<br>For that, you wrote a <code>torch.utils.data.Dataset</code> class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.</p>
<p>For a more complete example, which includes multi-machine / multi-gpu training, check <code>references/detection/train.py</code>, which is present in the <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision GitHub repo</a>. </p>
<p>#<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">coordinates:坐标, 座标,协调, 配合, 接应</span><br><span class="line">segmentation:分割</span><br><span class="line">backbone:主干</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch1.5.1官网教程</tag>
        <tag>pytorch</tag>
        <tag>Pytorch1.5.1官网教程-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N翻译版</title>
    <url>/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/</url>
    <content><![CDATA[<p>斯坦福大学深度学习课程笔记翻译版<br><a id="more"></a><br><a href="https://githubzhangshuai.github.io/2020/07/29/CS231N/#more" target="_blank" rel="noopener">英文版</a></p>
<h1 id="Standford-CS231n-2017-Summary"><a href="#Standford-CS231n-2017-Summary" class="headerlink" title="Standford CS231n 2017 Summary"></a>Standford CS231n 2017 Summary</h1><p>在观看了2017年著名的斯坦福<a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">CS231n</a>课程的所有视频后，我决定对整个课程进行总结，以帮助我记住并向任何想知道它的人。有些课我跳过了一些内容，因为这对我来说并不重要。</p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li><a href="#standford-cs231n-2017-summary">Standford CS231n 2017 Summary</a><ul>
<li><a href="#目录">目录</a></li>
<li><a href="#课程信息">课程信息</a></li>
<li><a href="#01-introduction-to-cnn-for-visual-recognition">01. CNN视觉识别简介</a></li>
<li><a href="#02-image-classification">02. 图像分类</a></li>
<li><a href="#03-loss-function-and-optimization">03. 损失函数与最优化</a></li>
<li><a href="#04-introduction-to-neural-network">04. 神经网络概论</a></li>
<li><a href="#05-convolutional-neural-networks-cnns">05. 卷积神经网络</a></li>
<li><a href="#06-training-neural-networks-i">06. 神经网络训练I</a></li>
<li><a href="#07-training-neural-networks-ii">07. 训练神经网络2</a></li>
<li><a href="#08-deep-learning-software">08. 深度学习软件</a></li>
<li><a href="#09-cnn-architectures">09. CNN架构</a></li>
<li><a href="#10-recurrent-neural-networks">10. 循环神经网络</a></li>
<li><a href="#11-detection-and-segmentation">11. 检测与分割</a></li>
<li><a href="#12-visualizing-and-understanding">12. 形象化和理解</a></li>
<li><a href="#13-generative-models">13. 生成模型</a></li>
<li><a href="#14-deep-reinforcement-learning">14. 深度强化学习</a></li>
<li><a href="#15-efficient-methods-and-hardware-for-deep-learning">15. 有效的深度学习方法和硬件</a></li>
<li><a href="#16-adversarial-examples-and-adversarial-training">16. 对抗性范例与对抗性训练</a></li>
</ul>
</li>
</ul>
<h2 id="课程信息"><a href="#课程信息" class="headerlink" title="课程信息"></a>课程信息</h2><ul>
<li><p>网站: <a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">http://cs231n.stanford.edu/</a></p>
</li>
<li><p>讲座链接: <a href="https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk</a></p>
</li>
<li><p>完整教学大纲链接: <a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">http://cs231n.stanford.edu/syllabus.html</a></p>
</li>
<li><p>作业解决方案: <a href="https://github.com/Burton2000/CS231n-2017" target="_blank" rel="noopener">https://github.com/Burton2000/CS231n-2017</a></p>
</li>
<li><p>讲座次数: <strong>16</strong></p>
</li>
<li><p>课程描述:</p>
<ul>
<li><blockquote>
<p>计算机视觉在我们的社会中已经变得无处不在，在搜索、图像理解、应用程序、地图、医学、无人机和自动驾驶汽车等领域都有应用。这些应用的核心是视觉识别任务，如图像分类、定位和检测。神经网络（又名“深度学习”）方法的最新发展极大地提高了这些最先进的视觉识别系统的性能。本课程深入研究深度学习架构的细节，重点学习这些任务的端到端模型，特别是图像分类。在为期10周的课程中，学生们将学习如何实现、训练和调试自己的神经网络，并详细了解计算机视觉领域的前沿研究。最后的任务是训练一个数百万参数的卷积神经网络，并将其应用于最大的图像分类数据集（ImageNet）。我们将重点教授如何设置图像识别问题、学习算法（如反向传播）、训练和微调网络的实用工程技巧，并指导学生通过实际作业和最终课程项目。本课程的大部分背景和材料都将来自<a href="http://image-net.org/challenges/LSVRC/2014/index" target="_blank" rel="noopener">ImageNet挑战赛</a>。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="01-Introduction-to-CNN-for-visual-recognition"><a href="#01-Introduction-to-CNN-for-visual-recognition" class="headerlink" title="01. Introduction to CNN for visual recognition"></a>01. Introduction to CNN for visual recognition</h2><ul>
<li>20世纪60年代末至2017年计算机视觉发展简史</li>
<li>计算机视觉问题包括图像分类、目标定位、目标检测和场景理解。</li>
<li><a href="http://www.image-net.org/" target="_blank" rel="noopener">Imagenet</a> 是目前可用的最大的图像分类数据集之一</li>
<li>从2012年的Imagenet竞赛开始，CNN（卷积神经网络）总是胜出</li>
<li>CNN实际上是由<a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">Yann Lecun</a>在1997年发明的 .</li>
</ul>
<h2 id="02-Image-classification"><a href="#02-Image-classification" class="headerlink" title="02. Image classification"></a>02. Image classification</h2><ul>
<li>图像分类问题有很多挑战，比如光照和视角<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/39.jpeg" alt></li>
</ul>
</li>
<li>一种图像分类算法可以用<strong>K近邻</strong>(KNN)来解决，但它不能很好地解决这个问题。KNN的性质是:<ul>
<li>KNN的超参数是：k和距离测度</li>
<li>K是我们正在比较的邻居结点的数目。</li>
<li>距离测量包括:<ul>
<li>L2距离（欧几里得距离）<ul>
<li>最适合非坐标点</li>
</ul>
</li>
<li>L1距离（曼哈顿距离）<ul>
<li>最适合坐标点</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>超参数可以使用交叉验证进行优化，如下所示（在我们的例子中，我们尝试tp predict K）:<ol>
<li>把数据集分成k份</li>
<li>给定预测超参数：<ul>
<li>先找出f-1份训练你的算法，用剩余的1份来测试它。对每份都要重复这个动作。</li>
</ul>
</li>
<li>选择提供最佳训练值的超参数（所有份数的平均值）</li>
</ol>
</li>
<li><strong>线性支持向量机</strong>分类器是解决图像分类问题的一种选择，但维数灾难使其在某些情况下停止了改进</li>
<li><strong>逻辑回归</strong>也是解决图像分类问题的一种方法，但图像分类问题是非线性的</li>
<li>线性分类器必须遵从以下等式: <code>Y = wX + b</code> <ul>
<li>w的形状与x相同，b的形状为1。</li>
</ul>
</li>
<li>我们可以在X向量上加1，去掉偏差，像这样: <code>Y = wX</code><ul>
<li>x的形状是旧的x+1的形状，w与x相同</li>
</ul>
</li>
<li>我们需要知道如何得到w和b，使分类器运行得最好。</li>
</ul>
<h2 id="03-Loss-function-and-optimization"><a href="#03-Loss-function-and-optimization" class="headerlink" title="03. Loss function and optimization"></a>03. Loss function and optimization</h2><ul>
<li><p>在最后一节中，我们讨论了线性分类器，但是我们没有讨论如何训练模型的参数以获得最佳的w和b。</p>
</li>
<li><p>我们需要一个损失函数来衡量我们当前参数的好坏。</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Loss = L[i] =(f(X[i],W),Y[i])</span><br><span class="line">Loss_for_all = <span class="number">1</span>/N * Sum(Li(f(X[i],W),Y[i]))      <span class="comment">#表示平均值</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>然后在给定参数的情况下，找到了一种最小化损失函数的方法。这叫做<strong>优化</strong>.</p>
</li>
<li><p>线性支持<strong>向量机</strong>分类器的损失函数:</p>
<ul>
<li><code>L[i] = 除预测类外的所有类的和 (max(0, s[j] - s[y[i]] + 1))</code></li>
<li>我们叫它 <strong><em>the hinge loss</em></strong>.</li>
<li>损失函数意味着，如果最佳预测值与真实值相同，则我们很高兴；反之，我们给出的误差为1。</li>
<li>例子:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/40.jpg" alt></li>
<li>在这个例子中，我们要计算这个图像的损失。</li>
<li><code>L = max (0, 437.9 - (-96.8) + 1) + max(0, 61.95 - (-96.8) + 1) = max(0, 535.7) + max(0, 159.75) = 695.45</code></li>
<li>最后的损失是695.45分，这是一个很大的数字，反映出猫的分数需要是最好的所有类别，作为它的最低值现在。 我们需要把损失降到最低。</li>
</ul>
</li>
<li>边距为1是可以的。但它也是一个超参数。</li>
</ul>
</li>
<li><p>如果你的损失函数为零，这个值和你的参数值一样吗？不，有很多参数可以给你最好的分数。</p>
</li>
<li><p>你有时会听到人们用<strong>the squared hinge loss SVM</strong>(or <strong>L2-SVM</strong>)来代替. 这对违规利润的惩罚力度更大（二次方而不是线性）。 无平方的版本更为标准，但在某些数据集中，<strong>the squared hinge loss</strong>可以更好地工作。</p>
</li>
<li><p>我们对损失函数进行了<strong>正则化</strong>处理，这样发现的模型不会过度拟合数据。</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Loss = L = <span class="number">1</span>/N * Sum(Li(f(X[i],W),Y[i])) + <span class="keyword">lambda</span> * R(W)</span><br></pre></td></tr></table></figure>
<ul>
<li>其中<code>R</code>是正则化子，<code>lambda</code>是正则化项</li>
</ul>
</li>
<li><p>有不同的正则化技术:</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>正则化矩阵</th>
<th>等式</th>
<th>注释</th>
</tr>
</thead>
<tbody>
<tr>
<td>L2</td>
<td><code>R(W) = Sum(W^2)</code></td>
<td>所有W的平方和</td>
</tr>
<tr>
<td>L1</td>
<td><code>R(W) = Sum(lWl)</code></td>
<td>所有W的绝对值之和</td>
</tr>
<tr>
<td>Elastic net (L1 + L2)</td>
<td><code>R(W) = beta * Sum(W^2) + Sum(lWl)</code></td>
<td></td>
</tr>
<tr>
<td>Dropout</td>
<td></td>
<td>没有等式</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>正则化倾向于较小的<code>W</code> s而不是大的<code>W</code> s。</p>
</li>
<li><p>正则化称为权重衰减。偏置不应包括在正规化中。</p>
</li>
<li><p>Softmax损失（类似线性回归，但适用于2个以上类别）：</p>
<ul>
<li><p>Softmax function:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A[L] = e^(score[L]) / sum(e^(score[L]), NoOfClasses)</span><br></pre></td></tr></table></figure>
</li>
<li><p>矢量和应为1</p>
</li>
<li><p>Softmax loss:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Loss = -logP(Y = y[i]|X = x[i])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>好类比的概率的log值。我们希望它接近1，所以我们加了个负数。</p>
</li>
<li><p>Softmax损失称为交叉熵损失。</p>
</li>
</ul>
</li>
<li><p>在计算Softmax时，请考虑这个数值问题:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3个分类的示例，每个分类都有很高的分数</span></span><br><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) </span><br><span class="line"><span class="comment"># 坏：数字问题，潜在爆炸</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># instead: 首先将f的值移位，以使最大数为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line"><span class="comment"># safe to do, gives the correct answer</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>最优化</strong>:</p>
<ul>
<li>如何优化我们讨论过的损失函数?</li>
<li>策略一:<ul>
<li>给定一个随机参数，并尝试所有的损失，得到最佳的损失。但这是个坏主意。</li>
</ul>
</li>
<li><p>策略二:</p>
<ul>
<li><p>沿着斜坡走。</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/41.png" alt></li>
<li>Image <a href="https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png" target="_blank" rel="noopener">source</a>.</li>
</ul>
</li>
<li><p>我们的目标是计算每个参数的梯度。</p>
<ul>
<li><strong>数值梯度</strong>: 近似，缓慢，易于书写。（但它在调试中很有用。）</li>
<li><strong>解析梯度</strong>: 准确、快速、容易出错。（实践中经常使用）</li>
</ul>
</li>
<li><p>计算参数梯度后，计算梯度下降:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = W - learning_rate * W_grad</span><br></pre></td></tr></table></figure>
</li>
<li><p>学习速率是一个非常重要的超参数，在所有的超参数中，首先要得到它的最佳值。</p>
</li>
<li><p>随机梯度下降:</p>
<ul>
<li>不要使用所有的数据，而是使用小批量的示例（通常使用32/64/128）来获得更快的结果。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="04-Introduction-to-Neural-network"><a href="#04-Introduction-to-Neural-network" class="headerlink" title="04. Introduction to Neural network"></a>04. Introduction to Neural network</h2><ul>
<li><p>计算任意复函数的解析梯度:</p>
<ul>
<li><p>什么是计算图?</p>
<ul>
<li>用节点来表示任何函数</li>
<li>使用计算图可以很容易地引导我们使用一种称为反向传播的技术。即使是像CNN和RNN这样的复杂模型。</li>
</ul>
</li>
<li><p>反向传播简单示例:</p>
<ul>
<li><p>假设我们有 <code>f(x,y,z) = (x+y)z</code></p>
</li>
<li><p>然后计算图可以被这种方式表示:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X         </span><br><span class="line">  \</span><br><span class="line">   (+)--&gt; q ---(*)--&gt; f</span><br><span class="line">  &#x2F;           &#x2F;</span><br><span class="line">Y            &#x2F;</span><br><span class="line">            &#x2F;</span><br><span class="line">           &#x2F;</span><br><span class="line">Z---------&#x2F;</span><br></pre></td></tr></table></figure>
</li>
<li><p>我们制作了一个中间变量<code>q</code>来保存<code>x+y</code>的值</p>
</li>
<li><p>然后我们有:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q = (x+y)              <span class="comment"># dq/dx = 1 , dq/dy = 1</span></span><br><span class="line">f = qz                 <span class="comment"># df/dq = z , df/dz = q</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>然后:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df/dq = z</span><br><span class="line">df/dz = q</span><br><span class="line">df/dx = df/dq * dq/dx = z * <span class="number">1</span> = z       <span class="comment"># Chain rule</span></span><br><span class="line">df/dy = df/dq * dq/dy = z * <span class="number">1</span> = z       <span class="comment"># Chain rule</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>所以在计算图中，我们称每个运算为 <code>f</code>。 对于每一个<code>f</code>，我们在进行反向传播之前计算局部梯度，然后使用链式法则计算关于损失函数的梯度。</p>
</li>
<li><p>在计算图中，你可以将每个操作拆分为你想要的那样简单，但是节点会很多。如果希望节点更小到可以计算该节点的梯度。</p>
</li>
<li><p>一个更大的例子:</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/01.png" alt></li>
<li>提示：两个节点从后面到一个节点的反向传播是通过添加两个导数来实现的。</li>
</ul>
</li>
<li><p>模块化实现：正向/反向API（示例乘法代码）:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultuplyGate</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  x,y are scalars</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z = x*y</span><br><span class="line">    self.x = x  <span class="comment"># Cache</span></span><br><span class="line">    self.y = y	<span class="comment"># Cache</span></span><br><span class="line">    <span class="comment"># We cache x and y because we know that the derivatives contains them.</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(dz)</span>:</span></span><br><span class="line">    dx = self.y * dz         <span class="comment">#self.y is dx</span></span><br><span class="line">    dy = self.x * dz</span><br><span class="line">    <span class="keyword">return</span> [dx, dy]</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果你看一个深度学习框架，你会发现它遵循模块化的实现，每个类都有正向传播和反向传播的定义。例如：</p>
<ul>
<li>Multiplication</li>
<li>Max</li>
<li>Plus</li>
<li>Minus</li>
<li>Sigmoid</li>
<li>Convolution</li>
</ul>
</li>
</ul>
</li>
<li><p>把神经网络定义为一个函数:</p>
<ul>
<li>（之前）线性评分函数: <code>f = Wx</code></li>
<li>（现在）2层神经网络:    <code>f = W2*max(0,W1*x)</code> <ul>
<li>其中max是RELU非线性函数</li>
</ul>
</li>
<li>（现在）三层神经网络:    <code>f = W3*max(0,W2*max(0,W1*x)</code></li>
<li>等等..</li>
</ul>
</li>
<li><p>神经网络是一些简单操作的堆栈，形成复杂的操作。</p>
</li>
</ul>
<h2 id="05-Convolutional-neural-networks-CNNs"><a href="#05-Convolutional-neural-networks-CNNs" class="headerlink" title="05. Convolutional neural networks (CNNs)"></a>05. Convolutional neural networks (CNNs)</h2><ul>
<li>神经网络历史:<ul>
<li>第一台感知器机器是弗兰克罗森布拉特在1957年开发的。它被用来识别字母表中的字母。反向传播还没有发展。</li>
<li>多层感知器是由Adaline/Madaline于1960年开发的。反向传播还没有发展。</li>
<li>反向传播是Rumeelhart在1986年发明的。</li>
<li>有一段时间，NN没有什么新的事情发生。由于计算资源和数据有限。</li>
<li><a href="www.cs.toronto.edu/~fritz/absps/netflix.pdf">2006</a>年，Hinton发表了一篇论文，表明我们可以使用受限的Boltzmann机器来训练一个深度神经网络来初始化权值，然后再进行反向传播。</li>
<li>2012年，Hinton在<a href="http://ieeexplore.ieee.org/document/6296526/" target="_blank" rel="noopener">语音识别</a>领域取得了第一个强劲的成果。而<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Alexnet</a> 的“卷积神经网络”在2012年赢得了图像网络，也是由Hinton的团队完成的</li>
<li>之后，神经网络被广泛应用于各种应用中。</li>
</ul>
</li>
<li>卷积神经网络历史：<ul>
<li>Hubel和Wisel在1959年至1968年对猫大脑皮层的实验发现，大脑皮层有一个地形图，神经元具有从简单到复杂的特殊组织。</li>
<li>1998年，Yann-Lecun提出了一篇<a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">基于梯度的学习应用于文档识别的论文</a>，引入了卷积神经网络。它很好地识别邮政编码，但不能运行在更复杂的例子</li>
<li>2012年，<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a> 使用了同样的Yan Lecun架构，并赢得了图像网络挑战赛。 与1998年不同的是，现在我们有了一个可以使用的大数据集，gpu的强大功能解决了很多性能问题。</li>
<li>从2012年开始，CNN将用于各种任务（以下是一些应用程序）:<ul>
<li>图像分类.</li>
<li>图像检索.<ul>
<li>利用神经网络提取特征，然后进行相似性匹配.</li>
</ul>
</li>
<li>目标检测.</li>
<li>分割.<ul>
<li>图像中的每个像素都有一个标签.</li>
</ul>
</li>
<li>人脸识别.</li>
<li>姿势识别.</li>
<li>医学图像.</li>
<li>用强化学习玩阿塔里游戏.</li>
<li>星系分类.</li>
<li>路标识别.</li>
<li>图片字幕.</li>
<li>风格迁移.</li>
</ul>
</li>
</ul>
</li>
<li>ConvNet体系结构明确假设输入是图像，这允许我们将某些属性编码到体系结构中</li>
<li>ConvNet中有几种不同类型的层（例如CONV/FC/RELU/POOL是目前最流行的）</li>
<li>每个层可能有参数，也可能没有参数（例如CONV/FC 会，RELU/POOL 不会）</li>
<li>每一层可能有也可能没有额外的超参数（例如CONV/FC/POOL 有，RELU没有）</li>
<li>卷积神经网络的工作原理?<ul>
<li>完全连接层是所有神经元都连接在一起的层. 有时我们称之为稠密层<ul>
<li>If input shape is <code>(X, M)</code> the weighs shape for this will be <code>(NoOfHiddenNeurons, X)</code></li>
</ul>
</li>
<li>卷积层是一层，在这个层中，我们将通过一个滤波器来保持输入的结构，该滤波器将穿过所有图像。<ul>
<li>我们用点积：W.T*X+b。这个等式使用广播技术。</li>
<li>所以我们需要得到W和b的值</li>
<li>我们通常把滤波器（W）当作向量而不是矩阵来处理。</li>
</ul>
</li>
<li>我们称之为卷积激活图的输出。我们需要多个激活图。<ul>
<li>例如，如果我们有6个过滤器，下面是形状：<ul>
<li>输入图像                        <code>(32,32,3)</code></li>
<li>滤波器尺寸                              <code>(5,5,3)</code><ul>
<li>我们使用6个过滤器。深度必须为3，因为输入图的深度为3。</li>
</ul>
</li>
<li>卷积的输出.                 <code>(28,28,6)</code> <ul>
<li>如果是一个滤波器会是   <code>(28,28,1)</code></li>
</ul>
</li>
<li>RELU之后                          <code>(28,28,6)</code> </li>
<li>另一个滤波器                     <code>(5,5,6)</code></li>
<li>卷积的输出.                 <code>(24,24,10)</code></li>
</ul>
</li>
</ul>
</li>
<li>事实证明，卷积神经网络在第一层学习低级特性，然后学习中级特性，然后学习高级特性。</li>
<li>在卷积层之后，我们可以为分类任务提供一个线性分类器。</li>
<li>在卷积神经网络中，我们通常有一些（Conv==&gt;Relu），然后我们应用池操作来降低激活的大小。</li>
</ul>
</li>
<li><p>我们做卷积的时候步幅是什么:</p>
<ul>
<li>在做一个转换层时，我们有很多选择要做，关于我们将采取的步骤。我将用例子来解释这一点。</li>
<li>步幅是在滑行时跳跃。默认为1。</li>
<li>给定形状为<code>（7,7）</code>的矩阵和形状为<code>（3,3）</code>的滤波器：<ul>
<li>If stride is <code>1</code> then the output shape will be <code>(5,5)</code>              <code># 2 are dropped</code></li>
<li>If stride is <code>2</code> then the output shape will be <code>(3,3)</code>             <code># 4 are dropped</code></li>
<li>If stride is <code>3</code> it doesn’t work.</li>
</ul>
</li>
<li>一般的公式是 <code>((N-F)/stride +1)</code><ul>
<li>If stride is <code>1</code> then <code>O = ((7-3)/1)+1 = 4 + 1 = 5</code></li>
<li>If stride is <code>2</code> then <code>O = ((7-3)/2)+1 = 2 + 1 = 3</code></li>
<li>If stride is <code>3</code> then <code>O = ((7-3)/3)+1 = 1.33 + 1 = 2.33</code>        <code># doesn&#39;t work</code></li>
</ul>
</li>
</ul>
</li>
<li><p>在实践中，零填充边界是很常见的。   <code># 从两侧填充.</code></p>
<ul>
<li>将步幅设为1是这个等式的共同点：（F-1）/2，其中F是滤波器的尺寸<ul>
<li>Example <code>F = 3</code> ==&gt; Zero pad with <code>1</code></li>
<li>Example <code>F = 5</code> ==&gt; Zero pad with <code>2</code></li>
</ul>
</li>
<li>如果我们这样做，我们称之为相同的卷积。</li>
<li>添加0给边缘提供了另一个功能，这就是为什么有不同的填充技术，如填充角不是零，但在实践中，零是有效的！</li>
<li>我们这样做是为了保持输入的完整大小。如果我们不这样做的话，输入将收缩得太快，我们将丢失大量数据。</li>
</ul>
</li>
<li>例子:<ul>
<li>如果我们输入了<code>shape（32,32,3）</code>和10个形状是<code>（5,5）</code>的过滤器，步长为1和填充为2<ul>
<li>输出尺寸为 <code>(32,32,10)</code>                       <code># 我们保留了尺寸不变</code></li>
</ul>
</li>
<li>每个滤波器参数个数为 <code>= 5*5*3 + 1 = 76</code></li>
<li>所有的参数个数为 <code>= 76 * 10 = 76</code></li>
</ul>
</li>
<li>滤波器的数量通常是2的幂次方.           <code># 为了更好矢量化</code></li>
<li>这里是Conv层的参数:<ul>
<li>滤波器数量 K.<ul>
<li>通常是2的幂次方.</li>
</ul>
</li>
<li>空间内容大小F.<ul>
<li>3,5,7 ….</li>
</ul>
</li>
<li>步幅 S. <ul>
<li>通常是1或者2        (如果步幅较大，则会有一个下采样，但不同于池化) </li>
</ul>
</li>
<li>填充量<ul>
<li>如果我们希望输入形状作为输出形状，如果F是3则是1，如果F是5则是2，依此类推</li>
</ul>
</li>
</ul>
</li>
<li>池化使表示更小，更易于管理。</li>
<li>池化在每个激活映射上独立运行。</li>
<li>最大池化层的例子.<ul>
<li>最大池的参数是滤波器的大小和步长<ul>
<li>示例2x2，步幅为2 <code>#通常这两个参数是相同的2，2</code></li>
</ul>
</li>
</ul>
</li>
<li>平均池化层的例子.<ul>
<li>在这种情况下，这可能是可以学习的。</li>
</ul>
</li>
</ul>
<h2 id="06-Training-neural-networks-I"><a href="#06-Training-neural-networks-I" class="headerlink" title="06. Training neural networks I"></a>06. Training neural networks I</h2><ul>
<li><p>作为修订，这里是小批量随机梯度下降算法步骤：</p>
<ul>
<li>循环:<ol>
<li>抽样一批数据。</li>
<li>通过图（网络）向前推进并获得损失。</li>
<li>反向传播计算梯度.</li>
<li>用梯度更新参数</li>
</ol>
</li>
</ul>
</li>
<li><p>激活函数:</p>
<ul>
<li><p>激活函数的不同选择包括 Sigmoid, tanh, RELU, Leaky RELU, Maxout, and ELU.</p>
</li>
<li><p><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/42.png" alt></p>
</li>
<li><p>Sigmoid:</p>
<ul>
<li>压缩[0,1]之间的数字</li>
<li>像人脑一样用作发射率.</li>
<li><code>Sigmoid(x) = 1 / (1 + e^-x)</code></li>
<li>sigmoid的问题:<ul>
<li>大值神经元会破坏梯度。<ul>
<li>在大多数情况下，渐变接近0（大值/小值），如果图形/网络很大，则会终止更新。</li>
</ul>
</li>
<li>不是以零为中心。<ul>
<li>不会产生零均值数据</li>
</ul>
</li>
<li><code>exp()</code> 的计算开销有点大.<ul>
<li>只是想提一下。我们在深度学习中有更复杂的运算，比如卷积。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Tanh:</p>
<ul>
<li>压缩[-1,1]之间的数字</li>
<li>以0的中心.</li>
<li>大值神经元仍然会破坏梯度。</li>
<li><code>Tanh(x)</code> 是个方程.</li>
<li>1991年由Yann Lecun提出</li>
</ul>
</li>
<li><p>RELU (<code>Rectified</code>改正的 <code>linear</code>线性 <code>unit</code>单元):</p>
<ul>
<li><code>RELU(x) = max(0,x)</code></li>
<li>不会破坏梯度<ul>
<li>只有小值被杀死。在一半结束了梯度</li>
</ul>
</li>
<li>计算效率高。</li>
<li>比Sigmoid和Tanh快得多 <code>(6x)</code></li>
<li>在生物学上比sigmoid更可信。</li>
<li>由Alex Krizhevsky在2012年多伦多大学提出 (AlexNet)</li>
<li>问题:<ul>
<li>不以0为中心.</li>
</ul>
</li>
<li>如果权重初始化不好，可能75%的神经元会死亡，这是浪费计算。但它仍然有效。这是一个积极的研究领域，以优化这一点。</li>
<li>为了解决上述问题，人们可以将所有偏差初始化为0.01</li>
</ul>
</li>
<li><p>Leaky RELU:</p>
<ul>
<li><code>leaky_RELU(x) = max(0.01x,x)</code></li>
<li>不会破坏两边的梯度。</li>
<li>计算效率高</li>
<li>比Sigmoid和Tanh快得多 (6x)</li>
<li>不会挂掉</li>
<li>PRELU通过变量alpha来放置0.01，alpha作为参数学习</li>
</ul>
</li>
<li><p>ELU(<code>Exponential</code>指数 <code>linear</code>线性 <code>units</code>单元):</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ELU(x) &#x3D; &#123; x if x &gt; 0</span><br><span class="line">		   alpah *(exp(x) -1)	if x &lt;&#x3D; 0</span><br><span class="line">       # alpah are a learning parameter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>有RELU的所有优点</p>
</li>
<li><p>更接近于零的平均输出，并增加了一些噪声的鲁棒性。</p>
</li>
<li><p>问题</p>
<ul>
<li><code>exp()</code> 的计算开销有点大.</li>
</ul>
</li>
</ul>
</li>
<li><p>最大输出激活(Maxout activations):</p>
<ul>
<li><code>maxout(x) = max(w1.T*x + b1, w2.T*x + b2)</code></li>
<li>推广了RELU和Leaky RELU</li>
<li>不会挂掉</li>
<li>问题:<ul>
<li>每个神经元的参数数量加倍</li>
</ul>
</li>
</ul>
</li>
<li><p>在实践中:</p>
<ul>
<li>使用RELU。注意你的学习率。</li>
<li>尝试Leaky RELU/Maxout/ELU</li>
<li>尝试 tanh 但是不要过高的期望</li>
<li>不要用 sigmoid!</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>数据预处理</strong>:</p>
<ul>
<li><p>正则化数据:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 零中心数据。（计算每个输入的平均值）</span></span><br><span class="line"><span class="comment"># 之所以这样做，是因为我们需要数据介于正数和负数之间</span></span><br><span class="line"><span class="comment"># 而不是全部都是负数或正数。</span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后应用标准差。提示：在图像中我们不会这样做</span></span><br><span class="line">X /= np.std(X, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>正则化图片:</p>
<ul>
<li>减去平均图像（例如Alexnet）<ul>
<li>平均图像形状与输入图像相同。</li>
</ul>
</li>
<li>或减去每个通道的平均值<ul>
<li>平均值计算所有图像的每个通道的平均值。形状为3（3通道）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>权重初始化</strong>:</p>
<ul>
<li><p>用零初始化所有W时发生了什么?</p>
<ul>
<li>所有的神经元都会做同样的事情。他们将有相同的梯度，他们将有相同的更新</li>
<li>所以如果某个特定层的W相等，那么所描述的事情就发生了</li>
</ul>
</li>
<li><p>第一个想法是用小随机数初始化w:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.rand(D, H)</span><br><span class="line"><span class="comment"># 适用于小型网络，但对于较深的网络会产生问题</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>在更深的网络中，标准差将为零。而在深层网络中，这种梯度会很快消失。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = <span class="number">1</span> * np.random.rand(D, H) </span><br><span class="line"><span class="comment"># 适用于小型网络，但对于较深的网络会产生问题</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>网络会因为大值数据而没法计算</p>
</li>
</ul>
</li>
<li><p><strong><em>Xavier 初始化</em></strong>:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = np.random.rand(<span class="keyword">in</span>, out) / np.sqrt(<span class="keyword">in</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>它之所以有效，是因为我们希望输入的方差和输出的方差一样。</p>
</li>
<li><p>但它有一个问题，当你使用RELU时，它会崩溃</p>
</li>
</ul>
</li>
<li><p><strong><em>He 初始化</em></strong> (RELU问题的解决方案):</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = np.random.rand(<span class="keyword">in</span>, out) / np.sqrt(<span class="keyword">in</span>/<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>解决了RELU的问题。建议在使用RELU时使用</li>
</ul>
</li>
<li><p>适当的初始化是一个活跃的研究领域。</p>
</li>
</ul>
</li>
<li><p><strong>批次正则化(Batch normalization)</strong>:</p>
<ul>
<li>是一种为神经网络的任何层提供零均值/单位方差输入的技术.</li>
<li>它加快了训练的速度。你很想这么做。<ul>
<li>2015年由Sergey Ioffe和Christian Szegedy制作。</li>
</ul>
</li>
<li>我们在每一层进行高斯激活。通过计算均值和方差。</li>
<li>通常插入在（完全连接或卷积层）和（非线性之前）。</li>
<li>步骤（对于层的每个输出）<ol>
<li>首先，我们计算每一个特征批次的平均值和方差^2</li>
<li>我们通过减去平均值并除以（方差^2+epsilon）的平方根进行归一化<ul>
<li>加上epsilon为了不被零除</li>
</ul>
</li>
<li>然后我们制作一个尺寸和移位变量: <code>Result = gamma * normalizedX + beta</code>  <ul>
<li>gamma和beta是可学习的参数</li>
<li>基本上可以说“嘿！！我不想要零均值/单位方差输入，请把原始输入还给我，这样对我更好。”</li>
<li>嘿，根据你想要的改变和比例，而不仅仅是均值和方差！</li>
</ul>
</li>
</ol>
</li>
<li>该算法使每一层变得灵活（它选择它想要的分布）</li>
<li>我们初始化批处理范数参数以将输入转换为零均值/单位方差分布，但在训练过程中，他们可以了解到任何其他分布都可能更好。</li>
<li>在训练过程中，我们需要用加权平均法计算出每一层的总体均值和总体方差。</li>
<li><u>批次正则化的好处</u>:<ul>
<li>网络训练更快.</li>
<li>允许更高的学习率。</li>
<li>有助于降低对初始起始重量的敏感性</li>
<li>使更多的激活功能可行。</li>
<li>提供一些正则化。<ul>
<li>因为我们计算的是每个批次的均值和方差，这会产生轻微的正则化效果。</li>
</ul>
</li>
</ul>
</li>
<li>在conv层中，每个激活图有一个方差和一个平均值。</li>
<li>批次正则化在CONV和正则深层神经网络中效果最好，但对于递归神经网络和强化学习仍然是一个活跃的研究领域。<ul>
<li>它的挑战在于强化学习，因为它的批量很小。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>浅显易懂的学习过程(Baby sitting the learning process)</strong></p>
<ol>
<li>数据预处理。</li>
<li>选择架构。</li>
<li>正向传播并检查损失（禁用正则化）。检查损失是否合理</li>
<li>加上正规化，损失应该上去了！</li>
<li>再次禁用正则化，取少量数据，尝试训练损失并达到零损失。<ul>
<li>你应该在小数据集上过拟合</li>
</ul>
</li>
<li>把你的全部训练数据，和小正则化，然后尝试一些学习率的值。<ul>
<li>如果损失几乎没有变化，那么学习率设置的太小了。</li>
<li>如果你得到了NAN，那么你的NN爆炸了，你的学习率太高了。</li>
<li>通过尝试最小值（可以更改）和最大值（不会使网络爆炸）来获取学习速率范围。</li>
</ul>
</li>
<li>对超参数进行优化，得到最佳的超参数值。</li>
</ol>
</li>
<li><p>超参数优化</p>
<ul>
<li>尝试交叉验证策略。<ul>
<li>用几个Ephoc跑，试着优化范围。</li>
</ul>
</li>
<li>最好在log空间中进行优化。</li>
<li>请调整范围，然后再试一次</li>
<li>最好尝试随机搜索而不是网格搜索（在log空间中）</li>
</ul>
</li>
</ul>
<h2 id="07-Training-neural-networks-II"><a href="#07-Training-neural-networks-II" class="headerlink" title="07. Training neural networks II"></a>07. Training neural networks II</h2><ul>
<li><p><strong>优化算法</strong>:</p>
<ul>
<li><p>随机梯度下降问题:</p>
<ul>
<li>如果在一个方向上损失很快，而在另一个方向上损失缓慢（仅针对两个变量），那么沿着浅维度的进展将非常缓慢，在陡峭的方向上会出现抖动。我们的神经网络会有很多参数，然后问题会更多。</li>
<li>局部最小值或鞍点<ul>
<li>如果SGD进入局部极小值，我们将在这一点上卡住，因为梯度是零。</li>
<li>同样在鞍点，梯度为零，所以我们会卡住。</li>
<li>鞍点在某一点上说:<ul>
<li>一些梯度会增加损失。</li>
<li>一些梯度会降低损失。</li>
<li>而在高维（例如1亿维）中发生的更多</li>
</ul>
</li>
<li>深度神经网络的问题更多的是关于鞍点而不是局部最小值，因为深度神经网络具有高维（参数）</li>
<li>小批量是有噪音的，因为没有对整个批次采取梯度。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>SGD + momentum</strong>:</p>
<ul>
<li><p>建立速度作为梯度的运行平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算加权平均数。 rho best 在 [0.9 - 0.99]</span></span><br><span class="line">V[t+<span class="number">1</span>] = rho * v[t] + dx</span><br><span class="line">x[t+<span class="number">1</span>] = x[t] - learningRate * V[t+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>V[0]</code> 是0.</p>
</li>
<li><p>解决了鞍点和局部极小问题。</p>
</li>
<li><p>它越过极值点，然后仔回到了原来的状态。</p>
</li>
</ul>
</li>
<li><p><strong>Nestrov momentum</strong>:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dx = compute_gradient(x)</span><br><span class="line">old_v = v</span><br><span class="line">v = rho * v - learning_rate * dx</span><br><span class="line">x+= -rho * old_v + (<span class="number">1</span>+rho) * v</span><br></pre></td></tr></table></figure>
<ul>
<li>不会越过极值点，但比SGD + momentum慢</li>
</ul>
</li>
<li><p><strong>AdaGrad</strong></p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 这里有一个问题，梯度平方没有衰减（变的特别大）</span></span><br><span class="line">  grad_squared += dx * dx			</span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>RMSProp</strong></p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Solved ADAgra</span></span><br><span class="line">  grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-grad_squared) * dx * dx  </span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>人们用这个来代替AdaGrad</li>
</ul>
</li>
<li><p><strong>Adam</strong></p>
<ul>
<li>结合了momentum和RMSProp来计算梯度。</li>
<li>它需要一个固定偏差来确定梯度的起始点</li>
<li>是目前为止最好的技术在很多问题上都是最好的。</li>
<li>赋值 <code>beta1 = 0.9</code> 并且 <code>beta2 = 0.999</code> 并且 <code>learning_rate = 1e-3</code> 或者 <code>5e-4</code> 是许多模型的一个很好的选择！</li>
</ul>
</li>
<li><p><strong>学习衰退(Learning decay)</strong></p>
<ul>
<li>每隔几个时代学习率下降一半。</li>
<li>帮助学习率不反弹。</li>
<li>学习衰退在SGD+动量中很常见，但在Adam中不常见。</li>
<li>不要从一开始就用学习衰退来选择超参数。先试试，看看你是否需要衰退学习率</li>
</ul>
</li>
<li><p>以上算法都是一阶优化问题。</p>
</li>
<li><p><strong>二阶优化</strong></p>
<ul>
<li>使用梯度和Hessian到二次逼近</li>
<li>逼近的最小值</li>
<li>这次更新有什么好处?<ul>
<li>有些版本没有学习率。</li>
</ul>
</li>
<li>但对于深度学习是不现实的<ul>
<li>有 O(N^2) 个元素.</li>
<li>反转复杂度 O(N^3).</li>
</ul>
</li>
<li><strong>L-BFGS</strong> 是二阶优化的一个版本<ul>
<li>适用于批量优化，但不适用于小批量。</li>
</ul>
</li>
</ul>
</li>
<li><p>在实践中，首先使用ADAM，如果不起作用，再试试L-BFGS</p>
</li>
<li><p>有人说所有著名的深层架构都使用 <strong>SGS + Nestrov momentum</strong></p>
</li>
</ul>
</li>
<li><p><strong>正则化</strong></p>
<ul>
<li>到目前为止，我们已经讨论过减少训练误差，但我们最关心的是我们的模型将如何处理看不见的数据！</li>
<li>如果训练数据和验证数据之间的误差太大怎么办？</li>
<li>这种误差称为高方差</li>
<li><strong>模型集合</strong>:<ul>
<li>算法:<ul>
<li>训练具有不同初始化的同一体系结构的多个独立模型。</li>
<li>在测试时平均他们的结果。</li>
</ul>
</li>
<li>它可以给你额外的2%的性能</li>
<li>减少了泛化误差。</li>
<li>你可以在训练中使用你的神经网络的一些快照，把它们集合起来，然后获取结果。</li>
</ul>
</li>
<li>正则化解决了高方差问题。我们已经讨论过L1，L2正则化。</li>
<li>一些正则化技术只针对神经网络设计，可以做得更好。</li>
<li><strong>Drop out</strong>:<ul>
<li>在每一次正向传播中，随机将一些神经元设为零。drop概率是一个超参数，在大多数情况下为0.5。</li>
<li>所以你会选择一些激活，使它们为零。</li>
<li>它起作用是因为:<ul>
<li>它迫使网络具有冗余表示；防止特征的协同适应!</li>
<li>如果你想一想，它将一些模型集成在同一个模型中!</li>
</ul>
</li>
<li>在测试时，我们可以用每一个drop层乘以drop的概率</li>
<li>有时在测试时，我们不乘法任何东西，并保持原样。</li>
<li>带有drop需要更多的时间来训练。</li>
</ul>
</li>
<li><strong>数据增强</strong>:<ul>
<li>另一种使正则化的技术。</li>
<li>更改数据!</li>
<li>例如，翻转图像或旋转图像。</li>
<li>ResNet中的示例:<ul>
<li>训练时: 随机采样进行裁剪和缩放:<ol>
<li>在[256,480]范围内选择随机数L</li>
<li>调整训练图像大小，短边=L</li>
<li>随机抽样224x244补丁.</li>
</ol>
</li>
<li>测试时: 对一套固定裁剪取平均值<ol>
<li>按5个比例调整图像大小 {224, 256, 384, 480, 640}</li>
<li>对于每个尺寸，使用10(<code>(4+1)*2</code>)个224x224裁剪：4个角+中心+翻转</li>
</ol>
</li>
<li>应用颜色抖动或PCA</li>
<li>平移，旋转，伸展。</li>
</ul>
</li>
</ul>
</li>
<li>断开连接(Drop connect)<ul>
<li>就像辍学的想法一样，它达到了规则化的效果</li>
<li>我们没有将激活drop，而是随机地将权重归零</li>
</ul>
</li>
<li>部分最大池化(Fractional Max Pooling)<ul>
<li>很酷的正则化思想。不常用</li>
<li>随机化我们聚集的区域</li>
</ul>
</li>
<li>随机深度<ul>
<li>新想法.</li>
<li>直接消除层数而不是，而不是神经元</li>
<li>有类似drop的效果，但这是一个新的想法。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>迁移学习</strong>:</p>
<ul>
<li><p>有时，模型会过度拟合数据，因为数据很小，而不是因为正则化。</p>
</li>
<li><p>如果你想训练/使用CNNs，你需要大量的数据。</p>
</li>
<li><p>迁移学习的步骤</p>
<ol>
<li>在一个与你的数据集有共同特征的大数据集上训练。叫做预训练。</li>
<li>冻结除最后一层以外的所有层，并输入您的小数据集，以便只了解最后一层。</li>
<li>不仅最后一层可能会再次训练，您还可以根据您拥有的数据数量微调任意数量的层</li>
</ol>
</li>
<li><p>迁移学习使用指南:</p>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>非常相似的数据集</th>
<th>非常不同的数据集</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>很少的数据集</strong></td>
<td>在顶层使用线性分类器</td>
<td>你有麻烦了。。尝试不同阶段的线性分类器</td>
</tr>
<tr>
<td><strong>相当多的数据</strong></td>
<td>微调几层</td>
<td>精细调整一大层</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>迁移学习是正常现象，也不例外</li>
</ul>
<h2 id="08-Deep-learning-software"><a href="#08-Deep-learning-software" class="headerlink" title="08. Deep learning software"></a>08. Deep learning software</h2><ul>
<li>由于深度学习软件的急剧变化，CS231n的这一部分每年都会发生很大的变化。</li>
<li>CPU vs GPU<ul>
<li>图形卡的开发是为了渲染图形来玩游戏或制作3D媒体，。等。<ul>
<li>NVIDIA vs AMD<ul>
<li>深度学习选择NVIDIA而不是AMD GPU，因为NVIDIA正在推动研究的发展，深度学习也使它的架构更适合深度学习</li>
</ul>
</li>
</ul>
</li>
<li>CPU有更少的内核，但每个内核都更快，功能更强；擅长顺序任务。虽然gpu有更多的内核，但每个内核都慢得多“笨”；非常适合并行任务</li>
<li>GPU核心需要协同工作。有自己的记忆</li>
<li>矩阵乘法来自于适合gpu的运算。它有独立于MxN的操作，可以并行进行。</li>
<li>卷积运算也可以瘫痪，因为它有独立的运算。</li>
<li>编程GPU框架:<ul>
<li><strong>CUDA</strong> (仅限英伟达)<ul>
<li>编写直接在GPU上运行的类c代码。</li>
<li>很难构建一个运行在GPU上的好的优化代码。这就是为什么他们提供高级API。</li>
<li>高级API：cuBLAS、cuDNN等</li>
<li><strong>CuDNN</strong> 已经实施了反向传播，卷积，循环，对你来说更多！</li>
<li>实际上，您不用编写并行代码。你可以使用其他人实现和优化的代码！</li>
</ul>
</li>
<li><strong>OpenCl</strong><ul>
<li>类似于CUDA，但可以在任何GPU上运行</li>
<li>通常较慢。</li>
<li>目前还没有得到所有深度学习软件的支持。</li>
</ul>
</li>
</ul>
</li>
<li>学习并行编程有很多课程。</li>
<li>如果不小心，训练可能会成为读取数据和传输到GPU的瓶颈。 解决方案如下:<ul>
<li>将所有数据读入RAM. # If possible</li>
<li>使用SSD而不是HDD</li>
<li>使用多个CPU线程预取数据!<ul>
<li>当GPU进行计算时，CPU线程将为您获取数据</li>
<li>很多框架都为您实现了这一点，因为这有点痛苦！</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>深度学习框架</strong><ul>
<li>它的移动速度非常快!</li>
<li>当前可用的框架:<ul>
<li>Tensorflow (Google)</li>
<li>Caffe (UC Berkeley)</li>
<li>Caffe2 (Facebook)</li>
<li>Torch (NYU / Facebook)</li>
<li>PyTorch (Facebook)</li>
<li>Theano (U monteral) </li>
<li>Paddle (Baidu)</li>
<li>CNTK (Microsoft)</li>
<li>MXNet (Amazon)</li>
</ul>
</li>
<li>老师认为你应该把重点放在Tensorflow和Pythorch上。</li>
<li>深度学习框架的意义:<ul>
<li>轻松构建大型计算图.</li>
<li>在计算图中容易地计算梯度。</li>
<li>在GPU上高效运行（cuDNN-cuBLAS）</li>
</ul>
</li>
<li>Numpy不能在GPU上运行。</li>
<li>大多数框架在正向传播时都会像NUMPY一样，然后他们会为你计算梯度。</li>
</ul>
</li>
<li><strong>Tensorflow (Google)</strong><ul>
<li>代码由两部分组成:<ol>
<li>定义计算图.</li>
<li>运行图形并多次重用它。</li>
</ol>
</li>
<li>Tensorflow使用静态图形架构。</li>
<li>Tensorflow变量存在于图中。用占位符来挨个喂养数据</li>
<li>全局初始化器函数初始化图中的变量。</li>
<li>使用预定义的优化器和损失。</li>
<li>你可以用<code>layers.dense</code>函数来实现一个完整的网络层.</li>
<li><strong>Keras</strong> (高级包装机High level wrapper):<ul>
<li>Keras是tensorflow顶部的一层,使普通的事情容易做。</li>
<li>太受欢迎了</li>
<li>用几行代码训练一个完整的深层神经网络。</li>
</ul>
</li>
<li>有很多高级包装:<ul>
<li>Keras</li>
<li>TFLearn</li>
<li>TensorLayer</li>
<li>tf.layers   <code>#使用tensorflow</code></li>
<li>tf-Slim   <code>#使用tensorflow</code></li>
<li>tf.contrib.learn   <code>#使用tensorflow</code></li>
<li>Sonnet <code># 心灵深处的新事物</code></li>
</ul>
</li>
<li>Tensorflow有预先训练过的模型，您可以在使用转移学习时使用这些模型。</li>
<li>Tensorboard增加了记录损失和统计数据的功能。运行服务器，得到漂亮的图形!</li>
<li>如果您想在某些节点上拆分图，它具有分布式代码</li>
<li>Tensorflow的灵感来自Theano。它有相同的灵感和结构。</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>PyTorch (Facebook)</strong></p>
<ul>
<li>有三层抽象:<ul>
<li>Tensor: <code>ndarray</code>但在tensorflow中 #<code>类似于numpy数组的GPU上运行</code><ul>
<li>Variable:计算图中的结点; 存储数据和梯度 <code>#Like Tensor, Variable, Placeholders</code></li>
</ul>
</li>
<li>Module: A NN layer;可以存储状态或可学习的权重<code>#Like tf.layers in tensorflow</code></li>
</ul>
</li>
<li>在PyTorch中，图形运行在您正在执行的同一个循环中，这使得调试更加容易。这叫做动态图。</li>
<li>在PyTorch中，您可以通过向前和向后写入张量来定义自己的autograd函数。大多数时候它会为你实现。</li>
<li>torch.nn是一个类似于tensorflow中keras的高级api。你可以创建模型，然后继续下去<ul>
<li>您可以定义自己的nn模块</li>
</ul>
</li>
<li>Pythorch还包含tensorflow之类的优化器。</li>
<li>它包含一个数据加载器，它包装了一个数据集，并提供了minbatches、shuffling和多线程处理。</li>
<li>Pythorch包含最好和超级容易使用的预训练模型</li>
<li>Pythorch含有类似Tensorboard的Visdom。但Tensorboard 似乎更强大。</li>
<li>与Torch相比，PyTorch是一个新的且仍在不断发展。它仍然处于beta状态。</li>
<li>Pythorch最适合研究</li>
</ul>
</li>
<li><p>Tensorflow构建一次图形，然后运行多次（称为静态图形）</p>
</li>
<li><p>在每个PyTorch迭代中，我们构建一个新的图（称为动态图）</p>
</li>
<li><p><strong>静态图vs动态图</strong>:</p>
<ul>
<li><p>最优化(Optimization):</p>
<ul>
<li>使用静态图，框架可以在运行之前为您优化图形.</li>
</ul>
</li>
<li><p>序列化(Serialization)</p>
<ul>
<li><strong>Static</strong>: 一旦构建了graph，就可以序列化它并运行它，而不需要构建该图的代码。例如在c中使用图++</li>
<li><strong>Dynamic</strong>: 总是需要保留代码。</li>
</ul>
</li>
<li><p>条件(Conditional)</p>
<ul>
<li>在动态图中更容易,在静态图中更加复杂。</li>
</ul>
</li>
<li><p>循环(Loops):</p>
<ul>
<li>在动态图中更容易,在静态图中更加复杂。</li>
</ul>
</li>
</ul>
</li>
<li><p>Tensorflow fold 通过动态批处理使动态图在Tensorflow中更容易实现</p>
</li>
<li><p>动态图的应用包括：递归网络(recurrent networks and recursive networks).</p>
</li>
<li><p>Caffe2使用静态图，可以在python中训练模型，也可以在IOS和Android上运行</p>
</li>
<li><p>Tensorflow/Caffe2在生产中得到了广泛的应用，特别是在移动设备上</p>
</li>
</ul>
<h2 id="09-CNN-architectures"><a href="#09-CNN-architectures" class="headerlink" title="09. CNN architectures"></a>09. CNN architectures</h2><ul>
<li><p>本节介绍著名的CNN架构。关注自2012年以来赢得<a href="www.image-net.org/">ImageNet</a>竞赛的CNN架构。 </p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/43.png" alt></li>
</ul>
</li>
<li><p>这些体系结构包括: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a>, <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a>, <a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener">GoogLeNet</a>, and <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>.</p>
</li>
<li><p>我们还将讨论一些有趣的架构。</p>
</li>
<li><p>第一个ConvNet是Yann Lecun在1998年提出的<a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">LeNet-5</a> 体系结构</p>
<ul>
<li>结构是: <code>CONV-POOL-CONV-POOL-FC-FC-FC</code><ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/02.jpg" alt></li>
</ul>
</li>
<li>每个卷积层滤波器步幅为<code>1</code>且采用<code>5x5</code>尺寸</li>
<li>每个池化层滤波器步幅为<code>2</code>且采用<code>2x2</code>尺寸</li>
<li>它在数字识别中很有用。</li>
<li>特别是图像特征分布在整个图像上的特点，以及具有可学习参数的卷积是在多个位置用较少的参数提取相似特征的有效方法。</li>
<li>它正好包含 <strong><u>5</u></strong> 层</li>
</ul>
</li>
<li><p>In <a href="https://arxiv.org/abs/1003.0358" target="_blank" rel="noopener">2010</a> 丹·克劳迪乌西雷桑（Dan Claudiu Ciresan）和Jurgen Schmidhuber发表了GPU神经网络的最早实现之一。这种实现方法在NVIDIA GTX 280 图形处理器上实现了前向和后向两个层次的神经网络。</p>
</li>
<li><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener"><strong>AlexNet</strong></a> (2012):</p>
<ul>
<li>ConvNet开创了进化之路，并在2012年赢得了ImageNet.</li>
<li>结构如下: <code>CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8</code></li>
<li>包含 <strong><u>8</u></strong> 层，前5层为卷积层，后3层为完全连通层。</li>
<li>AlexNet的准确度误差为<code>16.4%</code></li>
<li>例如，如果输入为227 x 227 x3，则每层输出的形状如下：<ul>
<li>CONV1    (96 11 x 11 filters at stride 4, pad 0)<ul>
<li>Output shape <code>(55,55,96)</code>,   Number of weights are <code>(11*11*3*96)+96 = 34944</code></li>
</ul>
</li>
<li>MAXPOOL1 (3 x 3 filters applied at stride 2)<ul>
<li>Output shape <code>(27,27,96)</code>,   No Weights</li>
</ul>
</li>
<li>NORM1<ul>
<li>Output shape <code>(27,27,96)</code>,     We don’t do this any more</li>
</ul>
</li>
<li>CONV2 (256 5 x 5 filters at stride 1, pad 2)</li>
<li>MAXPOOL2 (3 x 3 filters at stride 2)</li>
<li>NORM2</li>
<li>CONV3 (384 3 x 3 filters ar stride 1, pad 1)</li>
<li>CONV4 (384 3 x 3 filters ar stride 1, pad 1)</li>
<li>CONV5 (256 3 x 3 filters ar stride 1, pad 1)</li>
<li>MAXPOOL3 (3 x 3 filters at stride 2)<ul>
<li>Output shape <code>(6,6,256)</code></li>
</ul>
</li>
<li>FC6 (4096)</li>
<li>FC7 (4096)</li>
<li>FC8 (1000 neurons for class score)</li>
</ul>
</li>
<li>一些其他细节:<ul>
<li>首次使用RELU.</li>
<li>标准层(Norm layers)，但不再使用。</li>
<li>海量数据扩充</li>
<li>Dropout <code>0.5</code></li>
<li>batch size <code>128</code></li>
<li>SGD momentum <code>0.9</code></li>
<li>Learning rate <code>1e-2</code> reduce by 10 at some iterations</li>
<li>7 CNN ensembles!</li>
</ul>
</li>
<li>AlexNet是在GTX 580 GPU上训练的，只有3GB，这不足以在一台机器上进行训练，所以他们将特征地图分为两半。第一个AlexNet是分布式的</li>
<li>在很多任务中，迁移学习仍然使用它</li>
<li>参数总数为<code>6000万</code></li>
</ul>
</li>
<li><p><a href="https://arxiv.org/abs/1311.2901" target="_blank" rel="noopener"><strong>ZFNet</strong></a> (2013)</p>
<ul>
<li>2013年获胜，错误率为11.7%</li>
<li>它具有相同的一般结构，但它们在超参数上有一些变化，以获得最佳输出。</li>
<li>也包含 <strong><u>8</u></strong> 层。</li>
<li>AlexNet but:<ul>
<li><code>CONV1</code>: 从 (11 x 11 stride 4) 变为 (7 x 7 stride 2)</li>
<li><code>CONV3,4,5</code>: 分别用 512, 1024, 512 滤波器来代替 384, 384, 256 滤波器</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://arxiv.org/abs/1312.6229" target="_blank" rel="noopener">OverFeat</a> (2013)</p>
<ul>
<li>2013赢得比赛</li>
<li>我们展示了如何在ConvNet中有效地实现多尺度滑动窗口方法。我们还介绍了一种新的深度学习方法，通过学习预测对象边界来定位。</li>
</ul>
</li>
<li><p><a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener"><strong>VGGNet</strong></a> (2014) (Oxford)</p>
<ul>
<li>更深层次的网络。</li>
<li>包含19层。</li>
<li>2014年和GoogleNet一起赢得比赛，错误率7.3%</li>
<li>具有较深层的较小过滤器</li>
<li>VGG的最大优点在于，连续多次3×3卷积可以模拟较大感受野的影响，例如5×5和7×7。</li>
<li>网络全程使用简单的3 x 3 Conv。<ul>
<li>3个（3 x 3）核的效果可以与一个（7 x 7）核相同</li>
</ul>
</li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/03.png" alt></li>
<li>该体系结构包含多个CONV层，然后是5层以上的池化层，然后是全连接层。</li>
<li>它每个图像的总内存为96MB，仅用于前向传播<ul>
<li>大多数内存都在前几层</li>
</ul>
</li>
<li>参数总数为1.38亿<ul>
<li>大多数参数都在完全连接的层中</li>
</ul>
</li>
<li>在训练中也有类似的细节。比如利用momentum 和 dropout。</li>
<li>VGG19是VGG16的升级版，性能稍好，但内存更大<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/04.png" alt></li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener"><strong>GoogleNet</strong></a> (2014)</p>
<ul>
<li>更深层次的网络。</li>
<li>包含22层。</li>
<li>它具有高效的 <strong><u>Inception</u></strong> 模块</li>
<li>只有500万个参数！比AlexNet少12倍</li>
<li>2014年赢得VGGNet，错误率为6.7%</li>
<li>Inception 模块:<ul>
<li>设计一个良好的局域网拓扑结构 (网络中的网络network within a network (NiN)) 然后将这些模块堆叠在一起。</li>
<li>它包括：<ul>
<li>对上一层的输入应用并行过滤操作<ul>
<li>多种规格的卷积 (1 x 1, 3 x 3, 5 x 5) <ul>
<li>添加填充以保持大小</li>
</ul>
</li>
<li>池化层操作. (Max Pooling)<ul>
<li>添加填充以保持大小</li>
</ul>
</li>
</ul>
</li>
<li>将所有滤波器输出串联在一起。</li>
</ul>
</li>
<li>比如:<ul>
<li>inception模块的输入是 28 x 28 x 256</li>
<li>然后应用并行滤波器:<ul>
<li>(1 x 1), 128 filter               <code># output shape (28,28,128)</code></li>
<li>(3 x 3), 192 filter                 <code># output shape (28,28,192)</code></li>
<li>(5 x 5), 96 filter                   <code># output shape (28,28,96)</code></li>
<li>(3 x 3) Max pooling            <code># output shape (28,28,256)</code></li>
</ul>
</li>
<li>连接后，这将是 <code>(28,28,672)</code></li>
</ul>
</li>
<li>通过这种设计，我们称之为天真(Naive)，它有很大的计算复杂性。<ul>
<li>最后一个例子将使:<ul>
<li>[1 x 1 conv, 128] ==&gt; 28 <em> 28 </em> 128 <em> 1 </em> 1 * 256 = 25 Million approx</li>
<li>[3 x 3 conv, 192] ==&gt; 28 <em> 28 </em> 192 <em>3 </em>3 * 256 = 346 Million approx</li>
<li>[5 x 5 conv, 96] ==&gt; 28 <em> 28 </em> 96 <em> 5 </em> 5 * 256 = 482 Million approx</li>
<li>总共约8.54亿次操作!</li>
</ul>
</li>
</ul>
</li>
<li>解决方案：使用1x1卷积来减少特征深度的瓶颈层<ul>
<li>受 NiN (<a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network in network</a>)的启发</li>
</ul>
</li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/05.png" alt></li>
<li>在这个例子中，瓶颈解决方案的总操作量将达到358M，与原始的实现相比是很好的。</li>
</ul>
</li>
<li>所以GoogleNet将这个初始模块进行多次堆叠，以获得一个完整的网络架构，可以在没有完全连接的层的情况下解决问题。</li>
<li>它在分类步骤之前的末尾使用平均池层。</li>
<li>完整体系结构:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/44.png" alt></li>
</ul>
</li>
<li>2015年2月，批量标准化初始作为Inception V2引入。批量标准化计算图层输出处所有特征图的平均值和标准差，并用这些值规范化它们的响应。</li>
<li><a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">2015</a>年12月，他们介绍了一篇论文“重新思考计算机视觉的初始架构”，这篇论文很好地解释了早期的Inception模型，并引入了新版本V3。</li>
</ul>
</li>
<li><p>第一个GoogleNet和VGG是在批处理规范化发明之前，所以他们有一些技巧来训练NN并很好地收敛。</p>
</li>
<li><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener"><strong>ResNet</strong></a> (2015) (Microsoft Research)</p>
<ul>
<li><p>ImageNet的152层模型。以3.57%胜出，这好于人为水平的误差。</p>
</li>
<li><p>这也是第一次训练一个超过100层甚至1000层的网络。</p>
</li>
<li><p>在ILSVRC’15和COCO’15横扫了所有的分类和探测比赛！</p>
</li>
<li><p>当我们继续在一个“普通”的卷积神经网络上堆积更深的层时会发生什么？</p>
<ul>
<li>较深的模型性能更差，但不是由于过度拟合造成的！</li>
<li>学习停止执行得很好，因为更深的神经网络更难优化！</li>
</ul>
</li>
<li><p>更深层次的模型应该至少能和浅层次的模型一样好</p>
</li>
<li><p>构造的解决方案是从较浅的模型复制学习的层，并将附加层设置为标识映射。</p>
</li>
<li><p>残差块:</p>
<ul>
<li><p>微软提出了一个残差块，它具有以下架构：:</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/45.png" alt></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Instead of us trying To learn a new representation, We learn only Residual</span></span><br><span class="line">Y = (W2* RELU(W1x+b1) + b2) + X</span><br></pre></td></tr></table></figure>
</li>
<li><p>假设你有一个直到N层深度的网络。你只想添加一个新的层，如果你得到了额外的添加层。</p>
</li>
<li><p>确保这个新的（N+1）层学习网络新知识的一种方法是同时提供输入（x），而无需对第（N+1）层的输出进行任何转换。这本质上驱动新层学习与输入已经编码的内容不同的内容。</p>
</li>
<li><p>另一个优点是这种连接有助于处理非常深的网络中的消失梯度问题。</p>
</li>
</ul>
</li>
<li><p>有了残差块，我们现在可以得到任何深度的深度神经网络，而不用担心我们无法优化网络。</p>
</li>
<li><p>ResNet在具有大量层的情况下开始使用类似于Inception瓶颈的瓶颈层(即1x1核)来降低维数。</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/07.jpg" alt></li>
</ul>
</li>
<li><p><strong><u>完整的ResNet架构</u></strong>:</p>
<ul>
<li>残差块堆积.<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/08.png" alt></li>
</ul>
</li>
<li>每个剩余块有两个3 x 3 conv层。</li>
<li>开始时的附加转换层</li>
<li>结尾没有FC层（只有FC 1000到输出类）</li>
<li>周期性地，两倍数量的过滤器和使用步长2（每个维度中的/2）在空间上进行下采样</li>
<li>ResNet实践培训:<ul>
<li>每层转换后Batch Normalization.</li>
<li>来自He等人的Xavier/2初始化.</li>
<li>SGD + Momentum (<code>0.9</code>) </li>
<li>Learning rate: 0.1, 当验证误差稳定时除以10</li>
<li>Mini-batch size <code>256</code></li>
<li>Weight decay of <code>1e-5</code></li>
<li>No dropout used.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="noopener">Inception-v4</a>: Resnet + Inception and was founded in 2016.</p>
</li>
<li><p>所有体系结构的复杂性：</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/09.png" alt></li>
<li>VGG:最高的内存，最多的操作.</li>
<li>GoogLeNet: 效率最高.</li>
</ul>
</li>
<li><p><strong>ResNets 改进</strong>:</p>
<ul>
<li>(<a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">2016</a>) <u>深剩余网络中的恒等映射</u><ul>
<li>来自ResNet的创造者.</li>
<li>提供更好的性能。</li>
</ul>
</li>
<li>(<a href="https://arxiv.org/abs/1605.07146" target="_blank" rel="noopener">2016</a>) <u>宽残差网络</u><ul>
<li>认为残差是重要因素，而不是深度</li>
<li>50层宽ResNet优于152层原始ResNet</li>
<li>增加宽度而不是深度更有效地计算（可并行化）</li>
</ul>
</li>
<li>(<a href="https://arxiv.org/abs/1603.09382" target="_blank" rel="noopener">2016</a>) 深度随机网络<ul>
<li>动机：在训练过程中通过短网络减少消失梯度和训练时间。</li>
<li>在每一次训练过程中随机放下一个子集的图层</li>
<li>在测试时使用全深度网络。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>超越 ResNets</strong>:</p>
<ul>
<li>(<a href="https://arxiv.org/abs/1605.07648" target="_blank" rel="noopener">2017</a>) <u>分形网络：无残差的超深神经网络</u><ul>
<li>认为关键是要有效地从浅层过渡到深层，不需要残差表示。.</li>
<li>通过退出子路径训练</li>
<li>测试时全网。</li>
</ul>
</li>
<li>(<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">2017</a>) <u>密接卷积网络</u></li>
<li>(<a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noopener">2017</a>) SqueezeNet:AlexNet级精度，参数减少50倍，型号尺寸&lt;0.5Mb<ul>
<li>有利于生产</li>
<li>它是对ResNet和Inception中的许多概念的重新散列，并表明，毕竟，一个更好的架构设计将提供较小的网络规模和参数，而不需要复杂的压缩算法。</li>
</ul>
</li>
</ul>
</li>
<li><p>结论:</p>
<ul>
<li>ResNet当前最佳默认值</li>
<li>网络极深的趋势</li>
<li>在过去的几年里，一些模型都使用像“ResNet”这样的快捷方式来改变梯度。</li>
</ul>
</li>
</ul>
<h2 id="10-Recurrent-Neural-networks"><a href="#10-Recurrent-Neural-networks" class="headerlink" title="10. Recurrent Neural networks"></a>10. Recurrent Neural networks</h2><ul>
<li>Vanilla 神经网络给神经网络“输入”，固定大小的输入经过一些隐藏单元，然后输出。我们称之为一对一网络。</li>
<li><p>递归神经网络RNN模型:</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/46.png" alt></li>
<li>一对多<ul>
<li>示例：图像字幕<ul>
<li>图像==&gt;单词序列</li>
</ul>
</li>
</ul>
</li>
<li>多对一<ul>
<li>示例：情绪分类<ul>
<li>词序==&gt;情感</li>
</ul>
</li>
</ul>
</li>
<li>多对多<ul>
<li>示例：机器翻译<ul>
<li>一种语言的单词序列==&gt;另一种语言的单词序列</li>
</ul>
</li>
<li>示例：帧级视频分类</li>
</ul>
</li>
</ul>
</li>
<li><p>RNN也可以用于非序列数据（一对一问题）</p>
<ul>
<li>它通过采取一系列“瞥见”的方式进行了数字分类<ul>
<li>“<a href="https://arxiv.org/abs/1412.7755" target="_blank" rel="noopener">视觉注意下的多目标识别</a>”, ICLR 2015.</li>
</ul>
</li>
<li>它致力于一次生成一幅图像<ul>
<li>比如生成 <a href="http://ieeexplore.ieee.org/document/7966808/" target="_blank" rel="noopener">验证码</a></li>
</ul>
</li>
</ul>
</li>
<li><p>那么什么是递归神经网络?</p>
<ul>
<li><p>接收输入x的循环核心单元，该单元具有每次读取输入时更新的内部状态。</p>
</li>
<li><p><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/47.png" alt></p>
</li>
<li><p>RNN块应该返回一个向量</p>
</li>
<li><p>我们可以通过在每个时间步应用递归公式来处理向量x序列：</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h[t] = fw (h[t<span class="number">-1</span>], x[t])			</span><br><span class="line"><span class="comment"># Where fw is some function with parameters W</span></span><br></pre></td></tr></table></figure>
<ul>
<li>每个时间步使用相同的函数和相同的参数集。</li>
</ul>
</li>
<li><p>(Vanilla) Recurrent Neural Network:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">h[t] &#x3D; tanh (W[h,h]*h[t-1] + W[x,h]*x[t])   </span><br><span class="line"># Then we save h[t]</span><br><span class="line">y[t] &#x3D; W[h,y]*h[t]</span><br></pre></td></tr></table></figure>
<ul>
<li>这是RNN最简单的例子。</li>
</ul>
</li>
<li><p>RNN处理一系列相关数据。</p>
</li>
</ul>
</li>
<li><p>递归神经网络计算图:</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/10.png" alt></li>
<li><code>h0</code>初始化为零</li>
<li>梯度W是所有已计算的W梯度之和</li>
<li>多对多图<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/11.png" alt></li>
<li>最后一个是所有损失的和，Y的权重是1，通过求和所有梯度来更新</li>
</ul>
</li>
<li>多对一图<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/12.png" alt></li>
</ul>
</li>
<li>一对多图<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/13.png" alt></li>
</ul>
</li>
<li>序列到图形<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/14.png" alt></li>
<li>编码器和解码器原理。</li>
</ul>
</li>
</ul>
</li>
<li><p>例子:</p>
<ul>
<li>假设我们使用字符来构建单词 。我们需要一个模型来预测序列的下一个特征。假设字符只有[h，e，l，o]，单词是[hello]<ul>
<li>训练:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/15.png" alt></li>
<li>这里只有第三个预言是正确的。损失需要优化。</li>
<li>我们可以通过输入整个单词来训练网络。</li>
</ul>
</li>
<li>测试:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/16.png" alt></li>
<li>在测试时，我们逐个处理。输出字符将是下一个输入，其他已保存的隐藏激活。</li>
<li>这个<a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="noopener">链接</a> 包含了所有的代码，但是使用了截短的反向传播，正如我们将要讨论的那样。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>反向传播通过时间向前通过整个序列来计算损耗，然后反向通过整个序列来计算梯度。</p>
<ul>
<li>但如果我们选择整个序列，它将是如此缓慢，占用大量内存，永远不会收敛！</li>
</ul>
</li>
<li><p>所以在实践中，人们做的是“通过时间截短的反向传播”，我们继续向前和向后运行序列的块，而不是整个序列</p>
<ul>
<li>然后将隐藏状态永远向前推进，但只对一些较小的步骤进行反向传播。</li>
</ul>
</li>
<li><p>图像字幕示例:</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/17.png" alt></li>
<li>他们用代币来完成跑步。</li>
<li>最大的图片字幕数据集是微软COCO</li>
</ul>
</li>
<li><p>注意图像字幕是一个项目，在这个项目中，RNN在生成字幕时，会看到图像的特定部分，而不是整个图像。</p>
<ul>
<li>在“视觉答疑”问题中也采用了带注意的图像字幕技术</li>
</ul>
</li>
<li><p>多层rnn通常使用一些层作为隐藏层，这些层被再次馈入。<strong>LSTM</strong>是一个多层RNN。</p>
</li>
<li><p>可能会以反方向消失。爆炸是通过渐变剪裁控制的。消失由加性相互作用控制（LSTM）</p>
</li>
<li><p>LSTM代表长时短时记忆。它的目的是帮助解决RNNs上的消失梯度问题。</p>
<ul>
<li>它包括:<ul>
<li>f: Forget gate, 是否删除单元格</li>
<li>i: Input gate, 是否写入单元格</li>
<li>g: Gate gate (?), 写多少个单元格</li>
<li>o: Output gate, 输出多少个单元格</li>
</ul>
</li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/18.png" alt></li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/18.1.png" alt></li>
<li>像ResNet一样，LSTM梯度很容易计算</li>
<li>LSTM将数据保存在长或短内存中，因为它训练意味着它不仅可以记住最后一层的内容，而且可以记住各个层的内容</li>
</ul>
</li>
<li><p>公路网是介于ResNet和LSTM之间的东西，目前仍在研究中。</p>
</li>
<li><p>更好/更简单的体系结构是当前研究的热点</p>
</li>
<li><p>需要更好的理解（理论和经验）</p>
</li>
<li><p>RNN用于更多地使用相关输入序列的问题。比如NLP和语音识别。</p>
</li>
</ul>
<h2 id="11-Detection-and-Segmentation"><a href="#11-Detection-and-Segmentation" class="headerlink" title="11. Detection and Segmentation"></a>11. Detection and Segmentation</h2><ul>
<li><p>到目前为止，我们讨论的是图像分类问题。在本节中，我们将讨论分割、定位和检测。</p>
</li>
<li><p><strong><u>语义切分(Semantic Segmentation)</u></strong></p>
<ul>
<li><p>我们想用分类标签来标记图像中的每个像素。</p>
</li>
<li><p><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/19.png" alt></p>
</li>
<li><p>当你看到图像中的奶牛时，语义分割并不区分实例，只关心像素。</p>
</li>
<li><p>第一个想法是使用<strong>滑动窗口</strong>。我们取一个小尺寸的窗口，把它滑到整个画面上。对于每个窗口，我们要标记中心像素。</p>
<ul>
<li>它可以工作，但这不是一个好主意，因为它的计算成本很高</li>
<li>效率很低！不重复使用重叠修补程序之间的共享功能</li>
<li>实际上没人用这个</li>
</ul>
</li>
<li><p>第二个想法是将一个网络设计成一组卷积层，以便一次对像素进行预测！</p>
<ul>
<li>输入是整个图像。输出是每个像素标记的图像。</li>
<li>我们需要很多标签数据。而且数据非常昂贵。</li>
<li>它需要很深的卷积层。</li>
<li>损失是提供的每个像素之间的交叉熵。</li>
<li>数据扩充在这里很好。</li>
<li>这种实现的问题是原始图像分辨率下的卷积将非常昂贵。</li>
<li>所以在实践中，我们现在还没有看到这样的情况。</li>
</ul>
</li>
<li><p>第三个想法是基于最后一个想法。不同的是，我们在网络内部进行下采样和上采样。</p>
<ul>
<li><p>我们下采样是因为使用整个图像是非常昂贵的。所以我们在最后进行了多层下采样和上采样。</p>
</li>
<li><p>下采样是一种类似于池化和跨步卷积的操作。.</p>
</li>
<li><p>Upsampling is like “Nearest Neighbor” or “Bed of Nails” or “Max unpooling”</p>
<ul>
<li><p><strong>Nearest Neighbor</strong> example:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input:   1  2               Output:   1  1  2  2</span><br><span class="line">         3  4                         1  1  2  2</span><br><span class="line">                                      3  3  4  4</span><br><span class="line">                                      3  3  4  4</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Bed of Nails</strong> example:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input:   1  2               Output:   1  0  2  0</span><br><span class="line">         3  4                         0  0  0  0</span><br><span class="line">                                      3  0  4  0</span><br><span class="line">                                      0  0  0  0</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Max unpooling</strong> 取决于Max pooling之前的步骤。填充发生最大池化的像素，然后用零填充其他像素。</p>
</li>
</ul>
</li>
<li><p>Max unpooling似乎是上采样的最佳方法。</p>
</li>
<li><p>有一种可学习的上采样的想法叫做 “<strong>Transpose Convolution</strong>“</p>
<ul>
<li>我们做的不是卷积，而是相反。</li>
<li>也叫做:<ul>
<li>上卷积(Upconvolution)</li>
<li>分步卷积(Fractionally strided convolution)</li>
<li>后跨步卷积(Backward strided convolution)</li>
</ul>
</li>
<li>学习上采样的艺术性请参考<a href="https://arxiv.org/abs/1603.07285" target="_blank" rel="noopener">此文章</a>第四章.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><u>Classification + Localization</u></strong>:</p>
<ul>
<li>在这个问题中，我们希望将图像中的主要对象及其位置分类为矩形。</li>
<li>我们假设有一个物体。</li>
<li>我们将创建一个多任务NN。体系结构如下:<ul>
<li>卷积网络层连接到<ul>
<li>对对象进行分类的FC层. <code># 我们知道的简单分类问题</code></li>
<li>连接到四个数字<code>（x、y、w、h）</code>的FC层<ul>
<li>我们将本地化(Localization)视为一个回归问题。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>这个问题会有两个损失:<ul>
<li>Softmax分类损失</li>
<li>局部回归（线性损失）（L2损失）</li>
</ul>
</li>
<li>Loss = SoftmaxLoss + L2 loss</li>
<li>通常，第一个Conv层是像AlexNet一样经过预训练的nn!</li>
<li>这种技术可以应用于许多其他问题，如：人体姿态估计。</li>
</ul>
</li>
<li><p><strong><u>Object Detection</u></strong></p>
<ul>
<li>计算机视觉的核心思想。我们将详细讨论这个问题。</li>
<li>“分类+定位”和这个问题的区别在于，我们要检测一个或多个不同的对象及其位置</li>
<li>第一个想法是使用滑动窗口<ul>
<li>干得好但是时间长</li>
<li>步骤是:<ul>
<li>将CNN应用于图像的许多不同裁剪，CNN将每种裁剪分类为对象或背景。</li>
</ul>
</li>
<li>问题是我们需要将CNN应用于大量的位置和规模，计算非常昂贵</li>
<li>成千上万的人会用上千次的蛮力来滑动窗口</li>
</ul>
</li>
<li>区域建议将帮助我们决定在哪个区域运行NN:<ul>
<li>查找可能包含对象的滴状(blobby)图像区域</li>
<li>运行速度相对较快；例如，选择性搜索在CPU上几秒钟内提供1000个区域建议</li>
</ul>
</li>
<li>所以现在我们可以应用其中一个区域提案网络，然后应用第一个想法</li>
<li>还有一个想法叫做R-CNN<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/20.png" alt></li>
<li>这个想法是不好的，因为它将图像的一部分——包括区域建议——如果大小不同，在将它们全部缩放到一个尺寸后，将其提供给CNN。缩放不好</li>
<li>而且他非常缓慢</li>
</ul>
</li>
<li>快速R-CNN是在R-CNN上发展起来的另一个想法<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/48.png" alt></li>
<li>它用一个CNN来做任何事情。</li>
</ul>
</li>
<li>Faster R-CNN 通过插入区域建议网络（RPN）来根据特征预测提案，从而完成自己的区域提案。<ul>
<li>最快的R-CNNs.</li>
</ul>
</li>
<li>另一个想法是没有建议的检测: YOLO / SSD<ul>
<li>YOLO 代表你只看一次.</li>
<li>YOLO/SDD 是两个独立的算法</li>
<li>速度更快但不够准确。</li>
</ul>
</li>
<li>Takeaways<ul>
<li>Faster R-CNN速度较慢，但更准确。</li>
<li>SSD/YOLO 速度快得多，但不够准确</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><u>Denese Captioning</u></strong></p>
<ul>
<li>Denese Captioning is “Object Detection + Captioning”</li>
<li>关于这个想法的<a href="https://arxiv.org/abs/1511.07571" target="_blank" rel="noopener">论文</a>可以在这里找到.</li>
</ul>
</li>
<li><p><strong><u>实例分割(Instance Segmentation)</u></strong></p>
<ul>
<li>问题就是这样</li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/49.png" alt></li>
<li>我们不想预测边界框，而是想知道哪个像素标签，同时还要区分它们。</li>
<li>有很多想法</li>
<li>有一个新的想法”Mask R-CNN”<ul>
<li>像R-CNN一样，但在它里面我们应用了语义分割</li>
<li>这篇论文有很多很好的结果。</li>
<li>它总结了我们在这堂课上讨论过的所有事情</li>
<li>这方面的表现似乎不错。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="12-Visualizing-and-Understanding"><a href="#12-Visualizing-and-Understanding" class="headerlink" title="12. Visualizing and Understanding"></a>12. Visualizing and Understanding</h2><ul>
<li><p>我们想知道ConvNets内部发生了什么？</p>
</li>
<li><p>人们希望相信黑匣子（CNN）并知道它是如何工作的，并做出正确的决定。</p>
</li>
<li><p>第一种方法是可视化第一层的过滤器</p>
<ul>
<li>可能第一层过滤器的形状是5×5×3，过滤器的数量是16。然后我们将有16个不同的“彩色”过滤器图像。</li>
<li>事实证明，这些过滤器像人脑一样学习原始形状和定向边缘。</li>
<li>这些过滤器在你将要训练的每个Conv网络上看起来都是一样的，例如如果你试图从AlexNet、VGG、GoogleNet或ResNet获取它。</li>
<li>这将告诉您第一个卷积层在图像中寻找什么。</li>
</ul>
</li>
<li><p>我们可以看到下一层的过滤器，但它们不会告诉我们任何信息。</p>
<ul>
<li>第一层过滤器的形状可能是5×5×20，过滤器的数量是16。然后我们将有16*20个不同的“灰色”滤镜图像。</li>
</ul>
</li>
<li><p>在AlexNet，最后有一些FC层。如果我们取4096维特征向量作为图像，并收集这些特征向量。</p>
<ul>
<li>如果我们在这些特征向量之间建立一个最近的邻域，并得到这些特征的真实图像，那么与直接在图像上运行KNN相比，我们将得到非常好的结果！</li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/21.png" alt></li>
<li>这种相似性告诉我们，这些cnn真正获得的是这些图像的语义，而不是像素级的语义！</li>
<li>我们可以对4096维特征进行降维，并将其压缩到二维。<ul>
<li>这可以通过PCA或t-SNE来实现。</li>
<li>t-SNE更多地与深度学习一起用于可视化数据。可以在<a href="http://cs.stanford.edu/people/karpathy/cnnembed/" target="_blank" rel="noopener">这里</a>找到示例</li>
</ul>
</li>
</ul>
</li>
<li><p>我们可以看到激活图</p>
<ul>
<li>例如，如果CONV5功能图是128 x 13 x 13，我们可以将其可视化为128 13 x 13灰度图像。</li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/50.png" alt></li>
<li>其中的一个特性是根据输入激活的，所以现在我们知道这个特定的映射正在寻找一些东西。</li>
<li>这是由吉辛斯基等人(Yosinski et.)完成的。更多信息在<a href="http://yosinski.com/deepvis#toolbox" target="_blank" rel="noopener">这里</a>。</li>
</ul>
</li>
<li><p>有一种叫做最大激活补丁(<strong>Maximally Activating Patches</strong>)的东西可以帮助我们可视化Convnets中的中间特性</p>
<ul>
<li>这样做的步骤如下:<ul>
<li>我们先选择一层神经元<ul>
<li>例如，我们在AlexNet中选择Conv5，它是128 x 13 x 13，然后选择通道（神经元）17/128</li>
</ul>
</li>
<li>通过网络运行许多图像，记录所选频道的值</li>
<li>可视化对应于最大激活的图像块<ul>
<li>我们会发现每个神经元都在观察图像的特定部分</li>
<li>利用感受野(eceptive field)提取(Extract)图像</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>另一个想法是遮挡实验(<strong>Occlusion Experiments</strong>)</p>
<ul>
<li>我们将图像的一部分蒙版后再输入CNN，在每个掩模位置绘制概率热图（输出为真）</li>
<li>它将给你形象中最重要的部分，从中我们学习到了。</li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/51.png" alt></li>
</ul>
</li>
<li><p>显著性映射(<strong>Saliency Maps</strong>)指出哪些像素对分类很重要</p>
<ul>
<li>类似于遮挡实验，但使用了完全不同的方法</li>
<li>我们计算（未规范化）类分数相对于图像像素的梯度，在RGB通道上取绝对值和最大值。它会给我们一个灰色的图像，代表图像中最重要的区域。</li>
<li>这有时可以用于语义分割。</li>
</ul>
</li>
<li><p>（引导的）backprop可以最大限度地激活补丁(<strong>Maximally Activating Patches</strong>)，但不同的是，它能获得我们所关心的像素。</p>
<ul>
<li>在该方法中，选择一个最大激活块的通道，然后计算神经元值相对于图像像素的梯度</li>
<li>如果您只通过每个RELU反向投影正渐变（guided backprop），图像会变得更好</li>
</ul>
</li>
<li><p>坡度上升(<strong>Gradient Ascent</strong>)</p>
<ul>
<li><p>生成一个最大限度地激活神经元的合成图像。</p>
</li>
<li><p>反向坡度下降。不是取最小值而是取最大值。</p>
</li>
<li><p>我们想用输入图像最大化神经元。因此，我们在这里尝试学习使激活最大化的图像：</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># R（I）是自然图像正则化器，f（I）是神经元值</span></span><br><span class="line">I *= argmax(f(I)) + R(I)</span><br></pre></td></tr></table></figure>
</li>
<li><p>坡度上升的步骤</p>
<ul>
<li>将图像初始化为零.</li>
<li>传播图像以计算当前分数。</li>
<li>Backprop获取神经元值相对于图像像素的梯度</li>
<li>对图像做一个小的更新</li>
</ul>
</li>
<li><p><code>R(I)</code> 可能等于生成图像的L2</p>
</li>
<li><p>为了得到更好的结果，我们使用了一个更好的正则化器:</p>
<ul>
<li>惩罚图像的L2范数；也在周期性优化期间:<ul>
<li>高斯模糊图像</li>
<li>将小值的像素剪裁为0</li>
<li>将小梯度的像素剪裁为0</li>
</ul>
</li>
</ul>
</li>
<li><p>一个更好的正则化器使图像更干净</p>
</li>
<li><p><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/22.png" alt></p>
</li>
<li><p>后一层的结果似乎比其他层更有意义</p>
</li>
</ul>
</li>
<li><p>我们可以用这个程序愚弄CNN:</p>
<ul>
<li>从任意图像开始.            <code># 无根据的随机图片</code></li>
<li>选择任意类 <code># Random class</code></li>
<li>修改图像以最大化类</li>
<li>重复，直到网络被愚弄。</li>
</ul>
</li>
<li><p>愚弄网络的结果令人惊讶</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/23.png" alt></li>
<li>对于人眼来说，它们是一样的，但它只是通过添加一些噪音来愚弄网络</li>
</ul>
</li>
<li><p><strong>DeepDream</strong>: 扩大现有功能(Amplify existing features)</p>
<ul>
<li>谷歌在他们的网站上发布了DeepDream</li>
<li>它的实际操作与我们讨论过的愚弄神经网络的过程是一样的，但不是合成一个图像来最大化特定的神经元，而是试图放大网络中某一层的神经元激活。</li>
<li>Steps:<ul>
<li>Forward: 计算所选层的激活.        <code># form an input image (Any image)</code></li>
<li>设置所选层的梯度等于其激活.<ul>
<li>相当于<code>I* = arg max[I] sum(f(I)^2)</code></li>
</ul>
</li>
<li>Backward: 计算图像的梯度</li>
<li>更新图像.</li>
</ul>
</li>
<li>deep dream 的代码是在线的，你可以自己下载和检查yourself.</li>
</ul>
</li>
<li><p>特征反演(<strong>Feature Inversion</strong>)</p>
<ul>
<li>让我们知道不同层次的网络元素被捕捉到了什么</li>
<li>给定一个图像的CNN特征向量，找到一个新的图像<ul>
<li>匹配给定的特征向量</li>
<li>看起来自然（图像优先正则化）</li>
</ul>
</li>
</ul>
</li>
<li><p>纹理合成(<strong>Texture Synthesis</strong>)</p>
<ul>
<li>计算机图形学中的老问题。</li>
<li>给定一个纹理的样本块，我们能生成一个更大的相同纹理的图像吗</li>
<li>有一种算法不依赖于神经网络:<ul>
<li>Wei and Levoy, 基于树结构矢量量化的快速纹理合成，SIGGRAPH 2000</li>
<li>这是一个非常简单的算法</li>
</ul>
</li>
<li>这里的想法是，这是一个老问题，有很多算法已经解决了这个问题，但是简单的算法在复杂的纹理上效果不佳</li>
<li>2015年提出了一种基于梯度上升的神经网络方法，称之为“神经纹理合成”<ul>
<li>它依赖于一种叫做Gram矩阵的东西。</li>
</ul>
</li>
</ul>
</li>
<li><p>Neural Style Transfer =  Feature + Gram Reconstruction</p>
<ul>
<li>Gatys, Ecker, and Bethge, 使用卷积神经网络的图像风格传输(Image style transfer using Convolutional neural<br>networks)，CVPR 2016</li>
<li>Implementation by pytorch <a href="https://github.com/jcjohnson/neural-style" target="_blank" rel="noopener">here</a>.</li>
</ul>
</li>
<li><p>风格转换需要许多向前/向后通过VGG；非常慢</p>
<ul>
<li>训练另一个神经网络为我们进行风格转换</li>
<li>快速风格转换是解决方案</li>
<li>Johnson, Alahi, and Fei-Fei, 实时风格转换和超分辨率感知损失(Perceptual Losses for Real-Time Style Transfer and Super-Resolution)，ECCV 2016</li>
<li><a href="https://github.com/jcjohnson/fast-neural-style" target="_blank" rel="noopener">https://github.com/jcjohnson/fast-neural-style</a></li>
</ul>
</li>
<li><p>有很多关于这种风格转换的工作，而且一直持续到现在</p>
</li>
<li><p>总结:</p>
<ul>
<li>Activations: 最近邻，降维，最大面片，遮挡( Nearest neighbors, Dimensionality reduction, maximal patches,<br>occlusion)</li>
<li>Gradients: 显著图，类可视化，愚弄图像，特征反演(Saliency maps, class visualization, fooling images, feature inversion)</li>
<li>Fun: 深度梦想，风格转换(DeepDream, Style Transfer)</li>
</ul>
</li>
</ul>
<h2 id="13-Generative-models"><a href="#13-Generative-models" class="headerlink" title="13. Generative models"></a>13. Generative models</h2><ul>
<li><p>生成模型是一种无监督学习</p>
</li>
<li><p>有监督vs无监督学习:</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>有监督学习</th>
<th>无监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据结构</td>
<td>Data: (x, y), x是数据，y是标签</td>
<td>Data: x, 只有数据，没有标签！</td>
</tr>
<tr>
<td>数据代价</td>
<td>在很多情况下，培训数据是昂贵的。</td>
<td>训练数据很便宜！</td>
</tr>
<tr>
<td>目标</td>
<td>学习映射x-&gt;y的函数</td>
<td>了解一些隐藏的数据结构</td>
</tr>
<tr>
<td>例子</td>
<td>分类，回归，目标检测，语义分割，图像字幕</td>
<td>聚类，降维，特征学习，密度估计</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>自动编码器是一种特征学习技术。</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/24.png" alt></li>
<li>它包含一个编码器和一个解码器。编码器对图像进行降采样，而解码器对特征进行上采样。</li>
<li>损失为L2损失。</li>
</ul>
</li>
<li><p>密度估计是我们要学习/估计数据的底层分布的地方</p>
</li>
<li><p>与有监督学习相比，无监督学习还存在许多研究性开放性问题！</p>
</li>
<li><p>生成模型(<strong>Generative Models</strong>)</p>
<ul>
<li>给定训练数据，从同一分布生成新样本。</li>
<li>解决了无监督学习中的一个核心问题——密度估计。</li>
<li>我们有不同的方法：<ul>
<li>显式密度估计：明确定义和求解学习模型</li>
<li>学习模型，它可以从学习模型中进行采样，而无需显式地定义它</li>
</ul>
</li>
<li>为什么是生成模型<ul>
<li>艺术作品、超分辨率、彩色化等的逼真样品</li>
<li>时间序列数据的生成模型可用于模拟和规划（强化学习应用程序！）</li>
<li>训练生成模型还可以推理潜在的表示，这些表示可以作为一般特征有用</li>
</ul>
</li>
<li>生成模型分类法:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/52.png" alt></li>
</ul>
</li>
<li>在这节课中，我们将讨论: PixelRNN/CNN, 变分自动编码器和GANs，因为它们是目前研究中流行的模型。</li>
</ul>
</li>
<li><p><strong>PixelRNN</strong> and <strong>PixelCNN</strong></p>
<ul>
<li>在完全可见的信念网络中，我们使用链式规则将图像x的似然分解为一维分布的乘积<ul>
<li><code>p(x) = sum(p(x[i]| x[1]x[2]....x[i-1]))</code></li>
<li>其中p（x）是图像x的可能性，x[i]是给定所有先前像素的第i个像素值的概率。</li>
</ul>
</li>
<li>为了解决这个问题，我们需要最大化训练数据的可能性，但是像素值的分布非常复杂。</li>
<li>我们还需要定义先前像素的顺序。</li>
<li>PixelRNN<ul>
<li>Founded by [van der Oord et al. 2016]</li>
<li>对使用RNN（LSTM）建模的先前像素的依赖性</li>
<li>从角点开始生成图像像素</li>
<li>缺点：顺序生成速度慢！因为你必须逐像素地生成</li>
</ul>
</li>
<li>PixelCNN<ul>
<li>Also Founded by [van der Oord et al. 2016]</li>
<li>仍然生成从角开始的图像像素。</li>
<li>对先前像素的依赖现在使用CNN的上下文区域建模</li>
<li>训练比pixerlnn快（可以并行卷积，因为从训练图像知道上下文区域值）</li>
<li>生成仍然必须循序渐进，仍然缓慢。</li>
</ul>
</li>
<li>有一些技巧需要改进 PixelRNN &amp; PixelCNN.</li>
<li>PixelRNN and PixelCNN 能产生良好的样本，目前仍是研究的活跃领域。</li>
</ul>
</li>
<li><p><strong>Autoencoders</strong></p>
<ul>
<li>无监督方法从未标记的训练数据中学习低维特征表示。</li>
<li>由编码器和解码器组成。</li>
<li>编码器:<ul>
<li>将输入x转换为特性z。z应小于x才能从输入中获取重要值。我们可以称之为降维。</li>
<li>编码器可以用:<ul>
<li>线性或非线性层(earlier days days)</li>
<li>Deep fully connected NN (Then)</li>
<li>RELU CNN (Currently we use this on images)</li>
</ul>
</li>
</ul>
</li>
<li>解码器:<ul>
<li>我们希望编码器映射我们已经产生的特性，以输出类似于x或相同x的东西。</li>
<li>解码器可以用同样的技术，我们做编码器，目前它使用一个RELU CNN。</li>
</ul>
</li>
<li>编码器是conv层，而解码器是deconv层！意思是先降后升。</li>
<li>损失函数为L2损失函数:<ul>
<li><code>L[i] = |y[i] - y&#39;[i]|^2</code><ul>
<li>经过训练，我们把解码器拆掉了.<code># 现在我们有了我们需要的功能</code></li>
</ul>
</li>
</ul>
</li>
<li>我们可以用这个编码器我们要做一个有监督的模型<ul>
<li>这一点的价值在于它可以学习到一个好的特征来表示你所拥有的输入</li>
<li>很多时候我们会有少量的数据来解决问题。解决这个问题的一个方法是使用一个自动编码器来学习如何从图像中获取特征，并在模型上训练你的小数据集</li>
</ul>
</li>
<li>问题是我们能从这个自动编码器生成数据（图像）吗</li>
</ul>
</li>
<li><p>变分自动编码器<strong>Variational Autoencoders (VAE)</strong></p>
<ul>
<li>自动编码器上的概率自旋-将让我们从模型中取样生成数据</li>
<li>我们将z作为使用编码器形成的特征向量。</li>
<li>然后我们选择先验p（z）是简单的，例如高斯。<ul>
<li>合理的隐藏属性：例如姿势，微笑的程度。</li>
</ul>
</li>
<li>条件p（x | z）是复杂的（生成图像）=&gt;用神经网络表示</li>
<li>但是我们不能用下面的方程来计算P（z）P（x | z）dz的积分:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/25.png" alt></li>
</ul>
</li>
<li>在解决了最后一个方程的所有方程之后，我们应该:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/26.png" alt></li>
</ul>
</li>
<li>变分自动编码器是生成模型的一种方法，但与最先进的（GANs）相比，样本更模糊，质量更低</li>
<li>活跃的研究领域:<ul>
<li>更灵活的近似，例如更丰富的后验近似，而不是对角高斯</li>
<li>在潜在变量中加入结构</li>
</ul>
</li>
</ul>
</li>
<li><p>生成性对抗网络<strong>Generative Adversarial Networks (GANs)</strong></p>
<ul>
<li><p>GAN不适用于任何明确的密度函数</p>
</li>
<li><p>首先，采取博弈论的方法：通过两人博弈，从训练分配中学习生成。</p>
</li>
<li><p>在Facebook负责人工智能研究的Yann LeCun称GANs:</p>
<ul>
<li><blockquote>
<p>近20年来深度学习中最酷的想法</p>
</blockquote>
</li>
</ul>
</li>
<li><p>问题：想从复杂的、高维的培训分布中取样。我们已经讨论过了，没有直接的方法！</p>
</li>
<li><p>解决方案：从简单分布中取样，例如随机噪声。学习向培训分配的转变。</p>
</li>
<li><p>因此，我们创建一个噪声图像，从简单的分布中提取出来，将其馈送给神经网络，我们将其称为一个发电机网络，应该学会将其转换成我们想要的分布。</p>
</li>
<li><p>Training GANs: Two-player game:</p>
<ul>
<li><strong>Generator network</strong>: 试图通过生成真实感图像来愚弄鉴别器</li>
<li><strong>Discriminator network</strong>: 试着区分真假图像。</li>
</ul>
</li>
<li><p>如果我们能够很好地训练鉴别器，那么我们就可以训练生成器生成正确的图像。</p>
</li>
<li><p>这里给出了GANs作为极小极大对策的损失函数</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/27.png" alt></li>
</ul>
</li>
<li><p>生成器网络的标签为0，实际图像为1。</p>
</li>
<li><p>为了训练我们的网络:</p>
<ul>
<li>鉴别器上的梯度上升。</li>
<li>生成器上的梯度上升，但损耗不同。</li>
</ul>
</li>
<li><p>你可以在这里阅读完整的算法和方程式</p>
<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/28.png" alt></li>
</ul>
</li>
<li><p>旁白：联合培养两个网络是有挑战性的，可以是不稳定的。选择具有更好损失景观的目标有助于培训是一个活跃的研究领域。</p>
</li>
<li><p>卷积结构(Convolutional Architectures):</p>
<ul>
<li>生成器是一个上采样网络的分数阶跃卷积鉴别器是一个卷积网络。</li>
<li>稳定深部Conv-GANs指南:<ul>
<li>用跨步卷积（鉴别器）替换任何池层，用（生成器）替换部分跨步卷积</li>
<li>对两个网络使用批处理规范</li>
<li>移除完全连接的隐藏层以获得更深层的架构。</li>
<li>在生成器中对所有层使用RELU激活，除了使用Tanh的输出</li>
<li>所有层在鉴别器中使用leaky RELU</li>
</ul>
</li>
</ul>
</li>
<li><p>2017年是GANS年！它爆炸了，有一些非常好的结果</p>
</li>
<li><p>研究的活跃领域也是GANs的各种应用领域。</p>
</li>
<li><p>可以在这里找到 GAN zoo: <a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">https://github.com/hindupuravinash/the-gan-zoo</a></p>
</li>
<li><p>使用GANs的提示和技巧: <a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">https://github.com/soumith/ganhacks</a></p>
</li>
<li><p>NIPS 2016 GANS教程: <a href="https://www.youtube.com/watch?v=AJVyzd0rqdc" target="_blank" rel="noopener">https://www.youtube.com/watch?v=AJVyzd0rqdc</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="14-Deep-reinforcement-learning"><a href="#14-Deep-reinforcement-learning" class="headerlink" title="14. Deep reinforcement learning"></a>14. Deep reinforcement learning</h2><ul>
<li>这一节包含了很多数学知识。</li>
<li>强化学习问题涉及到一个智能体与一个环境的交互作用，它提供了数字奖励信号。</li>
<li>步骤是:<ul>
<li>Environment —&gt; State <code>s[t]</code> —&gt; Agent —&gt; Action <code>a[t]</code> —&gt; Environment —&gt; <code>Reward r[t]</code> + Next state <code>s[t+1]</code> —&gt; Agent —&gt; and so on..</li>
</ul>
</li>
<li>我们的目标是学习如何采取行动以获得最大的回报。</li>
<li>机器人移动就是一个例子:<ul>
<li>目标：使机器人向前移动</li>
<li>状态：关节的角度和位置</li>
<li>行为：施加在接头上的扭矩</li>
<li>每次1步直立+向前移动</li>
</ul>
</li>
<li>另一个例子是Atari Games：<ul>
<li>深度学习在这个问题上有着很好的研究现状。</li>
<li>目标：以最高分数完成游戏。</li>
<li>状态：游戏状态的原始像素输入。</li>
<li>行为：游戏控制，如左、右、上、下</li>
<li>奖励：每一个时间步增加/减少分数</li>
</ul>
</li>
<li>围棋游戏是AlphaGo团队在去年（2016年）赢得的又一个例子，这对于人工智能和深度学习来说是一个巨大的成就，因为问题太难了。</li>
<li>利用马尔可夫决策过程(<u><strong>Markov Decision Process</strong></u>)，我们可以在数学上表示强化学习</li>
<li>马尔可夫决策过程(<strong>Markov Decision Process</strong>)<ul>
<li>由（S，A，R，P，Y）定义，其中<ul>
<li><code>S</code>: set of possible states.</li>
<li><code>A</code>: set of possible actions</li>
<li><code>R</code>: distribution of reward given (state, action) pair</li>
<li><code>P</code>: 转移概率，即下一状态给定（状态，动作）对的分布</li>
<li><code>Y</code>: discount factor    <code># 我们对即将到来的奖励有多重视，请稍后讨论</code></li>
</ul>
</li>
<li>算法:<ul>
<li>在时间步长t=0时，环境采样初始状态s[0]</li>
<li>然后，t=0直到完成:<ul>
<li>代理选择动作a[t]</li>
<li>R的环境样品奖励（s[t]，a[t]）</li>
<li>环境用（s[t]，a[t]）从P中采样下一个状态</li>
<li>代理人获得奖励r[t]和下一状态s[t+1]</li>
</ul>
</li>
</ul>
</li>
<li>策略pi是从S到A的函数，它指定在每个状态下要执行的操作</li>
<li>目标：找到使累计折扣奖励最大化的策略pi<em>: `Sum(Y^t </em> r[t], t&gt;0)`</li>
<li>例子:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/29.png" alt></li>
</ul>
</li>
<li>解决方案是:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/30.png" alt></li>
</ul>
</li>
</ul>
</li>
<li>状态s的值函数是从s状态遵循策略得到的预期累积回报:<ul>
<li><code>V[pi](s) = Sum(Y^t * r[t], t&gt;0) given s0 = s, pi</code></li>
</ul>
</li>
<li>状态s和行动a的Q值函数是在s状态下采取行动a，然后遵循策略所得到的预期累积回报:<ul>
<li><code>Q[pi](s,a) = Sum(Y^t * r[t], t&gt;0) given s0 = s,a0 = a, pi</code></li>
</ul>
</li>
<li>最优Q值函数Q*是给定（状态、动作）对所能达到的最大期望累积报酬:<ul>
<li><code>Q*[s,a] = Max(for all of pi on (Sum(Y^t * r[t], t&gt;0) given s0 = s,a0 = a, pi))</code></li>
</ul>
</li>
<li>贝尔曼方程(Bellman equation)<ul>
<li>重要的是RL</li>
<li>给定任何一个状态-动作对，这个对的值将是你将得到的回报r加上你结束的状态的值。</li>
<li><code>Q*[s,a] = r + Y * max Q*(s&#39;,a&#39;) given s,a  # 提示等式中没有策略</code></li>
<li>最优策略pi<em>对应于在Q指定的任何状态下采取最佳操作</em></li>
</ul>
</li>
<li>利用Bellman方程作为迭代更新的值迭代算法，可以得到最优策略<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/31.png" alt></li>
</ul>
</li>
<li>由于实际应用中空间维数巨大，我们将使用函数逼近器来估计Q（s，a）。E、 神经网络！这叫做Q学习(<strong>Q-learning</strong>)<ul>
<li>任何时候我们有一个复杂的函数，我们不能代表我们使用神经网络！</li>
</ul>
</li>
<li><strong>Q-learning</strong><ul>
<li>第一个解决RL的深度学习算法</li>
<li>使用函数逼近器估计动作值函数</li>
<li>如果函数逼近器是深度神经网络=&gt;深度q-学习</li>
<li>损失函数zh:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/32.png" alt></li>
</ul>
</li>
</ul>
</li>
<li>现在让我们考虑一下“玩雅达利游戏”的问题:<ul>
<li>我们的总奖励通常是我们在屏幕顶部看到的奖励。</li>
<li>Q网络体系结构:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/33.png" alt></li>
</ul>
</li>
<li>从成批的连续样本中学习是一个问题。如果我们记录了一个训练数据，并设置了神经网络来处理它，如果数据不够，我们将走向高偏差误差。因此，我们应该使用“经验回放”而不是连续的样本，在这种情况下，神经网络将不断尝试游戏，直到它掌握了它。</li>
<li>随着游戏（体验）情节的播放，不断更新转换（s[t]、a[t]、r[t]、s[t+1]）的回放记忆表。</li>
<li>从重放存储器中随机训练Q网络，而不是连续的样本。</li>
<li>完整算法:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/34.png" alt></li>
</ul>
</li>
<li>在这里可以找到演示Atari游戏算法的视频: “<a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=V1eYniJ0Rnk</a>“</li>
</ul>
</li>
<li>政策梯度(<strong>Policy Gradients</strong>)<ul>
<li>解决RL的第二个深度学习算法。</li>
<li>Q函数的问题是Q函数可能非常复杂。<ul>
<li>机器人抓取物体的状态非常高维。</li>
<li>但政策可以简单得多：只需握紧你的手。</li>
</ul>
</li>
<li>我们是否可以直接学习策略，例如从一组策略中找到最佳策略？</li>
<li>政策梯度方程:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/35.png" alt></li>
</ul>
</li>
<li>收敛到J（ceta）的局部极小值，通常足够好</li>
<li>强化算法是一种能够得到/预测最优策略的算法</li>
<li>补强算法的方程与直观性:<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/36.png" alt></li>
<li>问题是这个方程的高方差，我们能解决吗？</li>
<li>方差缩减是一个活跃的研究领域！</li>
</ul>
</li>
<li>递归注意模型（RAM）是一种基于增强算法的图像分类算法:<ul>
<li>采取一系列的“瞥见”选择性地聚焦在图像的区域，以预测类<ul>
<li>灵感来自人类的感知和眼球运动。</li>
<li>节省计算资源=&gt;可伸缩性<ul>
<li>如果一个高分辨率的图像可以节省大量的计算</li>
</ul>
</li>
<li>能够忽略图像的杂乱/不相关部分</li>
</ul>
</li>
<li>RAM在许多任务中得到了广泛的应用：包括细粒度的图像识别、图像字幕和可视问答</li>
</ul>
</li>
<li>AlphaGo使用了监督学习和强化学习的混合，它也使用了策略梯度。</li>
</ul>
</li>
<li>斯坦福德关于深度强化学习的好课程<ul>
<li><a href="http://web.stanford.edu/class/cs234/index.html" target="_blank" rel="noopener">http://web.stanford.edu/class/cs234/index.html</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX</a></li>
</ul>
</li>
<li>深度强化学习好课程（2017）<ul>
<li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener">http://rll.berkeley.edu/deeprlcourse/</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3</a></li>
</ul>
</li>
<li>一篇好文章<ul>
<li><a href="https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html" target="_blank" rel="noopener">https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html</a></li>
</ul>
</li>
</ul>
<h2 id="15-Efficient-Methods-and-Hardware-for-Deep-Learning"><a href="#15-Efficient-Methods-and-Hardware-for-Deep-Learning" class="headerlink" title="15. Efficient Methods and Hardware for Deep Learning"></a>15. Efficient Methods and Hardware for Deep Learning</h2><ul>
<li>最初的讲座是由斯坦福大学的博士生宋晗做的。The original lecture was given by Song Han a PhD Candidate at standford.</li>
<li>深度转换网络、递归网络和深度强化学习正在形成许多应用程序，并改变了我们的生活。<ul>
<li>比如自动驾驶汽车，机器翻译，alphaGo等等。</li>
</ul>
</li>
<li>但现在的趋势是，如果我们想要高精度，我们需要更大（更深）的模型。<ul>
<li>2012年至2015年，ImageNet Competition的模型尺寸增加了16倍，以达到较高的精确度。</li>
<li>Deep speech 2的训练操作是Deep speech 1的10倍，而且这只需要一年！#在百度</li>
</ul>
</li>
<li>我们从中得到了三个挑战<ul>
<li><strong>模型尺寸(Model Size)</strong><ul>
<li>很难在我们的个人电脑、手机或汽车上部署更大的型号</li>
</ul>
</li>
<li><strong>速度(Speed)</strong><ul>
<li>ResNet152训练耗时1.5周，准确率为6.16%</li>
<li>长时间的培训限制了研究人员的生产力</li>
</ul>
</li>
<li><strong>能源效率(Energy Efficiency)</strong><ul>
<li>AlphaGo:1920个CPU和280个GPU。每场3000每元电费</li>
<li>如果我们在手机上使用它，它会耗尽电池电量。</li>
<li>谷歌在他们的博客中提到如果所有的用户使用谷歌语音3分钟，他们必须加倍他们的数据中心</li>
<li>能源消耗在哪里?<ul>
<li>更大的模型=&gt;更多的内存引用=&gt;更多的能量</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>通过算法硬件协同设计，可以提高深度学习的效率。<ul>
<li>从硬件和算法的角度。</li>
</ul>
</li>
<li>Hardware 101: the Family<ul>
<li>General Purpose # Used for any hardware<ul>
<li>CPU                <code># 面向延迟，单一强线程，就像一个元素</code></li>
<li>GPU            <code># 面向吞吐量，所以很多小线程就像很多蚂蚁</code></li>
<li>GPGPU<ul>
<li>专业硬件(<strong>Specialized HW</strong>)        <code>#针对应用程序领域进行了调整</code><ul>
<li>现场可编程门阵列（field-programmable gate array）FPGA  <code>#可编程逻辑，它便宜但效率低</code></li>
<li>特定用途集成电路（Application Specific Integrated Circuit）ASIC <code># 固定逻辑，为特定应用而设计（可用于深度学习应用程序）</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Hardware 101: 数字表示法（Number Representation）<ul>
<li>计算机中的数字用离散存储器表示。</li>
<li>在浮点运算中，硬件从32位变为16位是非常好和节能的。</li>
</ul>
</li>
<li>Part 1: 有效推理算法（<strong><u>Algorithms for Efficient Inference</u></strong>）<ul>
<li>修剪神经网络（<strong>Pruning neural networks</strong>）<ul>
<li>我们的想法是，我们可以去掉一些权重/神经元，而神经网络的行为仍然是一样的吗?</li>
<li>2015年，将AlexNet参数从6000万到600万！用修剪的方法。</li>
<li>剪枝可以应用于CNN和RNN中，迭代地达到与原始剪枝相同的精度。</li>
<li>修剪实际上发生在人类身上:<ul>
<li>新生儿（50万亿突触）==&gt; 1岁（1000万亿突触）==&gt; 青少年（500万亿突触）</li>
</ul>
</li>
<li>算法:<ol>
<li>得到训练的网络。</li>
<li>评估神经元的重要性</li>
<li>移除最不重要的神经元。</li>
<li>微调网络。</li>
<li>如果我们需要继续修剪，我们再次进入第2步，否则我们停止。</li>
</ol>
</li>
</ul>
</li>
<li>权重分担(<strong>Weight Sharing</strong>)<ul>
<li>我们的想法是，我们想让我们的模型数量更少。</li>
<li>训练量子化:<ul>
<li>示例：2.09、2.12、1.92、1.87的所有权重值将替换为2</li>
<li>为了做到这一点，我们可以使k均值聚类在一个过滤器上，并减少它的数目。通过使用此方法，我们还可以减少计算坡度所需的操作数。</li>
<li>经过训练的量化后，权重是离散的。</li>
<li>经过训练的量化可以显著减少每层数字所需的比特数。</li>
</ul>
</li>
<li>剪枝+训练量化可以共同减小模型的大小</li>
<li>哈夫曼编码(Huffman Coding)<ul>
<li>我们可以使用哈夫曼编码来减少/压缩权重的位数。</li>
<li>不频繁权重：使用更多的位来表示。</li>
<li>频繁权重：使用较少的位来表示。</li>
</ul>
</li>
<li>使用剪枝+训练量化+霍夫曼编码被称为深度压缩<ul>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/37.png" alt></li>
<li><img src="/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/38.png" alt></li>
<li><strong>SqueezeNet</strong><ul>
<li>到目前为止，我们讨论的所有模型都是使用预先训练过的模型。我们能做一个新的节省内存和计算的程序吗?</li>
<li>SqueezeNet获得了alexnet的精确度，参数减少了50倍，模型尺寸为0.5倍.</li>
</ul>
</li>
<li>挤压网甚至可以进一步压缩通过应用深度压缩他们</li>
<li>现在的模型更节能，速度也快了很多。</li>
<li>深度压缩通过facebook和百度应用于行业</li>
</ul>
</li>
</ul>
</li>
<li><strong>Quantization</strong><ul>
<li>算法（量化权重和激活）:<ul>
<li>用浮点数训练</li>
<li>量化重量和激活:<ul>
<li>收集体重和活动的统计数据。</li>
<li>选择合适的基点位置。</li>
</ul>
</li>
<li>微调浮动格式。</li>
<li>转换为定点格式。</li>
</ul>
</li>
</ul>
</li>
<li><strong>Low Rank Approximation</strong><ul>
<li>是CNN使用的另一种大小缩减算法。</li>
<li>想法是分解conv层，然后尝试两个组合层。</li>
</ul>
</li>
<li><strong>Binary / Ternary Net</strong><ul>
<li>我们能只用三个数字来表示神经网络中的权重吗？</li>
<li>如果只有-1，0，1，大小就会小得多</li>
<li>这是2017年发表的一个新观点”Zhu, Han, Mao, Dally. Trained Ternary Quantization, ICLR’17”</li>
<li>Works after training.</li>
<li>他们已经在AlexNet上尝试过了，它几乎达到了与AlexNet相同的错误</li>
<li>每个寄存器的操作数将增加: <a href="https://xnor.ai/" target="_blank" rel="noopener">https://xnor.ai/</a></li>
</ul>
</li>
<li><strong>Winograd Transformation</strong><ul>
<li>基于3x3 WINOGRAD卷积，比普通的卷积运算更少</li>
<li>cuDNN 5使用了WINOGRAD卷积，提高了速度</li>
</ul>
</li>
</ul>
</li>
<li>Part 2:有效的硬件推理(<strong><u>Hardware for Efficient Inference</u></strong>)<ul>
<li>我们为深入学习开发了许多ASIC。所有这些都有一个相同的目标，即最小化内存访问。<ul>
<li>麻省理工学院(Eyeriss MIT)</li>
<li>DaDiannao</li>
<li>TPU Google (Tensor processing unit)<ul>
<li>它可以用来替换服务器中的磁盘。</li>
<li>每台服务器最多4张卡</li>
<li>这种硬件所消耗的功率比GPU要小得多，芯片的尺寸也更小。</li>
</ul>
</li>
<li>EIE Standford<ul>
<li>By Han at 2016 [et al. ISCA’16]</li>
<li>我们不会保存零权重，也不会对硬件中的数字进行量化。</li>
<li>他说EIE有更好的吞吐量和能源效率。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Part 3: <strong><u>Algorithms for Efficient Training</u></strong><ul>
<li><strong>Parallelization</strong><ul>
<li><strong>Data Parallel</strong> – 并行运行多个输入<ul>
<li>同时运行两个图像！</li>
<li>并行运行多个培训示例</li>
<li>受批量大小限制</li>
<li>渐变必须由主节点应用</li>
</ul>
</li>
<li><strong>Model Parallel</strong><ul>
<li>拆分模型-即网络</li>
<li>按层将模型拆分到多个处理器上</li>
</ul>
</li>
<li>Hyper-Parameter Parallel<ul>
<li>尝试多个并行的替代网络。</li>
<li>易于获得16-64 GPU并行训练一个模型</li>
</ul>
</li>
</ul>
</li>
<li><strong>Mixed Precision</strong> with FP16 and FP32<ul>
<li>我们已经讨论过，如果我们在整个模型中使用16位实数，那么能耗将减少4倍</li>
<li>我们可以使用一个完全16位数字的模型吗？我们可以用混合的FP16和FP32来实现这一点。我们在任何地方都使用16位，但有些地方我们需要FP32</li>
<li>以FP16乘以FP16为例，我们需要FP32</li>
<li>在你训练模型后，你可以成为一个近乎精确的著名模型，如AlexNet和ResNet。</li>
</ul>
</li>
<li><strong>Model Distillation</strong><ul>
<li>问题是我们能否使用一个经过高级（良好）训练的神经网络，并使它们指导学生（新的）神经网络</li>
<li>欲了解更多信息，请参阅Hinton等人。暗知识/神经网络中知识的提取</li>
</ul>
</li>
<li>DSD: 密集稀疏密集训练(Dense-Sparse-Dense Training)<ul>
<li>Han et al. “深度神经网络的密集稀疏密集训练(DSD: Dense-Sparse-Dense Training for Deep Neural Networks)”, ICLR 2017</li>
<li>有更好的规则化</li>
<li>我们的想法是训练模型，我们称之为稠密，然后对其应用修剪，让我们称之为稀疏</li>
<li>DSD产生相同的模型结构，但能找到更好的优化解，达到更好的局部极小值，达到更高的预测精度。</li>
<li>在以上两个步骤之后，我们去连接剩下的连接并再次学习它们（再次密集）。</li>
<li>这大大提高了许多深度学习模型的性能。</li>
</ul>
</li>
</ul>
</li>
<li>Part 4: <strong><u>Hardware for Efficient Training</u></strong><ul>
<li>GPUs for training:<ul>
<li>Nvidia PASCAL GP100 (2016)</li>
<li>Nvidia Volta GV100 (2017)<ul>
<li>可以进行混合精确操作</li>
<li>如此强大。</li>
<li>新的内克尔炸弹(The new neclar bomb)!</li>
</ul>
</li>
</ul>
</li>
<li>Google Announced “Google Cloud TPU” on May 2017!<ul>
<li>云TPU提供高达180兆次的浮点运算来训练和运行机器学习模型.</li>
<li>我们的一个新的大型翻译模型曾经需要一整天的时间来训练32个最好的商用gpu，现在它只需要一个TPU吊舱的八分之一就可以在一个下午训练到同样的精度</li>
</ul>
</li>
</ul>
</li>
<li>我们已经从PC时代==&gt;移动第一时代==&gt;人工智能第一时代</li>
</ul>
<h2 id="16-Adversarial-Examples-and-Adversarial-Training"><a href="#16-Adversarial-Examples-and-Adversarial-Training" class="headerlink" title="16. Adversarial Examples and Adversarial Training"></a>16. Adversarial Examples and Adversarial Training</h2><ul>
<li>什么是对抗性的例子(<strong><u>What are adversarial examples?</u></strong>)<ul>
<li>自2013年以来，深度神经网络已经在<ul>
<li>人脸识别</li>
<li>物体识别</li>
<li>验证码识别<ul>
<li>因为它的准确度比人类高，所以网站试图找到另一种解决方法，而不是验证码。</li>
</ul>
</li>
<li>以及其他任务</li>
</ul>
</li>
<li>2013年以前，如果看到电脑出错，没人会感到惊讶！但是现在存在深度学习，了解问题和原因是非常重要的。</li>
<li>对抗性是深度学习所犯的问题和不寻常的错误。</li>
<li>这个话题直到现在的深度学习能比人类做得越来越好时才成为热门话题</li>
<li>对手方是一个被仔细计算后被错误分类的例子</li>
<li>从人的角度看，在很多情况下，对手形象与原始形象相比并没有太大变化。</li>
<li>近代论文史:<ul>
<li>Biggio <a href="https://link.springer.com/chapter/10.1007/978-3-642-40994-3_25" target="_blank" rel="noopener">2013</a>: fool neural nets.</li>
<li>Szegedy et al 2013: fool ImageNet classifiers imperceptibly</li>
<li>Goodfellow et al <a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">2014</a>: cheap, closed form attack.</li>
</ul>
</li>
<li>所以第一个故事发生在2013年。当塞格迪有一个CNN可以很好的分类图像。<ul>
<li>他想了解更多关于CNN如何改进它的工作。</li>
<li>他给出了一个物体的图像，通过梯度上升，他试图更新图像，使之成为另一个物体。</li>
<li>奇怪的是，他发现从人类的角度来看，结果图像并没有太大变化！</li>
<li>如果你尝试了，你不会通知任何变化，你会认为这是一个错误！但这并不是说，如果你去追求形象，你会发现他们完全不同</li>
</ul>
</li>
<li>这些错误几乎可以在我们研究过的任何深度学习算法中找到<ul>
<li>结果表明，RBF（Radial Basis Network）可以抵抗这种情况</li>
<li>用于密度估计的深层模型可以抵抗这种情况</li>
</ul>
</li>
<li>不仅仅是因为神经网络可以被愚弄<ul>
<li>Linear models<ul>
<li>Logistic regression</li>
<li>Softmax regression</li>
<li>SVMs</li>
</ul>
</li>
<li>Decision trees </li>
<li>Nearest neighbors</li>
</ul>
</li>
</ul>
</li>
<li>为什么会发生敌对(<strong><u>Why do adversarial happen?</u></strong>)<ul>
<li>在试图了解发生了什么的过程中，2016年，他们认为这是由于高维数据案例中的过度拟合模型造成的。<ul>
<li>因为在如此高的维中，我们可以发现一些随机误差。</li>
<li>所以如果我们用另一个参数训练一个模型，它不会犯同样的错误吗?</li>
<li>他们发现那是不对的。模型也出现了同样的错误，所以这并不意味着它是过度拟合的。</li>
</ul>
</li>
<li>在前面提到的实验中发现，问题是由系统性的东西引起的，而不是随机的。<ul>
<li>如果他们给一个例子添加一些向量，它将被错误地分类为任何模型。</li>
</ul>
</li>
<li>也许他们是因为不合身而不是过度合身。</li>
<li>现代深网是非常分段线性的<ul>
<li>Rectified linear unit</li>
<li>Carefully tuned sigmoid  <code># 大多数时候我们都在线性曲线内</code></li>
<li>Maxout</li>
<li>LSTM</li>
</ul>
</li>
<li>参数与输出之间的关系是非线性的，因为它是相乘的，这使得训练神经网络困难，而从输入和输出的线性映射是线性的，而且容易得多。</li>
</ul>
</li>
<li>如何利用对抗性来破坏机器学习系统？(<strong><u>How can adversarial be used to compromise machine learning systems?</u></strong>)<ul>
<li>如果我们正在试验一个NN有多容易被欺骗，我们要确保我们实际上是在愚弄它，而不仅仅是改变输出类，如果我们是攻击者，我们希望对NN（Get hole）采取这种行为。</li>
<li>当我们建立对抗性的例子时，我们使用最大范数来约束扰动。</li>
<li>快速梯度符号法:<ul>
<li>这种方法来自于这样一个事实，即几乎所有的神经网络都使用线性激活（如RELU），这是我们之前说过的假设</li>
<li>任何像素的变化都不能超过一定量的epsilon</li>
<li>快速的方法是用你用来训练网络的代价的梯度，然后用这个梯度的符号乘以epsilon。</li>
<li>Equation:<ul>
<li><code>Xdash = x + epslion * (sign of the gradient)</code></li>
<li>其中Xdash是对抗性示例，x是普通示例</li>
</ul>
</li>
<li>所以只要用符号（方向）和一些epsilon就可以检测到。</li>
</ul>
</li>
<li>一些攻击基于ADAM优化器。</li>
<li>敌对的例子不是随机的噪音！</li>
<li>神经网络训练在某个分布上，并且在该分布中表现良好。但是如果你改变这个分布，神经网络将不能回答正确的答案。他们很容易被愚弄。</li>
<li>深RL也可以被愚弄。</li>
<li>Attack of the weights:<ul>
<li>在线性模型中，我们可以将学习到的权重图像取出来，取图像的符号并将其添加到任何一个例子中，以迫使权重类为真。Andrej Karpathy，“打破ImageNet上的线性分类器”</li>
</ul>
</li>
<li>事实证明，有些利纳的型号表现很好（我们很难从他们那里得到广告）<ul>
<li>特别是浅层RBFs网络采用快速梯度符号法抵抗对抗性扰动<ul>
<li>问题是径向基函数在数据集上没有得到太多的精确性，因为它只是一个浅层模型，如果你试图让这个模型更深入，几乎所有层的梯度都将变为零。</li>
<li>rbf神经网络即使在批处理范数下也很难训练。算法。</li>
<li>伊恩认为，如果我们有一个更好的超参数或一个更好的梯度下降优化算法，我们将能够训练径向基函数，解决对抗性问题</li>
</ul>
</li>
</ul>
</li>
<li>我们也可以用另一个模型来愚弄当前的模型。例如使用支持向量机来愚弄深层神经网络。<ul>
<li>欲了解更多详情，请参考该报：“Papernot 2016”</li>
</ul>
</li>
<li>可转移攻击(Transferability Attack)<ol>
<li>具有未知权重的目标模型、机器学习算法、训练集；可能不可微</li>
<li>使用来自您的输入从这个模型中生成训练集，将它们发送到模型，然后从模型中获取输出</li>
<li>训练你自己的模特。“遵循Papernot 2016的一些表格”</li>
<li>在你的模型上创建一个对抗性的例子。</li>
<li>针对目标模型使用这些示例。</li>
<li>你几乎有可能得到好的结果并愚弄这个目标！</li>
</ol>
</li>
<li>在可转移性攻击中，通过100%的概率增加你的欺骗网络，你可以使不止一个模型可能是五个模型，然后应用它们。 “(Liu et al, 2016)”</li>
<li>对抗性的例子对人脑也是有用的！比如那些能骗过你眼睛的图像。他们在网上很多</li>
<li>在实践中，一些研究愚弄了真实的模型(MetaMind, Amazon, Google)</li>
<li>有人在facebook上上传了一些微扰，facebook被愚弄了</li>
</ul>
</li>
<li>防御措施是什么(<strong><u>What are the defenses?</u></strong>)<ul>
<li>伊恩尝试的很多防御措施都失败了，真的很糟糕！包括:<ul>
<li>Ensembles</li>
<li>Weight decay</li>
<li>Dropout</li>
<li>Adding noise at train time or at test time</li>
<li>Removing perturbation with an autoencoder </li>
<li>Generative modeling</li>
</ul>
</li>
<li>通用逼近定理<ul>
<li>无论我们希望我们的分类函数有一个足够大的神经网络可以使它成为任何形状。</li>
<li>我们可以训练一个神经网络来探测敌方</li>
</ul>
</li>
<li>线性模型和KNN比NN更容易被愚弄。神经网络实际上比其他模型更安全。在任何机器学习模型的对抗性例子中，对抗性训练的神经网络具有最佳的经验成功率<ul>
<li>深层神经网络可以用非线性函数进行训练，但我们只需要一种好的优化技术或使用像“RELU”这样的线性激活器来解决问题</li>
</ul>
</li>
</ul>
</li>
<li>在没有对手的情况下，如何使用对抗性示例来改进机器学习(<strong><u>How to use adversarial examples to improve machine learning, even when there is no adversary?</u></strong>)<ul>
<li>通用工程机械（基于模型的优化）        <code>#被伊恩称为通用工程机械</code><ul>
<li>For example:<ul>
<li>想象一下我们想要设计一辆速度快的汽车</li>
<li>我们训练了一个神经网络来查看汽车的设计图，并告诉我们蓝图是否能使我们成为一辆快车。</li>
<li>T这里的想法是优化网络的输入，使输出达到最大，这可以给我们一个最好的汽车蓝图！</li>
</ul>
</li>
<li>通过寻找使模型的预测性能最大化的输入来进行新的发明</li>
<li>现在，通过使用敌对的例子，我们只是得到了我们不喜欢的结果，但是如果我们解决了这个问题，我们就可以拥有最快的汽车、最好的GPU、最好的椅子、新药…。。</li>
</ul>
</li>
<li>整个对抗是一个活跃的研究领域，尤其是网络防御</li>
</ul>
</li>
<li>Conclusion<ul>
<li>进攻很容易</li>
<li>防守很难</li>
<li>对抗训练提供正规化和半监督学习</li>
<li>域外输入问题是基于模型优化的瓶颈问题</li>
</ul>
</li>
<li>有一个Github代码可以让你通过代码（构建在tensorflow之上）来了解敌方的一切:<ul>
<li>一个对抗性的示例库，用于构建攻击、构建防御和基准测试: <a href="https://github.com/tensorflow/cleverhans" target="_blank" rel="noopener">https://github.com/tensorflow/cleverhans</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CS231N</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N</title>
    <url>/2020/07/29/CS231N/</url>
    <content><![CDATA[<p>斯坦福大学深度学习cv课程笔记<br><a id="more"></a></p>
<p><a href="https://githubzhangshuai.github.io/2020/08/03/CS231N%E7%BF%BB%E8%AF%91%E7%89%88/#more" target="_blank" rel="noopener">翻译版</a></p>
<h1 id="Standford-CS231n-2017-Summary"><a href="#Standford-CS231n-2017-Summary" class="headerlink" title="Standford CS231n 2017 Summary"></a>Standford CS231n 2017 Summary</h1><p>After watching all the videos of the famous Standford’s <a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">CS231n</a> course that took place in 2017, i decided to take summary of the whole course to help me to remember and to anyone who would like to know about it. I’ve skipped some contents in some lectures as it wasn’t important to me.</p>
<h2 id="Table-of-contents"><a href="#Table-of-contents" class="headerlink" title="Table of contents"></a>Table of contents</h2><ul>
<li><a href="#standford-cs231n-2017-summary">Standford CS231n 2017 Summary</a><ul>
<li><a href="#table-of-contents">Table of contents</a></li>
<li><a href="#course-info">Course Info</a></li>
<li><a href="#01-introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</a></li>
<li><a href="#02-image-classification">02. Image classification</a></li>
<li><a href="#03-loss-function-and-optimization">03. Loss function and optimization</a></li>
<li><a href="#04-introduction-to-neural-network">04. Introduction to Neural network</a></li>
<li><a href="#05-convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</a></li>
<li><a href="#06-training-neural-networks-i">06. Training neural networks I</a></li>
<li><a href="#07-training-neural-networks-ii">07. Training neural networks II</a></li>
<li><a href="#08-deep-learning-software">08. Deep learning software</a></li>
<li><a href="#09-cnn-architectures">09. CNN architectures</a></li>
<li><a href="#10-recurrent-neural-networks">10. Recurrent Neural networks</a></li>
<li><a href="#11-detection-and-segmentation">11. Detection and Segmentation</a></li>
<li><a href="#12-visualizing-and-understanding">12. Visualizing and Understanding</a></li>
<li><a href="#13-generative-models">13. Generative models</a></li>
<li><a href="#14-deep-reinforcement-learning">14. Deep reinforcement learning</a></li>
<li><a href="#15-efficient-methods-and-hardware-for-deep-learning">15. Efficient Methods and Hardware for Deep Learning</a></li>
<li><a href="#16-adversarial-examples-and-adversarial-training">16. Adversarial Examples and Adversarial Training</a></li>
</ul>
</li>
</ul>
<h2 id="Course-Info"><a href="#Course-Info" class="headerlink" title="Course Info"></a>Course Info</h2><ul>
<li><p>Website: <a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">http://cs231n.stanford.edu/</a></p>
</li>
<li><p>Lectures link: <a href="https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk</a></p>
</li>
<li><p>Full syllabus link: <a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">http://cs231n.stanford.edu/syllabus.html</a></p>
</li>
<li><p>Assignments solutions: <a href="https://github.com/Burton2000/CS231n-2017" target="_blank" rel="noopener">https://github.com/Burton2000/CS231n-2017</a></p>
</li>
<li><p>Number of lectures: <strong>16</strong></p>
</li>
<li><p>Course description:</p>
<ul>
<li><blockquote>
<p>Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the <a href="http://image-net.org/challenges/LSVRC/2014/index" target="_blank" rel="noopener">ImageNet Challenge</a>.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="01-Introduction-to-CNN-for-visual-recognition"><a href="#01-Introduction-to-CNN-for-visual-recognition" class="headerlink" title="01. Introduction to CNN for visual recognition"></a>01. Introduction to CNN for visual recognition</h2><ul>
<li>A brief history of Computer vision starting from the late 1960s to 2017.</li>
<li>Computer vision problems includes image classification, object localization, object detection, and scene understanding.</li>
<li><a href="http://www.image-net.org/" target="_blank" rel="noopener">Imagenet</a> is one of the biggest datasets in image classification available right now.</li>
<li>Starting 2012 in the Imagenet competition, CNN (Convolutional neural networks) is always winning.</li>
<li>CNN actually has been invented in 1997 by <a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">Yann Lecun</a>.</li>
</ul>
<h2 id="02-Image-classification"><a href="#02-Image-classification" class="headerlink" title="02. Image classification"></a>02. Image classification</h2><ul>
<li>Image classification problem has a lot of challenges like illumination and viewpoints.<ul>
<li><img src="/2020/07/29/CS231N/39.jpeg" alt></li>
</ul>
</li>
<li>An image classification algorithm can be solved with <strong>K nearest neighborhood</strong> (KNN) but it can poorly solve the problem. The properties of KNN are:<ul>
<li>Hyperparameters of KNN are: k and the distance measure</li>
<li>K is the number of neighbors we are comparing to.</li>
<li>Distance measures include:<ul>
<li>L2 distance (Euclidean distance)<ul>
<li>Best for non coordinate points</li>
</ul>
</li>
<li>L1 distance (Manhattan distance)<ul>
<li>Best for coordinate points</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Hyperparameters can be optimized using Cross-validation as following (In our case we are trying tp predict K):<ol>
<li>Split your dataset into <code>f</code> folds.</li>
<li>Given predicted hyperparameters:<ul>
<li>Train your algorithm with f-1 folds and test it with the remain flood. and repeat this with every fold.</li>
</ul>
</li>
<li>Choose the hyperparameters that gives the best training values (Average over all folds)</li>
</ol>
</li>
<li><strong>Linear SVM</strong> classifier is an option for solving the image classification problem, but the curse of dimensions makes it stop improving at some point.</li>
<li><strong>Logistic regression</strong> is a also a solution for image classification problem, but image classification problem is non linear!</li>
<li>Linear classifiers has to run the following equation: <code>Y = wX + b</code> <ul>
<li>shape of <code>w</code> is the same as <code>x</code> and shape of <code>b</code> is 1.</li>
</ul>
</li>
<li>We can add 1 to X vector and remove the bias so that: <code>Y = wX</code><ul>
<li>shape of <code>x</code> is <code>oldX+1</code> and <code>w</code> is the same as <code>x</code></li>
</ul>
</li>
<li>We need to know how can we get <code>w</code>‘s and <code>b</code>‘s that makes the classifier runs at best.</li>
</ul>
<h2 id="03-Loss-function-and-optimization"><a href="#03-Loss-function-and-optimization" class="headerlink" title="03. Loss function and optimization"></a>03. Loss function and optimization</h2><ul>
<li><p>In the last section we talked about linear classifier but we didn’t discussed how we could <strong>train</strong> the parameters of that model to get best <code>w</code>‘s and <code>b</code>‘s.</p>
</li>
<li><p>We need a loss function to measure how good or bad our current parameters.</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Loss = L[i] =(f(X[i],W),Y[i])</span><br><span class="line">Loss_for_all = <span class="number">1</span>/N * Sum(Li(f(X[i],W),Y[i]))      <span class="comment"># Indicates the average</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Then we find a way to minimize the loss function given some parameters. This is called <strong>optimization</strong>.</p>
</li>
<li><p>Loss function for a linear <strong>SVM</strong> classifier:</p>
<ul>
<li><code>L[i] = Sum where all classes except the predicted class (max(0, s[j] - s[y[i]] + 1))</code></li>
<li>We call this <strong><em>the hinge loss</em></strong>.</li>
<li>Loss function means we are happy if the best prediction are the same as the true value other wise we give an error with 1 margin.</li>
<li>Example:<ul>
<li><img src="/2020/07/29/CS231N/40.jpg" alt></li>
<li>Given this example we want to compute the loss of this image.</li>
<li><code>L = max (0, 437.9 - (-96.8) + 1) + max(0, 61.95 - (-96.8) + 1) = max(0, 535.7) + max(0, 159.75) = 695.45</code></li>
<li>Final loss is 695.45 which is big and reflects that the cat score needs to be the best over all classes as its the lowest value now. We need to minimize that loss.</li>
</ul>
</li>
<li>Its OK for the margin to be 1. But its a hyperparameter too.</li>
</ul>
</li>
<li><p>If your loss function gives you zero, are this value is the same value for your parameter? No there are a lot of parameters that can give you best score.</p>
</li>
<li><p>You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM). that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better.</p>
</li>
<li><p>We add <strong>regularization</strong> for the loss function so that the discovered model don’t overfit the data.</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Loss = L = <span class="number">1</span>/N * Sum(Li(f(X[i],W),Y[i])) + <span class="keyword">lambda</span> * R(W)</span><br></pre></td></tr></table></figure>
<ul>
<li>Where <code>R</code> is the regularizer, and <code>lambda</code> is the regularization term.</li>
</ul>
</li>
<li><p>There are different regularizations techniques:</p>
<ul>
<li>| Regularizer           | Equation                            | Comments               |<br>| ——————————- | —————————————————- | ——————————— |<br>| L2                    | <code>R(W) = Sum(W^2)</code>                   | Sum all the W squared  |<br>| L1                    | <code>R(W) = Sum(lWl)</code>                   | Sum of all Ws with abs |<br>| Elastic net (L1 + L2) | <code>R(W) = beta * Sum(W^2) + Sum(lWl)</code> |                        |<br>| Dropout               |                                     | No Equation            |</li>
</ul>
</li>
<li><p>Regularization prefers smaller <code>W</code>s over big <code>W</code>s.</p>
</li>
<li><p>Regularizations is called weight decay. biases should not included in regularization.</p>
</li>
<li><p>Softmax loss (Like linear regression but works for more than 2 classes):</p>
<ul>
<li><p>Softmax function:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A[L] = e^(score[L]) / sum(e^(score[L]), NoOfClasses)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Sum of the vector should be 1.</p>
</li>
<li><p>Softmax loss:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Loss = -logP(Y = y[i]|X = x[i])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Log of the probability of the good class. We want it to be near 1 thats why we added a minus.</p>
</li>
<li><p>Softmax loss is called cross-entropy loss.</p>
</li>
</ul>
</li>
<li><p>Consider this numerical problem when you are computing Softmax:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example with 3 classes and each having large scores</span></span><br><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) </span><br><span class="line"><span class="comment"># Bad: Numeric problem, potential blowup</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># instead: first shift the values of f so that the highest number is 0:</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line"><span class="comment"># safe to do, gives the correct answer</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>Optimization</strong>:</p>
<ul>
<li>How we can optimize loss functions we discussed?</li>
<li>Strategy one:<ul>
<li>Get a random parameters and try all of them on the loss and get the best loss. But its a bad idea.</li>
</ul>
</li>
<li><p>Strategy two:</p>
<ul>
<li><p>Follow the slope.</p>
<ul>
<li><img src="/2020/07/29/CS231N/41.png" alt></li>
<li>Image <a href="https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png" target="_blank" rel="noopener">source</a>.</li>
</ul>
</li>
<li><p>Our goal is to compute the gradient of each parameter we have.</p>
<ul>
<li><strong>Numerical gradient</strong>: Approximate, slow, easy to write.   (But its useful in debugging.)</li>
<li><strong>Analytic gradient</strong>: Exact, Fast, Error-prone.   (Always used in practice)</li>
</ul>
</li>
<li><p>After we compute the gradient of our parameters, we compute the gradient descent:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = W - learning_rate * W_grad</span><br></pre></td></tr></table></figure>
</li>
<li><p>learning_rate is so important hyper parameter you should get the best value of it first of all the hyperparameters.</p>
</li>
<li><p>stochastic gradient descent:</p>
<ul>
<li>Instead of using all the data, use a mini batch of examples (32/64/128 are commonly used) for faster results.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="04-Introduction-to-Neural-network"><a href="#04-Introduction-to-Neural-network" class="headerlink" title="04. Introduction to Neural network"></a>04. Introduction to Neural network</h2><ul>
<li><p>Computing the analytic gradient for arbitrary complex functions:</p>
<ul>
<li><p>What is a Computational graphs?</p>
<ul>
<li>Used to represent any function. with nodes.</li>
<li>Using Computational graphs can easy lead us to use a technique that called back-propagation. Even with complex models like CNN and RNN.</li>
</ul>
</li>
<li><p>Back-propagation simple example:</p>
<ul>
<li><p>Suppose we have <code>f(x,y,z) = (x+y)z</code></p>
</li>
<li><p>Then graph can be represented this way:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X         </span><br><span class="line">  \</span><br><span class="line">   (+)--&gt; q ---(*)--&gt; f</span><br><span class="line">  &#x2F;           &#x2F;</span><br><span class="line">Y            &#x2F;</span><br><span class="line">            &#x2F;</span><br><span class="line">           &#x2F;</span><br><span class="line">Z---------&#x2F;</span><br></pre></td></tr></table></figure>
</li>
<li><p>We made an intermediate variable <code>q</code>  to hold the values of <code>x+y</code></p>
</li>
<li><p>Then we have:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q = (x+y)              <span class="comment"># dq/dx = 1 , dq/dy = 1</span></span><br><span class="line">f = qz                 <span class="comment"># df/dq = z , df/dz = q</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Then:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df/dq = z</span><br><span class="line">df/dz = q</span><br><span class="line">df/dx = df/dq * dq/dx = z * <span class="number">1</span> = z       <span class="comment"># Chain rule</span></span><br><span class="line">df/dy = df/dq * dq/dy = z * <span class="number">1</span> = z       <span class="comment"># Chain rule</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>So in the Computational graphs, we call each operation <code>f</code>. For each <code>f</code> we calculate the local gradient before we go on back propagation and then we compute the gradients in respect of the loss function using the chain rule.</p>
</li>
<li><p>In the Computational graphs you can split each operation to as simple as you want but the nodes will be a lot. if you want the nodes to be smaller be sure that you can compute the gradient of this node.</p>
</li>
<li><p>A bigger example:</p>
<ul>
<li><img src="/2020/07/29/CS231N/01.png" alt></li>
<li>Hint: the back propagation of two nodes going to one node from the back is by adding the two derivatives.</li>
</ul>
</li>
<li><p>Modularized implementation: forward/ backward API (example multiply code):</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultuplyGate</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  x,y are scalars</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z = x*y</span><br><span class="line">    self.x = x  <span class="comment"># Cache</span></span><br><span class="line">    self.y = y	<span class="comment"># Cache</span></span><br><span class="line">    <span class="comment"># We cache x and y because we know that the derivatives contains them.</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(dz)</span>:</span></span><br><span class="line">    dx = self.y * dz         <span class="comment">#self.y is dx</span></span><br><span class="line">    dy = self.x * dz</span><br><span class="line">    <span class="keyword">return</span> [dx, dy]</span><br></pre></td></tr></table></figure>
</li>
<li><p>If you look at a deep learning framework you will find it follow the Modularized implementation where each class has a definition for forward and backward. For example:</p>
<ul>
<li>Multiplication</li>
<li>Max</li>
<li>Plus</li>
<li>Minus</li>
<li>Sigmoid</li>
<li>Convolution</li>
</ul>
</li>
</ul>
</li>
<li><p>So to define neural network as a function:</p>
<ul>
<li>(Before) Linear score function: <code>f = Wx</code></li>
<li>(Now) 2-layer neural network:    <code>f = W2*max(0,W1*x)</code> <ul>
<li>Where max is the RELU non linear function</li>
</ul>
</li>
<li>(Now) 3-layer neural network:    <code>f = W3*max(0,W2*max(0,W1*x)</code></li>
<li>And so on..</li>
</ul>
</li>
<li><p>Neural networks is a stack of some simple operation that forms complex operations.</p>
</li>
</ul>
<h2 id="05-Convolutional-neural-networks-CNNs"><a href="#05-Convolutional-neural-networks-CNNs" class="headerlink" title="05. Convolutional neural networks (CNNs)"></a>05. Convolutional neural networks (CNNs)</h2><ul>
<li>Neural networks history:<ul>
<li>First perceptron machine was developed by Frank Rosenblatt in 1957. It was used to recognize letters of the alphabet. Back propagation wasn’t developed yet.</li>
<li>Multilayer perceptron was developed in 1960 by Adaline/Madaline. Back propagation wasn’t developed yet.</li>
<li>Back propagation was developed in 1986 by Rumeelhart.</li>
<li>There was a period which nothing new was happening with NN. Cause of the limited computing resources and data.</li>
<li>In <a href="www.cs.toronto.edu/~fritz/absps/netflix.pdf">2006</a> Hinton released a paper that shows that we can train a deep neural network using Restricted Boltzmann machines to initialize the weights then back propagation.</li>
<li>The first strong results was in 2012 by Hinton in <a href="http://ieeexplore.ieee.org/document/6296526/" target="_blank" rel="noopener">speech recognition</a>. And the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Alexnet</a> “Convolutional neural networks” that wins the image net in 2012 also by Hinton’s team.</li>
<li>After that NN is widely used in various applications.</li>
</ul>
</li>
<li>Convolutional neural networks history:<ul>
<li>Hubel &amp; Wisel in 1959 to 1968 experiments on cats cortex found that there are a topographical mapping in the cortex and that the neurons has hireical organization from simple to complex.</li>
<li>In 1998, Yann Lecun gives the paper <a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">Gradient-based learning applied to document recognition</a> that introduced the Convolutional neural networks. It was good for recognizing zip letters but couldn’t run on a more complex examples.</li>
<li>In 2012 <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a> used the same Yan Lecun architecture and won the image net challenge. The difference from 1998 that now we have a large data sets that can be used also the power of the GPUs solved a lot of performance problems.</li>
<li>Starting from 2012 there are CNN that are used for various tasks (Here are some applications):<ul>
<li>Image classification.</li>
<li>Image retrieval.<ul>
<li>Extracting features using a NN and then do a similarity matching.</li>
</ul>
</li>
<li>Object detection.</li>
<li>Segmentation.<ul>
<li>Each pixel in an image takes a label.</li>
</ul>
</li>
<li>Face recognition.</li>
<li>Pose recognition.</li>
<li>Medical images.</li>
<li>Playing Atari games with reinforcement learning.</li>
<li>Galaxies classification.</li>
<li>Street signs recognition.</li>
<li>Image captioning.</li>
<li>Deep dream.</li>
</ul>
</li>
</ul>
</li>
<li>ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture.</li>
<li>There are a few distinct types of Layers in ConvNet (e.g. CONV/FC/RELU/POOL are by far the most popular)</li>
<li>Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)</li>
<li>Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)</li>
<li>How Convolutional neural networks works?<ul>
<li>A fully connected layer is a layer in which all the neurons is connected. Sometimes we call it a dense layer.<ul>
<li>If input shape is <code>(X, M)</code> the weighs shape for this will be <code>(NoOfHiddenNeurons, X)</code></li>
</ul>
</li>
<li>Convolution layer is a layer in which we will keep the structure of the input by a filter that goes through all the image.<ul>
<li>We do this with dot product: <code>W.T*X + b</code>. This equation uses the broadcasting technique.</li>
<li>So we need to get the values of <code>W</code> and <code>b</code></li>
<li>We usually deal with the filter (<code>W</code>) as a vector not a matrix.</li>
</ul>
</li>
<li>We call output of the convolution activation map. We need to have multiple activation map.<ul>
<li>Example if we have 6 filters, here are the shapes:<ul>
<li>Input image                        <code>(32,32,3)</code></li>
<li>filter size                              <code>(5,5,3)</code><ul>
<li>We apply 6 filters. The depth must be three because the input map has depth of three.</li>
</ul>
</li>
<li>Output of Conv.                 <code>(28,28,6)</code> <ul>
<li>if one filter it will be   <code>(28,28,1)</code></li>
</ul>
</li>
<li>After RELU                          <code>(28,28,6)</code> </li>
<li>Another filter                     <code>(5,5,6)</code></li>
<li>Output of Conv.                 <code>(24,24,10)</code></li>
</ul>
</li>
</ul>
</li>
<li>It turns out that convNets learns in the first layers the low features and then the mid-level features and then the high level features.</li>
<li>After the Convnets we can have a linear classifier for a classification task.</li>
<li>In Convolutional neural networks usually we have some (Conv ==&gt; Relu)s and then we apply a pool operation to downsample the size of the activation.</li>
</ul>
</li>
<li><p>What is stride when we are doing convolution:</p>
<ul>
<li>While doing a conv layer we have many choices to make regarding the stride of which we will take. I will explain this by examples.</li>
<li>Stride is skipping while sliding. By default its 1.</li>
<li>Given a matrix with shape of <code>(7,7)</code> and a filter with shape <code>(3,3)</code>:<ul>
<li>If stride is <code>1</code> then the output shape will be <code>(5,5)</code>              <code># 2 are dropped</code></li>
<li>If stride is <code>2</code> then the output shape will be <code>(3,3)</code>             <code># 4 are dropped</code></li>
<li>If stride is <code>3</code> it doesn’t work.</li>
</ul>
</li>
<li>A general formula would be <code>((N-F)/stride +1)</code><ul>
<li>If stride is <code>1</code> then <code>O = ((7-3)/1)+1 = 4 + 1 = 5</code></li>
<li>If stride is <code>2</code> then <code>O = ((7-3)/2)+1 = 2 + 1 = 3</code></li>
<li>If stride is <code>3</code> then <code>O = ((7-3)/3)+1 = 1.33 + 1 = 2.33</code>        <code># doesn&#39;t work</code></li>
</ul>
</li>
</ul>
</li>
<li><p>In practice its common to zero pad the border.   <code># Padding from both sides.</code></p>
<ul>
<li>Give a stride of <code>1</code> its common to pad to this equation:  <code>(F-1)/2</code> where F is the filter size<ul>
<li>Example <code>F = 3</code> ==&gt; Zero pad with <code>1</code></li>
<li>Example <code>F = 5</code> ==&gt; Zero pad with <code>2</code></li>
</ul>
</li>
<li>If we pad this way we call this same convolution.</li>
<li>Adding zeros gives another features to the edges thats why there are different padding techniques like padding the corners not zeros but in practice zeros works!</li>
<li>We do this to maintain our full size of the input. If we didn’t do that the input will be shrinking too fast and we will lose a lot of data.</li>
</ul>
</li>
<li>Example:<ul>
<li>If we have input of shape <code>(32,32,3)</code> and ten filters with shape is <code>(5,5)</code> with stride <code>1</code> and pad <code>2</code><ul>
<li>Output size will be <code>(32,32,10)</code>                       <code># We maintain the size.</code></li>
</ul>
</li>
<li>Size of parameters per filter <code>= 5*5*3 + 1 = 76</code></li>
<li>All parameters <code>= 76 * 10 = 76</code></li>
</ul>
</li>
<li>Number of filters is usually common to be to the power of 2.           <code># To vectorize well.</code></li>
<li>So here are the parameters for the Conv layer:<ul>
<li>Number of filters K.<ul>
<li>Usually a power of 2.</li>
</ul>
</li>
<li>Spatial content size F.<ul>
<li>3,5,7 ….</li>
</ul>
</li>
<li>The stride S. <ul>
<li>Usually 1 or 2        (If the stride is big there will be a downsampling but different of pooling) </li>
</ul>
</li>
<li>Amount of Padding<ul>
<li>If we want the input shape to be as the output shape, based on the F if 3 its 1, if F is 5 the 2 and so on.</li>
</ul>
</li>
</ul>
</li>
<li>Pooling makes the representation smaller and more manageable.</li>
<li>Pooling Operates over each activation map independently.</li>
<li>Example of pooling is the maxpooling.<ul>
<li>Parameters of max pooling is the size of the filter and the stride”<ul>
<li>Example <code>2x2</code> with stride <code>2</code>                     <code># Usually the two parameters are the same 2 , 2</code></li>
</ul>
</li>
</ul>
</li>
<li>Also example of pooling is average pooling.<ul>
<li>In this case it might be learnable.</li>
</ul>
</li>
</ul>
<h2 id="06-Training-neural-networks-I"><a href="#06-Training-neural-networks-I" class="headerlink" title="06. Training neural networks I"></a>06. Training neural networks I</h2><ul>
<li><p>As a revision here are the Mini batch stochastic gradient descent algorithm steps:</p>
<ul>
<li>Loop:<ol>
<li>Sample a batch of data.</li>
<li>Forward prop it through the graph (network) and get loss.</li>
<li>Backprop to calculate the gradients.</li>
<li>Update the parameters using the gradients.</li>
</ol>
</li>
</ul>
</li>
<li><p>Activation functions:</p>
<ul>
<li><p>Different choices for activation function includes Sigmoid, tanh, RELU, Leaky RELU, Maxout, and ELU.</p>
</li>
<li><p><img src="/2020/07/29/CS231N/42.png" alt></p>
</li>
<li><p>Sigmoid:</p>
<ul>
<li>Squashes the numbers between [0,1]</li>
<li>Used as a firing rate like human brains.</li>
<li><code>Sigmoid(x) = 1 / (1 + e^-x)</code></li>
<li>Problems with sigmoid:<ul>
<li>big values neurons <strong><em>kill</em></strong> the gradients.<ul>
<li>Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.</li>
</ul>
</li>
<li>Not Zero-centered.<ul>
<li>Didn’t produce zero-mean data.</li>
</ul>
</li>
<li><code>exp()</code> is a bit compute expensive.<ul>
<li>just to mention. We have a more complex operations in deep learning like convolution.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Tanh:</p>
<ul>
<li>Squashes the numbers between [-1,1]</li>
<li>Zero centered.</li>
<li>Still big values neurons “kill” the gradients.</li>
<li><code>Tanh(x)</code> is the equation.</li>
<li>Proposed by Yann Lecun in 1991.</li>
</ul>
</li>
<li><p>RELU (Rectified linear unit):</p>
<ul>
<li><code>RELU(x) = max(0,x)</code></li>
<li>Doesn’t kill the gradients.<ul>
<li>Only small values that are killed. Killed the gradient in the half</li>
</ul>
</li>
<li>Computationally efficient.</li>
<li>Converges much faster than Sigmoid and Tanh <code>(6x)</code></li>
<li>More biologically plausible than sigmoid.</li>
<li>Proposed by Alex Krizhevsky in 2012 Toronto university. (AlexNet)</li>
<li>Problems:<ul>
<li>Not zero centered.</li>
</ul>
</li>
<li>If weights aren’t initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.</li>
<li>To solve the issue mentioned above, people might initialize all the biases by 0.01</li>
</ul>
</li>
<li><p>Leaky RELU:</p>
<ul>
<li><code>leaky_RELU(x) = max(0.01x,x)</code></li>
<li>Doesn’t kill the gradients from both sides.</li>
<li>Computationally efficient.</li>
<li>Converges much faster than Sigmoid and Tanh (6x)</li>
<li>Will not die.</li>
<li>PRELU is placing the 0.01 by a variable alpha which is learned as a parameter.</li>
</ul>
</li>
<li><p>Exponential linear units (ELU):</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ELU(x) &#x3D; &#123; x if x &gt; 0</span><br><span class="line">		   alpah *(exp(x) -1)	if x &lt;&#x3D; 0</span><br><span class="line">       # alpah are a learning parameter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>It has all the benefits of RELU</p>
</li>
<li><p>Closer to zero mean outputs and adds some robustness to noise.</p>
</li>
<li><p>problems</p>
<ul>
<li><code>exp()</code> is a bit compute expensive. </li>
</ul>
</li>
</ul>
</li>
<li><p>Maxout activations:</p>
<ul>
<li><code>maxout(x) = max(w1.T*x + b1, w2.T*x + b2)</code></li>
<li>Generalizes RELU and Leaky RELU</li>
<li>Doesn’t die!</li>
<li>Problems:<ul>
<li>doubles the number of parameters per neuron</li>
</ul>
</li>
</ul>
</li>
<li><p>In practice:</p>
<ul>
<li>Use RELU. Be careful for your learning rates.</li>
<li>Try out Leaky RELU/Maxout/ELU</li>
<li>Try out tanh but don’t expect much.</li>
<li>Don’t use sigmoid!</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Data preprocessing</strong>:</p>
<ul>
<li><p>Normalize the data:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Zero centered data. (Calculate the mean for every input).</span></span><br><span class="line"><span class="comment"># On of the reasons we do this is because we need data to be between </span></span><br><span class="line"><span class="comment"># positive and negative and not all the be negative or positive. </span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then apply the standard deviation. Hint: in images we don't do this.</span></span><br><span class="line">X /= np.std(X, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>To normalize images:</p>
<ul>
<li>Subtract the mean image (E.g. Alexnet)<ul>
<li>Mean image shape is the same as the input images.</li>
</ul>
</li>
<li>Or Subtract per-channel mean <ul>
<li>Means calculate the mean for each channel of all images. Shape is 3 (3 channels)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Weight initialization</strong>:</p>
<ul>
<li><p>What happened when initialize all Ws with zeros?</p>
<ul>
<li>All the neurons will do exactly the same thing. They will have the same gradient and they will have the same update.</li>
<li>So if W’s of a specific layer is equal the thing described happened</li>
</ul>
</li>
<li><p>First idea is to initialize the w’s with small random numbers:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.rand(D, H)</span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>The standard deviations is going to zero in deeper networks. and the gradient will vanish sooner in deep networks.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = <span class="number">1</span> * np.random.rand(D, H) </span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>The network will explode with big numbers!</p>
</li>
</ul>
</li>
<li><p><strong><em>Xavier initialization</em></strong>:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = np.random.rand(<span class="keyword">in</span>, out) / np.sqrt(<span class="keyword">in</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>It works because we want the variance of the input to be as the variance of the output.</p>
</li>
<li><p>But it has an issue, It breaks when you are using RELU.</p>
</li>
</ul>
</li>
<li><p><strong><em>He initialization</em></strong> (Solution for the RELU issue):</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = np.random.rand(<span class="keyword">in</span>, out) / np.sqrt(<span class="keyword">in</span>/<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Solves the issue with RELU. Its recommended when you are using RELU</li>
</ul>
</li>
<li><p>Proper initialization is an active area of research.</p>
</li>
</ul>
</li>
<li><p><strong>Batch normalization</strong>:</p>
<ul>
<li>is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance.</li>
<li>It speeds up the training. You want to do this a lot.<ul>
<li>Made by Sergey Ioffe and Christian Szegedy at 2015.</li>
</ul>
</li>
<li>We make a Gaussian activations in each layer. by calculating the mean and the variance.</li>
<li>Usually inserted after (fully connected or Convolutional layers) and (before nonlinearity).</li>
<li>Steps (For each output of a layer)<ol>
<li>First we compute the mean and variance^2 of the batch for each feature.</li>
<li>We normalize by subtracting the mean and dividing by square root of (variance^2 + epsilon)<ul>
<li>epsilon to not divide by zero</li>
</ul>
</li>
<li>Then we make a scale and shift variables: <code>Result = gamma * normalizedX + beta</code>  <ul>
<li>gamma and beta are learnable parameters.</li>
<li>it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.”</li>
<li>Hey shift and scale by what you want not just the mean and variance!</li>
</ul>
</li>
</ol>
</li>
<li>The algorithm makes each layer flexible (It chooses which distribution it wants)</li>
<li>We initialize the BatchNorm Parameters to transform the input to zero mean/unit variance distributions but during training they can learn that any other distribution might be better.</li>
<li>During the running of the training we need to calculate the globalMean and globalVariance for each layer by using weighted average.</li>
<li><u>Benefits of Batch Normalization</u>:<ul>
<li>Networks train faster.</li>
<li>Allows higher learning rates.</li>
<li>helps reduce the sensitivity to the initial starting weights.</li>
<li>Makes more activation functions viable.</li>
<li>Provides some regularization.<ul>
<li>Because we are calculating mean and variance for each batch that gives a slight regularization effect.</li>
</ul>
</li>
</ul>
</li>
<li>In conv layers, we will have one variance and one mean per activation map.</li>
<li>Batch normalization have worked best for CONV and regular deep NN, But for recurrent NN and reinforcement learning its still an active research area.<ul>
<li>Its challengey in reinforcement learning because the batch is small.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Baby sitting the learning process</strong></p>
<ol>
<li>Preprocessing of data.</li>
<li>Choose the architecture.</li>
<li>Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.</li>
<li>Add regularization, the loss should go up!</li>
<li>Disable the regularization again and take a small number of data and try to train the loss and reach zero loss.<ul>
<li>You should overfit perfectly for small datasets.</li>
</ul>
</li>
<li>Take your full training data, and small regularization then try some value of learning rate.<ul>
<li>If loss is barely changing, then the learning rate is small.</li>
<li>If you got <code>NAN</code> then your NN exploded and your learning rate is high.</li>
<li>Get your learning rate range by trying the min value (That can change) and the max value that doesn’t explode the network.</li>
</ul>
</li>
<li>Do Hyperparameters optimization to get the best hyperparameters values.</li>
</ol>
</li>
<li><p>Hyperparameter Optimization</p>
<ul>
<li>Try Cross validation strategy.<ul>
<li>Run with a few ephocs, and try to optimize the ranges.</li>
</ul>
</li>
<li>Its best to optimize in log space.</li>
<li>Adjust your ranges and try again.</li>
<li>Its better to try random search instead of grid searches (In log space)</li>
</ul>
</li>
</ul>
<h2 id="07-Training-neural-networks-II"><a href="#07-Training-neural-networks-II" class="headerlink" title="07. Training neural networks II"></a>07. Training neural networks II</h2><ul>
<li><p><strong>Optimization algorithms</strong>:</p>
<ul>
<li><p>Problems with stochastic gradient descent:</p>
<ul>
<li>if loss quickly in one direction and slowly in another (For only two variables), you will get very slow progress along shallow dimension, jitter along steep direction. Our NN will have a lot of parameters then the problem will be more.</li>
<li>Local minimum or saddle points<ul>
<li>If SGD went into local minimum we will stuck at this point because the gradient is zero.</li>
<li>Also in saddle points the gradient will be zero so we will stuck.</li>
<li>Saddle points says that at some point:<ul>
<li>Some gradients will get the loss up.</li>
<li>Some gradients will get the loss down.</li>
<li>And that happens more in high dimensional (100 million dimension for example)</li>
</ul>
</li>
<li>The problem of deep NN is more about saddle points than about local minimum because deep NN has high dimensions (Parameters)</li>
<li>Mini batches are noisy because the gradient is not taken for the whole batch.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>SGD + momentum</strong>:</p>
<ul>
<li><p>Build up velocity as a running mean of gradients:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Computing weighted average. rho best is in range [0.9 - 0.99]</span></span><br><span class="line">V[t+<span class="number">1</span>] = rho * v[t] + dx</span><br><span class="line">x[t+<span class="number">1</span>] = x[t] - learningRate * V[t+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>V[0]</code> is zero.</p>
</li>
<li><p>Solves the saddle point and local minimum problems.</p>
</li>
<li><p>It overshoots the problem and returns to it back.</p>
</li>
</ul>
</li>
<li><p><strong>Nestrov momentum</strong>:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dx = compute_gradient(x)</span><br><span class="line">old_v = v</span><br><span class="line">v = rho * v - learning_rate * dx</span><br><span class="line">x+= -rho * old_v + (<span class="number">1</span>+rho) * v</span><br></pre></td></tr></table></figure>
<ul>
<li>Doesn’t overshoot the problem but slower than SGD + momentum</li>
</ul>
</li>
<li><p><strong>AdaGrad</strong></p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># here is a problem, the grad_squared isn't decayed (gets so large)</span></span><br><span class="line">  grad_squared += dx * dx			</span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>RMSProp</strong></p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Solved ADAgra</span></span><br><span class="line">  grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-grad_squared) * dx * dx  </span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>People uses this instead of AdaGrad</li>
</ul>
</li>
<li><p><strong>Adam</strong></p>
<ul>
<li>Calculates the momentum and RMSProp as the gradients.</li>
<li>It need a Fixing bias to fix starts of gradients.</li>
<li>Is the best technique so far runs best on a lot of problems.</li>
<li>With <code>beta1 = 0.9</code> and <code>beta2 = 0.999</code> and <code>learning_rate = 1e-3</code> or <code>5e-4</code> is a great starting point for many models!</li>
</ul>
</li>
<li><p><strong>Learning decay</strong></p>
<ul>
<li>Ex. decay learning rate by half every few epochs.</li>
<li>To help the learning rate not to bounce out.</li>
<li>Learning decay is common with SGD+momentum but not common with Adam.</li>
<li>Dont use learning decay from the start at choosing your hyperparameters. Try first and check if you need decay or not.</li>
</ul>
</li>
<li><p>All the above algorithms we have discussed is a first order optimization.</p>
</li>
<li><p><strong>Second order optimization</strong></p>
<ul>
<li>Use gradient and Hessian to from quadratic approximation.</li>
<li>Step to the minima of the approximation.</li>
<li>What is nice about this update?<ul>
<li>It doesn’t has a learning rate in some of the versions.</li>
</ul>
</li>
<li>But its unpractical for deep learning<ul>
<li>Has O(N^2) elements.</li>
<li>Inverting takes O(N^3).</li>
</ul>
</li>
<li><strong>L-BFGS</strong> is a version of second order optimization<ul>
<li>Works with batch optimization but not with mini-batches.</li>
</ul>
</li>
</ul>
</li>
<li><p>In practice first use ADAM and if it didn’t work try L-BFGS.</p>
</li>
<li><p>Some says all the famous deep architectures uses <strong>SGS + Nestrov momentum</strong></p>
</li>
</ul>
</li>
<li><p><strong>Regularization</strong></p>
<ul>
<li>So far we have talked about reducing the training error, but we care about most is how our model will handle unseen data!</li>
<li>What if the gab of the error between training data and validation data are too large?</li>
<li>This error is called high variance.</li>
<li><strong>Model Ensembles</strong>:<ul>
<li>Algorithm:<ul>
<li>Train multiple independent models of the same architecture with different initializations.</li>
<li>At test time average their results.</li>
</ul>
</li>
<li>It can get you extra 2% performance.</li>
<li>It reduces the generalization error.</li>
<li>You can use some snapshots of your NN at the training ensembles them and take the results.</li>
</ul>
</li>
<li>Regularization solves the high variance problem. We have talked about L1, L2 Regularization.</li>
<li>Some Regularization techniques are designed for only NN and can do better.</li>
<li><strong>Drop out</strong>:<ul>
<li>In each forward pass, randomly set some of the neurons to zero. Probability of dropping is a hyperparameter that are 0.5 for almost cases.</li>
<li>So you will chooses some activation and makes them zero.</li>
<li>It works because:<ul>
<li>It forces the network to have redundant representation; prevent co-adaption of features!</li>
<li>If you think about this, It ensemble some of the models in the same model!</li>
</ul>
</li>
<li>At test time we might multiply each dropout layer by the probability of the dropout.</li>
<li>Sometimes at test time we don’t multiply anything and leave it as it is.</li>
<li>With drop out it takes more time to train.</li>
</ul>
</li>
<li><strong>Data augmentation</strong>:<ul>
<li>Another technique that makes Regularization.</li>
<li>Change the data!</li>
<li>For example flip the image, or rotate it.</li>
<li>Example in ResNet:<ul>
<li>Training: Sample random crops and scales:<ol>
<li>Pick random L in range [256,480]</li>
<li>Resize training image, short side = L</li>
<li>Sample random 224x244 patch.</li>
</ol>
</li>
<li>Testing: average a fixed set of crops<ol>
<li>Resize image at 5 scales: {224, 256, 384, 480, 640}</li>
<li>For each size, use 10 224x224 crops: 4 corners + center + flips</li>
</ol>
</li>
<li>Apply Color jitter or PCA</li>
<li>Translation, rotation, stretching.</li>
</ul>
</li>
</ul>
</li>
<li>Drop connect<ul>
<li>Like drop out idea it makes a regularization.</li>
<li>Instead of dropping the activation, we randomly zeroing the weights.</li>
</ul>
</li>
<li>Fractional Max Pooling<ul>
<li>Cool regularization idea. Not commonly used.</li>
<li>Randomize the regions in which we pool.</li>
</ul>
</li>
<li>Stochastic depth<ul>
<li>New idea.</li>
<li>Eliminate layers, instead on neurons.</li>
<li>Has the similar effect of drop out but its a new idea.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Transfer learning</strong>:</p>
<ul>
<li><p>Some times your data is overfitted by your model because the data is small not because of regularization.</p>
</li>
<li><p>You need a lot of data if you want to train/use CNNs.</p>
</li>
<li><p>Steps of transfer learning</p>
<ol>
<li>Train on a big dataset that has common features with your dataset. Called pretraining.</li>
<li>Freeze the layers except the last layer and feed your small dataset to learn only the last layer.</li>
<li>Not only the last layer maybe trained again, you can fine tune any number of layers you want based on the number of data you have</li>
</ol>
</li>
<li><p>Guide to use transfer learning:</p>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Very Similar dataset</th>
<th>very different dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>very little dataset</strong></td>
<td>Use Linear classifier on top layer</td>
<td>You’re in trouble.. Try linear classifier from different stages</td>
</tr>
<tr>
<td><strong>quite a lot of data</strong></td>
<td>Finetune a few layers</td>
<td>Finetune a large layers</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Transfer learning is the normal not an exception.</li>
</ul>
<h2 id="08-Deep-learning-software"><a href="#08-Deep-learning-software" class="headerlink" title="08. Deep learning software"></a>08. Deep learning software</h2><ul>
<li>This section changes a lot every year in CS231n due to rabid changes in the deep learning softwares.</li>
<li>CPU vs GPU<ul>
<li>GPU The graphics card was developed to render graphics to play games or make 3D media,. etc.<ul>
<li>NVIDIA vs AMD<ul>
<li>Deep learning choose NVIDIA over AMD GPU because NVIDIA is pushing research forward deep learning also makes it architecture more suitable for deep learning.</li>
</ul>
</li>
</ul>
</li>
<li>CPU has fewer cores but each core is much faster and much more capable; great at sequential tasks. While GPUs has more cores but each core is much slower “dumber”; great for parallel tasks.</li>
<li>GPU cores needs to work together. and has its own memory.</li>
<li>Matrix multiplication is from the operations that are suited for GPUs. It has MxN independent operations that can be done on parallel.</li>
<li>Convolution operation also can be paralyzed because it has independent operations.</li>
<li>Programming GPUs frameworks:<ul>
<li><strong>CUDA</strong> (NVIDIA only)<ul>
<li>Write c-like code that runs directly on the GPU.</li>
<li>Its hard to build a good optimized code that runs on GPU. Thats why they provided high level APIs.</li>
<li>Higher level APIs: cuBLAS, cuDNN, etc</li>
<li><strong>CuDNN</strong> has implemented back prop. , convolution, recurrent and a lot more for you!</li>
<li>In practice you won’t write a parallel code. You will use the code implemented and optimized by others!</li>
</ul>
</li>
<li><strong>OpenCl</strong><ul>
<li>Similar to CUDA, but runs on any GPU.</li>
<li>Usually Slower .</li>
<li>Haven’t much support yet from all deep learning softwares.</li>
</ul>
</li>
</ul>
</li>
<li>There are a lot of courses for learning parallel programming.</li>
<li>If you aren’t careful, training can bottleneck on reading data and transferring to GPU. So the solutions are:<ul>
<li>Read all the data into RAM. # If possible</li>
<li>Use SSD instead of HDD</li>
<li>Use multiple CPU threads to prefetch data!<ul>
<li>While the GPU are computing, a CPU thread will fetch the data for you.</li>
<li>A lot of frameworks implemented that for you because its a little bit painful!</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Deep learning Frameworks</strong><ul>
<li>Its super fast moving!</li>
<li>Currently available frameworks:<ul>
<li>Tensorflow (Google)</li>
<li>Caffe (UC Berkeley)</li>
<li>Caffe2 (Facebook)</li>
<li>Torch (NYU / Facebook)</li>
<li>PyTorch (Facebook)</li>
<li>Theano (U monteral) </li>
<li>Paddle (Baidu)</li>
<li>CNTK (Microsoft)</li>
<li>MXNet (Amazon)</li>
</ul>
</li>
<li>The instructor thinks that you should focus on Tensorflow and PyTorch.</li>
<li>The point of deep learning frameworks:<ul>
<li>Easily build big computational graphs.</li>
<li>Easily compute gradients in computational graphs.</li>
<li>Run it efficiently on GPU (cuDNN - cuBLAS)</li>
</ul>
</li>
<li>Numpy doesn’t run on GPU.</li>
<li>Most of the frameworks tries to be like NUMPY in the forward pass and then they compute the gradients for you.</li>
</ul>
</li>
<li><strong>Tensorflow (Google)</strong><ul>
<li>Code are two parts:<ol>
<li>Define computational graph.</li>
<li>Run the graph and reuse it many times.</li>
</ol>
</li>
<li>Tensorflow uses a static graph architecture.</li>
<li>Tensorflow variables live in the graph. while the placeholders are feed each run.</li>
<li>Global initializer function initializes the variables that lives in the graph.</li>
<li>Use predefined optimizers and losses.</li>
<li>You can make a full layers with layers.dense function.</li>
<li><strong>Keras</strong> (High level wrapper):<ul>
<li>Keras is a layer on top pf Tensorflow, makes common things easy to do.</li>
<li>So popular!</li>
<li>Trains a full deep NN in a few lines of codes.</li>
</ul>
</li>
<li>There are a lot high level wrappers:<ul>
<li>Keras</li>
<li>TFLearn</li>
<li>TensorLayer</li>
<li>tf.layers   <code>#Ships with tensorflow</code></li>
<li>tf-Slim   <code>#Ships with tensorflow</code></li>
<li>tf.contrib.learn   <code>#Ships with tensorflow</code></li>
<li>Sonnet <code># New from deep mind</code></li>
</ul>
</li>
<li>Tensorflow has pretrained models that you can use while you are using transfer learning.</li>
<li>Tensorboard adds logging to record loss, stats. Run server and get pretty graphs!</li>
<li>It has distributed code if you want to split your graph on some nodes.</li>
<li>Tensorflow is actually inspired from Theano. It has the same inspirations and structure.</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>PyTorch (Facebook)</strong></p>
<ul>
<li>Has three layers of abstraction:<ul>
<li>Tensor: <code>ndarray</code> but runs on GPU     <code>#Like numpy arrays in tensorflow</code><ul>
<li>Variable: Node in a computational graphs; stores data and gradient <code>#Like Tensor, Variable, Placeholders</code></li>
</ul>
</li>
<li>Module: A NN layer; may store state or learnable weights<code>#Like tf.layers in tensorflow</code></li>
</ul>
</li>
<li>In PyTorch the graphs runs in the same loop you are executing which makes it easier for debugging. This is called a dynamic graph.</li>
<li>In PyTorch you can define your own autograd functions by writing forward and backward for tensors. Most of the times it will implemented for you.</li>
<li>Torch.nn is a high level api like keras in tensorflow. You can create the models and go on and on.<ul>
<li>You can define your own nn module!</li>
</ul>
</li>
<li>Also Pytorch contains optimizers like tensorflow.</li>
<li>It contains a data loader that wraps a Dataset and provides minbatches, shuffling and multithreading.</li>
<li>PyTorch contains the best and super easy to use pretrained models</li>
<li>PyTorch contains Visdom that are like tensorboard. but Tensorboard seems to be more powerful.</li>
<li>PyTorch is new and still evolving compared to Torch. Its still in beta state.</li>
<li>PyTorch is best for research.</li>
</ul>
</li>
<li><p>Tensorflow builds the graph once, then run them many times (Called static graph)</p>
</li>
<li><p>In each PyTorch iteration we build a new graph (Called dynamic graph)</p>
</li>
<li><p><strong>Static vs dynamic graphs</strong>:</p>
<ul>
<li><p>Optimization:</p>
<ul>
<li>With static graphs, framework can optimize the graph for you before it runs.</li>
</ul>
</li>
<li><p>Serialization</p>
<ul>
<li><strong>Static</strong>: Once graph is built, can serialize it and run it without the code that built the graph. Ex use the graph in c++</li>
<li><strong>Dynamic</strong>: Always need to keep the code around.</li>
</ul>
</li>
<li><p>Conditional</p>
<ul>
<li>Is easier in dynamic graphs. And more complicated in static graphs.</li>
</ul>
</li>
<li><p>Loops:</p>
<ul>
<li>Is easier in dynamic graphs. And more complicated in static graphs.</li>
</ul>
</li>
</ul>
</li>
<li><p>Tensorflow fold make dynamic graphs easier in Tensorflow through dynamic batching.</p>
</li>
<li><p>Dynamic graph applications include: recurrent networks and recursive networks.</p>
</li>
<li><p>Caffe2 uses static graphs and can train model in python also works on IOS and Android</p>
</li>
<li><p>Tensorflow/Caffe2 are used a lot in production especially on mobile.</p>
</li>
</ul>
<h2 id="09-CNN-architectures"><a href="#09-CNN-architectures" class="headerlink" title="09. CNN architectures"></a>09. CNN architectures</h2><ul>
<li><p>This section talks about the famous CNN architectures. Focuses on CNN architectures that won <a href="www.image-net.org/">ImageNet</a> competition since 2012.</p>
<ul>
<li><img src="/2020/07/29/CS231N/43.png" alt></li>
</ul>
</li>
<li><p>These architectures includes: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a>, <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a>, <a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener">GoogLeNet</a>, and <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>.</p>
</li>
<li><p>Also we will discuss some interesting architectures as we go.</p>
</li>
<li><p>The first ConvNet that was made was <a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">LeNet-5</a> architectures are:by Yann Lecun at 1998.</p>
<ul>
<li>Architecture are: <code>CONV-POOL-CONV-POOL-FC-FC-FC</code><ul>
<li><img src="/2020/07/29/CS231N/02.jpg" alt></li>
</ul>
</li>
<li>Each conv filters was <code>5x5</code> applied at stride 1</li>
<li>Each pool was <code>2x2</code> applied at stride <code>2</code></li>
<li>It was useful in Digit recognition.</li>
<li>In particular the insight that image features are distributed across the entire image, and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters.</li>
<li>It contains exactly <strong><u>5 </u></strong> layers</li>
</ul>
</li>
<li><p>In <a href="https://arxiv.org/abs/1003.0358" target="_blank" rel="noopener">2010</a> Dan Claudiu Ciresan and Jurgen Schmidhuber published one of the very fist implementations of GPU Neural nets. This implementation had both forward and backward implemented on a a NVIDIA GTX 280 graphic processor of an up to 9 layers neural network.</p>
</li>
<li><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener"><strong>AlexNet</strong></a> (2012):</p>
<ul>
<li>ConvNet that started the evolution and wins the ImageNet at 2012.</li>
<li>Architecture are: <code>CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8</code></li>
<li>Contains exactly <strong><u>8</u></strong> layers the first 5 are Convolutional and the last 3 are fully connected layers.</li>
<li>AlexNet accuracy error was <code>16.4%</code></li>
<li>For example if the input is 227 x 227 x3 then these are the shapes of the of the outputs at each layer:<ul>
<li>CONV1    (96 11 x 11 filters at stride 4, pad 0)<ul>
<li>Output shape <code>(55,55,96)</code>,   Number of weights are <code>(11*11*3*96)+96 = 34944</code></li>
</ul>
</li>
<li>MAXPOOL1 (3 x 3 filters applied at stride 2)<ul>
<li>Output shape <code>(27,27,96)</code>,   No Weights</li>
</ul>
</li>
<li>NORM1<ul>
<li>Output shape <code>(27,27,96)</code>,     We don’t do this any more</li>
</ul>
</li>
<li>CONV2 (256 5 x 5 filters at stride 1, pad 2)</li>
<li>MAXPOOL2 (3 x 3 filters at stride 2)</li>
<li>NORM2</li>
<li>CONV3 (384 3 x 3 filters ar stride 1, pad 1)</li>
<li>CONV4 (384 3 x 3 filters ar stride 1, pad 1)</li>
<li>CONV5 (256 3 x 3 filters ar stride 1, pad 1)</li>
<li>MAXPOOL3 (3 x 3 filters at stride 2)<ul>
<li>Output shape <code>(6,6,256)</code></li>
</ul>
</li>
<li>FC6 (4096)</li>
<li>FC7 (4096)</li>
<li>FC8 (1000 neurons for class score)</li>
</ul>
</li>
<li>Some other details:<ul>
<li>First use of RELU.</li>
<li>Norm layers but not used any more.</li>
<li>heavy data augmentation</li>
<li>Dropout <code>0.5</code></li>
<li>batch size <code>128</code></li>
<li>SGD momentum <code>0.9</code></li>
<li>Learning rate <code>1e-2</code> reduce by 10 at some iterations</li>
<li>7 CNN ensembles!</li>
</ul>
</li>
<li>AlexNet was trained on GTX 580 GPU with only 3 GB which wasn’t enough to train in one machine so they have spread the feature maps in half. The first AlexNet was distributed!</li>
<li>Its still used in transfer learning in a lot of tasks.</li>
<li>Total number of parameters are <code>60 million</code></li>
</ul>
</li>
<li><p><a href="https://arxiv.org/abs/1311.2901" target="_blank" rel="noopener"><strong>ZFNet</strong></a> (2013)</p>
<ul>
<li>Won in 2013 with error 11.7%</li>
<li>It has the same general structure but they changed a little in hyperparameters to get the best output.</li>
<li>Also contains <strong><u>8</u></strong> layers.</li>
<li>AlexNet but:<ul>
<li><code>CONV1</code>: change from (11 x 11 stride 4) to (7 x 7 stride 2)</li>
<li><code>CONV3,4,5</code>: instead of 384, 384, 256 filters use 512, 1024, 512</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://arxiv.org/abs/1312.6229" target="_blank" rel="noopener">OverFeat</a> (2013)</p>
<ul>
<li>Won the localization in imageNet in 2013</li>
<li>We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries.</li>
</ul>
</li>
<li><p><a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener"><strong>VGGNet</strong></a> (2014) (Oxford)</p>
<ul>
<li>Deeper network with more layers.</li>
<li>Contains 19 layers.</li>
<li>Won on 2014 with GoogleNet with error 7.3%</li>
<li>Smaller filters with deeper layers.</li>
<li>The great advantage of VGG was the insight that multiple 3 × 3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5 × 5 and 7 × 7.</li>
<li>Used the simple 3 x 3 Conv all through the network.<ul>
<li>3 (3 x 3) filters has the same effect as 7 x 7</li>
</ul>
</li>
<li><img src="/2020/07/29/CS231N/03.png" alt></li>
<li>The Architecture contains several CONV layers then POOL layer over 5 times and then the full connected layers.</li>
<li>It has a total memory of 96MB per image for only forward propagation!<ul>
<li>Most memory are in the earlier layers</li>
</ul>
</li>
<li>Total number of parameters are 138 million<ul>
<li>Most of the parameters are in the fully connected layers</li>
</ul>
</li>
<li>Has a similar details in training like AlexNet. Like using momentum and dropout.</li>
<li>VGG19 are an upgrade for VGG16 that are slightly better but with more memory<ul>
<li><img src="/2020/07/29/CS231N/04.png" alt></li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener"><strong>GoogleNet</strong></a> (2014)</p>
<ul>
<li>Deeper network with more layers.</li>
<li>Contains 22 layers.</li>
<li>It has Efficient <strong><u>Inception</u></strong> module.</li>
<li>Only 5 million parameters! 12x less than AlexNet</li>
<li>Won on 2014 with VGGNet with error 6.7%</li>
<li>Inception module:<ul>
<li>Design a good local network topology (network within a network (NiN)) and then stack these modules on top of each other.</li>
<li>It consists of:<ul>
<li>Apply parallel filter operations on the input from previous layer<ul>
<li>Multiple convs of sizes (1 x 1, 3 x 3, 5 x 5) <ul>
<li>Adds padding to maintain the sizes.</li>
</ul>
</li>
<li>Pooling operation. (Max Pooling)<ul>
<li>Adds padding to maintain the sizes.</li>
</ul>
</li>
</ul>
</li>
<li>Concatenate all filter outputs together depth-wise.</li>
</ul>
</li>
<li>For example:<ul>
<li>Input for inception module is 28 x 28 x 256</li>
<li>Then the parallel filters applied:<ul>
<li>(1 x 1), 128 filter               <code># output shape (28,28,128)</code></li>
<li>(3 x 3), 192 filter                 <code># output shape (28,28,192)</code></li>
<li>(5 x 5), 96 filter                   <code># output shape (28,28,96)</code></li>
<li>(3 x 3) Max pooling            <code># output shape (28,28,256)</code></li>
</ul>
</li>
<li>After concatenation this will be <code>(28,28,672)</code></li>
</ul>
</li>
<li>By this design -We call Naive- it has a big computation complexity.<ul>
<li>The last example will make:<ul>
<li>[1 x 1 conv, 128] ==&gt; 28 <em> 28 </em> 128 <em> 1 </em> 1 * 256 = 25 Million approx</li>
<li>[3 x 3 conv, 192] ==&gt; 28 <em> 28 </em> 192 <em>3 </em>3 * 256 = 346 Million approx</li>
<li>[5 x 5 conv, 96] ==&gt; 28 <em> 28 </em> 96 <em> 5 </em> 5 * 256 = 482 Million approx</li>
<li>In total around 854 Million operation!</li>
</ul>
</li>
</ul>
</li>
<li>Solution: <strong>bottleneck</strong> layers that use 1x1 convolutions to reduce feature depth.<ul>
<li>Inspired from NiN (<a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network in network</a>)</li>
</ul>
</li>
<li><img src="/2020/07/29/CS231N/05.png" alt></li>
<li>The bottleneck solution will make a total operations of 358M on this example which is good compared with the naive implementation.</li>
</ul>
</li>
<li>So GoogleNet stacks this Inception module multiple times to get a full architecture of a network that can solve a problem without the Fully connected layers.</li>
<li>Just to mention, it uses an average pooling layer at the end before the classification step.</li>
<li>Full architecture:<ul>
<li><img src="/2020/07/29/CS231N/44.png" alt></li>
</ul>
</li>
<li>In February 2015 Batch-normalized Inception was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values.</li>
<li>In December <a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">2015</a> they introduced a paper “Rethinking the Inception Architecture for Computer Vision” which explains the older inception models well also introducing a new version V3.</li>
</ul>
</li>
<li><p>The first GoogleNet and VGG was before batch normalization invented so they had some hacks to train the NN and converge well.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener"><strong>ResNet</strong></a> (2015) (Microsoft Research)</p>
<ul>
<li><p>152-layer model for ImageNet. Winner by 3.57% which is more than human level error.</p>
</li>
<li><p>This is also the very first time that a network of &gt; hundred, even 1000 layers was trained.</p>
</li>
<li><p>Swept all classification and detection competitions in ILSVRC’15 and COCO’15!</p>
</li>
<li><p>What happens when we continue stacking deeper layers on a “plain” Convolutional neural network?</p>
<ul>
<li>The deeper model performs worse, but it’s not caused by overfitting!</li>
<li>The learning stops performs well somehow because deeper NN are harder to optimize!</li>
</ul>
</li>
<li><p>The deeper model should be able to perform at least as well as the shallower model.</p>
</li>
<li><p>A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.</p>
</li>
<li><p>Residual block:</p>
<ul>
<li><p>Microsoft came with the Residual block which has this architecture:</p>
<ul>
<li><img src="/2020/07/29/CS231N/45.png" alt></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Instead of us trying To learn a new representation, We learn only Residual</span></span><br><span class="line">Y = (W2* RELU(W1x+b1) + b2) + X</span><br></pre></td></tr></table></figure>
</li>
<li><p>Say you have a network till a depth of N layers. You only want to add a new layer if you get something extra out of adding that layer.</p>
</li>
<li><p>One way to ensure this new (N+1)th layer learns something new about your network is to also provide the input(x) without any transformation to the output of the (N+1)th layer. This essentially drives the new layer to learn something different from what the input has already encoded.</p>
</li>
<li><p>The other advantage is such connections help in handling the Vanishing gradient problem in very deep networks.</p>
</li>
</ul>
</li>
<li><p>With the Residual block we can now have a deep NN of any depth without the fearing that we can’t optimize the network.</p>
</li>
<li><p>ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck to reduce the dimensions.</p>
<ul>
<li><img src="/2020/07/29/CS231N/07.jpg" alt></li>
</ul>
</li>
<li><p><strong><u>Full ResNet architecture</u></strong>:</p>
<ul>
<li>Stack residual blocks.<ul>
<li><img src="/2020/07/29/CS231N/08.png" alt></li>
</ul>
</li>
<li>Every residual block has two 3 x 3 conv layers.</li>
<li>Additional conv layer at the beginning.</li>
<li>No FC layers at the end (only FC 1000 to output classes)</li>
<li>Periodically, double number of filters and downsample spatially using stride 2 (/2 in each dimension)</li>
<li>Training ResNet in practice:<ul>
<li>Batch Normalization after every CONV layer.</li>
<li>Xavier/2 initialization from He et al.</li>
<li>SGD + Momentum (<code>0.9</code>) </li>
<li>Learning rate: 0.1, divided by 10 when validation error plateaus</li>
<li>Mini-batch size <code>256</code></li>
<li>Weight decay of <code>1e-5</code></li>
<li>No dropout used.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="noopener">Inception-v4</a>: Resnet + Inception and was founded in 2016.</p>
</li>
<li><p>The complexity comparing over all the architectures:</p>
<ul>
<li><img src="/2020/07/29/CS231N/09.png" alt></li>
<li>VGG: Highest memory, most operations.</li>
<li>GoogLeNet: most efficient.</li>
</ul>
</li>
<li><p><strong>ResNets Improvements</strong>:</p>
<ul>
<li>(<a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">2016</a>) <u>Identity Mappings in Deep Residual Networks</u><ul>
<li>From the creators of ResNet.</li>
<li>Gives better performance.</li>
</ul>
</li>
<li>(<a href="https://arxiv.org/abs/1605.07146" target="_blank" rel="noopener">2016</a>) <u>Wide Residual Networks</u><ul>
<li>Argues that residuals are the important factor, not depth</li>
<li>50-layer wide ResNet outperforms 152-layer original ResNet</li>
<li>Increasing width instead of depth more computationally efficient (parallelizable)</li>
</ul>
</li>
<li>(<a href="https://arxiv.org/abs/1603.09382" target="_blank" rel="noopener">2016</a>) Deep Networks with Stochastic Depth<ul>
<li>Motivation: reduce vanishing gradients and training time through short networks during training.</li>
<li>Randomly drop a subset of layers during each training pass</li>
<li>Use full deep network at test time.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Beyond ResNets</strong>:</p>
<ul>
<li>(<a href="https://arxiv.org/abs/1605.07648" target="_blank" rel="noopener">2017</a>) <u>FractalNet: Ultra-Deep Neural Networks without Residuals</u><ul>
<li>Argues that key is transitioning effectively from shallow to deep and residual representations are not necessary.</li>
<li>Trained with dropping out sub-paths</li>
<li>Full network at test time.</li>
</ul>
</li>
<li>(<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">2017</a>) <u>Densely Connected Convolutional Networks</u></li>
<li>(<a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noopener">2017</a>) SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and &lt;0.5Mb Model Size<ul>
<li>Good for production.</li>
<li>It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.</li>
</ul>
</li>
</ul>
</li>
<li><p>Conclusion:</p>
<ul>
<li>ResNet current best default.</li>
<li>Trend towards extremely deep networks</li>
<li>In the last couple of years, some models all using the shortcuts like “ResNet” to eaisly flow the gradients.</li>
</ul>
</li>
</ul>
<h2 id="10-Recurrent-Neural-networks"><a href="#10-Recurrent-Neural-networks" class="headerlink" title="10. Recurrent Neural networks"></a>10. Recurrent Neural networks</h2><ul>
<li><p>Vanilla Neural Networks “Feed neural networks”, input of fixed size goes through some hidden units and then go to output. We call it a one to one network.</p>
</li>
<li><p>Recurrent Neural Networks RNN Models:</p>
<ul>
<li><img src="/2020/07/29/CS231N/46.png" alt></li>
<li>One to many<ul>
<li>Example: Image Captioning<ul>
<li>image ==&gt; sequence of words</li>
</ul>
</li>
</ul>
</li>
<li>Many to One<ul>
<li>Example: Sentiment Classification<ul>
<li>sequence of words ==&gt; sentiment</li>
</ul>
</li>
</ul>
</li>
<li>Many to many<ul>
<li>Example: Machine Translation<ul>
<li>seq of words in one language ==&gt; seq of words in another language</li>
</ul>
</li>
<li>Example: Video classification on frame level</li>
</ul>
</li>
</ul>
</li>
<li><p>RNNs can also work for Non-Sequence Data (One to One problems)</p>
<ul>
<li>It worked in Digit classification through taking a series of “glimpses”<ul>
<li>“<a href="https://arxiv.org/abs/1412.7755" target="_blank" rel="noopener">Multiple Object Recognition with Visual Attention</a>”, ICLR 2015.</li>
</ul>
</li>
<li>It worked on generating images one piece at a time<ul>
<li>i.e generating a <a href="http://ieeexplore.ieee.org/document/7966808/" target="_blank" rel="noopener">captcha</a></li>
</ul>
</li>
</ul>
</li>
<li><p>So what is a recurrent neural network?</p>
<ul>
<li><p>Recurrent core cell that take an input x and that cell has an internal state that are updated each time it reads an input.</p>
</li>
<li><p><img src="/2020/07/29/CS231N/47.png" alt></p>
</li>
<li><p>The RNN block should return a vector.</p>
</li>
<li><p>We can process a sequence of vectors x by applying a recurrence formula at every time step:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h[t] = fw (h[t<span class="number">-1</span>], x[t])			</span><br><span class="line"><span class="comment"># Where fw is some function with parameters W</span></span><br></pre></td></tr></table></figure>
<ul>
<li>The same function and the same set of parameters are used at every time step.</li>
</ul>
</li>
<li><p>(Vanilla) Recurrent Neural Network:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">h[t] &#x3D; tanh (W[h,h]*h[t-1] + W[x,h]*x[t])    # Then we save h[t]</span><br><span class="line">y[t] &#x3D; W[h,y]*h[t]</span><br></pre></td></tr></table></figure>
<ul>
<li>This is the simplest example of a RNN.</li>
</ul>
</li>
<li><p>RNN works on a sequence of related data.</p>
</li>
</ul>
</li>
<li><p>Recurrent NN Computational graph:</p>
<ul>
<li><img src="/2020/07/29/CS231N/10.png" alt></li>
<li><code>h0</code> are initialized to zero.</li>
<li>Gradient of <code>W</code> is the sum of all the <code>W</code> gradients that has been calculated!</li>
<li>A many to many graph:<ul>
<li><img src="/2020/07/29/CS231N/11.png" alt></li>
<li>Also the last is the sum of all losses and the weights of Y is one and is updated through summing all the gradients!</li>
</ul>
</li>
<li>A many to one graph:<ul>
<li><img src="/2020/07/29/CS231N/12.png" alt></li>
</ul>
</li>
<li>A one to many graph:<ul>
<li><img src="/2020/07/29/CS231N/13.png" alt></li>
</ul>
</li>
<li>sequence to sequence graph:<ul>
<li><img src="/2020/07/29/CS231N/14.png" alt></li>
<li>Encoder and decoder philosophy.</li>
</ul>
</li>
</ul>
</li>
<li><p>Examples:</p>
<ul>
<li>Suppose we are building words using characters. We want a model to predict the next character of a sequence. Lets say that the characters are only <code>[h, e, l, o]</code> and the words are [hello]<ul>
<li>Training:<ul>
<li><img src="/2020/07/29/CS231N/15.png" alt></li>
<li>Only the third prediction here is true. The loss needs to be optimized.</li>
<li>We can train the network by feeding the whole word(s).</li>
</ul>
</li>
<li>Testing time:<ul>
<li><img src="/2020/07/29/CS231N/16.png" alt></li>
<li>At test time we work with a character by character. The output character will be the next input with the other saved hidden activations.</li>
<li>This <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="noopener">link</a> contains all the code but uses Truncated Backpropagation through time as we will discuss.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Backpropagation through time Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient.</p>
<ul>
<li>But if we choose the whole sequence it will be so slow and take so much memory and will never converge!</li>
</ul>
</li>
<li><p>So in practice people are doing “Truncated Backpropagation through time” as we go on we Run forward and backward through chunks of the sequence instead of whole sequence</p>
<ul>
<li>Then Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.</li>
</ul>
</li>
<li><p>Example on image captioning:</p>
<ul>
<li><img src="/2020/07/29/CS231N/17.png" alt></li>
<li>They use <End> token to finish running.</End></li>
<li>The biggest dataset for image captioning is Microsoft COCO.</li>
</ul>
</li>
<li><p>Image Captioning with Attention is a project in which when the RNN is generating captions, it looks at a specific part of the image not the whole image.</p>
<ul>
<li>Image Captioning with Attention technique is also used in “Visual Question Answering” problem</li>
</ul>
</li>
<li><p>Multilayer RNNs is generally using some layers as the hidden layer that are feed into again. <strong>LSTM</strong> is a multilayer RNNs.</p>
</li>
<li><p>Backward flow of gradients in RNN can explode or vanish. Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)</p>
</li>
<li><p>LSTM stands for Long Short Term Memory. It was designed to help the vanishing gradient problem on RNNs.</p>
<ul>
<li>It consists of:<ul>
<li>f: Forget gate, Whether to erase cell</li>
<li>i: Input gate, whether to write to cell</li>
<li>g: Gate gate (?), How much to write to cell</li>
<li>o: Output gate, How much to reveal cell</li>
</ul>
</li>
<li><img src="/2020/07/29/CS231N/18.png" alt></li>
<li><img src="/2020/07/29/CS231N/18.1.png" alt></li>
<li>The LSTM gradients are easily computed like ResNet</li>
<li>The LSTM is keeping data on the long or short memory as it trains means it can remember not just the things from last layer but layers.</li>
</ul>
</li>
<li><p>Highway networks is something between ResNet and LSTM that is still in research.</p>
</li>
<li><p>Better/simpler architectures are a hot topic of current research</p>
</li>
<li><p>Better understanding (both theoretical and empirical) is needed.</p>
</li>
<li><p>RNN is used for problems that uses sequences of related inputs more. Like NLP and Speech recognition.</p>
</li>
</ul>
<h2 id="11-Detection-and-Segmentation"><a href="#11-Detection-and-Segmentation" class="headerlink" title="11. Detection and Segmentation"></a>11. Detection and Segmentation</h2><ul>
<li><p>So far we are talking about image classification problem. In this section we will talk about Segmentation, Localization, Detection.</p>
</li>
<li><p><strong><u>Semantic Segmentation</u></strong></p>
<ul>
<li><p>We want to Label each pixel in the image with a category label.</p>
</li>
<li><p><img src="/2020/07/29/CS231N/19.png" alt></p>
</li>
<li><p>As you see the cows in the image, Semantic Segmentation Don’t differentiate instances, only care about pixels.</p>
</li>
<li><p>The first idea is to use a <strong>sliding window</strong>. We take a small window size and slide it all over the picture. For each window we want to label the center pixel.</p>
<ul>
<li>It will work but its not a good idea because it will be computational expensive!</li>
<li>Very inefficient! Not reusing shared features between overlapping patches.</li>
<li>In practice nobody uses this.</li>
</ul>
</li>
<li><p>The second idea is designing a network as a bunch of Convolutional layers to make predictions for pixels all at once!</p>
<ul>
<li>Input is the whole image. Output is the image with each pixel labeled.</li>
<li>We need a lot of labeled data. And its very expensive data.</li>
<li>It needs a deep Conv. layers.</li>
<li>The loss is cross entropy between each pixel provided.</li>
<li>Data augmentation are good here.</li>
<li>The problem with this implementation that convolutions at original image resolution will be very expensive.</li>
<li>So in practice we don’t see something like this right now.</li>
</ul>
</li>
<li><p>The third idea is based on the last idea. The difference is that we are downsampling and upsampling inside the network.</p>
<ul>
<li><p>We downsample because using the whole image as it is very expensive. So we go on multiple layers downsampling and then upsampling in the end.</p>
</li>
<li><p>Downsampling is an operation like Pooling and strided convolution.</p>
</li>
<li><p>Upsampling is like “Nearest Neighbor” or “Bed of Nails” or “Max unpooling”</p>
<ul>
<li><p><strong>Nearest Neighbor</strong> example:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input:   1  2               Output:   1  1  2  2</span><br><span class="line">         3  4                         1  1  2  2</span><br><span class="line">                                      3  3  4  4</span><br><span class="line">                                      3  3  4  4</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Bed of Nails</strong> example:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input:   1  2               Output:   1  0  2  0</span><br><span class="line">         3  4                         0  0  0  0</span><br><span class="line">                                      3  0  4  0</span><br><span class="line">                                      0  0  0  0</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Max unpooling</strong> is depending on the earlier steps that was made by max pooling. You fill the pixel where max pooling took place and then fill other pixels by zero.</p>
</li>
</ul>
</li>
<li><p>Max unpooling seems to be the best idea for upsampling.</p>
</li>
<li><p>There are an idea of Learnable Upsampling called “<strong>Transpose Convolution</strong>“</p>
<ul>
<li>Rather than making a convolution we make the reverse. </li>
<li>Also called:<ul>
<li>Upconvolution.</li>
<li>Fractionally strided convolution</li>
<li>Backward strided convolution</li>
</ul>
</li>
<li>Learn the artimitic of the upsampling please refer to chapter 4 in this <a href="https://arxiv.org/abs/1603.07285" target="_blank" rel="noopener">paper</a>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><u>Classification + Localization</u></strong>:</p>
<ul>
<li>In this problem we want to classify the main object in the image and its location as a rectangle.</li>
<li>We assume there are one object.</li>
<li>We will create a multi task NN. The architecture are as following:<ul>
<li>Convolution network layers connected to:<ul>
<li>FC layers that classify the object. <code># The plain classification problem we know</code></li>
<li>FC layers that connects to a four numbers <code>(x,y,w,h)</code><ul>
<li>We treat Localization as a regression problem.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>This problem will have two losses:<ul>
<li>Softmax loss for classification</li>
<li>Regression (Linear loss) for the localization (L2 loss)</li>
</ul>
</li>
<li>Loss = SoftmaxLoss + L2 loss</li>
<li>Often the first Conv layers are pretrained NNs like AlexNet!</li>
<li>This technique can be used in so many other problems like:  Human Pose Estimation.</li>
</ul>
</li>
<li><p><strong><u>Object Detection</u></strong></p>
<ul>
<li>A core idea of computer vision. We will talk by details in this problem.</li>
<li>The difference between “Classification + Localization” and this problem is that here we want to detect one or mode different objects and its locations!</li>
<li>First idea is to use a sliding window<ul>
<li>Worked well and long time.</li>
<li>The steps are:<ul>
<li>Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.</li>
</ul>
</li>
<li>The problem is we need  to apply CNN to huge number of locations and scales, very computationally expensive!</li>
<li>The brute force sliding window will make us take thousands of thousands of time.</li>
</ul>
</li>
<li>Region Proposals will help us deciding which region we should run our NN at:<ul>
<li>Find <strong>blobby</strong> image regions that are likely to contain objects.</li>
<li>Relatively fast to run; e.g. Selective Search gives 1000 region proposals in a few seconds on CPU</li>
</ul>
</li>
<li>So now we can apply one of the Region proposals networks and then apply the first idea.</li>
<li>There is another idea which is called R-CNN<ul>
<li><img src="/2020/07/29/CS231N/20.png" alt></li>
<li>The idea is bad because its taking parts of the image -With Region Proposals- if different sizes and feed it to CNN after scaling them all to one size. Scaling is bad</li>
<li>Also its very slow.</li>
</ul>
</li>
<li>Fast R-CNN is another idea that developed on R-CNN<ul>
<li><img src="/2020/07/29/CS231N/48.png" alt></li>
<li>It uses one CNN to do everything.</li>
</ul>
</li>
<li>Faster R-CNN does its own region proposals by Inserting Region Proposal Network (RPN) to predict proposals from features.<ul>
<li>The fastest of the R-CNNs.</li>
</ul>
</li>
<li>Another idea is Detection without Proposals: YOLO / SSD<ul>
<li>YOLO stands for you only look once.</li>
<li>YOLO/SDD is two separate algorithms.</li>
<li>Faster but not as accurate.</li>
</ul>
</li>
<li>Takeaways<ul>
<li>Faster R-CNN is slower but more accurate.</li>
<li>SSD/YOLO is much faster but not as accurate.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><u>Denese Captioning</u></strong></p>
<ul>
<li>Denese Captioning is “Object Detection + Captioning”</li>
<li>Paper that covers this idea can be found <a href="https://arxiv.org/abs/1511.07571" target="_blank" rel="noopener">here</a>.</li>
</ul>
</li>
<li><p><strong><u>Instance Segmentation</u></strong></p>
<ul>
<li>This is like the full problem.</li>
<li><img src="/2020/07/29/CS231N/49.png" alt></li>
<li>Rather than we want to predict the bounding box, we want to know which pixel label but also distinguish them.</li>
<li>There are a lot of ideas.</li>
<li>There are a new idea “Mask R-CNN”<ul>
<li>Like R-CNN but inside it we apply the Semantic Segmentation</li>
<li>There are a lot of good results out of this paper.</li>
<li>It sums all the things that we have discussed in this lecture.</li>
<li>Performance of this seems good.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="12-Visualizing-and-Understanding"><a href="#12-Visualizing-and-Understanding" class="headerlink" title="12. Visualizing and Understanding"></a>12. Visualizing and Understanding</h2><ul>
<li><p>We want to know what’s going on inside ConvNets?</p>
</li>
<li><p>People want to trust the black box (CNN) and know how it exactly works and give and good decisions.</p>
</li>
<li><p>A first approach is to visualize filters of the first layer.</p>
<ul>
<li>Maybe the shape of the first layer filter is 5 x 5 x 3, and the number of filters are 16. Then we will have 16 different “colored” filter images.</li>
<li>It turns out that these filters learns primitive shapes and oriented edges like the human brain does.</li>
<li>These filters really looks the same on each Conv net you will train, Ex if you tried to get it out of AlexNet, VGG, GoogleNet, or ResNet.</li>
<li>This will tell you what is the first convolution layer is looking for in the image.</li>
</ul>
</li>
<li><p>We can visualize filters from the next layers but they won’t tell us anything.</p>
<ul>
<li>Maybe the shape of the first layer filter is 5 x 5 x 20, and the number of filters are 16. Then we will have 16*20 different “gray” filter images.</li>
</ul>
</li>
<li><p>In AlexNet, there was some FC layers in the end. If we took the 4096-dimensional feature vector for an image, and collecting these feature vectors.</p>
<ul>
<li>If we made a nearest neighbors between these feature vectors and get the real images of these features we will get something very good compared with running the KNN on the images directly!</li>
<li><img src="/2020/07/29/CS231N/21.png" alt></li>
<li>This similarity tells us that these CNNs are really getting the semantic meaning of these images instead of on the pixels level!</li>
<li>We can make a dimensionality reduction on the 4096 dimensional feature and compress it to 2 dimensions.<ul>
<li>This can be made by PCA, or t-SNE.</li>
<li>t-SNE are used more with deep learning to visualize the data. Example can be found <a href="http://cs.stanford.edu/people/karpathy/cnnembed/" target="_blank" rel="noopener">here</a>.</li>
</ul>
</li>
</ul>
</li>
<li><p>We can Visualize the activation maps.</p>
<ul>
<li>For example if CONV5 feature map is 128 x 13 x 13, We can visualize it as 128 13 x 13 gray-scale images.</li>
<li><img src="/2020/07/29/CS231N/50.png" alt></li>
<li>One of these features are activated corresponding to the input, so now we know that this particular map are looking for something.</li>
<li>Its done by Yosinski et. More info are <a href="http://yosinski.com/deepvis#toolbox" target="_blank" rel="noopener">here</a>.</li>
</ul>
</li>
<li><p>There are something called <strong>Maximally Activating Patches</strong> that can help us visualize the intermediate features in Convnets</p>
<ul>
<li>The steps of doing this is as following:<ul>
<li>We choose a layer then a neuron<ul>
<li>Ex. We choose Conv5 in AlexNet which is 128 x 13 x 13 then pick channel (Neuron) 17/128</li>
</ul>
</li>
<li>Run many images through the network, record values of chosen channel.</li>
<li>Visualize image patches that correspond to maximal activations.<ul>
<li>We will find that each neuron is looking into a specific part of the image.</li>
<li>Extracted images are extracted using receptive field.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Another idea is <strong>Occlusion Experiments</strong></p>
<ul>
<li>We mask part of the image before feeding to CNN, draw heat-map of probability (Output is true) at each mask location</li>
<li>It will give you the most important parts of the image in which the Conv. Network has learned from.</li>
<li><img src="/2020/07/29/CS231N/51.png" alt></li>
</ul>
</li>
<li><p><strong>Saliency Maps</strong> tells which pixels matter for classification</p>
<ul>
<li>Like Occlusion Experiments but with a completely different approach</li>
<li>We Compute gradient of (unnormalized) class score with respect to image pixels, take absolute value and max over RGB channels. It will get us a gray image that represents the most important areas in the image.</li>
<li>This can be used for Semantic Segmentation sometimes.</li>
</ul>
</li>
<li><p>(guided) backprop Makes something like <strong>Maximally Activating Patches</strong> but unlike it gets the pixels in which we are caring of.</p>
<ul>
<li>In this technique choose a channel like Maximally Activating Patches and then compute gradient of neuron value with respect to image pixels</li>
<li>Images come out nicer if you only backprop positive gradients through each RELU (guided backprop)</li>
</ul>
</li>
<li><p><strong>Gradient Ascent</strong></p>
<ul>
<li><p>Generate a synthetic image that maximally activates a neuron.</p>
</li>
<li><p>Reverse of gradient decent. Instead of taking the minimum it takes the maximum.</p>
</li>
<li><p>We want to maximize the neuron with the input image. So here instead we are trying to learn the image that maximize the activation:</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># R(I) is Natural image regularizer, f(I) is the neuron value.</span></span><br><span class="line">I *= argmax(f(I)) + R(I)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Steps of gradient ascent</p>
<ul>
<li>Initialize image to zeros.</li>
<li>Forward image to compute current scores.</li>
<li>Backprop to get gradient of neuron value with respect to image pixels.</li>
<li>Make a small update to the image</li>
</ul>
</li>
<li><p><code>R(I)</code> may equal to L2 of generated image.</p>
</li>
<li><p>To get a better results we use a better regularizer:</p>
<ul>
<li>penalize L2 norm of image; also during optimization periodically:<ul>
<li>Gaussian blur image</li>
<li>Clip pixels with small values to 0</li>
<li>Clip pixels with small gradients to 0</li>
</ul>
</li>
</ul>
</li>
<li><p>A better regularizer makes out images cleaner!</p>
</li>
<li><p><img src="/2020/07/29/CS231N/22.png" alt></p>
</li>
<li><p>The results in the latter layers seems to mean something more than the other layers.</p>
</li>
</ul>
</li>
<li><p>We can fool CNN by using this procedure:</p>
<ul>
<li>Start from an arbitrary image.            <code># Random picture based on nothing.</code></li>
<li>Pick an arbitrary class. <code># Random class</code></li>
<li>Modify the image to maximize the class.</li>
<li>Repeat until network is fooled.</li>
</ul>
</li>
<li><p>Results on fooling the network is pretty surprising!</p>
<ul>
<li><img src="/2020/07/29/CS231N/23.png" alt></li>
<li>For human eyes they are the same, but it fooled the network by adding just some noise!</li>
</ul>
</li>
<li><p><strong>DeepDream</strong>: Amplify existing features</p>
<ul>
<li>Google released deep dream on their website.</li>
<li>What its actually doing is the same procedure as fooling the NN that we discussed, but rather than synthesizing an image to maximize a specific neuron, instead try to amplify the neuron activations at some layer in the network.</li>
<li>Steps:<ul>
<li>Forward: compute activations at chosen layer.        <code># form an input image (Any image)</code></li>
<li>Set gradient of chosen layer equal to its activation.<ul>
<li>Equivalent to <code>I* = arg max[I] sum(f(I)^2)</code></li>
</ul>
</li>
<li>Backward: Compute gradient on image.</li>
<li>Update image.</li>
</ul>
</li>
<li>The code of deep dream is online you can download and check it yourself.</li>
</ul>
</li>
<li><p><strong>Feature Inversion</strong></p>
<ul>
<li>Gives us to know what types of elements parts of the image are captured at different layers in the network.</li>
<li>Given a CNN feature vector for an image, find a new image that: <ul>
<li>Matches the given feature vector.</li>
<li><em>looks natural</em> (image prior regularization) </li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Texture Synthesis</strong></p>
<ul>
<li>Old problem in computer graphics.</li>
<li>Given a sample patch of some texture, can we generate a bigger image of the same texture?</li>
<li>There is an algorithm which doesn’t depend on NN:<ul>
<li>Wei and Levoy, Fast Texture Synthesis using Tree-structured Vector Quantization, SIGGRAPH 2000</li>
<li>Its a really simple algorithm</li>
</ul>
</li>
<li>The idea here is that this is an old problem and there are a lot of algorithms that has already solved it but simple algorithms doesn’t work well on complex textures!</li>
<li>An idea of using NN has been proposed on 2015 based on gradient ascent and called it “Neural Texture Synthesis”<ul>
<li>It depends on something called Gram matrix.</li>
</ul>
</li>
</ul>
</li>
<li><p>Neural Style Transfer =  Feature + Gram Reconstruction</p>
<ul>
<li>Gatys, Ecker, and Bethge, Image style transfer using Convolutional neural networks, CVPR 2016</li>
<li>Implementation by pytorch <a href="https://github.com/jcjohnson/neural-style" target="_blank" rel="noopener">here</a>.</li>
</ul>
</li>
<li><p>Style transfer requires many forward / backward passes through VGG; very slow!</p>
<ul>
<li>Train another neural network to perform style transfer for us!</li>
<li>Fast Style Transfer is the solution.</li>
<li>Johnson, Alahi, and Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, ECCV 2016</li>
<li><a href="https://github.com/jcjohnson/fast-neural-style" target="_blank" rel="noopener">https://github.com/jcjohnson/fast-neural-style</a></li>
</ul>
</li>
<li><p>There are a lot of work on these style transfer and it continues till now!</p>
</li>
<li><p>Summary:</p>
<ul>
<li>Activations: Nearest neighbors, Dimensionality reduction, maximal patches, occlusion</li>
<li>Gradients: Saliency maps, class visualization, fooling images, feature inversion</li>
<li>Fun: DeepDream, Style Transfer</li>
</ul>
</li>
</ul>
<h2 id="13-Generative-models"><a href="#13-Generative-models" class="headerlink" title="13. Generative models"></a>13. Generative models</h2><ul>
<li><p>Generative models are type of Unsupervised learning.</p>
</li>
<li><p>Supervised vs Unsupervised Learning:</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Supervised Learning</th>
<th>Unsupervised Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data structure</td>
<td>Data: (x, y), and x is data, y is label</td>
<td>Data: x, Just data, no labels!</td>
</tr>
<tr>
<td>Data price</td>
<td>Training data is expensive in a lot of cases.</td>
<td>Training data are cheap!</td>
</tr>
<tr>
<td>Goal</td>
<td>Learn a function to map x -&gt; y</td>
<td>Learn some underlying hidden structure of the data</td>
</tr>
<tr>
<td>Examples</td>
<td>Classification, regression, object detection, semantic segmentation, image captioning</td>
<td>Clustering, dimensionality reduction, feature learning, density estimation</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>Autoencoders are a Feature learning technique.</p>
<ul>
<li><img src="/2020/07/29/CS231N/24.png" alt></li>
<li>It contains an encoder and a decoder. The encoder downsamples the image while the decoder upsamples the features.</li>
<li>The loss are L2 loss.</li>
</ul>
</li>
<li><p>Density estimation is where we want to learn/estimate the underlaying distribution for the data!</p>
</li>
<li><p>There are a lot of research open problems in unsupervised learning compared with supervised learning!</p>
</li>
<li><p><strong>Generative Models</strong></p>
<ul>
<li>Given training data, generate new samples from same distribution.</li>
<li>Addresses density estimation, a core problem in unsupervised learning.</li>
<li>We have different ways to do this:<ul>
<li>Explicit density estimation: explicitly define and solve for the learning model.</li>
<li>Learn model that can sample from the learning model without explicitly defining it.</li>
</ul>
</li>
<li>Why Generative Models?<ul>
<li>Realistic samples for artwork, super-resolution, colorization, etc</li>
<li>Generative models of time-series data can be used for simulation and planning (reinforcement learning applications!)</li>
<li>Training generative models can also enable inference of latent representations that can be useful as general features</li>
</ul>
</li>
<li>Taxonomy of Generative Models:<ul>
<li><img src="/2020/07/29/CS231N/52.png" alt></li>
</ul>
</li>
<li>In this lecture we will discuss: PixelRNN/CNN, Variational Autoencoder, and GANs as they are the popular models in research now.</li>
</ul>
</li>
<li><p><strong>PixelRNN</strong> and <strong>PixelCNN</strong></p>
<ul>
<li>In a full visible belief network we use the chain rule to decompose likelihood of an image x into product of 1-d distributions<ul>
<li><code>p(x) = sum(p(x[i]| x[1]x[2]....x[i-1]))</code></li>
<li>Where p(x) is the Likelihood of image x and x[i] is Probability of i’th pixel value given all previous pixels.</li>
</ul>
</li>
<li>To solve the problem we need to maximize the likelihood of training data but the distribution is so complex over pixel values.</li>
<li>Also we will need to define ordering of <u>previous pixels</u>.</li>
<li>PixelRNN<ul>
<li>Founded by [van der Oord et al. 2016]</li>
<li>Dependency on previous pixels modeled using an RNN (LSTM)</li>
<li>Generate image pixels starting from corner</li>
<li>Drawback: sequential generation is slow! because you have to generate pixel by pixel!</li>
</ul>
</li>
<li>PixelCNN<ul>
<li>Also Founded by [van der Oord et al. 2016]</li>
<li>Still generate image pixels starting from corner.</li>
<li>Dependency on previous pixels now modeled using a CNN over context region</li>
<li>Training is faster than PixelRNN (can parallelize convolutions since context region values known from training images)</li>
<li>Generation must still proceed sequentially still slow.</li>
</ul>
</li>
<li>There are some tricks to improve PixelRNN &amp; PixelCNN.</li>
<li>PixelRNN and PixelCNN can generate good samples and are still active area of research.</li>
</ul>
</li>
<li><p><strong>Autoencoders</strong></p>
<ul>
<li>Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data.</li>
<li>Consists of Encoder and decoder.</li>
<li>The encoder:<ul>
<li>Converts the input x to the features z. z should be smaller than x to get only the important values out of the input. We can call this dimensionality reduction.</li>
<li>The encoder can be made with:<ul>
<li>Linear or non linear layers (earlier days days)</li>
<li>Deep fully connected NN (Then)</li>
<li>RELU CNN (Currently we use this on images)</li>
</ul>
</li>
</ul>
</li>
<li>The decoder:<ul>
<li>We want the encoder to map the features we have produced to output something similar to x or the same x.</li>
<li>The decoder can be made with the same techniques we made the encoder and currently it uses a RELU CNN.</li>
</ul>
</li>
<li>The encoder is a conv layer while the decoder is deconv layer! Means Decreasing and then increasing.</li>
<li>The loss function is L2 loss function:<ul>
<li><code>L[i] = |y[i] - y&#39;[i]|^2</code><ul>
<li>After training we though away the decoder.<code># Now we have the features we need</code></li>
</ul>
</li>
</ul>
</li>
<li>We can use this encoder we have to make a supervised model.<ul>
<li>The value of this it can learn a good feature representation to the input you have.</li>
<li>A lot of times we will have a small amount of data to solve problem. One way to tackle this is to use an Autoencoder that learns how to get features from images and train your small dataset on top of that model.</li>
</ul>
</li>
<li>The question is can we generate data (Images) from this Autoencoder?</li>
</ul>
</li>
<li><p><strong>Variational Autoencoders (VAE)</strong></p>
<ul>
<li>Probabilistic spin on Autoencoders - will let us sample from the model to generate data!</li>
<li>We have z as the features vector that has been formed using the encoder.</li>
<li>We then choose prior p(z) to be simple, e.g. Gaussian. <ul>
<li>Reasonable for hidden attributes: e.g. pose, how much smile.</li>
</ul>
</li>
<li>Conditional p(x|z) is complex (generates image) =&gt; represent with neural network</li>
<li>But we cant compute integral for P(z)p(x|z)dz as the following equation:<ul>
<li><img src="/2020/07/29/CS231N/25.png" alt></li>
</ul>
</li>
<li>After resolving all the equations that solves the last equation we should get this:<ul>
<li><img src="/2020/07/29/CS231N/26.png" alt></li>
</ul>
</li>
<li>Variational Autoencoder are an approach to generative models but Samples blurrier and lower quality compared to state-of-the-art (GANs)</li>
<li>Active areas of research:<ul>
<li>More flexible approximations, e.g. richer approximate posterior instead of diagonal Gaussian</li>
<li>Incorporating structure in latent variables</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Generative Adversarial Networks (GANs)</strong></p>
<ul>
<li><p>GANs don’t work with any explicit density function!</p>
</li>
<li><p>Instead, take game-theoretic approach: learn to generate from training distribution through 2-player game.</p>
</li>
<li><p>Yann LeCun, who oversees AI research at Facebook, has called GANs:</p>
<ul>
<li><blockquote>
<p>The coolest idea in deep learning in the last 20 years</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Problem: Want to sample from complex, high-dimensional training distribution. No direct way to do this as we have discussed!</p>
</li>
<li><p>Solution: Sample from a simple distribution, e.g. random noise. Learn transformation to training distribution.</p>
</li>
<li><p>So we create a noise image which are drawn from simple distribution feed it to NN we will call it a generator network that should learn to transform this into the distribution we want.</p>
</li>
<li><p>Training GANs: Two-player game:</p>
<ul>
<li><strong>Generator network</strong>: try to fool the discriminator by generating real-looking images.</li>
<li><strong>Discriminator network</strong>: try to distinguish between real and fake images.</li>
</ul>
</li>
<li><p>If we are able to train the Discriminator well then we can train the generator to generate the right images.</p>
</li>
<li><p>The loss function of GANs as minimax game are here:</p>
<ul>
<li><img src="/2020/07/29/CS231N/27.png" alt></li>
</ul>
</li>
<li><p>The label of the generator network will be 0 and the real images are 1.</p>
</li>
<li><p>To train the network we will do:</p>
<ul>
<li>Gradient ascent on discriminator.</li>
<li>Gradient ascent on generator but with different loss.</li>
</ul>
</li>
<li><p>You can read the full algorithm with the equations here:</p>
<ul>
<li><img src="/2020/07/29/CS231N/28.png" alt></li>
</ul>
</li>
<li><p>Aside: Jointly training two networks is challenging, can be unstable. Choosing objectives with better loss landscapes helps training is an active area of research.</p>
</li>
<li><p>Convolutional Architectures:</p>
<ul>
<li>Generator is an upsampling network with fractionally-strided convolutions Discriminator is a Convolutional network.</li>
<li>Guidelines for stable deep Conv GANs:<ul>
<li>Replace any pooling layers with strided convs (discriminator) and fractional-strided convs with (Generator).</li>
<li>Use batch norm for both networks.</li>
<li>Remove fully connected hidden layers for deeper architectures.</li>
<li>Use RELU activation in generator for all layers except the output which uses Tanh</li>
<li>Use leaky RELU in discriminator for all the layers.</li>
</ul>
</li>
</ul>
</li>
<li><p>2017 is the year of the GANs! it has exploded and there are some really good results.</p>
</li>
<li><p>Active areas of research also is GANs for all kinds of applications.</p>
</li>
<li><p>The GAN zoo can be found here: <a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">https://github.com/hindupuravinash/the-gan-zoo</a></p>
</li>
<li><p>Tips and tricks for using GANs: <a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">https://github.com/soumith/ganhacks</a></p>
</li>
<li><p>NIPS 2016 Tutorial GANs: <a href="https://www.youtube.com/watch?v=AJVyzd0rqdc" target="_blank" rel="noopener">https://www.youtube.com/watch?v=AJVyzd0rqdc</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="14-Deep-reinforcement-learning"><a href="#14-Deep-reinforcement-learning" class="headerlink" title="14. Deep reinforcement learning"></a>14. Deep reinforcement learning</h2><ul>
<li>This section contains a lot of math.</li>
<li>Reinforcement learning problems are involving an agent interacting with an environment, which provides numeric reward signals.</li>
<li>Steps are:<ul>
<li>Environment —&gt; State <code>s[t]</code> —&gt; Agent —&gt; Action <code>a[t]</code> —&gt; Environment —&gt; <code>Reward r[t]</code> + Next state <code>s[t+1]</code> —&gt; Agent —&gt; and so on..</li>
</ul>
</li>
<li>Our goal is learn how to take actions in order to maximize reward.</li>
<li>An example is Robot Locomotion:<ul>
<li>Objective: Make the robot move forward</li>
<li>State: Angle and position of the joints</li>
<li>Action: Torques applied on joints</li>
<li>1 at each time step upright + forward movement</li>
</ul>
</li>
<li>Another example is Atari Games:<ul>
<li>Deep learning has a good state of art in this problem.</li>
<li>Objective: Complete the game with the highest score.</li>
<li>State: Raw pixel inputs of the game state.</li>
<li>Action: Game controls e.g. Left, Right, Up, Down</li>
<li>Reward: Score increase/decrease at each time step</li>
</ul>
</li>
<li>Go game is another example which AlphaGo team won in the last year (2016) was a big achievement for AI and deep learning because the problem was so hard.</li>
<li>We can mathematically formulate the RL (reinforcement learning) by using <u><strong>Markov Decision Process</strong></u></li>
<li><strong>Markov Decision Process</strong><ul>
<li>Defined by (<code>S</code>, <code>A</code>, <code>R</code>, <code>P</code>, <code>Y</code>) where:<ul>
<li><code>S</code>: set of possible states.</li>
<li><code>A</code>: set of possible actions</li>
<li><code>R</code>: distribution of reward given (state, action) pair</li>
<li><code>P</code>: transition probability i.e. distribution over next state given (state, action) pair</li>
<li><code>Y</code>: discount factor    <code># How much we value rewards coming up soon verses later on.</code></li>
</ul>
</li>
<li>Algorithm:<ul>
<li>At time step <code>t=0</code>, environment samples initial state <code>s[0]</code></li>
<li>Then, for t=0 until done:<ul>
<li>Agent selects action <code>a[t]</code></li>
<li>Environment samples reward from <code>R</code> with (<code>s[t]</code>, <code>a[t]</code>)</li>
<li>Environment samples next state from <code>P</code> with (<code>s[t]</code>, <code>a[t]</code>)</li>
<li>Agent receives reward <code>r[t]</code> and next state <code>s[t+1]</code></li>
</ul>
</li>
</ul>
</li>
<li>A policy <code>pi</code>  is a function from S to A that specifies what action to take in each state.</li>
<li>Objective: find policy <code>pi*</code> that maximizes cumulative discounted reward: <code>Sum(Y^t * r[t], t&gt;0)</code></li>
<li>For example:<ul>
<li><img src="/2020/07/29/CS231N/29.png" alt></li>
</ul>
</li>
<li>Solution would be:<ul>
<li><img src="/2020/07/29/CS231N/30.png" alt></li>
</ul>
</li>
</ul>
</li>
<li>The value function at state <code>s</code>, is the expected cumulative reward from following the policy from state <code>s</code>:<ul>
<li><code>V[pi](s) = Sum(Y^t * r[t], t&gt;0) given s0 = s, pi</code></li>
</ul>
</li>
<li>The Q-value function at state s and action <code>a</code>, is the expected cumulative reward from taking action <code>a</code> in state <code>s</code> and then following the policy:<ul>
<li><code>Q[pi](s,a) = Sum(Y^t * r[t], t&gt;0) given s0 = s,a0 = a, pi</code></li>
</ul>
</li>
<li>The optimal Q-value function <code>Q*</code> is the maximum expected cumulative reward achievable from a given (state, action) pair:<ul>
<li><code>Q*[s,a] = Max(for all of pi on (Sum(Y^t * r[t], t&gt;0) given s0 = s,a0 = a, pi))</code></li>
</ul>
</li>
<li>Bellman equation<ul>
<li>Important thing is RL.</li>
<li>Given any state action pair (s,a) the value of this pair is going to be the reward that you are going to get r plus the value of the state that you end in.</li>
<li><code>Q*[s,a] = r + Y * max Q*(s&#39;,a&#39;) given s,a  # Hint there is no policy in the equation</code></li>
<li>The optimal policy <code>pi*</code> corresponds to taking the best action in any state as specified by <code>Q*</code></li>
</ul>
</li>
<li>We can get the optimal policy using the value iteration algorithm that uses the Bellman equation as an iterative update<ul>
<li><img src="/2020/07/29/CS231N/31.png" alt></li>
</ul>
</li>
<li>Due to the huge space dimensions in real world applications we will use a function approximator to estimate <code>Q(s,a)</code>. E.g. a neural network! this is called <strong>Q-learning</strong><ul>
<li>Any time we have a complex function that we cannot represent we use Neural networks!</li>
</ul>
</li>
<li><strong>Q-learning</strong><ul>
<li>The first deep learning algorithm that solves the RL.</li>
<li>Use a function approximator to estimate the action-value function</li>
<li>If the function approximator is a deep neural network =&gt; deep q-learning</li>
<li>The loss function:<ul>
<li><img src="/2020/07/29/CS231N/32.png" alt></li>
</ul>
</li>
</ul>
</li>
<li>Now lets consider the “Playing Atari Games” problem:<ul>
<li>Our total reward are usually the reward we are seeing in the top of the screen.</li>
<li>Q-network Architecture:<ul>
<li><img src="/2020/07/29/CS231N/33.png" alt></li>
</ul>
</li>
<li>Learning from batches of consecutive samples is a problem. If we recorded a training data and set the NN to work with it, if the data aren’t enough we will go to a high bias error. so we should use “experience replay” instead of consecutive samples where the NN will try the game again and again until it masters it.</li>
<li>Continually update a replay memory table of transitions (<code>s[t]</code> , <code>a[t]</code> , <code>r[t]</code> , <code>s[t+1]</code>) as game (experience) episodes are played.</li>
<li>Train Q-network on random minibatches of transitions from the replay memory, instead of consecutive samples.</li>
<li>The full algorithm:<ul>
<li><img src="/2020/07/29/CS231N/34.png" alt></li>
</ul>
</li>
<li>A video that demonstrate the algorithm on Atari game can be found here: “<a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=V1eYniJ0Rnk</a>“</li>
</ul>
</li>
<li><strong>Policy Gradients</strong><ul>
<li>The second deep learning algorithm that solves the RL.</li>
<li>The problem with Q-function is that the Q-function can be very complicated.<ul>
<li>Example: a robot grasping an object has a very high-dimensional state.</li>
<li>But the policy can be much simpler: just close your hand.</li>
</ul>
</li>
<li>Can we learn a policy directly, e.g. finding the best policy from a collection of policies?</li>
<li>Policy Gradients equations:<ul>
<li><img src="/2020/07/29/CS231N/35.png" alt></li>
</ul>
</li>
<li>Converges to a local minima of <code>J(ceta)</code>, often good enough!</li>
<li>REINFORCE algorithm is the algorithm that will get/predict us the best policy</li>
<li>Equation and intuition of the Reinforce algorithm:<ul>
<li><img src="/2020/07/29/CS231N/36.png" alt></li>
<li>the problem was high variance with this equation can we solve this?</li>
<li>variance reduction is an active research area!</li>
</ul>
</li>
<li>Recurrent Attention Model (RAM) is an algorithm that are based on REINFORCE algorithm and is used for image classification problems:<ul>
<li>Take a sequence of “glimpses” selectively focusing on regions of the image, to predict class<ul>
<li>Inspiration from human perception and eye movements.</li>
<li>Saves computational resources =&gt; scalability<ul>
<li>If an image with high resolution you can save a lot of computations</li>
</ul>
</li>
<li>Able to ignore clutter / irrelevant parts of image</li>
</ul>
</li>
<li>RAM is used now in a lot of tasks: including fine-grained image recognition, image captioning, and visual question-answering</li>
</ul>
</li>
<li>AlphaGo are using a mix of supervised learning and reinforcement learning, It also using policy gradients.</li>
</ul>
</li>
<li>A good course from Standford on deep reinforcement learning<ul>
<li><a href="http://web.stanford.edu/class/cs234/index.html" target="_blank" rel="noopener">http://web.stanford.edu/class/cs234/index.html</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX</a></li>
</ul>
</li>
<li>A good course on deep reinforcement learning (2017)<ul>
<li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener">http://rll.berkeley.edu/deeprlcourse/</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3</a></li>
</ul>
</li>
<li>A good article<ul>
<li><a href="https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html" target="_blank" rel="noopener">https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html</a></li>
</ul>
</li>
</ul>
<h2 id="15-Efficient-Methods-and-Hardware-for-Deep-Learning"><a href="#15-Efficient-Methods-and-Hardware-for-Deep-Learning" class="headerlink" title="15. Efficient Methods and Hardware for Deep Learning"></a>15. Efficient Methods and Hardware for Deep Learning</h2><ul>
<li>The original lecture was given by Song Han a PhD Candidate at standford.</li>
<li>Deep Conv nets, Recurrent nets, and deep reinforcement learning are shaping a lot of applications and changing a lot of our lives.<ul>
<li>Like self driving cars, machine translations, alphaGo and so on.</li>
</ul>
</li>
<li>But the trend now says that if we want a high accuracy we need a larger (Deeper) models.<ul>
<li>The model size in ImageNet competation from 2012 to 2015 has increased 16x to achieve a high accurecy.</li>
<li>Deep speech 2 has 10x training operations than deep speech 1 and thats in only one year! <code># At Baidu</code></li>
</ul>
</li>
<li>There are three challenges we got from this<ul>
<li><strong>Model Size</strong><ul>
<li>Its hard to deploy larger models on our PCs, mobiles, or cars.</li>
</ul>
</li>
<li><strong>Speed</strong><ul>
<li>ResNet152 took 1.5 weeks to train and give the 6.16% accurecy!</li>
<li>Long training time limits ML researcher’s productivity</li>
</ul>
</li>
<li><strong>Energy Efficiency</strong><ul>
<li>AlphaGo: 1920 CPUs and 280 GPUs. $3000 electric bill per game</li>
<li>If we use this on our mobile it will drain the battery.</li>
<li>Google mentioned in thier blog if all the users used google speech for 3 minutes, they have to double thier data-center!</li>
<li>Where is the Energy Consumed?<ul>
<li>larger model =&gt; more memory reference =&gt; more energy</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>We can improve the Efficiency of Deep Learning by Algorithm-Hardware Co-Design.<ul>
<li>From both the hardware and the algorithm perspectives.</li>
</ul>
</li>
<li>Hardware 101: the Family<ul>
<li><strong>General Purpose</strong>            <code># Used for any hardware</code><ul>
<li>CPU                <code># Latency oriented, Single strong threaded like a single elepahnt</code></li>
<li>GPU            <code># Throughput oriented, So many small threads like a lot of ants</code></li>
<li>GPGPU<ul>
<li><strong>Specialized HW</strong>        <code>#Tuned for a domain of applications</code><ul>
<li>FPGA <code># Programmable logic, Its cheaper but less effiecnet</code></li>
<li>ASIC <code># Fixed logic, Designed for a certian applications (Can be designed for deep learning applications)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Hardware 101: Number Representation<ul>
<li>Numbers in computer are represented with a discrete memory.</li>
<li>Its very good and energy efficent for hardware to go from 32 bit to 16 bit in float point operations.</li>
</ul>
</li>
<li>Part 1: <strong><u>Algorithms for Efficient Inference</u></strong><ul>
<li><strong>Pruning neural networks</strong><ul>
<li>Idea is can we remove some of the weights/neurons and the NN still behave the same?</li>
<li>In 2015 Han made AlexNet parameters from 60 million to 6 Million! by using the idea of Pruning.</li>
<li>Pruning can be applied to CNN and RNN, iteratively it will reach the same accurecy as the original.</li>
<li>Pruning actually happends to humans:<ul>
<li>Newborn(50 Trillion Synapses) ==&gt; 1 year old(1000 Trillion Synapses) ==&gt; Adolescent(500 Trillion Synapses)</li>
</ul>
</li>
<li>Algorithm:<ol>
<li>Get Trained network.</li>
<li>Evaluate importance of neurons.</li>
<li>Remove the least important neuron.</li>
<li>Fine tune the network.</li>
<li>If we need to continue Pruning we go to step 2 again else we stop.</li>
</ol>
</li>
</ul>
</li>
<li><strong>Weight Sharing</strong><ul>
<li>The idea is that we want to make the numbers is our models less.</li>
<li>Trained Quantization:<ul>
<li>Example: all weight values that are 2.09, 2.12, 1.92, 1.87 will be replaced by 2</li>
<li>To do that we can make k means clustering on a filter for example and reduce the numbers in it. By using this we can also reduce the number of operations that are used from calculating the gradients.</li>
<li>After Trained Quantization the Weights are Discrete.</li>
<li>Trained Quantization can reduce the number of bits we need for a number in each layer significantly.</li>
</ul>
</li>
<li>Pruning + Trained Quantization can Work Together to reduce the size of the model.</li>
<li>Huffman Coding<ul>
<li>We can use Huffman Coding to reduce/compress the number of bits of the weight.</li>
<li>In-frequent weights: use more bits to represent.</li>
<li>Frequent weights: use less bits to represent.</li>
</ul>
</li>
<li>Using Pruning + Trained Quantization + Huffman Coding is called deep compression.<ul>
<li><img src="/2020/07/29/CS231N/37.png" alt></li>
<li><img src="/2020/07/29/CS231N/38.png" alt></li>
<li><strong>SqueezeNet</strong><ul>
<li>All the models we have talked about till now was using a pretrained models. Can we make a new arcitecutre that saves memory and computations?</li>
<li>SqueezeNet gets the alexnet accurecy with 50x fewer parameters and 0.5 model size.</li>
</ul>
</li>
<li>SqueezeNet can even be further compressed by applying deep compression on them.</li>
<li>Models are now more energy efficient and has speed up a lot.</li>
<li>Deep compression was applied in Industry through facebook and Baidu.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Quantization</strong><ul>
<li>Algorithm (Quantizing the Weight and Activation):<ul>
<li>Train with float.</li>
<li>Quantizing the weight and activation:<ul>
<li>Gather the statistics for weight and activation.</li>
<li>Choose proper radix point position.</li>
</ul>
</li>
<li>Fine-tune in float format.</li>
<li>Convert to fixed-point format.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Low Rank Approximation</strong><ul>
<li>Is another size reduction algorithm that are used for CNN.</li>
<li>Idea is decompose the conv layer and then try both of the composed layers.</li>
</ul>
</li>
<li><strong>Binary / Ternary Net</strong><ul>
<li>Can we only use three numbers to represent weights in NN?</li>
<li>The size will be much less with only -1, 0, 1.</li>
<li>This is a new idea that was published in 2017 “Zhu, Han, Mao, Dally. Trained Ternary Quantization, ICLR’17”</li>
<li>Works after training.</li>
<li>They have tried it on AlexNet and it has reached almost the same error as AlexNet.</li>
<li>Number of operation will increase per register: <a href="https://xnor.ai/" target="_blank" rel="noopener">https://xnor.ai/</a></li>
</ul>
</li>
<li><strong>Winograd Transformation</strong><ul>
<li>Based on 3x3 WINOGRAD Convolutions which makes less operations than the ordiany convolution</li>
<li>cuDNN 5 uses the WINOGRAD Convolutions which has improved the speed.</li>
</ul>
</li>
</ul>
</li>
<li>Part 2: <strong><u>Hardware for Efficient Inference</u></strong><ul>
<li>There are a lot of ASICs that we developed for deep learning. All in which has the same goal of minimize memory access.<ul>
<li>Eyeriss MIT</li>
<li>DaDiannao</li>
<li>TPU Google (Tensor processing unit)<ul>
<li>It can be put to replace the disk in the server.</li>
<li>Up to 4 cards per server.</li>
<li>Power consumed by this hardware is a lot less than a GPU and the size of the chip is less.</li>
</ul>
</li>
<li>EIE Standford<ul>
<li>By Han at 2016 [et al. ISCA’16]</li>
<li>We don’t save zero weights and make quantization for the numbers from the hardware.</li>
<li>He says that EIE has a better Throughput and energy efficient.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Part 3: <strong><u>Algorithms for Efficient Training</u></strong><ul>
<li><strong>Parallelization</strong><ul>
<li><strong>Data Parallel</strong> – Run multiple inputs in parallel<ul>
<li>Ex. Run two images in the same time!</li>
<li>Run multiple training examples in parallel.</li>
<li>Limited by batch size.</li>
<li>Gradients have to be applied by a master node.</li>
</ul>
</li>
<li><strong>Model Parallel</strong><ul>
<li>Split up the Model – i.e. the network</li>
<li>Split model over multiple processors By layer.</li>
</ul>
</li>
<li>Hyper-Parameter Parallel<ul>
<li>Try many alternative networks in parallel.</li>
<li>Easy to get 16-64 GPUs training one model in parallel.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Mixed Precision</strong> with FP16 and FP32<ul>
<li>We have discussed that if we use 16 bit real numbers all over the model the energy cost will be less by x4.</li>
<li>Can we use a model entirely with 16 bit number? We can partially do this with mixed FP16 and FP32. We use 16 bit everywhere but at some points we need the FP32.</li>
<li>By example in multiplying FP16 by FP16 we will need FP32.</li>
<li>After you train the model you can be a near accuracy of the famous models like AlexNet and ResNet.</li>
</ul>
</li>
<li><strong>Model Distillation</strong><ul>
<li>The question is can we use a senior (Good) trained neural network(s) and make them guide a student (New) neural network?</li>
<li>For more information look at Hinton et al. Dark knowledge / Distilling the Knowledge in a Neural Network</li>
</ul>
</li>
<li>DSD: Dense-Sparse-Dense Training<ul>
<li>Han et al. “DSD: Dense-Sparse-Dense Training for Deep Neural Networks”, ICLR 2017</li>
<li>Has a better regularization.</li>
<li>The idea is Train the model lets call this the Dense, we then apply Pruning to it lets call this sparse.</li>
<li>DSD produces same model architecture but can find better optimization solution arrives at better local minima, and achieves higher prediction accuracy.</li>
<li>After the above two steps we go connect the remain connection and learn them again (To dense again).</li>
<li>This improves the performace a lot in many deep learning models.</li>
</ul>
</li>
</ul>
</li>
<li>Part 4: <strong><u>Hardware for Efficient Training</u></strong><ul>
<li>GPUs for training:<ul>
<li>Nvidia PASCAL GP100 (2016)</li>
<li>Nvidia Volta GV100 (2017)<ul>
<li>Can make mixed precision operations!</li>
<li>So powerful.</li>
<li>The new neclar bomb!</li>
</ul>
</li>
</ul>
</li>
<li>Google Announced “Google Cloud TPU” on May 2017!<ul>
<li>Cloud TPU delivers up to 180 teraflops to train and run machine learning models.</li>
<li>One of our new large-scale translation models used to take a full day to train on 32 of the best commercially-available GPUs—now it trains to the same accuracy in an afternoon using just one eighth of a TPU pod.</li>
</ul>
</li>
</ul>
</li>
<li>We have moved from PC Era ==&gt; Mobile-First Era ==&gt; AI-First Era</li>
</ul>
<h2 id="16-Adversarial-Examples-and-Adversarial-Training"><a href="#16-Adversarial-Examples-and-Adversarial-Training" class="headerlink" title="16. Adversarial Examples and Adversarial Training"></a>16. Adversarial Examples and Adversarial Training</h2><ul>
<li><strong><u>What are adversarial examples?</u></strong><ul>
<li>Since 2013, deep neural networks have matched human performance at..<ul>
<li>Face recognition</li>
<li>Object recognition</li>
<li>Captcha recognition<ul>
<li>Because its accuracy was higher than humans, Websites tried to find another solution than Captcha.</li>
</ul>
</li>
<li>And other tasks..</li>
</ul>
</li>
<li>Before 2013 no body was surprised if they saw a computer made a mistake! But now the deep learning exists and its so important to know the problems and the causes.</li>
<li>Adversarial are problems and unusual mistake that deep learning make.</li>
<li>This topic wasn’t hot until deep learning can now do better and better than human!</li>
<li>An adversarial is an example that has been carefully computed to to be misclassified.</li>
<li>In a lot of cases the adversarial image isn’t changed much compared to the original image from the human perspective.</li>
<li>History of recent papers:<ul>
<li>Biggio <a href="https://link.springer.com/chapter/10.1007/978-3-642-40994-3_25" target="_blank" rel="noopener">2013</a>: fool neural nets.</li>
<li>Szegedy et al 2013: fool ImageNet classifiers imperceptibly</li>
<li>Goodfellow et al <a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">2014</a>: cheap, closed form attack.</li>
</ul>
</li>
<li>So the first story was in 2013. When Szegedy had a CNN that can classify images very well.<ul>
<li>He wanted to understand more about how CNN works to improve it.</li>
<li>He give an image of an object and by using gradient ascent he tried to update the images so that it can be another object.</li>
<li>Strangely he found that the result image hasn’t changed much from the human perspective!</li>
<li>If you tried it you won’t notify any change and you will think that this is a bug! but it isn’t if you go for the image you will notice that they are completely different!</li>
</ul>
</li>
<li>These mistakes can be found in almost any deep learning algorithm we have studied!<ul>
<li>It turns out that RBF (Radial Basis Network) can resist this.</li>
<li>Deep Models for Density Estimation can resist this.</li>
</ul>
</li>
<li>Not just for neural nets can be fooled:<ul>
<li>Linear models<ul>
<li>Logistic regression</li>
<li>Softmax regression</li>
<li>SVMs</li>
</ul>
</li>
<li>Decision trees </li>
<li>Nearest neighbors</li>
</ul>
</li>
</ul>
</li>
<li><strong><u>Why do adversarial happen?</u></strong><ul>
<li>In the process in trying to understand what is happening, in 2016 they thought it was from overfitting models in the high dimensional data case.<ul>
<li>Because in such high dimensions we could have some random errors which can be found.</li>
<li>So if we trained a model with another parameters it should not make the same mistake?</li>
<li>They found that not right. Models are reaching to the same mistakes so it doesn’t mean its overfitting.</li>
</ul>
</li>
<li>In the previous mentioned experiment the found that the problem is caused by systematic thing not a random.<ul>
<li>If they add some vector to an example it would misclassified to any model.</li>
</ul>
</li>
<li>Maybe they are coming from underfitting not overfitting.</li>
<li>Modern deep nets are very piecewise linear<ul>
<li>Rectified linear unit</li>
<li>Carefully tuned sigmoid  <code># Most of the time we are inside the linear curve</code></li>
<li>Maxout</li>
<li>LSTM</li>
</ul>
</li>
<li>Relation between the parameter and the output are non linear because it’s multiplied together thats what make training NN difficult, while mapping from linear from input and output are linear and much easier.</li>
</ul>
</li>
<li><strong><u>How can adversarial be used to compromise machine learning systems?</u></strong><ul>
<li>If we are experimenting how easy a NN to fool, We want to make sure we are actually fooling it not just changing the output class, and if we are attackers we want to make this behavior to the NN (Get hole).</li>
<li>When we build Adversarial example we use the max norm constrain to perturbation.</li>
<li>The fast gradient sign method:<ul>
<li>This method comes from the fact that almost all NN are using a linear activations (Like RELU) the assumption we have told before.</li>
<li>No pixel can be changed more than some amount epsilon.</li>
<li>Fast way is to take the gradient of the cost you used to train the network with respect to the input and then take the sign of that gradient multiply this by epsilon.</li>
<li>Equation:<ul>
<li><code>Xdash = x + epslion * (sign of the gradient)</code></li>
<li>Where Xdash is the adversarial example and x is the normal example</li>
</ul>
</li>
<li>So it can be detected by just using the sign (direction) and some epsilon.</li>
</ul>
</li>
<li>Some attacks are based on ADAM optimizer.</li>
<li>Adversarial examples are not random noises!</li>
<li>NN are trained on some distribution and behaves well in that distribution. But if you shift this distribution the NN won’t answer the right answers. They will be so easy to fool.</li>
<li>deep RL can also be fooled.</li>
<li>Attack of the weights:<ul>
<li>In linear models, We can take the learned weights image, take the signs of the image and add it to any example to force the class of the weights to be true. Andrej Karpathy, “Breaking Linear Classifiers on ImageNet”</li>
</ul>
</li>
<li>It turns out that some of the linaer models performs well (We cant get advertisal from them easily)<ul>
<li>In particular Shallow RBFs network resist adversarial perturbation # By The fast gradient sign method<ul>
<li>The problem is RBFs doesn’t get so much accuracy on the datasets because its just a shallow model and if you tried to get this model deeper the gradients will become zero in almost all the layers.</li>
<li>RBFs are so difficult to train even with batch norm. algorithm.</li>
<li>Ian thinks if we have a better hyper parameters or a better optimization algorithm that gradient decent we will be able to train RBFs and solve the adversarial problem!</li>
</ul>
</li>
</ul>
</li>
<li>We also can use another model to fool current model. Ex use an SVM to fool a deep NN.<ul>
<li>For more details follow the paper: “Papernot 2016”</li>
</ul>
</li>
<li>Transferability Attack<ol>
<li>Target model with unknown weights, machine learning algorithm, training set; maybe non differentiable</li>
<li>Make your training set from this model using inputs from you, send them to the model and then get outputs from the model</li>
<li>Train you own model. “Following some table from Papernot 2016”</li>
<li>Create an Adversarial example on your model.</li>
<li>Use these examples against the model you are targeting.</li>
<li>You are almost likely to get good results and fool this target!</li>
</ol>
</li>
<li>In Transferability Attack to increase your probability by 100% of fooling a network, You can make more than just one model may be five models and then apply them. “(Liu et al, 2016)”</li>
<li>Adversarial Examples are works for human brain also! for example images that tricks your eyes. They are a lot over the Internet.</li>
<li>In practice some researches have fooled real models from (MetaMind, Amazon, Google)</li>
<li>Someone has uploaded some perturbation into facebook and facebook was fooled :D</li>
</ul>
</li>
<li><strong><u>What are the defenses?</u></strong><ul>
<li>A lot of defenses Ian tried failed really bad! Including:<ul>
<li>Ensembles</li>
<li>Weight decay</li>
<li>Dropout</li>
<li>Adding noise at train time or at test time</li>
<li>Removing perturbation with an autoencoder </li>
<li>Generative modeling</li>
</ul>
</li>
<li>Universal approximator theorem<ul>
<li>Whatever shape we would like our classification function to have a big enough NN can make it.</li>
<li>We could have train a NN that detects the Adversarial!</li>
</ul>
</li>
<li>Linear models &amp; KNN can be fooled easier than NN. Neural nets can actually become more secure than other models. Adversarial trained neural nets have the best empirical success rate on adversarial examples of any machine learning model.<ul>
<li>Deep NNs can be trained with non linear functions but we will just need a good optimization technique or solve the problem with using such linear activator like “RELU”</li>
</ul>
</li>
</ul>
</li>
<li><strong><u>How to use adversarial examples to improve machine learning, even when there is no adversary?</u></strong><ul>
<li>Universal engineering machine (model-based optimization)        <code>#Is called Universal engineering machine by Ian</code><ul>
<li>For example:<ul>
<li>Imagine that we want to design a car that are fast.</li>
<li>We trained a NN to look at the blueprints of a car and tell us if the blueprint will make us a fast car or not.</li>
<li>The idea here is to optimize the input to the network so that the output will max this could give us the best blueprint for a car!</li>
</ul>
</li>
<li>Make new inventions by finding input that maximizes model’s predicted performance.</li>
<li>Right now by using adversarial examples we are just getting the results we don’t like but if we have solve this problem we can have the fastest car, the best GPU, the best chair, new drugs…..</li>
</ul>
</li>
<li>The whole adversarial is an active area of research especially defending the network!</li>
</ul>
</li>
<li>Conclusion<ul>
<li>Attacking is easy</li>
<li>Defending is difficult</li>
<li>Adversarial training provides regularization and semi-supervised learning </li>
<li>The out-of-domain input problem is a bottleneck for model-based optimization generally</li>
</ul>
</li>
<li>There are a Github code that can make you learn everything about adversarial by code (Built above tensorflow):<ul>
<li>An adversarial example library for constructing attacks, building defenses, and benchmarking both: <a href="https://github.com/tensorflow/cleverhans" target="_blank" rel="noopener">https://github.com/tensorflow/cleverhans</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CS231N</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>论文-卷积经典模型-AlexNet</title>
    <url>/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/</url>
    <content><![CDATA[<p><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">点击下载原文pdf</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;papers.nips.cc&#x2F;paper&#x2F;4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</span><br></pre></td></tr></table></figure><br><a id="more"></a></p>
<p><a href="#" onclick="window.print()">下载/导出/打印本文</a></p>
<h1 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h1><div style="text-align:center;font-size:2rem">ImageNet Classification with Deep Convolutional Neural Networks</div>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes.On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art.The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective.We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting.<br>Until recently, datasets of labeled images were relatively small — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations.For example, the currentbest error rate on the MNIST digit-recognition task (&lt;0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets.And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images.The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.</p>
<p>To learn about thousands of objects from millions of images, we need a model with a large learning capacity.However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have.Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies).Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.</p>
<p>Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images.Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.</p>
<p>The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets.We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly $^1$.Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3.The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4.Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance.</p>
<hr>
<p>$^1$<a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="noopener">http://code.google.com/p/cuda-convnet/</a></p>
<p>In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate.Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.</p>
<h2 id="2-The-Dataset"><a href="#2-The-Dataset" class="headerlink" title="2.The Dataset"></a>2.The Dataset</h2><p>ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories.The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool.Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.</p>
<p>ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments.Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable.On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.</p>
<p>ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality.Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image.We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.</p>
<h2 id="3-The-Architecture"><a href="#3-The-Architecture" class="headerlink" title="3.The Architecture"></a>3.The Architecture</h2><p>The architecture of our network is summarized in Figure 2. It contains eight learned layers — five convolutional and three fully-connected. Below, we describe some of the novel or unusual features of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first.</p>
<h3 id="3-1-ReLU-Nonlinearity"><a href="#3-1-ReLU-Nonlinearity" class="headerlink" title="3.1 ReLU Nonlinearity"></a>3.1 ReLU Nonlinearity</h3><p>The standard way to model a neuron’s output f as a function of its input x is with f(x) = tanh(x) or f(x) = (1 + $e^x$)−1. In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x) = max(0, x). Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of iterations required to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional network. This plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models.</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure1.png" alt></p>
<p>Figure 1: A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen independently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.</p>
<p>We are not the first to consider alternatives to traditional neuron models in CNNs. For example, Jarrett etal.[11] claim that the nonlinearityf(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset. However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the performance of large models trained on large datasets.</p>
<h3 id="3-2-Training-on-Multiple-GPUs"><a href="#3-2-Training-on-Multiple-GPUs" class="headerlink" title="3.2 Training on Multiple GPUs"></a>3.2 Training on Multiple GPUs</h3><p>A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another’s memory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.</p>
<p>The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Cire¸ san et al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU  ${net}^2$。</p>
<hr>
<p>$^2$The one-GPU net actually has the same number of kernels as the two-GPU net in the final convolutional layer. This is because most of the net’s parameters are in the first fully-connected layer, which takes the last convolutional layer as input. So to make the two nets have approximately the same number of parameters, we did not halve the size of the final convolutional layer (nor the fully-conneced layers which follow). Therefore this comparison is biased in favor of the one-GPU net, since it is bigger than “half the size” of the two-GPU net.</p>
<h3 id="3-3-Local-Response-Normalization"><a href="#3-3-Local-Response-Normalization" class="headerlink" title="3.3 Local Response Normalization"></a>3.3 Local Response Normalization</h3><p>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization. Denoting by $a^i_{x, y}$ the activity of a neuron computed by applying kernel i at position (x, y) and then applying the ReLU nonlinearity, the response-normalized activity $b^i_{x, y}$ given by the expression</p>
<script type="math/tex; mode=display">b^i_{x,y} = a^i_{x,y}/ (k+a\sum^{min(N-1,i+n/2)}_{j=max(0,i-n/2)} {a^i_{x,y}}^2)^\beta</script><p>where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined before training begins. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = $10^{-4}$, and β = 0.75. We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).</p>
<p>This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11], but ours would be more correctly termed “brightness normalization”, since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization $^3$.</p>
<hr>
<p>$^3$We cannot describe this network in detail due to space constraints, but it is specified precisely by the code and parameter files provided here: <a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="noopener">http://code.google.com/p/cuda-convnet/</a></p>
<h3 id="3-4Overlapping-Pooling"><a href="#3-4Overlapping-Pooling" class="headerlink" title="3.4Overlapping Pooling"></a>3.4Overlapping Pooling</h3><p>Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map.  Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g.,[17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z × z centered at the location of the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. If we set s &lt; z, we obtain overlapping pooling. This is what we use throughout our network, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme s = 2, z = 2, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.</p>
<h3 id="3-5-Overall-Architecture"><a href="#3-5-Overall-Architecture" class="headerlink" title="3.5 Overall Architecture"></a>3.5 Overall Architecture</h3><p>Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fullyconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution. The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the previous layer.Response-normalization layers follow the first and second convolutional layers. Max-poolinglayers, ofthekinddescribedinSection 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer. The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure2.png" alt></p>
<p>Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and the number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264– 4096–4096–1000.</p>
<h2 id="4-Reducing-Overfitting"><a href="#4-Reducing-Overfitting" class="headerlink" title="4 Reducing Overfitting"></a>4 Reducing Overfitting</h2><p>Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting.</p>
<h3 id="4-1-Data-Augmentation"><a href="#4-1-Data-Augmentation" class="headerlink" title="4.1 Data Augmentation"></a>4.1 Data Augmentation</h3><p>The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free.</p>
<p>The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224×224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches $^4$. This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches.</p>
<hr>
<p>$^4$This is the reason why the input images in Figure 2 are 224 × 224 × 3-dimensional.</p>
<p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, we add multiples of the found principal components,with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel $I_{xy}=[I_{xy}^R,I_{xy}^G,I_{xy}^B]$ we add the following quantity:</p>
<script type="math/tex; mode=display">[P1,P2,P3][\alpha1 \lambda1,\alpha2 \lambda2,\alpha3 \lambda3]^T</script><p>where pi and λi are ith eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and αiis the aforementioned random variable. Each αiis drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1%.</p>
<h3 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h3><p>Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in backpropagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks. We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.</p>
<h2 id="5-Details-of-learning"><a href="#5-Details-of-learning" class="headerlink" title="5 Details of learning"></a>5 Details of learning</h2><p>We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error. The update rule for weight w was</p>
<script type="math/tex; mode=display">v_{i+1}:=0.9v_{i}-0.0005\epsilon w_i-\epsilon \langle \frac{\delta L}{\delta W }|_{w_i}\rangle _{D_i}</script><script type="math/tex; mode=display">w_{i+1}:=w_i+v_{i+1}</script><p>where i is the iteration index,v is the momentum variable,$\epsilon $is the learning rate,$\langle \frac{\delta L}{\delta W }|_{w_i}\rangle _{D_i}$is the average over the ith batch Diof the derivative of the objective with respect to w, evaluated at wi.<br><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure3.png" alt></p>
<p>Figure 3: 96 convolutional kernels of size 11×11×3learned by the first convolutional layer on the 224×224×3 input images. The top 48 kernels were learned on GPU 1 while the bottom 48 kernels were learned on GPU 2. See Section 6.1 for details.</p>
<p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0.</p>
<p>We used an equal learning rate for all layers, which we adjusted manually throughout training.<br>The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination. We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.</p>
<h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6 Results"></a>6 Results</h2><p>Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% $^5$.</p>
<hr>
<p>$^5 $ The error rates without averaging predictions over ten patches as described in Section 4.1 are 39.0% and 18.3%. The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/table1.png" alt><br>Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/table2.png" alt><br>Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were “pre-trained” to classify the entire ImageNet 2011 Fall release. See Section 6 for details.</p>
<p>We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7]. </p>
<p>Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].</p>
<h3 id="6-1-Qualitative-Evaluations"><a href="#6-1-Qualitative-Evaluations" class="headerlink" title="6.1 Qualitative Evaluations"></a>6.1 Qualitative Evaluations</h3><p>Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The network has learned a variety of frequency- and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs).</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure4.png" alt></p>
<p>Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.</p>
<p>In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example, only other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph.</p>
<p>Another way to probe the network’s visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar. Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the results for many more test images in the supplementary material.</p>
<p>Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vec-torsisinefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes. This should produce a much better image retrieval method than applying auto-encoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar.</p>
<h2 id="7-Discussion"><a href="#7-Discussion" class="headerlink" title="7 Discussion"></a>7 Discussion</h2><p>Our results show that a large, deep convolutional neural network is capable of achieving recordbreaking results on a highly challenging dataset using purely supervised learning. It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.</p>
<p>To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data. Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system. Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] R.M. Bell and Y . Koren. Lessons from the netflixprizechallenge. ACM SIGKDD Explorations Newsletter,<br>9(2):75–79, 2007.<br>[2] A. Berg, J. Deng, and L. Fei-Fei.  Large scale visual recognition challenge 2010. www.image-<br>net.org/challenges. 2010.<br>[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.<br>[4] D. Cire¸ san, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.<br>Arxiv preprint arXiv:1202.2745, 2012.<br>[5] D.C. Cire¸ san, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural<br>networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.<br>[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical<br>Image Database. In CVPR09, 2009.<br>[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012.  URL<br><a href="http://www.image-net.org/challenges/LSVRC/2012/" target="_blank" rel="noopener">http://www.image-net.org/challenges/LSVRC/2012/</a>.<br>[8] L. Fei-Fei, R. Fergus, and P . Perona. Learning generative visual models from few training examples: An<br>incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand-<br>ing, 106(1):59–70, 2007.<br>[9] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, Cali-<br>fornia Institute of Technology, 2007. URL <a href="http://authors.library.caltech.edu/7694" target="_blank" rel="noopener">http://authors.library.caltech.edu/7694</a>.<br>[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural net-<br>works by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.<br>[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y . LeCun. What is the best multi-stage architecture for<br>object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.<br>[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of<br>Computer Science, University of Toronto, 2009.<br>[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.<br>[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In<br>ESANN, 2011.<br>[15] Y . Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand-<br>written digit recognition with a back-propagation network. In Advances in neural information processing<br>systems, 1990.<br>[16] Y . LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to<br>pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the<br>2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.<br>[17] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In<br>Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256.<br>IEEE, 2010.<br>[18] H. Lee, R. Grosse, R. Ranganath, and A.Y . Ng. Convolutional deep belief networks for scalable unsuper-<br>vised learning of hierarchical representations. InProceedings of the 26th Annual International Conference<br>on Machine Learning, pages 609–616. ACM, 2009.<br>[19] T. Mensink, J. V erbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classifi-<br>cation: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer<br>Vision, Florence, Italy, October 2012.<br>[20] V . Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th<br>International Conference on Machine Learning, 2010.<br>[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computa-<br>tional biology, 4(1):e27, 2008.<br>[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering<br>good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,<br>2009.<br>[23] B.C. Russell, A. Torralba, K.P . Murphy, and W.T. Freeman. Labelme: a database and web-based tool for<br>image annotation. International journal of computer vision, 77(1):157–173, 2008.<br>[24] J. Sánchez and F. Perronnin. High-dimensional signature compression for large-scale image classification.<br>InComputer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,<br>2011.<br>[25] P .Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to<br>visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis<br>and Recognition, volume 2, pages 958–962, 2003.<br>[26] S.C. Turaga, J.F. Murray, V . Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H.S. Seung. Con-<br>volutional networks can learn to generate affinity graphs for image segmentation. Neural Computation,<br>22(2):511–538, 2010.</p>
<h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><div style="text-align:center;font-size:2rem">基于深卷积神经网络的图像网络分类</div>

<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们训练了一个大型的深卷积神经网络，将ImageNetLSVRC-2010竞赛中的120万张高分辨率图像分成1000个不同的类别。在测试数据上，我们获得了前1名和前5名的错误率分别为37.5%和17.0%，这比以前的最新技术要好得多。这个神经网络有6000万个参数和65万个神经元，由5个卷积层组成，其中一些是最大池化层，还有3个完全连接的层，最后是1000-way的softmax。为了加快训练速度，我们使用非饱和神经元和一个非常有效的GPU实现卷积运算。为了减少完全连接层中的过拟合，我们采用了一种最近发展起来的正则化方法，称为“dropout”，这被证明是非常有效的。我们还在ILSVRC-2012竞赛中输入了该模型的一个变体，并获得了15.3%的前5名测试错误率，而第二名的测试错误率为26.2%。</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>当前的目标识别方法主要利用机器学习方法。为了提高它们的性能，我们可以收集更大的数据集，学习更强大的模型，并使用更好的技术来防止过度拟合。直到最近，标记图像的数据集相对较小，只有数万张图像（例如，NORB[16]、Caltech-101/256[8,9]和CIFAR-10/100[12]）。使用这种大小的数据集可以很好地解决简单的识别任务，特别是如果使用保留标签的转换对它们进行扩充的话。例如，MNIST数字识别任务的当前最佳错误率（&lt;0.3%）接近人因绩效[4]。但是现实环境中的物体表现出相当大的可变性，所以要学会识别它们就必须使用更大的训练集。事实上，小图像数据集的缺点已经被广泛地认识到（例如，Pinto等人。[21]），但直到最近才有可能收集带有数百万图像的标记数据集。新的更大的数据集包括LabelMe[23]和ImageNet[6]，前者由数十万张全分割图像组成，后者由超过22000个类别的1500万张高分辨率图像组成。</p>
<p>要从数百万幅图像中了解成千上万的物体，我们需要一个具有很大学习能力的模型。然而，对象识别任务的巨大复杂性意味着即使像ImageNet这样大的数据集也不能指定这个问题，因此我们的模型还应该有大量的先验知识来弥补我们没有的所有数据。卷积神经网络(CNNs)就是这样一类模型[16,11,13,18,15,22,26]。它们的容量可以通过改变深度和宽度来控制，而且它们还可以对图像的性质(即统计的平稳性和像素依赖的局部性)做出强有力的、基本正确的假设。因此，与具有相似层大小的标准前馈神经网络相比，cnn具有更少的连接和参数，因此更容易训练，而理论上最好的性能可能只会稍微差一点。</p>
<p>尽管cnn具有吸引人的品质，尽管其本地架构相对高效，但大规模应用于高分辨率图像的成本仍令人望而却步。幸运的是，目前的gpu，加上一个高度优化的2D卷积实现，已经足够强大来方便训练大的cnn，而最近的数据集，比如ImageNet包含了足够多的带标记的例子来训练这样的模型，而不会出现严重的过拟合。</p>
<p>本文的具体贡献如下:我们在ILSVRC-2010和ILSVRC-2012竞赛[2]中使用的ImageNet子集训练了迄今为止最大的卷积神经网络之一，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的二维卷积GPU实现，以及训练卷积神经网络所固有的所有其他操作，这些操作我们已经公开提供了 $^1$。我们的网络包含了许多新的和不寻常的特性，这些特性提高了它的性能，减少了它的训练时间，这在第3节中详细介绍。我们的网络的规模使得过拟合成为一个严重的问题，即使有120万个标记的训练样本，所以我们使用了几种有效的技术来防止过拟合，这些技术将在第4节中描述。我们最终的网络包含5个卷积层和3个全连接层，而这种深度似乎很重要:我们发现，删除任何卷积层(每个卷积层包含的参数不超过模型的1%)都会导致性能下降。</p>
<hr>
<p>$^1$<a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="noopener">http://code.google.com/p/cuda-convnet/</a></p>
<p>最后，网络的大小主要受到当前gpu上可用的内存数量和我们愿意容忍的训练时间的限制。我们的网络在两个GTX 580 3GB gpu上需要5到6天的训练。我们所有的实验都表明，我们的结果可以通过等待更快的gpu和更大的数据集得到改善。</p>
<h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2.数据集"></a>2.数据集</h2><p>ImageNet是一个包含超过1500万张高分辨率图像的数据集，大约属于22,000个类别。这些图片是从网上收集的，并由人使用Amazon’s Mechanical Turk crowd-sourcing tool进行手工标记。从2010年开始，作为Pascal视觉物体挑战赛的一部分，一年一度的ImageNet大型视觉识别挑战赛(ILSVRC)已经举行。从2010年开始，作为Pascal视觉物体挑战赛的一部分，一年一度的ImageNet大型视觉识别挑战赛(ILSVRC)已经举行。</p>
<p>ILSVRC-2010是唯一一个有测试集标签的ILSVRC版本，所以我们在这个版本上执行了大部分实验。由于我们也在ILSVRC-2012竞赛中输入了我们的模型，所以在第6节中，我们也报告了这个版本的数据集的结果，因为测试集标签是不可用的。在ImageNet上，通常会报告两个错误率:top-1和top-5，其中top-5错误率是测试图像中正确的标签不在模型认为最可能出现的5个标签中所占的比例。</p>
<p>ImageNet由不同分辨率的图像组成，而我们的系统需要一个恒定的输入维数。因此，我们将图像下采样到固定分辨率为256×256。对于一个矩形图像，我们首先对图像进行重新标定，使较短的边的长度为256，然后从得到的图像中裁剪出中间的256×256的patch。除了从每个像素中减去训练集上的平均活跃度之外，我们没有采用任何其他方法对图像进行预处理。因此，我们根据像素(居中)的原始RGB值训练我们的网络。</p>
<h2 id="3-体系结构"><a href="#3-体系结构" class="headerlink" title="3.体系结构"></a>3.体系结构</h2><p>图2总结了我们的网络架构。它包含8个学习层——5个卷积层和3个完全连接层。下面，我们将描述我们网络架构的一些新奇或不寻常的特征。3.1-3.4部分根据我们对其重要性的估计进行排序，最重要的放在前面。</p>
<h3 id="3-1-非线性的ReLU"><a href="#3-1-非线性的ReLU" class="headerlink" title="3.1 非线性的ReLU"></a>3.1 非线性的ReLU</h3><p>将神经元的输出f建模为其输入x的函数的标准方法是:f(x) = tanh(x)或f(x) = (1 + $e^x$)−1。在梯度下降的训练时间方面，这些饱和非线性比非饱和非线性f(x) = max(0, x)要慢得多。根据Nair和Hinton[20]，我们将具有这种非线性的神经元称为改良的直线单元(Rectified Linear Units简称ReLUs)。使用ReLUs的深度卷积神经网络的训练速度比使用tanh单元的神经网络快好几倍。如图1所示，它显示了对于一个特定的四层卷积网络，在CIFAR-10数据集上达到25%的训练误差所需的迭代次数。这张图表明，如果我们使用传统的饱和神经元模型，我们就无法用这么大的神经网络进行实验。</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure1.png" alt></p>
<p>图1:带ReLUs(实线)的四层卷积神经网络在CIFAR-10上的训练错误率达到25%，比等效的带tanh神经元网络(虚线)快6倍。每个网络的学习率都是独立选择的，以便尽可能快地进行训练。没有采用任何形式的正规化。这里显示的影响的大小随着网络结构的不同而不同，但是有ReLUs的网络始终比有饱和神经元的网络学习速度快几倍。</p>
<p>我们不是第一个考虑替代传统神经元模型的cnn。例如，Jarrett etal.[11]声称非线性函数f(x) = |tanh(x)|与Caltech-101数据集上的对比归一化和本地平均池化层的类型一起工作得特别好。然而，在这个数据集上，主要的关注点是防止过拟合，因此他们观察到的效果与我们在使用ReLUs时报告的快速拟合训练集的能力不同。快速学习对在大数据集上训练的大模型的性能有很大的影响。这意味着，例如，第3层的内核从第2层的所有内核映射中获取输入。然而，层4中的内核只从层3中驻留在同一GPU上的内核映射中获取输入。</p>
<h3 id="3-2-在多GPU上的训练"><a href="#3-2-在多GPU上的训练" class="headerlink" title="3.2 在多GPU上的训练"></a>3.2 在多GPU上的训练</h3><p>一个GTX 580 GPU只有3GB的内存，这限制了它能训练的最大网络大小。事实证明，120万个训练例子足以训练那些大到无法在一个GPU上运行的网络。因此，我们将网络分散到两个gpu上。当前的gpu特别适合交叉gpu并行，因为它们能够直接读写彼此的内存，而不需要通过主机内存。我们采用的并行化方案本质上是将一半的内核(或神经元)放在每个GPU上，还有一个额外的技巧:GPU只在特定的层上通信。 选择连接模式是交叉验证的一个问题，但这允许我们精确地调整通信量，直到它成为计算量的一个可接受的部分。</p>
<p>这种架构与Cire¸ san et al. [5]所使用的“柱状”CNN有些相似，只是我们的专栏不是独立的(见图2)。 与在一个GPU上训练每个卷积层内核数量减半的网络相比，该方案将我们的top-1和top-5错误率分别降低了1.7%和1.2%。双gpu网络的训练时间略少于单gpu $网络^2$。</p>
<hr>
<p>$^2$在最终的卷积层中，单gpu网络实际上拥有与双gpu网络相同数量的内核。这是因为网络的大多数参数都在第一个完全连接层中，它将最后一个卷积层作为输入。 因此，为了使这两个网络具有大致相同数量的参数，我们没有将最后的卷积层(也没有将后面的完全连接层)的大小减半。因此这种比较偏向于单gpu网络，因为它大于双gpu网络的“一半大小”。</p>
<h3 id="3-3局部响应归一化"><a href="#3-3局部响应归一化" class="headerlink" title="3.3局部响应归一化"></a>3.3局部响应归一化</h3><p>ReLUs具有理想的特性，它们不需要输入归一化来防止饱和。如果至少有一些训练例子对一个ReLU产生了积极的输入，学习就会在那个神经元中发生。然而，我们仍然发现下面的局部归一化方案有助于推广。用$a^i_{x, y}$表示在位置(x, y)处应用核i计算的神经元的活性，再应用ReLU非线性，响应归一化的活性$b^i_{x, y}$由表达式给出</p>
<script type="math/tex; mode=display">b^i_{x,y} = a^i_{x,y}/ (k+a\sum^{min(N-1,i+n/2)}_{j=max(0,i-n/2)} {a^i_{x,y}}^2)^\beta</script><p>其中，和在同一空间位置的n个“相邻”核映射上运行，n是这一层的核总数。内核映射的顺序当然是任意的，在训练开始之前就确定了。这种响应归一化实现了一种形式的侧向抑制，这种抑制是由在真实神经元中发现的类型激发的，在使用不同内核计算的神经元输出之间产生对大型活动的竞争。常数k, n，以及，和，都是超参数，它们的值是通过验证集确定的;我们使用k = 2, n = 5，$\alpha$= $10^{-4}$，和$\beta$ = 0.75。我们在某些层中应用ReLU非线性后应用了这种归一化(见3.5节)。</p>
<p>这种方案与Jarrett等人[11]的局部对比度归一化方案有一些相似之处，但我们的方案更准确地称为“亮度归一化”，因为我们没有减去平均活动。 响应归一化使前1和前5的错误率分别降低了1.4%和1.2%。我们还验证了该方案在CIFAR-10数据集上的有效性:一个四层CNN在不进行归一化的情况下测试错误率为13%，在进行归一化的情况下测试错误率为11% $^3$。</p>
<hr>
<p>$^3$由于空间的限制，我们无法详细描述这个网络，但在这里提供的代码和参数文件中有精确的说明:<a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="noopener">http://code.google.com/p/cuda-convnet/</a></p>
<h3 id="3-4-重叠池化层"><a href="#3-4-重叠池化层" class="headerlink" title="3.4 重叠池化层"></a>3.4 重叠池化层</h3><p>网络神经网络的池化层总结同一核图中相邻神经元的输出。传统上，由相邻的共用单元汇总的社区并不重叠(如[17,11,4])。更精确地说，一个池化层可以被认为是由一个间隔s像素的池化单元网格组成，每个池化单元汇总一个大小为z×z的以池化单元位置为中心的邻域。设s = z，得到CNNs中常用的传统局部池。如果我们设置s &lt; z，我们得到重叠池。这是我们在整个网络中使用的，s = 2和z = 3。与不重叠的方案s = 2、z = 2相比，该方案的top-1和top-5错误率分别降低了0.4%和0.3%，产生了等效维数的输出。 我们通常在训练过程中观察到，使用重叠池的模型很难过度适应。</p>
<h3 id="3-5总体架构"><a href="#3-5总体架构" class="headerlink" title="3.5总体架构"></a>3.5总体架构</h3><p>现在我们准备描述CNN的整体架构。如图2所示，该网包含8个带有权重的层;前5个是卷积的，其余3个是完全连通的。最后的全连接层的输出被馈给一个1000-way的softmax，它产生超过1000类标签的分布。我们的网络最大化了多项logistic回归的目标，它等价于在预测分布下最大化正确标签的对数概率的训练案例的平均值。 第二、四、五卷积层的内核只与上一层位于同一GPU上的内核映射相连(见图2)。第三卷积层的内核与第二层的所有内核映射相连。完全连接层中的神经元与前一层的所有神经元相连。响应归一化层紧随第一个第二个卷积层。 3.4节提到的最大池化层遵循两个响应规范化层和第五个卷积层。每个卷积层和全连通层的输出均采用ReLU非线性。第一卷积层用96个11×11×3的核对224×224×3输入图像进行滤波，步长为4个像素（这是相邻的感受野中心之间的距离）。 第二个卷积层将第一个卷积层的（响应规范化和池化）输出作为输入，并用大小为5×5×48的256个核对其进行滤波。第三、第四和第五卷积层彼此连接而没有任何中间的池或规范化层。第三卷积层有384个大小为3×3×256的核，连接到第二个卷积层的（标准化的、混合的）输出端。第四卷积层有384个3×3×192的核，第五个卷积层有256个3×3×192的核。完全连接的层每层有4096个神经元。</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure2.png" alt></p>
<p>图2：我们的CNN架构图，明确显示了两个GPU之间的职责划分。一个GPU运行图形顶部的层部件，而另一个GPU运行底部的层部件。gpu只在某些层进行通信。网络的输入为150528维，网络剩余层中的神经元数量为253440–186624–64896–64896–43264–4096–4096–1000。</p>
<h2 id="4-减少过拟合"><a href="#4-减少过拟合" class="headerlink" title="4.减少过拟合"></a>4.减少过拟合</h2><p>我们的神经网络架构有6000万个参数。虽然ILSVRC的1000个类使得每个训练示例对从图像到标签的映射施加10个位的约束，但这并不足以在不过度拟合的情况下学习这么多参数。下面，我们将介绍我们对抗过度拟合的两种主要方法。</p>
<p>减少图像数据过度拟合的最简单和最常见的方法是使用保留标签的变换（例如[25,4,5]）人为地放大数据集。我们采用了两种不同的数据增强形式，这两种方式都允许从原始图像生成经过变换的图像，而无需将转换后的图像存储在磁盘上。在我们的实现中，转换后的图像是在CPU上用Python代码生成的，而GPU正在对前一批图像进行训练。因此，这些数据扩充方案实际上是无需计算的。</p>
<p>数据增强的第一种形式包括生成图像平移和水平反射。我们通过从256×256幅图像中随机提取224×224个面片（及其水平反射），并在这些提取的斑块上训练我们的网络$^4$。这使我们的培训规模增加了2048倍，尽管最终的培训示例当然是高度相互依赖的。如果没有这个方案，我们的网络将遭受严重的过度拟合，这将迫使我们使用更小的网络。在测试时，网络通过提取五个224×224个面片（四角面片和中心面片）及其水平反射（因此总共十个面片）进行预测，并将网络的softmax层在这十个面片上所做的预测取平均值。</p>
<hr>
<p>$^4$ 这就是为什么图2中的输入图像是224×224×3维的。</p>
<p>第二种形式的数据增强包括改变训练图像中RGB通道的强度。具体地说，我们在ImageNet训练集中对RGB像素值集执行PCA。在每个训练图像中，我们添加找到的主成分的倍数，量值与相应的特征值乘以从平均值为零，标准偏差为0.1的高斯随机变量。因此，对每个RGB图像像素$I_{xy}=[I_{xy}^R,I_{xy}^G,I_{xy}^B]$加上以下数量：</p>
<script type="math/tex; mode=display">[P1,P2,P3][\alpha1 \lambda1,\alpha2 \lambda2,\alpha3 \lambda3]^T</script><p>其中，pi和λi分别为RGB像素值的3×3协方差矩阵的特征向量和特征值，α为上述随机变量。每个αi只为一个特定训练图像的所有像素绘制一次，直到该图像再次用于训练，此时重新绘制该图像。该方案近似地捕捉到了自然图像的一个重要特性，即物体的同一性随光照强度和颜色的变化而变化。该方案使前1个错误率降低了1%以上。</p>
<h3 id="4-2-Dropout-1"><a href="#4-2-Dropout-1" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h3><p>将许多不同模型的预测结合起来是减少测试误差的一种非常成功的方法[1,3]，但对于已经花费数天时间进行训练的大型神经网络来说，这似乎太贵了。然而，有一个非常有效的模型组合版本，在培训期间只需花费大约两倍的成本。最近引入的技术称为“dropout”[10]，包括将每个隐藏神经元的输出设为0，概率为0.5。 以这种方式“退出”的神经元不参与前向传递，也不参与反向传播。 所以每次输入被提出时，神经网络都会对不同的结构进行采样，但所有这些结构都共享权重。 这种技术减少了神经元复杂的共适应，因为一个神经元不能依赖于其他特定神经元的存在。 因此，它被迫学习更健壮的特征，这些特征与其他神经元的许多不同的随机子集结合起来是有用的。在测试时，我们使用所有的神经元，但将它们的输出乘以0.5，这是一个合理的近似值，取指数型多个丢失网络产生的预测分布的几何平均值。我们在图2中的前两个完全连接的层中使用dropout。在没有dropout的情况下，我们的网络表现出严重的过度适应。Dropout大约是收敛所需迭代次数的两倍。</p>
<h2 id="5-学习细节"><a href="#5-学习细节" class="headerlink" title="5.学习细节"></a>5.学习细节</h2><p>我们使用随机梯度下降训练我们的模型，批量大小为128个例子，动量为0.9，权重衰减为0.0005。我们发现这个小的重量衰减对模型的学习很重要。换句话说，这里的权重衰减不仅仅是一个正则化器：它减少了模型的训练误差。权重w的更新规则是</p>
<script type="math/tex; mode=display">v_{i+1}:=0.9v_{i}-0.0005\epsilon w_i-\epsilon \langle \frac{\delta L}{\delta W }|_{w_i}\rangle _{D_i}</script><script type="math/tex; mode=display">w_{i+1}:=w_i+v_{i+1}</script><p>其中i是迭代索引,v是动量变量，$\epsilon$ 是学习率，$\langle \frac{\delta L}{\delta W }|_{w_i}\rangle _{D_i}$是第i批中目标相对于w的导数的平均值，在wi处计算。</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure3.png" alt><br>图3:224×224×3输入图像第一卷积层学习的96个卷积核，大小为11×11×3。前48个内核在gpu1上学习，而后48个内核在gpu2上学习。详见第6.1节。</p>
<p>我们用标准差为0.01的零均值高斯分布初始化每个层的权重。我们用常数1初始化第二、第四和第五卷积层以及完全连接的隐藏层中的神经元偏差。这种初始化通过向ReLUs提供积极的输入加快了学习的早期阶段。我们用常数0初始化剩余层中的神经元偏差。</p>
<p>我们对所有层使用相同的学习率，在整个培训过程中我们手动调整。我们遵循的启发式方法是，当验证错误率不再随当前学习率提高时，将学习率除以10。学习率初始化为0.01，终止前降低3倍。我们通过120万张图像的训练集训练了网络大约90个周期，在两个 NVIDIA GTX 5803 gb gpu上花了5到6天的时间。</p>
<h2 id="6-Results-1"><a href="#6-Results-1" class="headerlink" title="6 Results"></a>6 Results</h2><p>表1总结了我们在ILSVRC-2010上的结果。我们的网络实现了前1名和前5名测试集错误率分别为37.5%和17.0% $^5$。</p>
<hr>
<p>$^5$ 如第4.1节所述，未对10个补丁进行平均预测的错误率分别为39.0%和18.3%。在ILSVRC2010竞赛中取得的最佳性能分别为47.1%和28.2%，采用的方法是平均六个针对不同特征训练的稀疏编码模型生成的预测值[2]，从那时起，最好的发布结果分别为45.7%和25.7%，使用一种方法，即使用两种密集采样特征计算出的Fisher向量（fv）训练的两个分类器的预测值的平均值[24]。</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/table1.png" alt><br>表1:ILSVRC2010测试集结果比较。斜体字是别人取得的最好成绩。</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/table2.png" alt><br>表2:ILSVRC-2012验证集和测试集的错误率比较。斜体字是别人取得的最好成绩。带有星号*的模型经过“预先训练”，对ImageNet 2011秋季发布的整个版本进行分类。详见第6节。</p>
<p>我们还在ILSVRC-2012竞赛中输入了我们的模型，并在表2中报告了我们的结果。由于ILSVRC-2012测试集标签不公开，我们无法报告我们尝试的所有模型的测试错误率。在本段的其余部分中，我们交替使用验证和测试错误率，因为根据我们的经验，它们之间的差异不超过0.1%（见表2）。本文描述的CNN的前5位错误率为18.2%。对5个相似的cnn进行平均预测，误差率为16.4%。训练一个CNN，在最后一个池层上增加一个第六个卷积层，对整个ImageNet Fall 2011发布版（15M图像，22K类别）进行分类，然后在ILSVRC-2012上对其进行“微调”，错误率为16.6%。对2011年秋季发布的两个CNN的预测值进行平均，这两个CNN在上述五个CNN中进行了预训练，其预测误差率为15.3%。第二名最佳竞赛条目的错误率为26.2%，其方法是将训练在基于不同类型密集采样特征的FVs上的几个分类器的预测平均化[7]。</p>
<p>最后，我们还报告了2009年秋季版本的ImageNet的错误率，该版本包含10184个类别和890万个图像。在这个数据集上，我们遵循文献中的惯例，使用一半的图像进行训练，另一半用于测试。由于没有已建立的测试集，我们的分割必然不同于先前作者使用的分割，但这不会对结果产生明显的影响。我们在这个数据集上的前1名和前5名的错误率分别为67.4%和40.9%，通过上述网络实现，但是在最后一个池层上有一个额外的第六个卷积层。在这个数据集上发表的最佳结果分别是78.1%和60.9%[19]。</p>
<h3 id="6-1定性评价"><a href="#6-1定性评价" class="headerlink" title="6.1定性评价"></a>6.1定性评价</h3><p>图3显示了由网络的两个数据连接层学习的卷积核。网络已经学习了各种频率和方向选择性核，以及各种颜色的斑点。 请注意这两个gpu所显示的专门化，这是第3.5节中描述的受限连接的结果。gpu1上的内核基本上是颜色不可知的，而gpu2上的内核主要是颜色特定的。这种专门化发生在每次运行期间，并且独立于任何特定的随机权重初始化（对gpu进行模化重新编号）。</p>
<p><img src="/2020/07/31/%E8%AE%BA%E6%96%87-%E5%8D%B7%E7%A7%AF%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B-AlexNet/figure4.png" alt></p>
<p>图4：（左）8个ILSVRC-2010测试图像和我们模型认为最有可能的五个标签。正确的标签写在每个图像下面，并且分配给正确标签的概率也用红色条显示（如果它正好在前5位）。（右）第一列中的五幅ILSVRC-2010测试图像。剩下的列显示了六个训练图像，它们在最后一个隐藏层中生成特征向量，与测试图像的特征向量的欧几里德距离最小。</p>
<p>在图4的左侧面板中，我们通过计算8张测试图像的前5位预测，定性地评估了网络所学的知识。请注意，即使是偏离中心的对象，如左上角的斜接体，也可以被网络识别。前五名中的大多数似乎都是合理的。例如，只有其他种类的猫才被认为是豹的合理标签。在某些情况下（格栅，樱桃），有真正的含糊不清的预定焦点的照片。</p>
<p>探索网络视觉知识的另一种方法是考虑图像在最后4096维隐藏层引起的特征激活。如果两幅图像产生的特征激活向量具有很小的欧几里德分离，我们可以说，较高层次的神经网络认为他们是相似的。图4显示了来自测试集的五个图像和来自训练集的六个图像，这些图像与此度量中的每一个最相似。注意，在像素级别，检索到的训练图像在L2中通常与第一列中的查询图像不太接近。例如，被找回的狗和大象以各种姿势出现。我们在补充材料中展示了更多测试图像的结果。</p>
<p>利用两个4096维实值向量之间的欧几里德距离来计算相似性是低效的，但是通过训练nauto编码器将这些向量压缩成短的二进制代码，可以提高效率。这将产生比对原始像素应用自动编码器更好的图像检索方法[14]，后者不使用图像标签，因此倾向于检索具有相似边缘模式的图像，无论它们在语义上是否相似。</p>
<h2 id="7讨论"><a href="#7讨论" class="headerlink" title="7讨论"></a>7讨论</h2><p>我们的结果表明，一个大的，深卷积神经网络能够在一个高挑战性的数据集上使用纯监督学习获得破纪录的结果。值得注意的是，如果去掉单个卷积层，我们的网络性能会下降。例如，删除任何中间层都会导致网络的前1性能损失约2%。所以深度对我们的结果很重要。</p>
<p>为了简化我们的实验，我们没有使用任何无监督的预训练，即使我们期望它会有所帮助，特别是如果我们获得足够的计算能力来显著地增加网络的规模，而不获得相应增加的标记数据量。到目前为止，我们的结果已经有所改善，因为我们已经扩大了我们的网络并训练了它更长的时间，但我们仍然有许多数量级的工作要去匹配人类视觉系统的时间-时间路径。最后，我们希望在视频序列上使用非常大和很深的卷积网络，其中时间结构提供了非常有用的信息，而这些信息在静态图像中是缺失的或不太明显的。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1] R.M. Bell and Y . Koren. Lessons from the netflixprizechallenge. ACM SIGKDD Explorations Newsletter,<br>9(2):75–79, 2007.<br>[2] A. Berg, J. Deng, and L. Fei-Fei.  Large scale visual recognition challenge 2010. www.image-<br>net.org/challenges. 2010.<br>[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.<br>[4] D. Cire¸ san, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.<br>Arxiv preprint arXiv:1202.2745, 2012.<br>[5] D.C. Cire¸ san, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural<br>networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.<br>[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical<br>Image Database. In CVPR09, 2009.<br>[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012.  URL<br><a href="http://www.image-net.org/challenges/LSVRC/2012/" target="_blank" rel="noopener">http://www.image-net.org/challenges/LSVRC/2012/</a>.<br>[8] L. Fei-Fei, R. Fergus, and P . Perona. Learning generative visual models from few training examples: An<br>incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand-<br>ing, 106(1):59–70, 2007.<br>[9] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, Cali-<br>fornia Institute of Technology, 2007. URL <a href="http://authors.library.caltech.edu/7694" target="_blank" rel="noopener">http://authors.library.caltech.edu/7694</a>.<br>[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural net-<br>works by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.<br>[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y . LeCun. What is the best multi-stage architecture for<br>object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.<br>[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of<br>Computer Science, University of Toronto, 2009.<br>[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.<br>[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In<br>ESANN, 2011.<br>[15] Y . Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand-<br>written digit recognition with a back-propagation network. In Advances in neural information processing<br>systems, 1990.<br>[16] Y . LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to<br>pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the<br>2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.<br>[17] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In<br>Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256.<br>IEEE, 2010.<br>[18] H. Lee, R. Grosse, R. Ranganath, and A.Y . Ng. Convolutional deep belief networks for scalable unsuper-<br>vised learning of hierarchical representations. InProceedings of the 26th Annual International Conference<br>on Machine Learning, pages 609–616. ACM, 2009.<br>[19] T. Mensink, J. V erbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classifi-<br>cation: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer<br>Vision, Florence, Italy, October 2012.<br>[20] V . Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th<br>International Conference on Machine Learning, 2010.<br>[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computa-<br>tional biology, 4(1):e27, 2008.<br>[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering<br>good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,<br>2009.<br>[23] B.C. Russell, A. Torralba, K.P . Murphy, and W.T. Freeman. Labelme: a database and web-based tool for<br>image annotation. International journal of computer vision, 77(1):157–173, 2008.<br>[24] J. Sánchez and F. Perronnin. High-dimensional signature compression for large-scale image classification.<br>InComputer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,<br>2011.<br>[25] P .Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to<br>visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis<br>and Recognition, volume 2, pages 958–962, 2003.<br>[26] S.C. Turaga, J.F. Murray, V . Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H.S. Seung. Con-<br>volutional networks can learn to generate affinity graphs for image segmentation. Neural Computation,<br>22(2):511–538, 2010.</p>
<h1 id="我的总结"><a href="#我的总结" class="headerlink" title="我的总结"></a>我的总结</h1><ul>
<li>1.现实环境中的物体表现出相当大的可变性，需要大的数据集</li>
<li>2.需要大的数据集，为此需要学习能力很强的模型。</li>
<li>3.事实上数据集没法提供的无穷大，需要先验知识来弥补所没有的数据集</li>
<li>4.先验知识:卷积神经网络的容量可以通过改变深度和宽度来控制,还可以对图像的性质(即统计的平稳性和像素依赖的局部性)做出强有力的、基本正确的假设。</li>
<li>5.与具有相似层大小的标准前馈神经网络相比，cnn具有更少的连接和参数，因此更容易训练，而理论上最好的性能可能只会稍微差一点。</li>
<li>6.ImageNet的数据集里的图片分辨率不一样，需要将图像下采样到固定分辨率为256×256,因为系统需要一个恒定的输入维数</li>
<li>7.Rectified Linear Units简称ReLUs比sigmoid快好几倍</li>
<li>8.因为一个GTX 580 GPU只有3GB的内存,所以用两个GPU来并行计算,用两个GPU直接读写彼此的内存，而不需要通过主机内存。</li>
<li>9.双gpu网络的训练时间略少于单gpu网络(在一个GPU上训练每个卷积层内核数量减半的网络),因为使这两个网络具有大致相同数量的参数，我们没有将最后的卷积层(也没有将后面的完全连接层)的大小减半。</li>
<li>10.局部相应归一化</li>
<li>11.重叠池化层</li>
<li>12.数据增强(1图像平移水平翻转,2通道增强)</li>
</ul>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>卷积经典模型论文</tag>
      </tags>
  </entry>
  <entry>
    <title>Python进阶</title>
    <url>/2020/07/27/Python%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<p>参考<a href="https://eastlakeside.gitbook.io/interpy-zh/" target="_blank" rel="noopener">此处</a></p>
<h1 id="Python进阶"><a href="#Python进阶" class="headerlink" title="Python进阶 "></a>Python进阶 </h1><p>《Python进阶》是《Intermediate Python》的中文译本, 谨以此献给进击的 Python 和 Python 程序员们!<br><a id="more"></a></p>
<h3 id="快速阅读传送门"><a href="#快速阅读传送门" class="headerlink" title="快速阅读传送门"></a>快速阅读传送门</h3><ul>
<li>Github快速阅读任一章节：<a href="https://github.com/eastlakeside/interpy-zh/blob/master/SUMMARY.md" target="_blank" rel="noopener">进入目录</a></li>
<li>Gitbook完整顺序地阅读：<a href="https://eastlakeside.gitbooks.io/interpy-zh/content/" target="_blank" rel="noopener">进入Gitbook</a></li>
<li>本地或kindle上阅读：<a href="https://github.com/eastlakeside/interpy-zh/releases" target="_blank" rel="noopener">下载pdf/epub/mobi</a></li>
<li>国内推荐镜像（实时同步）：<a href="http://wiki.jikexueyuan.com/project/interpy-zh/" target="_blank" rel="noopener">极客学院收录</a></li>
<li>其他镜像（不定期同步）：<a href="http://docs.pythontab.com/interpy/" target="_blank" rel="noopener">Pythontab收录</a></li>
<li>纯代码阅读和演示：<a href="https://github.com/eastlakeside/interpy-zh/tree/master/code/" target="_blank" rel="noopener">进入code目录</a></li>
</ul>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Python，作为一个”老练”、”小清新”的开发语言，已受到广大才男俊女的喜爱。我们也从最基础的Python粉，经过时间的摧残慢慢的变成了Python老鬼。</p>
<p>IntermediatePython这本书具有如下几个优点：</p>
<ol>
<li>简单</li>
<li>易读</li>
<li>易译</li>
</ol>
<p>这些都不是重点，重点是：<strong>它是一本开脑洞的书</strong>。无论你是Python初学者，还是Python高手，它显现给你的永远是Python里最美好的事物。</p>
<blockquote>
<p>世上语言千万种<br>美好事物藏其中</p>
</blockquote>
<p>译者在翻译过程中，慢慢发现，本书作者的行文方式有着科普作家的风范，—那就是能将晦涩难懂的技术用比较清晰简洁的方式进行呈现，深入浅出的风格在每个章节的讨论中都得到了体现：</p>
<ul>
<li>每个章节都非常精简，5分钟就能看完，用最简洁的例子精辟地展现了原理</li>
<li>每个章节都会通过疑问，来引导读者主动思考答案</li>
<li>每个章节都引导读者做延伸阅读，让有兴趣的读者能进一步举一反三</li>
<li>每个章节都是独立的，你可以挑选任意的章节开始阅读，而不受影响</li>
</ul>
<p>总之，这本书非常方便随时选取一个章节进行阅读，而且每次阅读一个章节，你都可能会有一些新的发现。</p>
<h2 id="原书作者"><a href="#原书作者" class="headerlink" title="原书作者"></a>原书作者</h2><p>感谢英文原著作者 @yasoob《<a href="https://github.com/yasoob/intermediatePython" target="_blank" rel="noopener">Intermediate Python</a>》，有了他才有了这里的一切</p>
<h1 id="序"><a href="#序" class="headerlink" title="序"></a>序</h1><p>这是一本<a href="https://github.com/yasoob/intermediatePython" target="_blank" rel="noopener">Intermediate Python</a> 的中文译本, 谨以此献给进击的 Python 和 Python 程序员们!</p>
<p>这是一次团队建设、一次尝鲜、一次对自我的提升。相信每个有为青年，心里想藏着一个小宇宙：<strong>我想要做一件有意思的事</strong>。<script type="math/tex">什么是有意思的事？</script> <strong>别闹</strong></p>
<p>Python，作为一个”老练”、”小清新”的开发语言，已受到广大才男俊女的喜爱。我们也从最基础的Python粉，经过时间的摧残慢慢的变成了Python老鬼。因此一开始 @大牙 提出要翻译点什么的时候，我还是挺兴奋的，团队一起协作，不单可以磨练自己，也能加强团队之间的协作。为此在经过短暂的讨论后，翻译的内容就定为：《Intermediate Python》。</p>
<p>IntermediatePython这本书具有如下几个优点：</p>
<ol>
<li>简单</li>
<li>易读</li>
<li>易译</li>
</ol>
<p>这些都不是重点，重点是：<strong>它是一本开脑洞的书</strong>。无论你是Python初学者，还是Python高手，它显现给你的永远是Python里最美好的事物。</p>
<h1 id="关于原作者"><a href="#关于原作者" class="headerlink" title="关于原作者"></a>关于原作者</h1><p>我是 Muhammad Yasoob Ullah Khalid. </p>
<p>我已经广泛使用 Python 编程3年多了. 同时参与了很多开源项目. 并定期在<a href="http://pythontips.com/" target="_blank" rel="noopener">我的博客</a>里写一些关于Python有趣的话题. </p>
<p>2014年我在柏林举办的欧洲最大的Python会议<strong>EuroPython</strong>上做过精彩的演讲. </p>
<blockquote>
<p>译者注：分享的主题为：《Session: Web Scraping in Python 101》<br>地址：<a href="https://ep2014.europython.eu/en/schedule/sessions/20/" target="_blank" rel="noopener">https://ep2014.europython.eu/en/schedule/sessions/20/</a></p>
</blockquote>
<p>如果你能给我有意思的工作机会, 请联系我哦.</p>
<blockquote>
<p>译者注：嗯，硬广，你来中国么，HOHO</p>
</blockquote>
<h1 id="作者前言"><a href="#作者前言" class="headerlink" title="作者前言"></a>作者前言</h1><p>Hello 大家好! 我非常自豪地宣布我自己创作的书完成啦.<br>经过很多辛苦工作和决心, 终于将不可能变成了可能, “Intermediate Python”终于杀青.<br>ps: 它还将持续更新 :)</p>
<p>Python 是一门奇妙的语言, 还有一个强大而友爱的程序员社区.<br>然而, 在你理解消化掉 Python 的基础后, 关于下一步学习什么的资料比较缺乏. 而我正是要通过本书来解决这一问题.<br>我会给你一些可以进一步探索的有趣的话题的信息.</p>
<p>本书讨论的这些话题将会打开你的脑洞, 将你引导至 Python 语言的一些美好的地方. 我最开始学习 Python 时, 渴望见到Python最优雅的地方, 而本书正是这些渴望的结果.</p>
<p>无论你是个初学者, 中级或者甚至高级程序员, 你都会在这本书里有所收获.</p>
<p>请注意本书不是一个指导手册, 也不会教你 Python. 因为书中的话题并没有进行基础解释, 而只提供了展开讨论前所需的最少信息.</p>
<p>好啦，你肯定也和我一样兴奋, 那让我们开始吧!</p>
<h1 id="开源公告"><a href="#开源公告" class="headerlink" title="开源公告"></a>开源公告</h1><p>注意: 这本书是开源的, 也是一个持续进展中的工作. 如果你发现typo, 或者想添加更多内容进来, 或者可以改进的任意地方(我知道你会发现很多),  那么请慷慨地提交一个 pull request, 我会无比高兴地合并进来. :)</p>
<p>另外, 我决定将这本书免费发布!   我相信它会帮助到那些需要帮助的人. 祝你们好运!</p>
<p>这里是免费阅读链接:</p>
<ul>
<li><a href="http://book.pythontips.com/" target="_blank" rel="noopener">Html</a> </li>
<li><a href="http://readthedocs.org/projects/intermediatepythongithubio/downloads/pdf/latest/" target="_blank" rel="noopener">PDF</a></li>
<li><a href="https://github.com/IntermediatePython/intermediatePython" target="_blank" rel="noopener">GitHub</a></li>
</ul>
<h1 id="args-和-kwargs"><a href="#args-和-kwargs" class="headerlink" title="*args 和 **kwargs"></a><code>*args</code> 和 <code>**kwargs</code></h1><p>我观察到，大部分新的Python程序员都需要花上大量时间理解清楚 <code>*args</code> 和<code>**kwargs</code>这两个魔法变量。那么它们到底是什么? </p>
<p>首先让我告诉你, 其实并不是必须写成<code>*args</code> 和<code>**kwargs</code>。 只有变量前面的 <code>*</code>(星号)才是必须的. 你也可以写成<code>*var</code> 和<code>**vars</code>. 而写成<code>*args</code> 和<code>**kwargs</code>只是一个通俗的命名约定。 那就让我们先看一下<code>*args</code>吧。</p>
<h1 id="args-的用法"><a href="#args-的用法" class="headerlink" title="*args 的用法"></a>*args 的用法</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">和 &#96;&#96;&#96;**kwargs&#96;&#96;&#96; 主要用于函数定义。 你可以将不定数量的参数传递给一个函数。 </span><br><span class="line"></span><br><span class="line">这里的不定的意思是：预先并不知道, 函数使用者会传递多少个参数给你, 所以在这个场景下使用这两个关键字。 &#96;&#96;&#96;*args&#96;&#96;&#96; 是用来发送一个非键值对的可变数量的参数列表给一个函数. </span><br><span class="line"></span><br><span class="line">这里有个例子帮你理解这个概念:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">def test_var_args(f_arg, *argv):</span><br><span class="line">    print(&quot;first normal arg:&quot;, f_arg)</span><br><span class="line">    for arg in argv:</span><br><span class="line">        print(&quot;another arg through *argv:&quot;, arg)</span><br><span class="line"></span><br><span class="line">test_var_args(&#39;yasoob&#39;, &#39;python&#39;, &#39;eggs&#39;, &#39;test&#39;)</span><br></pre></td></tr></table></figure>
<p>这会产生如下输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">first normal arg: yasoob</span><br><span class="line">another arg through *argv: python</span><br><span class="line">another arg through *argv: eggs</span><br><span class="line">another arg through *argv: test</span><br></pre></td></tr></table></figure>
<p>我希望这解决了你所有的困惑. 那接下来让我们谈谈 <code>**kwargs</code></p>
<h1 id="kwargs-的用法"><a href="#kwargs-的用法" class="headerlink" title="**kwargs 的用法"></a>**kwargs 的用法</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">允许你将不定长度的**键值对**, 作为参数传递给一个函数。 如果你想要在一个函数里处理**带名字的参数**, 你应该使用&#96;&#96;&#96;**kwargs&#96;&#96;&#96;。 </span><br><span class="line"></span><br><span class="line">这里有个让你上手的例子:</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">def greet_me(**kwargs):</span><br><span class="line">    for key, value in kwargs.items():</span><br><span class="line">        print(&quot;&#123;0&#125; &#x3D;&#x3D; &#123;1&#125;&quot;.format(key, value))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; greet_me(name&#x3D;&quot;yasoob&quot;)</span><br><span class="line">name &#x3D;&#x3D; yasoob</span><br></pre></td></tr></table></figure>
<p>现在你可以看出我们怎样在一个函数里, 处理了一个<strong>键值对</strong>参数了。</p>
<p>这就是<code>**kwargs</code>的基础, 而且你可以看出它有多么管用。 接下来让我们谈谈，你怎样使用<code>*args</code> 和 <code>**kwargs</code>来调用一个参数为列表或者字典的函数。</p>
<h1 id="使用-args-和-kwargs-来调用函数"><a href="#使用-args-和-kwargs-来调用函数" class="headerlink" title="使用 *args 和 **kwargs 来调用函数"></a>使用 <code>*args</code> 和 <code>**kwargs</code> 来调用函数</h1><p>那现在我们将看到怎样使用<code>*args</code>和<code>**kwargs</code> 来调用一个函数。<br> 假设，你有这样一个小函数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_args_kwargs</span><span class="params">(arg1, arg2, arg3)</span>:</span></span><br><span class="line">    print(<span class="string">"arg1:"</span>, arg1)</span><br><span class="line">    print(<span class="string">"arg2:"</span>, arg2)</span><br><span class="line">    print(<span class="string">"arg3:"</span>, arg3)</span><br></pre></td></tr></table></figure></p>
<p>你可以使用<code>*args</code>或<code>**kwargs</code>来给这个小函数传递参数。<br>下面是怎样做：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先使用 *args</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>args = (<span class="string">"two"</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_args_kwargs(*args)</span><br><span class="line">arg1: two</span><br><span class="line">arg2: <span class="number">3</span></span><br><span class="line">arg3: <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在使用 **kwargs:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kwargs = &#123;<span class="string">"arg3"</span>: <span class="number">3</span>, <span class="string">"arg2"</span>: <span class="string">"two"</span>, <span class="string">"arg1"</span>: <span class="number">5</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_args_kwargs(**kwargs)</span><br><span class="line">arg1: <span class="number">5</span></span><br><span class="line">arg2: two</span><br><span class="line">arg3: <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<h3 id="标准参数与-args、-kwargs在使用时的顺序"><a href="#标准参数与-args、-kwargs在使用时的顺序" class="headerlink" title="标准参数与*args、**kwargs在使用时的顺序"></a>标准参数与<code>*args、**kwargs</code>在使用时的顺序</h3><p>那么如果你想在函数里同时使用所有这三种参数， 顺序是这样的：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">some_func(fargs, *args, **kwargs)</span><br></pre></td></tr></table></figure></p>
<h1 id="什么时候使用它们？"><a href="#什么时候使用它们？" class="headerlink" title="什么时候使用它们？"></a>什么时候使用它们？</h1><p>这还真的要看你的需求而定。</p>
<p>最常见的用例是在写函数装饰器的时候（会在另一章里讨论）。</p>
<p>此外它也可以用来做猴子补丁(monkey patching)。猴子补丁的意思是在程序运行时(runtime)修改某些代码。 打个比方，你有一个类，里面有个叫<code>get_info</code>的函数会调用一个API并返回相应的数据。如果我们想测试它，可以把API调用替换成一些测试数据。例如：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> someclass</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_info</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Test data"</span></span><br><span class="line"></span><br><span class="line">someclass.get_info = get_info</span><br></pre></td></tr></table></figure></p>
<p>我敢肯定你也可以想象到一些其他的用例。</p>
<h1 id="调试（Debugging）"><a href="#调试（Debugging）" class="headerlink" title="调试（Debugging）"></a>调试（Debugging）</h1><p>利用好调试，能大大提高你捕捉代码Bug的。大部分新人忽略了Python debugger(<code>pdb</code>)的重要性。 在这个章节我只会告诉你一些重要的命令，你可以从官方文档中学习到更多。</p>
<blockquote>
<p>译者注，参考：<a href="https://docs.python.org/2/library/pdb.html" target="_blank" rel="noopener">https://docs.python.org/2/library/pdb.html</a><br>Or <a href="https://docs.python.org/3/library/pdb.html" target="_blank" rel="noopener">https://docs.python.org/3/library/pdb.html</a></p>
</blockquote>
<h3 id="从命令行运行"><a href="#从命令行运行" class="headerlink" title="从命令行运行"></a>从命令行运行</h3><p>你可以在命令行使用Python debugger运行一个脚本， 举个例子：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python -m pdb my_script.py</span><br></pre></td></tr></table></figure></p>
<p>这会触发debugger在脚本第一行指令处停止执行。这在脚本很短时会很有帮助。你可以通过(Pdb)模式接着查看变量信息，并且逐行调试。</p>
<h3 id="从脚本内部运行"><a href="#从脚本内部运行" class="headerlink" title="从脚本内部运行"></a>从脚本内部运行</h3><p>同时，你也可以在脚本内部设置断点，这样就可以在某些特定点查看变量信息和各种执行时信息了。这里将使用<code>pdb.set_trace()</code>方法来实现。举个例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_bread</span><span class="params">()</span>:</span></span><br><span class="line">    pdb.set_trace()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"I don't have time"</span></span><br><span class="line"></span><br><span class="line">print(make_bread())</span><br></pre></td></tr></table></figure></p>
<p>试下保存上面的脚本后运行之。你会在运行时马上进入debugger模式。现在是时候了解下debugger模式下的一些命令了。</p>
<h5 id="命令列表："><a href="#命令列表：" class="headerlink" title="命令列表："></a>命令列表：</h5><ul>
<li><code>c</code>: 继续执行</li>
<li><code>w</code>: 显示当前正在执行的代码行的上下文信息</li>
<li><code>a</code>: 打印当前函数的参数列表</li>
<li><code>s</code>: 执行当前代码行，并停在第一个能停的地方（相当于单步进入）</li>
<li><code>n</code>: 继续执行到当前函数的下一行，或者当前行直接返回（单步跳过）</li>
</ul>
<p>单步跳过（<code>n</code>ext）和单步进入（<code>s</code>tep）的区别在于， 单步进入会进入当前行调用的函数内部并停在里面， 而单步跳过会（几乎）全速执行完当前行调用的函数，并停在当前函数的下一行。</p>
<p>pdb真的是一个很方便的功能，上面仅列举少量用法，更多的命令强烈推荐你去看官方文档。</p>
<h1 id="生成器（Generators）"><a href="#生成器（Generators）" class="headerlink" title="生成器（Generators）"></a>生成器（Generators）</h1><p>首先我们要理解迭代器(iterators)。根据维基百科，迭代器是一个让程序员可以遍历一个容器（特别是列表）的对象。然而，一个迭代器在遍历并读取一个容器的数据元素时，并不会执行一个迭代。你可能有点晕了，那我们来个慢动作。换句话说这里有三个部分：</p>
<ul>
<li>可迭代对象(Iterable)</li>
<li>迭代器(Iterator)</li>
<li>迭代(Iteration)</li>
</ul>
<p>上面这些部分互相联系。我们会先各个击破来讨论他们，然后再讨论生成器(generators).</p>
<h1 id="可迭代对象-Iterable"><a href="#可迭代对象-Iterable" class="headerlink" title="可迭代对象(Iterable)"></a>可迭代对象(Iterable)</h1><p>Python中任意的对象，只要它定义了可以返回一个迭代器的<code>__iter__</code>方法，或者定义了可以支持下标索引的<code>__getitem__</code>方法(这些双下划线方法会在其他章节中全面解释)，那么它就是一个可迭代对象。简单说，可迭代对象就是能提供迭代器的任意对象。那迭代器又是什么呢？</p>
<h1 id="迭代器-Iterator"><a href="#迭代器-Iterator" class="headerlink" title="迭代器(Iterator)"></a>迭代器(Iterator)</h1><p>任意对象，只要定义了<code>next</code>(Python2) 或者<code>__next__</code>方法，它就是一个迭代器。就这么简单。现在我们来理解迭代(iteration)</p>
<h1 id="迭代-Iteration"><a href="#迭代-Iteration" class="headerlink" title="迭代(Iteration)"></a>迭代(Iteration)</h1><p>用简单的话讲，它就是从某个地方（比如一个列表）取出一个元素的过程。当我们使用一个循环来遍历某个东西时，这个过程本身就叫迭代。现在既然我们有了这些术语的基本理解，那我们开始理解生成器吧。</p>
<h1 id="生成器-Generators"><a href="#生成器-Generators" class="headerlink" title="生成器(Generators)"></a>生成器(Generators)</h1><p>生成器也是一种迭代器，但是你只能对其迭代一次。这是因为它们并没有把所有的值存在内存中，而是在运行时生成值。你通过遍历来使用它们，要么用一个“for”循环，要么将它们传递给任意可以进行迭代的函数和结构。大多数时候生成器是以函数来实现的。然而，它们并不返回一个值，而是<code>yield</code>(暂且译作“生出”)一个值。这里有个生成器函数的简单例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_function</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> generator_function():</span><br><span class="line">    print(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># 7</span></span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line"><span class="comment"># 9</span></span><br></pre></td></tr></table></figure></p>
<p>这个案例并不是非常实用。生成器最佳应用场景是：你不想同一时间将所有计算出来的大量结果集分配到内存当中，特别是结果集里还包含循环。</p>
<blockquote>
<p>译者注：这样做会消耗大量资源 </p>
</blockquote>
<p>许多Python 2里的标准库函数都会返回列表，而Python 3都修改成了返回生成器，因为生成器占用更少的资源。  </p>
<p>下面是一个计算斐波那契数列的生成器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># generator version</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibon</span><span class="params">(n)</span>:</span></span><br><span class="line">    a = b = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line">        a, b = b, a + b</span><br></pre></td></tr></table></figure>
<p>函数使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for x in fibon(1000000):</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<p>用这种方式，我们可以不用担心它会使用大量资源。然而，之前如果我们这样来实现的话：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibon</span><span class="params">(n)</span>:</span></span><br><span class="line">    a = b = <span class="number">1</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        result.append(a)</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>这也许会在计算很大的输入参数时，用尽所有的资源。我们已经讨论过生成器使用一次迭代，但我们并没有测试过。在测试前你需要再知道一个Python内置函数：<code>next()</code>。它允许我们获取一个序列的下一个元素。那我们来验证下我们的理解：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_function</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">gen = generator_function()</span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: 0</span></span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: 1</span></span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: 2</span></span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: Traceback (most recent call last):</span></span><br><span class="line"><span class="comment">#            File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span></span><br><span class="line"><span class="comment">#         StopIteration</span></span><br></pre></td></tr></table></figure>
<p>我们可以看到，在<code>yield</code>掉所有的值后，<code>next()</code>触发了一个<code>StopIteration</code>的异常。基本上这个异常告诉我们，所有的值都已经被<code>yield</code>完了。你也许会奇怪，为什么我们在使用<code>for</code>循环时没有这个异常呢？啊哈，答案很简单。<code>for</code>循环会自动捕捉到这个异常并停止调用<code>next()</code>。你知不知道Python中一些内置数据类型也支持迭代哦？我们这就去看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_string = <span class="string">"Yasoob"</span></span><br><span class="line">next(my_string)</span><br><span class="line"><span class="comment"># Output: Traceback (most recent call last):</span></span><br><span class="line"><span class="comment">#      File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span></span><br><span class="line"><span class="comment">#    TypeError: str object is not an iterator</span></span><br></pre></td></tr></table></figure>
<p>好吧，这不是我们预期的。这个异常说那个<code>str</code>对象不是一个迭代器。对，就是这样！它是一个可迭代对象，而不是一个迭代器。这意味着它支持迭代，但我们不能直接对其进行迭代操作。那我们怎样才能对它实施迭代呢？是时候学习下另一个内置函数，<code>iter</code>。它将根据一个可迭代对象返回一个迭代器对象。这里是我们如何使用它：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_string = <span class="string">"Yasoob"</span></span><br><span class="line">my_iter = iter(my_string)</span><br><span class="line">next(my_iter)</span><br><span class="line"><span class="comment"># Output: 'Y'</span></span><br></pre></td></tr></table></figure><br>现在好多啦。我肯定你已经爱上了学习生成器。一定要记住，想要完全掌握这个概念，你只有使用它。确保你按照这个模式，并在生成器对你有意义的任何时候都使用它。你绝对不会失望的！</p>
<h1 id="Map，Filter-和-Reduce"><a href="#Map，Filter-和-Reduce" class="headerlink" title="Map，Filter 和 Reduce"></a>Map，Filter 和 Reduce</h1><p>Map，Filter 和 Reduce 三个函数能为函数式编程提供便利。我们会通过实例一个一个讨论并理解它们。</p>
<h1 id="Map"><a href="#Map" class="headerlink" title="Map"></a><code>Map</code></h1><p><code>Map</code>会将一个函数映射到一个输入列表的所有元素上。这是它的规范：</p>
<p><strong>规范</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">map(function_to_apply, list_of_inputs)</span><br></pre></td></tr></table></figure></p>
<p>大多数时候，我们要把列表中所有元素一个个地传递给一个函数，并收集输出。比方说：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">items = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">squared = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> items:</span><br><span class="line">    squared.append(i**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><code>Map</code>可以让我们用一种简单而漂亮得多的方式来实现。就是这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">items = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">squared = list(map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>, items))</span><br></pre></td></tr></table></figure>
<p>大多数时候，我们使用匿名函数(lambdas)来配合<code>map</code>, 所以我在上面也是这么做的。<br> 不仅用于一列表的输入， 我们甚至可以用于一列表的函数！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (x*x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (x+x)</span><br><span class="line"></span><br><span class="line">funcs = [multiply, add]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    value = map(<span class="keyword">lambda</span> x: x(i), funcs)</span><br><span class="line">    print(list(value))</span><br><span class="line">    <span class="comment"># 译者注：上面print时，加了list转换，是为了python2/3的兼容性</span></span><br><span class="line">    <span class="comment">#        在python2中map直接返回列表，但在python3中返回迭代器</span></span><br><span class="line">    <span class="comment">#        因此为了兼容python3, 需要list转换一下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># [0, 0]</span></span><br><span class="line"><span class="comment"># [1, 2]</span></span><br><span class="line"><span class="comment"># [4, 4]</span></span><br><span class="line"><span class="comment"># [9, 6]</span></span><br><span class="line"><span class="comment"># [16, 8]</span></span><br></pre></td></tr></table></figure>
<h1 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a><code>Filter</code></h1><p>顾名思义，<code>filter</code>过滤列表中的元素，并且返回一个由所有符合要求的元素所构成的列表，<code>符合要求</code>即函数映射到该元素时返回值为True. 这里是一个简短的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number_list = range(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">less_than_zero = filter(<span class="keyword">lambda</span> x: x &lt; <span class="number">0</span>, number_list)</span><br><span class="line">print(list(less_than_zero))  </span><br><span class="line"><span class="comment"># 译者注：上面print时，加了list转换，是为了python2/3的兼容性</span></span><br><span class="line"><span class="comment">#        在python2中filter直接返回列表，但在python3中返回迭代器</span></span><br><span class="line"><span class="comment">#        因此为了兼容python3, 需要list转换一下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: [-5, -4, -3, -2, -1]</span></span><br></pre></td></tr></table></figure>
<p>这个<code>filter</code>类似于一个<code>for</code>循环，但它是一个内置函数，并且更快。</p>
<p>注意：如果<code>map</code>和<code>filter</code>对你来说看起来并不优雅的话，那么你可以看看另外一章：列表/字典/元组推导式。</p>
<blockquote>
<p>译者注：大部分情况下推导式的可读性更好</p>
</blockquote>
<h1 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a><code>Reduce</code></h1><p>当需要对一个列表进行一些计算并返回结果时，<code>Reduce</code> 是个非常有用的函数。举个例子，当你需要计算一个整数列表的乘积时。</p>
<p>通常在 python 中你可能会使用基本的 for 循环来完成这个任务。</p>
<p>现在我们来试试 reduce：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">product &#x3D; reduce( (lambda x, y: x * y), [1, 2, 3, 4] )</span><br><span class="line"></span><br><span class="line"># Output: 24</span><br></pre></td></tr></table></figure>
<h1 id="set-集合-数据结构"><a href="#set-集合-数据结构" class="headerlink" title="set(集合)数据结构"></a><code>set</code>(集合)数据结构</h1><p><code>set</code>(集合)是一个非常有用的数据结构。它与列表(<code>list</code>)的行为类似，区别在于<code>set</code>不能包含重复的值。<br>这在很多情况下非常有用。例如你可能想检查列表中是否包含重复的元素，你有两个选择，第一个需要使用<code>for</code>循环，就像这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">some_list = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'d'</span>, <span class="string">'m'</span>, <span class="string">'n'</span>, <span class="string">'n'</span>]</span><br><span class="line"></span><br><span class="line">duplicates = []</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> some_list:</span><br><span class="line">    <span class="keyword">if</span> some_list.count(value) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> value <span class="keyword">not</span> <span class="keyword">in</span> duplicates:</span><br><span class="line">            duplicates.append(value)</span><br><span class="line"></span><br><span class="line">print(duplicates)</span><br><span class="line"><span class="comment">### 输出: ['b', 'n']</span></span><br></pre></td></tr></table></figure>
<p>但还有一种更简单更优雅的解决方案，那就是使用<code>集合(sets)</code>，你直接这样做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">some_list = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'d'</span>, <span class="string">'m'</span>, <span class="string">'n'</span>, <span class="string">'n'</span>]</span><br><span class="line">duplicates = set([x <span class="keyword">for</span> x <span class="keyword">in</span> some_list <span class="keyword">if</span> some_list.count(x) &gt; <span class="number">1</span>])</span><br><span class="line">print(duplicates)</span><br><span class="line"><span class="comment">### 输出: set(['b', 'n'])</span></span><br></pre></td></tr></table></figure>
<p>集合还有一些其它方法，下面我们介绍其中一部分。</p>
<h2 id="交集"><a href="#交集" class="headerlink" title="交集"></a>交集</h2><p>你可以对比两个集合的交集（两个集合中都有的数据），如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">valid = set([<span class="string">'yellow'</span>, <span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'black'</span>])</span><br><span class="line">input_set = set([<span class="string">'red'</span>, <span class="string">'brown'</span>])</span><br><span class="line">print(input_set.intersection(valid))</span><br><span class="line"><span class="comment">### 输出: set(['red'])</span></span><br></pre></td></tr></table></figure>
<h2 id="差集"><a href="#差集" class="headerlink" title="差集"></a>差集</h2><p>你可以用差集(difference)找出无效的数据，相当于用一个集合减去另一个集合的数据，例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">valid = set([<span class="string">'yellow'</span>, <span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'black'</span>])</span><br><span class="line">input_set = set([<span class="string">'red'</span>, <span class="string">'brown'</span>])</span><br><span class="line">print(input_set.difference(valid))</span><br><span class="line"><span class="comment">### 输出: set(['brown'])</span></span><br></pre></td></tr></table></figure>
<p>你也可以用<code>{}</code>符号来创建集合，如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a_set = &#123;<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>&#125;</span><br><span class="line">print(type(a_set))</span><br><span class="line"><span class="comment">### 输出: &lt;type 'set'&gt;</span></span><br></pre></td></tr></table></figure>
<p>集合还有一些其它方法，我会建议访问官方文档并做个快速阅读。</p>
<h1 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h1><p>三元运算符通常在Python里被称为条件表达式，这些表达式基于真(true)/假(false)的条件判断，在Python 2.4以上才有了三元操作。</p>
<p>下面是一个伪代码和例子：</p>
<p><strong>伪代码:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#如果条件为真，返回真 否则返回假</span></span><br><span class="line">condition_is_true <span class="keyword">if</span> condition <span class="keyword">else</span> condition_is_false</span><br></pre></td></tr></table></figure>
<p><strong>例子:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">is_fat = <span class="literal">True</span></span><br><span class="line">state = <span class="string">"fat"</span> <span class="keyword">if</span> is_fat <span class="keyword">else</span> <span class="string">"not fat"</span></span><br></pre></td></tr></table></figure>
<p>它允许用简单的一行快速判断，而不是使用复杂的多行<code>if</code>语句。<br>这在大多数时候非常有用，而且可以使代码简单可维护。</p>
<p>另一个晦涩一点的用法比较少见，它使用了元组，请继续看：</p>
<p><strong>伪代码:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#(返回假，返回真)[真或假]</span></span><br><span class="line">(if_test_is_false, if_test_is_true)[test]</span><br></pre></td></tr></table></figure>
<p><strong>例子:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fat = <span class="literal">True</span></span><br><span class="line">fitness = (<span class="string">"skinny"</span>, <span class="string">"fat"</span>)[fat]</span><br><span class="line">print(<span class="string">"Ali is"</span>, fitness)</span><br><span class="line"><span class="comment">#输出: Ali is fat</span></span><br></pre></td></tr></table></figure>
<p>这之所以能正常工作，是因为在Python中，True等于1，而False等于0，这就相当于在元组中使用0和1来选取数据。</p>
<p>上面的例子没有被广泛使用，而且Python玩家一般不喜欢那样，因为没有Python味儿(Pythonic)。这样的用法很容易把真正的数据与True/False弄混。</p>
<p>另外一个不使用元组条件表达式的缘故是因为在元组中会把两个条件都执行，而 <code>if-else</code> 的条件表达式不会这样。</p>
<p>例如:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">condition = <span class="literal">True</span></span><br><span class="line">print(<span class="number">2</span> <span class="keyword">if</span> condition <span class="keyword">else</span> <span class="number">1</span> / <span class="number">0</span>)</span><br><span class="line"><span class="comment">#输出: 2</span></span><br><span class="line"></span><br><span class="line">print((<span class="number">1</span> / <span class="number">0</span>, <span class="number">2</span>)[condition])</span><br><span class="line"><span class="comment">#输出ZeroDivisionError异常</span></span><br></pre></td></tr></table></figure>
<p>这是因为在元组中是先建数据，然后用True(1)/False(0)来索引到数据。<br>而<code>if-else</code>条件表达式遵循普通的<code>if-else</code>逻辑树，<br>因此，如果逻辑中的条件异常，或者是重计算型（计算较久）的情况下，最好尽量避免使用元组条件表达式。</p>
<h1 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h1><p>装饰器(Decorators)是Python的一个重要部分。简单地说：他们是修改其他函数的功能的函数。他们有助于让我们的代码更简短，也更Pythonic（Python范儿）。大多数初学者不知道在哪儿使用它们，所以我将要分享下，哪些区域里装饰器可以让你的代码更简洁。</p>
<p>首先，让我们讨论下如何写你自己的装饰器。</p>
<p>这可能是最难掌握的概念之一。我们会每次只讨论一个步骤，这样你能完全理解它。</p>
<h1 id="一切皆对象"><a href="#一切皆对象" class="headerlink" title="一切皆对象"></a>一切皆对象</h1><p>首先我们来理解下Python中的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"hi "</span> + name</span><br><span class="line"></span><br><span class="line">print(hi())</span><br><span class="line"><span class="comment"># output: 'hi yasoob'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们甚至可以将一个函数赋值给一个变量，比如</span></span><br><span class="line">greet = hi</span><br><span class="line"><span class="comment"># 我们这里没有在使用小括号，因为我们并不是在调用hi函数</span></span><br><span class="line"><span class="comment"># 而是在将它放在greet变量里头。我们尝试运行下这个</span></span><br><span class="line"></span><br><span class="line">print(greet())</span><br><span class="line"><span class="comment"># output: 'hi yasoob'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果我们删掉旧的hi函数，看看会发生什么！</span></span><br><span class="line"><span class="keyword">del</span> hi</span><br><span class="line">print(hi())</span><br><span class="line"><span class="comment">#outputs: NameError</span></span><br><span class="line"></span><br><span class="line">print(greet())</span><br><span class="line"><span class="comment">#outputs: 'hi yasoob'</span></span><br></pre></td></tr></table></figure>
<h1 id="在函数中定义函数"><a href="#在函数中定义函数" class="headerlink" title="在函数中定义函数"></a>在函数中定义函数</h1><p>刚才那些就是函数的基本知识了。我们来让你的知识更进一步。在Python中我们可以在一个函数中定义另一个函数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line">    print(<span class="string">"now you are inside the hi() function"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the greet() function"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">welcome</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the welcome() function"</span></span><br><span class="line"></span><br><span class="line">    print(greet())</span><br><span class="line">    print(welcome())</span><br><span class="line">    print(<span class="string">"now you are back in the hi() function"</span>)</span><br><span class="line"></span><br><span class="line">hi()</span><br><span class="line"><span class="comment">#output:now you are inside the hi() function</span></span><br><span class="line"><span class="comment">#       now you are in the greet() function</span></span><br><span class="line"><span class="comment">#       now you are in the welcome() function</span></span><br><span class="line"><span class="comment">#       now you are back in the hi() function</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面展示了无论何时你调用hi(), greet()和welcome()将会同时被调用。</span></span><br><span class="line"><span class="comment"># 然后greet()和welcome()函数在hi()函数之外是不能访问的，比如：</span></span><br><span class="line"></span><br><span class="line">greet()</span><br><span class="line"><span class="comment">#outputs: NameError: name 'greet' is not defined</span></span><br></pre></td></tr></table></figure><br>那现在我们知道了可以在函数中定义另外的函数。也就是说：我们可以创建嵌套的函数。现在你需要再多学一点，就是函数也能返回函数。</p>
<h1 id="从函数中返回函数"><a href="#从函数中返回函数" class="headerlink" title="从函数中返回函数"></a>从函数中返回函数</h1><p>其实并不需要在一个函数里去执行另一个函数，我们也可以将其作为输出返回出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the greet() function"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">welcome</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the welcome() function"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">"yasoob"</span>:</span><br><span class="line">        <span class="keyword">return</span> greet</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> welcome</span><br><span class="line"></span><br><span class="line">a = hi()</span><br><span class="line">print(a)</span><br><span class="line"><span class="comment">#outputs: &lt;function greet at 0x7f2143c01500&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#上面清晰地展示了`a`现在指向到hi()函数中的greet()函数</span></span><br><span class="line"><span class="comment">#现在试试这个</span></span><br><span class="line"></span><br><span class="line">print(a())</span><br><span class="line"><span class="comment">#outputs: now you are in the greet() function</span></span><br></pre></td></tr></table></figure>
<p>再次看看这个代码。在<code>if/else</code>语句中我们返回<code>greet</code>和<code>welcome</code>，而不是<code>greet()</code>和<code>welcome()</code>。为什么那样？这是因为当你把一对小括号放在后面，这个函数就会执行；然而如果你不放括号在它后面，那它可以被到处传递，并且可以赋值给别的变量而不去执行它。</p>
<p>你明白了吗？让我再稍微多解释点细节。</p>
<p>当我们写下<code>a = hi()</code>，<code>hi()</code>会被执行，而由于<code>name</code>参数默认是<em>yasoob</em>，所以函数<code>greet</code>被返回了。如果我们把语句改为<code>a = hi(name = &quot;ali&quot;)</code>，那么<code>welcome</code>函数将被返回。我们还可以打印出<code>hi()()</code>，这会输出<em>now you are in the greet() function</em>。</p>
<h1 id="将函数作为参数传给另一个函数"><a href="#将函数作为参数传给另一个函数" class="headerlink" title="将函数作为参数传给另一个函数"></a>将函数作为参数传给另一个函数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"hi yasoob!"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doSomethingBeforeHi</span><span class="params">(func)</span>:</span></span><br><span class="line">    print(<span class="string">"I am doing some boring work before executing hi()"</span>)</span><br><span class="line">    print(func())</span><br><span class="line"></span><br><span class="line">doSomethingBeforeHi(hi)</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing hi()</span></span><br><span class="line"><span class="comment">#        hi yasoob!</span></span><br></pre></td></tr></table></figure>
<p>现在你已经具备所有必需知识，来进一步学习装饰器真正是什么了。装饰器让你在一个函数的前后去执行代码。</p>
<h1 id="你的第一个装饰器"><a href="#你的第一个装饰器" class="headerlink" title="你的第一个装饰器"></a>你的第一个装饰器</h1><p>在上一个例子里，其实我们已经创建了一个装饰器！现在我们修改下上一个装饰器，并编写一个稍微更有用点的程序：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span><span class="params">(a_func)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span><span class="params">()</span>:</span></span><br><span class="line">        print(<span class="string">"I am doing some boring work before executing a_func()"</span>)</span><br><span class="line"></span><br><span class="line">        a_func()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"I am doing some boring work after executing a_func()"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: "I am the function which needs some decoration to remove my foul smell"</span></span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br><span class="line"><span class="comment">#now a_function_requiring_decoration is wrapped by wrapTheFunction()</span></span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#        I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#        I am doing some boring work after executing a_func()</span></span><br></pre></td></tr></table></figure>
<p>你看明白了吗？我们刚刚应用了之前学习到的原理。这正是python中装饰器做的事情！它们封装一个函数，并且用这样或者那样的方式来修改它的行为。现在你也许疑惑，我们在代码里并没有使用@符号？那只是一个简短的方式来生成一个被装饰的函数。这里是我们如何使用@来运行之前的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Hey you! Decorate me!"""</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to "</span></span><br><span class="line">          <span class="string">"remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#         I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#         I am doing some boring work after executing a_func()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#the @a_new_decorator is just a short way of saying:</span></span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br></pre></td></tr></table></figure>
<p>希望你现在对Python装饰器的工作原理有一个基本的理解。如果我们运行如下代码会存在一个问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(a_function_requiring_decoration.__name__)</span><br><span class="line"><span class="comment"># Output: wrapTheFunction</span></span><br></pre></td></tr></table></figure>
<p>这并不是我们想要的！Ouput输出应该是“a_function_requiring_decoration”。这里的函数被warpTheFunction替代了。它重写了我们函数的名字和注释文档(docstring)。幸运的是Python提供给我们一个简单的函数来解决这个问题，那就是functools.wraps。我们修改上一个例子来使用functools.wraps：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span><span class="params">(a_func)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(a_func)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span><span class="params">()</span>:</span></span><br><span class="line">        print(<span class="string">"I am doing some boring work before executing a_func()"</span>)</span><br><span class="line">        a_func()</span><br><span class="line">        print(<span class="string">"I am doing some boring work after executing a_func()"</span>)</span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"></span><br><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Hey yo! Decorate me!"""</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to "</span></span><br><span class="line">          <span class="string">"remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">print(a_function_requiring_decoration.__name__)</span><br><span class="line"><span class="comment"># Output: a_function_requiring_decoration</span></span><br></pre></td></tr></table></figure>
<p>现在好多了。我们接下来学习装饰器的一些常用场景。</p>
<p>蓝本规范:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorator_name</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorated</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> can_run:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"Function will not run"</span></span><br><span class="line">        <span class="keyword">return</span> f(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> decorated</span><br><span class="line"></span><br><span class="line"><span class="meta">@decorator_name</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span>(<span class="string">"Function is running"</span>)</span><br><span class="line"></span><br><span class="line">can_run = <span class="literal">True</span></span><br><span class="line">print(func())</span><br><span class="line"><span class="comment"># Output: Function is running</span></span><br><span class="line"></span><br><span class="line">can_run = <span class="literal">False</span></span><br><span class="line">print(func())</span><br><span class="line"><span class="comment"># Output: Function will not run</span></span><br></pre></td></tr></table></figure><br>注意：@wraps接受一个函数来进行装饰，并加入了复制函数名称、注释文档、参数列表等等的功能。这可以让我们在装饰器里面访问在装饰之前的函数的属性。</p>
<h1 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h1><p>现在我们来看一下装饰器在哪些地方特别耀眼，以及使用它可以让一些事情管理起来变得更简单。</p>
<h1 id="授权-Authorization"><a href="#授权-Authorization" class="headerlink" title="授权(Authorization)"></a>授权(Authorization)</h1><p>装饰器能有助于检查某个人是否被授权去使用一个web应用的端点(endpoint)。它们被大量使用于Flask和Django web框架中。这里是一个例子来使用基于装饰器的授权：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">requires_auth</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorated</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        auth = request.authorization</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> auth <span class="keyword">or</span> <span class="keyword">not</span> check_auth(auth.username, auth.password):</span><br><span class="line">            authenticate()</span><br><span class="line">        <span class="keyword">return</span> f(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> decorated</span><br></pre></td></tr></table></figure>
<h1 id="日志-Logging"><a href="#日志-Logging" class="headerlink" title="日志(Logging)"></a>日志(Logging)</h1><p>日志是装饰器运用的另一个亮点。这是个例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logit</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(func)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_logging</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        print(func.__name__ + <span class="string">" was called"</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> with_logging</span><br><span class="line"></span><br><span class="line"><span class="meta">@logit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addition_func</span><span class="params">(x)</span>:</span></span><br><span class="line">   <span class="string">"""Do some math."""</span></span><br><span class="line">   <span class="keyword">return</span> x + x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result = addition_func(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># Output: addition_func was called</span></span><br></pre></td></tr></table></figure></p>
<p>我敢肯定你已经在思考装饰器的一个其他聪明用法了。</p>
<h1 id="带参数的装饰器"><a href="#带参数的装饰器" class="headerlink" title="带参数的装饰器"></a>带参数的装饰器</h1><p>来想想这个问题，难道<code>@wraps</code>不也是个装饰器吗？但是，它接收一个参数，就像任何普通的函数能做的那样。那么，为什么我们不也那样做呢？</p>
<p>这是因为，当你使用<code>@my_decorator</code>语法时，你是在应用一个以单个函数作为参数的一个包裹函数。记住，Python里每个东西都是一个对象，而且这包括函数！记住了这些，我们可以编写一下能返回一个包裹函数的函数。</p>
<h1 id="在函数中嵌入装饰器"><a href="#在函数中嵌入装饰器" class="headerlink" title="在函数中嵌入装饰器"></a>在函数中嵌入装饰器</h1><p>我们回到日志的例子，并创建一个包裹函数，能让我们指定一个用于输出的日志文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logit</span><span class="params">(logfile=<span class="string">'out.log'</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">logging_decorator</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">        @wraps(func)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            log_string = func.__name__ + <span class="string">" was called"</span></span><br><span class="line">            print(log_string)</span><br><span class="line">            <span class="comment"># 打开logfile，并写入内容</span></span><br><span class="line">            <span class="keyword">with</span> open(logfile, <span class="string">'a'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                <span class="comment"># 现在将日志打到指定的logfile</span></span><br><span class="line">                opened_file.write(log_string + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapped_function</span><br><span class="line">    <span class="keyword">return</span> logging_decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@logit()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">myfunc1()</span><br><span class="line"><span class="comment"># Output: myfunc1 was called</span></span><br><span class="line"><span class="comment"># 现在一个叫做 out.log 的文件出现了，里面的内容就是上面的字符串</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@logit(logfile='func2.log')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">myfunc2()</span><br><span class="line"><span class="comment"># Output: myfunc2 was called</span></span><br><span class="line"><span class="comment"># 现在一个叫做 func2.log 的文件出现了，里面的内容就是上面的字符串</span></span><br></pre></td></tr></table></figure>
<h1 id="装饰器类"><a href="#装饰器类" class="headerlink" title="装饰器类"></a>装饰器类</h1><p>现在我们有了能用于正式环境的<code>logit</code>装饰器，但当我们的应用的某些部分还比较脆弱时，异常也许是需要更紧急关注的事情。比方说有时你只想打日志到一个文件。而有时你想把引起你注意的问题发送到一个email，同时也保留日志，留个记录。这是一个使用继承的场景，但目前为止我们只看到过用来构建装饰器的函数。</p>
<p>幸运的是，类也可以用来构建装饰器。那我们现在以一个类而不是一个函数的方式，来重新构建<code>logit</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">logit</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, logfile=<span class="string">'out.log'</span>)</span>:</span></span><br><span class="line">        self.logfile = logfile</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, func)</span>:</span></span><br><span class="line"><span class="meta">        @wraps(func)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            log_string = func.__name__ + <span class="string">" was called"</span></span><br><span class="line">            print(log_string)</span><br><span class="line">            <span class="comment"># 打开logfile并写入</span></span><br><span class="line">            <span class="keyword">with</span> open(self.logfile, <span class="string">'a'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                <span class="comment"># 现在将日志打到指定的文件</span></span><br><span class="line">                opened_file.write(log_string + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="comment"># 现在，发送一个通知</span></span><br><span class="line">            self.notify()</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapped_function</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">notify</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># logit只打日志，不做别的</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>这个实现有一个附加优势，在于比嵌套函数的方式更加整洁，而且包裹一个函数还是使用跟以前一样的语法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@logit()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>现在，我们给<code>logit</code>创建子类，来添加email的功能(虽然email这个话题不会在这里展开)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">email_logit</span><span class="params">(logit)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    一个logit的实现版本，可以在函数调用时发送email给管理员</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, email=<span class="string">'admin@myproject.com'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">        self.email = email</span><br><span class="line">        super(email_logit, self).__init__(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">notify</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 发送一封email到self.email</span></span><br><span class="line">        <span class="comment"># 这里就不做实现了</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>从现在起，<code>@email_logit</code>将会和<code>@logit</code>产生同样的效果，但是在打日志的基础上，还会多发送一封邮件给管理员。</p>
<h1 id="Global和Return"><a href="#Global和Return" class="headerlink" title="Global和Return"></a>Global和Return</h1><p>你也许遇到过, python中一些函数在最尾部有一个<code>return</code>关键字。你知道它是干嘛吗？它和其他语言的<code>return</code>类似。我们来检查下这个小函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1, value2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value1 + value2</span><br><span class="line"></span><br><span class="line">result = add(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># Output: 8</span></span><br></pre></td></tr></table></figure>
<p>上面这个函数将两个值作为输入，然后输出它们相加之和。我们也可以这样做：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1,value2)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> result</span><br><span class="line">    result = value1 + value2</span><br><span class="line"></span><br><span class="line">add(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># Output: 8</span></span><br></pre></td></tr></table></figure></p>
<p>那首先我们来谈谈第一段也就是包含<code>return</code>关键字的代码。那个函数把值赋给了调用它的变量（也就是例子中的result变量）。<br>大多数境况下，你并不需要使用<code>global</code>关键字。然而我们也来检查下另外一段也就是包含<code>global</code>关键字的代码。<br>那个函数生成了一个<code>global</code>（全局）变量result。</p>
<p><code>global</code>在这的意思是什么？<code>global</code>变量意味着我们可以在函数以外的区域都能访问这个变量。让我们通过一个例子来证明它：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先，是没有使用global变量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1, value2)</span>:</span></span><br><span class="line">    result = value1 + value2</span><br><span class="line"></span><br><span class="line">add(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Oh 糟了，我们遇到异常了。为什么会这样？</span></span><br><span class="line"><span class="comment"># python解释器报错说没有一个叫result的变量。</span></span><br><span class="line"><span class="comment"># 这是因为result变量只能在创建它的函数内部才允许访问，除非它是全局的(global)。</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">""</span>, line <span class="number">1</span>, <span class="keyword">in</span></span><br><span class="line">    result</span><br><span class="line">NameError: name <span class="string">'result'</span> <span class="keyword">is</span> <span class="keyword">not</span> defined</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在我们运行相同的代码，不过是在将result变量设为global之后</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1, value2)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> result</span><br><span class="line">    result = value1 + value2</span><br><span class="line"></span><br><span class="line">add(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>如我们所愿，在第二次运行时没有异常了。在实际的编程时，你应该试着避开<code>global</code>关键字，它只会让生活变得艰难，因为它引入了多余的变量到全局作用域了。</p>
<h1 id="多个return值"><a href="#多个return值" class="headerlink" title="多个return值"></a>多个return值</h1><p>那如果你想从一个函数里返回两个变量而不是一个呢？<br>新手们有若干种方法。最著名的方法，是使用<code>global</code>关键字。让我们看下这个没用的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">profile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> name</span><br><span class="line">    <span class="keyword">global</span> age</span><br><span class="line">    name = <span class="string">"Danny"</span></span><br><span class="line">    age = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">profile()</span><br><span class="line">print(name)</span><br><span class="line"><span class="comment"># Output: Danny</span></span><br><span class="line"></span><br><span class="line">print(age)</span><br><span class="line"><span class="comment"># Output: 30</span></span><br></pre></td></tr></table></figure>
<p><strong>注意:</strong> 不要试着使用上述方法。重要的事情说三遍，不要试着使用上述方法！</p>
<p>有些人试着在函数结束时，返回一个包含多个值的<code>tuple</code>(元组)，<code>list</code>(列表)或者<code>dict</code>(字典),来解决这个问题。这是一种可行的方式，而且使用起来像一个黑魔法：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">profile</span><span class="params">()</span>:</span></span><br><span class="line">    name = <span class="string">"Danny"</span></span><br><span class="line">    age = <span class="number">30</span></span><br><span class="line">    <span class="keyword">return</span> (name, age)</span><br><span class="line"></span><br><span class="line">profile_data = profile()</span><br><span class="line">print(profile_data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Output: Danny</span></span><br><span class="line"></span><br><span class="line">print(profile_data[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Output: 30</span></span><br></pre></td></tr></table></figure><br>或者按照更常见的惯例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">profile</span><span class="params">()</span>:</span></span><br><span class="line">    name = <span class="string">"Danny"</span></span><br><span class="line">    age = <span class="number">30</span></span><br><span class="line">    <span class="keyword">return</span> name, age</span><br></pre></td></tr></table></figure><br>这是一种比列表和字典更好的方式。不要使用<code>global</code>关键字，除非你知道你正在做什么。<code>global</code>也许在某些场景下是一个更好的选择（但其中大多数情况都不是）。</p>
<h1 id="对象变动-Mutation"><a href="#对象变动-Mutation" class="headerlink" title="对象变动(Mutation)"></a>对象变动(Mutation)</h1><p>Python中可变(<strong>mutable</strong>)与不可变(<strong>immutable</strong>)的数据类型让新手很是头痛。简单的说，可变(mutable)意味着”可以被改动”，而不可变(immutable)的意思是“常量(constant)”。想把脑筋转动起来吗？考虑下这个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">foo = [<span class="string">'hi'</span>]</span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi']</span></span><br><span class="line"></span><br><span class="line">bar = foo</span><br><span class="line">bar += [<span class="string">'bye'</span>]</span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi', 'bye']</span></span><br></pre></td></tr></table></figure>
<p>刚刚发生了什么？我们预期的不是那样！我们期望看到是这样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">foo = [<span class="string">'hi'</span>]</span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi']</span></span><br><span class="line"></span><br><span class="line">bar = foo</span><br><span class="line">bar += [<span class="string">'bye'</span>]</span><br><span class="line"></span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi']</span></span><br><span class="line"></span><br><span class="line">print(bar)</span><br><span class="line"><span class="comment"># Output: ['hi', 'bye']</span></span><br></pre></td></tr></table></figure>
<p>这不是一个bug。这是对象可变性(<strong>mutability</strong>)在作怪。每当你将一个变量赋值为另一个可变类型的变量时，对这个数据的任意改动会同时反映到这两个变量上去。新变量只不过是老变量的一个别名而已。这个情况只是针对可变数据类型。下面的函数和可变数据类型让你一下就明白了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(num, target=[])</span>:</span></span><br><span class="line">    target.append(num)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line"></span><br><span class="line">add_to(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Output: [1]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Output: [1, 2]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">3</span>)</span><br><span class="line"><span class="comment"># Output: [1, 2, 3]</span></span><br></pre></td></tr></table></figure>
<p>你可能预期它表现的不是这样子。你可能希望，当你调用<code>add_to</code>时，有一个新的列表被创建，就像这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(num, target=[])</span>:</span></span><br><span class="line">    target.append(num)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line"></span><br><span class="line">add_to(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Output: [1]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Output: [2]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">3</span>)</span><br><span class="line"><span class="comment"># Output: [3]</span></span><br></pre></td></tr></table></figure>
<p>啊哈！这次又没有达到预期，是列表的可变性在作怪。在Python中当函数被定义时，默认参数只会运算一次，而不是每次被调用时都会重新运算。你应该永远不要定义可变类型的默认参数，除非你知道你正在做什么。你应该像这样做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(element, target=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> target <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        target = []</span><br><span class="line">    target.append(element)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line">``` </span><br><span class="line">现在每当你在调用这个函数不传入```target```参数的时候，一个新的列表会被创建。举个例子：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">add_to(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Output: [42]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Output: [42]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Output: [42]</span></span><br></pre></td></tr></table></figure>
<h1 id="slots-魔法"><a href="#slots-魔法" class="headerlink" title="__slots__魔法"></a><code>__slots__</code>魔法</h1><p>在Python中，每个类都有实例属性。默认情况下Python用一个字典来保存一个对象的实例属性。这非常有用，因为它允许我们在运行时去设置任意的新属性。</p>
<p>然而，对于有着已知属性的小类来说，它可能是个瓶颈。这个字典浪费了很多内存。Python不能在对象创建时直接分配一个固定量的内存来保存所有的属性。因此如果你创建许多对象（我指的是成千上万个），它会消耗掉很多内存。<br>不过还是有一个方法来规避这个问题。这个方法需要使用<code>__slots__</code>来告诉Python不要使用字典，而且只给一个固定集合的属性分配空间。</p>
<p>这里是一个使用与不使用<code>__slots__</code>的例子：</p>
<ul>
<li><p>不使用 <code>__slots__</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.identifier = identifier</span><br><span class="line">        self.set_up()</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 <code>__slots__</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    __slots__ = [<span class="string">'name'</span>, <span class="string">'identifier'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.identifier = identifier</span><br><span class="line">        self.set_up()</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>第二段代码会为你的内存减轻负担。通过这个技巧，有些人已经看到内存占用率几乎40%~50%的减少。</p>
<p>稍微备注一下，你也许需要试一下PyPy。它已经默认地做了所有这些优化。</p>
<p>以下你可以看到一个例子，它用IPython来展示在有与没有<code>__slots__</code>情况下的精确内存占用，感谢 <a href="https://github.com/ianozsvald/ipython_memory_usage" target="_blank" rel="noopener">https://github.com/ianozsvald/ipython_memory_usage</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Python <span class="number">3.4</span><span class="number">.3</span> (default, Jun  <span class="number">6</span> <span class="number">2015</span>, <span class="number">13</span>:<span class="number">32</span>:<span class="number">34</span>)</span><br><span class="line">Type <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line"></span><br><span class="line">IPython <span class="number">4.0</span><span class="number">.0</span> -- An enhanced Interactive Python.</span><br><span class="line">?         -&gt; Introduction and overview of IPython's features.</span><br><span class="line">%quickref -&gt; Quick reference.</span><br><span class="line">help      -&gt; Python's own help system.</span><br><span class="line">object?   -&gt; Details about 'object', use 'object??' for extra details.</span><br><span class="line"></span><br><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> ipython_memory_usage.ipython_memory_usage <span class="keyword">as</span> imu</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: imu.start_watching_memory()</span><br><span class="line">In [<span class="number">2</span>] used <span class="number">0.0000</span> MiB RAM <span class="keyword">in</span> <span class="number">5.31</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">15.57</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: %cat slots.py</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">        __slots__ = [<span class="string">'name'</span>, <span class="string">'identifier'</span>]</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">                self.name = name</span><br><span class="line">                self.identifier = identifier</span><br><span class="line"></span><br><span class="line">num = <span class="number">1024</span>*<span class="number">256</span></span><br><span class="line">x = [MyClass(<span class="number">1</span>,<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">In [<span class="number">3</span>] used <span class="number">0.2305</span> MiB RAM <span class="keyword">in</span> <span class="number">0.12</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">15.80</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="keyword">from</span> slots <span class="keyword">import</span> *</span><br><span class="line">In [<span class="number">4</span>] used <span class="number">9.3008</span> MiB RAM <span class="keyword">in</span> <span class="number">0.72</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">25.10</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: %cat noslots.py</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">                self.name = name</span><br><span class="line">                self.identifier = identifier</span><br><span class="line"></span><br><span class="line">num = <span class="number">1024</span>*<span class="number">256</span></span><br><span class="line">x = [MyClass(<span class="number">1</span>,<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">In [<span class="number">5</span>] used <span class="number">0.1758</span> MiB RAM <span class="keyword">in</span> <span class="number">0.12</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">25.28</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: <span class="keyword">from</span> noslots <span class="keyword">import</span> *</span><br><span class="line">In [<span class="number">6</span>] used <span class="number">22.6680</span> MiB RAM <span class="keyword">in</span> <span class="number">0.80</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">47.95</span> MiB</span><br></pre></td></tr></table></figure>
<h1 id="虚拟环境-virtualenv"><a href="#虚拟环境-virtualenv" class="headerlink" title="虚拟环境(virtualenv)"></a>虚拟环境(virtualenv)</h1><h2 id="你听说过virtualenv吗？"><a href="#你听说过virtualenv吗？" class="headerlink" title="你听说过virtualenv吗？"></a>你听说过<code>virtualenv</code>吗？</h2><p>如果你是一位初学者，你可能没有听说过<code>virtualenv</code>；但如果你是位经验丰富的程序员，那么它可能是你的工具集的重要组成部分。</p>
<h2 id="那么，什么是virtualenv"><a href="#那么，什么是virtualenv" class="headerlink" title="那么，什么是virtualenv?"></a>那么，什么是<code>virtualenv</code>?</h2><p><code>Virtualenv</code> 是一个工具，它能够帮我们创建一个独立(隔离)的Python环境。想象你有一个应用程序，依赖于版本为2的第三方模块，但另一个程序依赖的版本是3，请问你如何使用和开发这些应用程序？</p>
<p>如果你把一切都安装到了<code>/usr/lib/python2.7/site-packages</code>（或者其它平台的标准位置），那很容易出现某个模块被升级而你却不知道的情况。</p>
<p>在另一种情况下，想象你有一个已经开发完成的程序，但是你不想更新它所依赖的第三方模块版本；但你已经开始另一个程序，需要这些第三方模块的版本。</p>
<h2 id="用什么方式解决？"><a href="#用什么方式解决？" class="headerlink" title="用什么方式解决？"></a>用什么方式解决？</h2><p>使用<code>virtualenv</code>！针对每个程序创建独立（隔离）的Python环境，而不是在全局安装所依赖的模块。</p>
<p>要安装它，只需要在命令行中输入以下命令：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ pip install virtualenv</span><br></pre></td></tr></table></figure>
<p>最重要的命令是：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ virtualenv myproject</span><br><span class="line">$ <span class="built_in">source</span> myproject/bin/activate</span><br></pre></td></tr></table></figure>
<p>执行第一个命令在<code>myproject</code>文件夹创建一个隔离的virtualenv环境，第二个命令激活这个隔离的环境(<code>virtualenv</code>)。</p>
<p>在创建virtualenv时，你必须做出决定：这个virtualenv是使用系统全局的模块呢？还是只使用这个virtualenv内的模块。<br>默认情况下，virtualenv不会使用系统全局模块。</p>
<p>如果你想让你的virtualenv使用系统全局模块，请使用<code>--system-site-packages</code>参数创建你的virtualenv，例如：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">virtualenv --system-site-packages mycoolproject</span><br></pre></td></tr></table></figure>
<p>使用以下命令可以退出这个virtualenv:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ deactivate</span><br></pre></td></tr></table></figure>
<p>运行之后将恢复使用你系统全局的Python模块。</p>
<h1 id="福利"><a href="#福利" class="headerlink" title="福利"></a>福利</h1><p>你可以使用<code>smartcd</code>来帮助你管理你的环境，当你切换目录时，它可以帮助你激活（activate）和退出（deactivate）你的virtualenv。我已经用了很多次，很喜欢它。你可以在github(<a href="https://github.com/cxreg/smartcd" target="_blank" rel="noopener">https://github.com/cxreg/smartcd</a>) 上找到更多关于它的资料。</p>
<p>这只是一个virtualenv的简短介绍，你可以在 <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/" target="_blank" rel="noopener">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a> 找到更多信息。</p>
<h1 id="容器-Collections"><a href="#容器-Collections" class="headerlink" title="容器(Collections)"></a>容器(<code>Collections</code>)</h1><p>Python附带一个模块，它包含许多容器数据类型，名字叫作<code>collections</code>。我们将讨论它的作用和用法。</p>
<p>我们将讨论的是：</p>
<ul>
<li>defaultdict</li>
<li>counter</li>
<li>deque</li>
<li>namedtuple</li>
<li>enum.Enum (包含在Python 3.4以上)</li>
</ul>
<h1 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a>defaultdict</h1><p>我个人使用<code>defaultdict</code>较多，与<code>dict</code>类型不同，你不需要检查<strong>key</strong>是否存在，所以我们能这样做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">colours = (</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Yellow'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Blue'</span>),</span><br><span class="line">    (<span class="string">'Arham'</span>, <span class="string">'Green'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Black'</span>),</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Red'</span>),</span><br><span class="line">    (<span class="string">'Ahmed'</span>, <span class="string">'Silver'</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">favourite_colours = defaultdict(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, colour <span class="keyword">in</span> colours:</span><br><span class="line">    favourite_colours[name].append(colour)</span><br><span class="line"></span><br><span class="line">print(favourite_colours)</span><br></pre></td></tr></table></figure>
<h2 id="运行输出"><a href="#运行输出" class="headerlink" title="运行输出"></a>运行输出</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># defaultdict(&lt;type 'list'&gt;,</span></span><br><span class="line"><span class="comment">#    &#123;'Arham': ['Green'],</span></span><br><span class="line"><span class="comment">#     'Yasoob': ['Yellow', 'Red'],</span></span><br><span class="line"><span class="comment">#     'Ahmed': ['Silver'],</span></span><br><span class="line"><span class="comment">#     'Ali': ['Blue', 'Black']</span></span><br><span class="line"><span class="comment"># &#125;)</span></span><br></pre></td></tr></table></figure>
<p>另一种重要的是例子就是：当你在一个字典中对一个键进行嵌套赋值时，如果这个键不存在，会触发<code>keyError</code>异常。 <code>defaultdict</code>允许我们用一个聪明的方式绕过这个问题。<br> 首先我分享一个使用<code>dict</code>触发<code>KeyError</code>的例子，然后提供一个使用<code>defaultdict</code>的解决方案。</p>
<p><strong>问题</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">some_dict = &#123;&#125;</span><br><span class="line">some_dict[<span class="string">'colours'</span>][<span class="string">'favourite'</span>] = <span class="string">"yellow"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 异常输出：KeyError: 'colours'</span></span><br></pre></td></tr></table></figure>
<p><strong>解决方案</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line">tree = <span class="keyword">lambda</span>: collections.defaultdict(tree)</span><br><span class="line">some_dict = tree()</span><br><span class="line">some_dict[<span class="string">'colours'</span>][<span class="string">'favourite'</span>] = <span class="string">"yellow"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 运行正常</span></span><br></pre></td></tr></table></figure>
<p>你可以用<code>json.dumps</code>打印出<code>some_dict</code>，例如：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">print(json.dumps(some_dict))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: &#123;"colours": &#123;"favourite": "yellow"&#125;&#125;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="counter"><a href="#counter" class="headerlink" title="counter"></a>counter</h1><p>Counter是一个计数器，它可以帮助我们针对某项数据进行计数。比如它可以用来计算每个人喜欢多少种颜色：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">colours = (</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Yellow'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Blue'</span>),</span><br><span class="line">    (<span class="string">'Arham'</span>, <span class="string">'Green'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Black'</span>),</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Red'</span>),</span><br><span class="line">    (<span class="string">'Ahmed'</span>, <span class="string">'Silver'</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">favs = Counter(name <span class="keyword">for</span> name, colour <span class="keyword">in</span> colours)</span><br><span class="line">print(favs)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出:</span></span><br><span class="line"><span class="comment">## Counter(&#123;</span></span><br><span class="line"><span class="comment">##     'Yasoob': 2,</span></span><br><span class="line"><span class="comment">##     'Ali': 2,</span></span><br><span class="line"><span class="comment">##     'Arham': 1,</span></span><br><span class="line"><span class="comment">##     'Ahmed': 1</span></span><br><span class="line"><span class="comment">##  &#125;)</span></span><br></pre></td></tr></table></figure>
<p>我们也可以在利用它统计一个文件，例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'filename'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    line_count = Counter(f)</span><br><span class="line">print(line_count)</span><br></pre></td></tr></table></figure>
<h1 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h1><p>deque提供了一个双端队列，你可以从头/尾两端添加或删除元素。要想使用它，首先我们要从<code>collections</code>中导入<code>deque</code>模块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br></pre></td></tr></table></figure>
<p>现在，你可以创建一个<code>deque</code>对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = deque()</span><br></pre></td></tr></table></figure>
<p>它的用法就像python的<code>list</code>，并且提供了类似的方法，例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = deque()</span><br><span class="line">d.append(<span class="string">'1'</span>)</span><br><span class="line">d.append(<span class="string">'2'</span>)</span><br><span class="line">d.append(<span class="string">'3'</span>)</span><br><span class="line"></span><br><span class="line">print(len(d))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 3</span></span><br><span class="line"></span><br><span class="line">print(d[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: '1'</span></span><br><span class="line"></span><br><span class="line">print(d[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: '3'</span></span><br></pre></td></tr></table></figure>
<p>你可以从两端取出(pop)数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = deque(range(<span class="number">5</span>))</span><br><span class="line">print(len(d))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 5</span></span><br><span class="line"></span><br><span class="line">d.popleft()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 0</span></span><br><span class="line"></span><br><span class="line">d.pop()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 4</span></span><br><span class="line"></span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: deque([1, 2, 3])</span></span><br></pre></td></tr></table></figure>
<p>我们也可以限制这个列表的大小，当超出你设定的限制时，数据会从对队列另一端被挤出去(pop)。<br>最好的解释是给出一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = deque(maxlen=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<p>现在当你插入30条数据时，最左边一端的数据将从队列中删除。</p>
<p>你还可以从任一端扩展这个队列中的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = deque([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">d.extendleft([<span class="number">0</span>])</span><br><span class="line">d.extend([<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: deque([0, 1, 2, 3, 4, 5, 6, 7, 8])</span></span><br></pre></td></tr></table></figure>
<h1 id="namedtuple"><a href="#namedtuple" class="headerlink" title="namedtuple"></a>namedtuple</h1><p>您可能已经熟悉元组。<br>一个元组是一个不可变的列表，你可以存储一个数据的序列，它和命名元组(<code>namedtuples</code>)非常像，但有几个关键的不同。<br>主要相似点是都不像列表，你不能修改元组中的数据。为了获取元组中的数据，你需要使用整数作为索引：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">man = (<span class="string">'Ali'</span>, <span class="number">30</span>)</span><br><span class="line">print(man[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: Ali</span></span><br></pre></td></tr></table></figure>
<p>嗯，那<code>namedtuples</code>是什么呢？它把元组变成一个针对简单任务的容器。你不必使用整数索引来访问一个<code>namedtuples</code>的数据。你可以像字典(<code>dict</code>)一样访问<code>namedtuples</code>，但<code>namedtuples</code>是不可变的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line"></span><br><span class="line">print(perry)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: Animal(name='perry', age=31, type='cat')</span></span><br><span class="line"></span><br><span class="line">print(perry.name)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 'perry'</span></span><br></pre></td></tr></table></figure>
<p>现在你可以看到，我们可以用名字来访问<code>namedtuple</code>中的数据。我们再继续分析它。一个命名元组(<code>namedtuple</code>)有两个必需的参数。它们是元组名称和字段名称。</p>
<p>在上面的例子中，我们的元组名称是<code>Animal</code>，字段名称是’name’，’age’和’type’。<br><code>namedtuple</code>让你的元组变得<strong>自文档</strong>了。你只要看一眼就很容易理解代码是做什么的。<br>你也不必使用整数索引来访问一个命名元组，这让你的代码更易于维护。<br>而且，<strong><code>namedtuple</code>的每个实例没有对象字典</strong>，所以它们很轻量，与普通的元组比，并不需要更多的内存。这使得它们比字典更快。</p>
<p>然而，要记住它是一个元组，属性值在<code>namedtuple</code>中是不可变的，所以下面的代码不能工作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line">perry.age = <span class="number">42</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出:</span></span><br><span class="line"><span class="comment">## Traceback (most recent call last):</span></span><br><span class="line"><span class="comment">##     File "", line 1, in</span></span><br><span class="line"><span class="comment">## AttributeError: can't set attribute</span></span><br></pre></td></tr></table></figure>
<p>你应该使用命名元组来让代码<strong>自文档</strong>，<strong>它们向后兼容于普通的元组</strong>，这意味着你可以既使用整数索引，也可以使用名称来访问<code>namedtuple</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line">print(perry[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: perry</span></span><br></pre></td></tr></table></figure>
<p>最后，你可以将一个命名元组转换为字典，方法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"Perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line">print(perry._asdict())</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: OrderedDict([('name', 'Perry'), ('age', 31), ...</span></span><br></pre></td></tr></table></figure>
<h1 id="enum-Enum-Python-3-4"><a href="#enum-Enum-Python-3-4" class="headerlink" title="enum.Enum (Python 3.4+)"></a>enum.Enum (Python 3.4+)</h1><p>另一个有用的容器是枚举对象，它属于<code>enum</code>模块，存在于Python 3.4以上版本中（同时作为一个独立的PyPI包<code>enum34</code>供老版本使用）。Enums(枚举类型)基本上是一种组织各种东西的方式。</p>
<p>让我们回顾一下上一个’Animal’命名元组的例子。<br>它有一个type字段，问题是，type是一个字符串。<br>那么问题来了，万一程序员输入了<code>Cat</code>，因为他按到了Shift键，或者输入了’CAT’，甚至’kitten’？</p>
<p>枚举可以帮助我们避免这个问题，通过不使用字符串。考虑以下这个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Species</span><span class="params">(Enum)</span>:</span></span><br><span class="line">    cat = <span class="number">1</span></span><br><span class="line">    dog = <span class="number">2</span></span><br><span class="line">    horse = <span class="number">3</span></span><br><span class="line">    aardvark = <span class="number">4</span></span><br><span class="line">    butterfly = <span class="number">5</span></span><br><span class="line">    owl = <span class="number">6</span></span><br><span class="line">    platypus = <span class="number">7</span></span><br><span class="line">    dragon = <span class="number">8</span></span><br><span class="line">    unicorn = <span class="number">9</span></span><br><span class="line">    <span class="comment"># 依次类推</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 但我们并不想关心同一物种的年龄，所以我们可以使用一个别名</span></span><br><span class="line">    kitten = <span class="number">1</span>  <span class="comment"># (译者注：幼小的猫咪)</span></span><br><span class="line">    puppy = <span class="number">2</span>   <span class="comment"># (译者注：幼小的狗狗)</span></span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"Perry"</span>, age=<span class="number">31</span>, type=Species.cat)</span><br><span class="line">drogon = Animal(name=<span class="string">"Drogon"</span>, age=<span class="number">4</span>, type=Species.dragon)</span><br><span class="line">tom = Animal(name=<span class="string">"Tom"</span>, age=<span class="number">75</span>, type=Species.cat)</span><br><span class="line">charlie = Animal(name=<span class="string">"Charlie"</span>, age=<span class="number">2</span>, type=Species.kitten)</span><br></pre></td></tr></table></figure>
<h2 id="现在，我们进行一些测试："><a href="#现在，我们进行一些测试：" class="headerlink" title="现在，我们进行一些测试："></a>现在，我们进行一些测试：</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>charlie.type == tom.type</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>charlie.type</span><br><span class="line">&lt;Species.cat: <span class="number">1</span>&gt;</span><br></pre></td></tr></table></figure>
<p>这样就没那么容易错误，我们必须更明确，而且我们应该只使用定义后的枚举类型。</p>
<p>有三种方法访问枚举数据，例如以下方法都可以获取到’cat’的值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Species(<span class="number">1</span>)</span><br><span class="line">Species[<span class="string">'cat'</span>]</span><br><span class="line">Species.cat</span><br></pre></td></tr></table></figure>
<p>这只是一个快速浏览<code>collections</code>模块的介绍，建议你阅读本文最后的官方文档。</p>
<h1 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h1><p>枚举(<code>enumerate</code>)是Python内置函数。它的用处很难在简单的一行中说明，但是大多数的新人，甚至一些高级程序员都没有意识到它。</p>
<p>它允许我们遍历数据并自动计数，</p>
<p>下面是一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> counter, value <span class="keyword">in</span> enumerate(some_list):</span><br><span class="line">    print(counter, value)</span><br></pre></td></tr></table></figure>
<p>不只如此，<code>enumerate</code>也接受一些可选参数，这使它更有用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grapes'</span>, <span class="string">'pear'</span>]</span><br><span class="line"><span class="keyword">for</span> c, value <span class="keyword">in</span> enumerate(my_list, <span class="number">1</span>):</span><br><span class="line">    print(c, value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'apple'</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">'banana'</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">'grapes'</span>)</span><br><span class="line">(<span class="number">4</span>, <span class="string">'pear'</span>)</span><br></pre></td></tr></table></figure>
<p>上面这个可选参数允许我们定制从哪个数字开始枚举。<br>你还可以用来创建包含索引的元组列表，<br>例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grapes'</span>, <span class="string">'pear'</span>]</span><br><span class="line">counter_list = list(enumerate(my_list, <span class="number">1</span>))</span><br><span class="line">print(counter_list)</span><br><span class="line"><span class="comment"># 输出: [(1, 'apple'), (2, 'banana'), (3, 'grapes'), (4, 'pear')]</span></span><br></pre></td></tr></table></figure>
<h1 id="对象自省"><a href="#对象自省" class="headerlink" title="对象自省"></a>对象自省</h1><p>自省(introspection)，在计算机编程领域里，是指在运行时来判断一个对象的类型的能力。它是Python的强项之一。Python中所有一切都是一个对象，而且我们可以仔细勘察那些对象。Python还包含了许多内置函数和模块来帮助我们。</p>
<h1 id="dir"><a href="#dir" class="headerlink" title="dir"></a><code>dir</code></h1><p>在这个小节里我们会学习到<code>dir</code>以及它在自省方面如何给我们提供便利。</p>
<p>它是用于自省的最重要的函数之一。它返回一个列表，列出了一个对象所拥有的属性和方法。这里是一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">dir(my_list)</span><br><span class="line"><span class="comment"># Output: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',</span></span><br><span class="line"><span class="comment"># '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',</span></span><br><span class="line"><span class="comment"># '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',</span></span><br><span class="line"><span class="comment"># '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',</span></span><br><span class="line"><span class="comment"># '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',</span></span><br><span class="line"><span class="comment"># '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',</span></span><br><span class="line"><span class="comment"># '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',</span></span><br><span class="line"><span class="comment"># 'remove', 'reverse', 'sort']</span></span><br></pre></td></tr></table></figure>
<p>上面的自省给了我们一个列表对象的所有方法的名字。当你没法回忆起一个方法的名字，这会非常有帮助。如果我们运行<code>dir()</code>而不传入参数，那么它会返回当前作用域的所有名字。</p>
<h1 id="type和id"><a href="#type和id" class="headerlink" title="type和id"></a><code>type</code>和<code>id</code></h1><p><code>type</code>函数返回一个对象的类型。举个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(type(<span class="string">''</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'str'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type([]))</span><br><span class="line"><span class="comment"># Output: &lt;type 'list'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(&#123;&#125;))</span><br><span class="line"><span class="comment"># Output: &lt;type 'dict'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(dict))</span><br><span class="line"><span class="comment"># Output: &lt;type 'type'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'int'&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>id()</code>函数返回任意不同种类对象的唯一ID，举个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name = <span class="string">"Yasoob"</span></span><br><span class="line">print(id(name))</span><br><span class="line"><span class="comment"># Output: 139972439030304</span></span><br></pre></td></tr></table></figure>
<h1 id="inspect模块"><a href="#inspect模块" class="headerlink" title="inspect模块"></a><code>inspect</code>模块</h1><p><code>inspect</code>模块也提供了许多有用的函数，来获取活跃对象的信息。比方说，你可以查看一个对象的成员，只需运行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line">print(inspect.getmembers(str))</span><br><span class="line"><span class="comment"># Output: [('__add__', &lt;slot wrapper '__add__' of ... ...</span></span><br></pre></td></tr></table></figure>
<p>还有好多个其他方法也能有助于自省。如果你愿意，你可以去探索它们。</p>
<h1 id="各种推导式-comprehensions"><a href="#各种推导式-comprehensions" class="headerlink" title="各种推导式(comprehensions)"></a>各种推导式(comprehensions)</h1><p>推导式（又称解析式）是Python的一种独有特性，如果我被迫离开了它，我会非常想念。推导式是可以从一个数据序列构建另一个新的数据序列的结构体。 共有三种推导，在Python2和3中都有支持：</p>
<ul>
<li>列表(<code>list</code>)推导式</li>
<li>字典(<code>dict</code>)推导式</li>
<li>集合(<code>set</code>)推导式</li>
</ul>
<p>我们将一一进行讨论。一旦你知道了使用列表推导式的诀窍，你就能轻易使用任意一种推导式了。</p>
<h1 id="列表推导式（list-comprehensions）"><a href="#列表推导式（list-comprehensions）" class="headerlink" title="列表推导式（list comprehensions）"></a>列表推导式（<code>list</code> comprehensions）</h1><p>列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表。<br>它的结构是在一个中括号里包含一个表达式，然后是一个<code>for</code>语句，然后是0个或多个<code>for</code>或者<code>if</code>语句。那个表达式可以是任意的，意思是你可以在列表中放入任意类型的对象。返回结果将是一个新的列表，在这个以<code>if</code>和<code>for</code>语句为上下文的表达式运行完成之后产生。</p>
<h3 id="规范"><a href="#规范" class="headerlink" title="规范"></a>规范</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">variable = [out_exp <span class="keyword">for</span> out_exp <span class="keyword">in</span> input_list <span class="keyword">if</span> out_exp == <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>这里是另外一个简明例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">multiples = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>) <span class="keyword">if</span> i % <span class="number">3</span> <span class="keyword">is</span> <span class="number">0</span>]</span><br><span class="line">print(multiples)</span><br><span class="line"><span class="comment"># Output: [0, 3, 6, 9, 12, 15, 18, 21, 24, 27]</span></span><br></pre></td></tr></table></figure>
<p>这将对快速生成列表非常有用。<br>有些人甚至更喜欢使用它而不是<code>filter</code>函数。<br>列表推导式在有些情况下超赞，特别是当你需要使用<code>for</code>循环来生成一个新列表。举个例子，你通常会这样做：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">squared = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    squared.append(x**<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>你可以使用列表推导式来简化它，就像这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">squared = [x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>
<h1 id="字典推导式（dict-comprehensions）"><a href="#字典推导式（dict-comprehensions）" class="headerlink" title="字典推导式（dict comprehensions）"></a>字典推导式（<code>dict</code> comprehensions）</h1><p>字典推导和列表推导的使用方法是类似的。这里有个我最近发现的例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mcase = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">34</span>, <span class="string">'A'</span>: <span class="number">7</span>, <span class="string">'Z'</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">mcase_frequency = &#123;</span><br><span class="line">    k.lower(): mcase.get(k.lower(), <span class="number">0</span>) + mcase.get(k.upper(), <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> mcase.keys()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># mcase_frequency == &#123;'a': 17, 'z': 3, 'b': 34&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>在上面的例子中我们把同一个字母但不同大小写的值合并起来了。  </p>
<p>就我个人来说没有大量使用字典推导式。</p>
<p>你还可以快速对换一个字典的键和值：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> some_dict.items()&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="集合推导式（set-comprehensions）"><a href="#集合推导式（set-comprehensions）" class="headerlink" title="集合推导式（set comprehensions）"></a>集合推导式（<code>set</code> comprehensions）</h1><p>它们跟列表推导式也是类似的。 唯一的区别在于它们使用大括号<code>{}</code>。 举个例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">squared = &#123;x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]&#125;</span><br><span class="line">print(squared)</span><br><span class="line"><span class="comment"># Output: &#123;1, 4&#125;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h1><p>异常处理是一种艺术，一旦你掌握，会授予你无穷的力量。我将要向你展示我们能处理异常的一些方式。</p>
<p>最基本的术语里我们知道了<code>try/except</code>从句。可能触发异常产生的代码会放到<code>try</code>语句块里，而处理异常的代码会在<code>except</code>语句块里实现。这是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'An IOError occurred. &#123;&#125;'</span>.format(e.args[<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure>
<p>上面的例子里，我们仅仅在处理一个<code>IOError</code>的异常。大部分初学者还不知道的是，我们可以处理多个异常。</p>
<h1 id="处理多个异常"><a href="#处理多个异常" class="headerlink" title="处理多个异常"></a>处理多个异常</h1><p>我们可以使用三种方法来处理多个异常。</p>
<p>第一种方法需要把所有可能发生的异常放到一个元组里。像这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> (IOError, EOFError) <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"An error occurred. &#123;&#125;"</span>.format(e.args[<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure>
<p>另外一种方式是对每个单独的异常在单独的<code>except</code>语句块中处理。我们想要多少个<code>except</code>语句块都可以。这里是个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> EOFError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"An EOF error occurred."</span>)</span><br><span class="line">    <span class="keyword">raise</span> e</span><br><span class="line"><span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"An error occurred."</span>)</span><br><span class="line">    <span class="keyword">raise</span> e</span><br></pre></td></tr></table></figure>
<p>上面这个方式中，如果异常没有被第一个<code>except</code>语句块处理，那么它也许被下一个语句块处理，或者根本不会被处理。</p>
<p>现在，最后一种方式会捕获<strong>所有</strong>异常：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="comment"># 打印一些异常日志，如果你想要的话</span></span><br><span class="line">    <span class="keyword">raise</span></span><br></pre></td></tr></table></figure>
<p>当你不知道你的程序会抛出什么样的异常时，上面的方式可能非常有帮助。</p>
<h1 id="finally从句"><a href="#finally从句" class="headerlink" title="finally从句"></a><code>finally</code>从句</h1><p>我们把我们的主程序代码包裹进了<code>try</code>从句。然后我们把一些代码包裹进一个<code>except</code>从句，它会在<code>try</code>从句中的代码触发异常时执行。</p>
<p>在下面的例子中，我们还会使用第三个从句，那就是<code>finally</code>从句。包裹到<code>finally</code>从句中的代码不管异常是否触发都将会被执行。这可以被用来在脚本执行之后做清理工作。这里是个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'An IOError occurred. &#123;&#125;'</span>.format(e.args[<span class="number">-1</span>]))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    print(<span class="string">"This would be printed whether or not an exception occurred!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: An IOError occurred. No such file or directory</span></span><br><span class="line"><span class="comment"># This would be printed whether or not an exception occurred!</span></span><br></pre></td></tr></table></figure>
<h1 id="try-else从句"><a href="#try-else从句" class="headerlink" title="try/else从句"></a><code>try/else</code>从句</h1><p>我们常常想在没有触发异常的时候执行一些代码。这可以很轻松地通过一个<code>else</code>从句来达到。</p>
<p>有人也许问了：如果你只是想让一些代码在没有触发异常的情况下执行，为啥你不直接把代码放在<code>try</code>里面呢？<br>回答是，那样的话这段代码中的任意异常都还是会被<code>try</code>捕获，而你并不一定想要那样。</p>
<p>大多数人并不使用<code>else</code>从句，而且坦率地讲我自己也没有大范围使用。这里是个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    print(<span class="string">'I am sure no exception is going to occur!'</span>)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">'exception'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 这里的代码只会在try语句里没有触发异常时运行,</span></span><br><span class="line">    <span class="comment"># 但是这里的异常将 *不会* 被捕获</span></span><br><span class="line">    print(<span class="string">'This would only run if no exception occurs. And an error here '</span></span><br><span class="line">          <span class="string">'would NOT be caught.'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    print(<span class="string">'This would be printed in every case.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: I am sure no exception is going to occur!</span></span><br><span class="line"><span class="comment"># This would only run if no exception occurs.</span></span><br><span class="line"><span class="comment"># This would be printed in every case.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># 17. &#96;&#96;&#96;lambda&#96;&#96;&#96;表达式</span><br><span class="line">&#96;lambda&#96;表达式是一行函数。  </span><br><span class="line">它们在其他语言中也被称为匿名函数。如果你不想在程序中对一个函数使用两次，你也许会想用lambda表达式，它们和普通的函数完全一样。</span><br><span class="line"></span><br><span class="line">__原型__</span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">    lambda 参数:操作(参数)</span><br></pre></td></tr></table></figure>
<p><strong>例子</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add = <span class="keyword">lambda</span> x, y: x + y</span><br><span class="line"></span><br><span class="line">print(add(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># Output: 8</span></span><br></pre></td></tr></table></figure></p>
<p>这还有一些lambda表达式的应用案例，可以在一些特殊情况下使用：</p>
<p><strong>列表排序</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">9</span>, <span class="number">10</span>), (<span class="number">13</span>, <span class="number">-3</span>)]</span><br><span class="line">a.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># Output: [(13, -3), (4, 1), (1, 2), (9, 10)]</span></span><br></pre></td></tr></table></figure></p>
<p><strong>列表并行排序</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = zip(list1, list2)</span><br><span class="line">data = sorted(data)</span><br><span class="line">list1, list2 = map(<span class="keyword">lambda</span> t: list(t), zip(*data))</span><br></pre></td></tr></table></figure></p>
<h1 id="18-一行式"><a href="#18-一行式" class="headerlink" title="18. 一行式"></a>18. 一行式</h1><p>本章节,我将向大家展示一些一行式的Python命令，这些程序将对你非常有帮助。</p>
<p><strong>简易Web Server</strong></p>
<p>你是否想过通过网络快速共享文件？好消息，Python为你提供了这样的功能。进入到你要共享文件的目录下并在命令行中运行下面的代码：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Python 2</span></span><br><span class="line">python -m SimpleHTTPServer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Python 3</span></span><br><span class="line">python -m http.server</span><br></pre></td></tr></table></figure>
<p><strong>漂亮的打印</strong></p>
<p>你可以在Python REPL漂亮的打印出列表和字典。这里是相关的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line">my_dict = &#123;<span class="string">'name'</span>: <span class="string">'Yasoob'</span>, <span class="string">'age'</span>: <span class="string">'undefined'</span>, <span class="string">'personality'</span>: <span class="string">'awesome'</span>&#125;</span><br><span class="line">pprint(my_dict)</span><br></pre></td></tr></table></figure>
<p>这种方法在字典上更为有效。此外，如果你想快速漂亮的从文件打印出json数据，那么你可以这么做：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">cat file.json | python -m json.tool</span><br></pre></td></tr></table></figure></p>
<p><strong>脚本性能分析</strong><br>这可能在定位你的脚本中的性能瓶颈时，会非常奏效：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">python -m cProfile my_script.py</span><br></pre></td></tr></table></figure>
<p>备注：<code>cProfile</code>是一个比<code>profile</code>更快的实现，因为它是用c写的</p>
<p><strong>CSV转换为json</strong></p>
<p>在命令行执行这条指令<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">python -c <span class="string">"import csv,json;print json.dumps(list(csv.reader(open('csv_file.csv'))))"</span></span><br></pre></td></tr></table></figure><br>确保更换<code>csv_file.csv</code>为你想要转换的csv文件</p>
<p><strong>列表辗平</strong></p>
<p>您可以通过使用<code>itertools</code>包中的<code>itertools.chain.from_iterable</code>轻松快速的辗平一个列表。下面是一个简单的例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a_list = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">print(list(itertools.chain.from_iterable(a_list)))</span><br><span class="line"><span class="comment"># Output: [1, 2, 3, 4, 5, 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">print(list(itertools.chain(*a_list)))</span><br><span class="line"><span class="comment"># Output: [1, 2, 3, 4, 5, 6]</span></span><br></pre></td></tr></table></figure></p>
<p><strong>一行式的构造器</strong></p>
<p>避免类初始化时大量重复的赋值语句<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, b, c, d, e, f)</span>:</span></span><br><span class="line">        self.__dict__.update(&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> locals().items() <span class="keyword">if</span> k != <span class="string">'self'</span>&#125;)</span><br></pre></td></tr></table></figure><br>更多的一行方法请参考<a href="https://wiki.python.org/moin/Powerful%20Python%20One-Liners" target="_blank" rel="noopener">Python官方文档</a>。</p>
<h1 id="For-Else"><a href="#For-Else" class="headerlink" title="For - Else"></a>For - Else</h1><p>循环是任何语言的一个必备要素。同样地，<code>for</code>循环就是Python的一个重要组成部分。然而还有一些东西是初学者并不知道的。我们将一个个讨论一下。</p>
<p>我们先从已经知道的开始。我们知道可以像这样使用<code>for</code>循环：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'mango'</span>]</span><br><span class="line"><span class="keyword">for</span> fruit <span class="keyword">in</span> fruits:</span><br><span class="line">    print(fruit.capitalize())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: Apple</span></span><br><span class="line"><span class="comment">#         Banana</span></span><br><span class="line"><span class="comment">#         Mango</span></span><br></pre></td></tr></table></figure>
<p>这是一个<code>for</code>循环非常基础的结构。现在我们继续看看，Python的<code>for</code>循环的一些鲜为人所知的特性。</p>
<h1 id="else从句"><a href="#else从句" class="headerlink" title="else从句"></a><code>else</code>从句</h1><figure class="highlight plain"><figcaption><span>一旦你掌握了何时何地使用它，它真的会非常有用。我自己对它真是相见恨晚。</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">有个常见的构造是跑一个循环，并查找一个元素。如果这个元素被找到了，我们使用&#96;&#96;&#96;break&#96;&#96;&#96;来中断这个循环。有两个场景会让循环停下来。</span><br><span class="line">- 第一个是当一个元素被找到，&#96;&#96;&#96;break&#96;&#96;&#96;被触发。</span><br><span class="line">- 第二个场景是循环结束。  </span><br><span class="line"></span><br><span class="line">现在我们也许想知道其中哪一个，才是导致循环完成的原因。一个方法是先设置一个标记，然后在循环结束时打上标记。另一个是使用&#96;&#96;&#96;else&#96;&#96;&#96;从句。</span><br><span class="line"></span><br><span class="line">这就是&#96;&#96;&#96;for&#x2F;else&#96;&#96;&#96;循环的基本结构：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">for item in container:</span><br><span class="line">    if search_something(item):</span><br><span class="line">        # Found it!</span><br><span class="line">        process(item)</span><br><span class="line">        break</span><br><span class="line">else:</span><br><span class="line">    # Didn&#39;t find anything..</span><br><span class="line">    not_found_in_container()</span><br></pre></td></tr></table></figure>
<p>考虑下这个简单的案例，它是我从官方文档里拿来的：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line">            print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n / x)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p>
<p>它会找出2到10之间的数字的因子。现在是趣味环节了。我们可以加上一个附加的else语句块，来抓住质数，并且告诉我们：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line">            print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n / x)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># loop fell through without finding a factor</span></span><br><span class="line">        print(n, <span class="string">'is a prime number'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="使用C扩展"><a href="#使用C扩展" class="headerlink" title="使用C扩展"></a>使用C扩展</h1><p>CPython还为开发者实现了一个有趣的特性，使用Python可以轻松调用C代码</p>
<p>开发者有三种方法可以在自己的Python代码中来调用C编写的函数-<code>ctypes</code>，<code>SWIG</code>，<code>Python/C API</code>。每种方式也都有各自的利弊。 </p>
<p>首先，我们要明确为什么要在Python中调用C？</p>
<p>常见原因如下：</p>
<ul>
<li>你要提升代码的运行速度，而且你知道C要比Python快50倍以上</li>
<li>C语言中有很多传统类库，而且有些正是你想要的，但你又不想用Python去重写它们</li>
<li>想对从内存到文件接口这样的底层资源进行访问</li>
<li>不需要理由，就是想这样做</li>
</ul>
<h1 id="CTypes"><a href="#CTypes" class="headerlink" title="CTypes"></a>CTypes</h1><p>Python中的<a href="https://docs.python.org/2/library/ctypes.html" target="_blank" rel="noopener">ctypes模块</a>可能是Python调用C方法中最简单的一种。ctypes模块提供了和C语言兼容的数据类型和函数来加载dll文件，因此在调用时不需对源文件做任何的修改。也正是如此奠定了这种方法的简单性。</p>
<p>示例如下</p>
<p>实现两数求和的C代码，保存为<code>add.c</code><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//sample C file to add 2 numbers - int and floats</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span>, <span class="keyword">float</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span> num1, <span class="keyword">float</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>接下来将C文件编译为<code>.so</code>文件(windows下为DLL)。下面操作会生成adder.so文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">For Linux</span></span><br><span class="line"><span class="meta">$</span><span class="bash">  gcc -shared -Wl,-soname,adder -o adder.so -fPIC add.c</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">For Mac</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gcc -shared -Wl,-install_name,adder.so -o adder.so -fPIC add.c</span></span><br></pre></td></tr></table></figure></p>
<p>现在在你的Python代码中来调用它<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">#load the shared object file</span></span><br><span class="line">adder = CDLL(<span class="string">'./adder.so'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find sum of integers</span></span><br><span class="line">res_int = adder.add_int(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of 4 and 5 = "</span> + str(res_int)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find sum of floats</span></span><br><span class="line">a = c_float(<span class="number">5.5</span>)</span><br><span class="line">b = c_float(<span class="number">4.1</span>)</span><br><span class="line"></span><br><span class="line">add_float = adder.add_float</span><br><span class="line">add_float.restype = c_float</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of 5.5 and 4.1 = "</span>, str(add_float(a, b))</span><br></pre></td></tr></table></figure></p>
<p>输出如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sum of 4 and 5 &#x3D; 9</span><br><span class="line">Sum of 5.5 and 4.1 &#x3D;  9.60000038147</span><br></pre></td></tr></table></figure></p>
<p>在这个例子中，C文件是自解释的，它包含两个函数，分别实现了整形求和和浮点型求和。</p>
<p>在Python文件中，一开始先导入ctypes模块，然后使用CDLL函数来加载我们创建的库文件。这样我们就可以通过变量<code>adder</code>来使用C类库中的函数了。当<code>adder.add_int()</code>被调用时，内部将发起一个对C函数<code>add_int</code>的调用。ctypes接口允许我们在调用C函数时使用原生Python中默认的字符串型和整型。</p>
<p>而对于其他类似布尔型和浮点型这样的类型，必须要使用正确的ctype类型才可以。如向<code>adder.add_float()</code>函数传参时, 我们要先将Python中的十进制值转化为c_float类型，然后才能传送给C函数。这种方法虽然简单，清晰，但是却很受限。例如，并不能在C中对对象进行操作。</p>
<h1 id="SWIG"><a href="#SWIG" class="headerlink" title="SWIG"></a>SWIG</h1><p>SWIG是Simplified Wrapper and Interface Generator的缩写。是Python中调用C代码的另一种方法。在这个方法中，开发人员必须编写一个额外的接口文件来作为SWIG(终端工具)的入口。</p>
<p>Python开发者一般不会采用这种方法，因为大多数情况它会带来不必要的复杂。而当你有一个C/C++代码库需要被多种语言调用时，这将是个非常不错的选择。</p>
<p>示例如下(来自<a href="http://www.swig.org/tutorial.html" target="_blank" rel="noopener">SWIG官网</a>)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#96;&#96;&#96;C</span><br><span class="line">#include &lt;time.h&gt;</span><br><span class="line">double My_variable &#x3D; 3.0;</span><br><span class="line"></span><br><span class="line">int fact(int n) &#123;</span><br><span class="line">    if (n &lt;&#x3D; 1) return 1;</span><br><span class="line">    else return n*fact(n-1);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int my_mod(int x, int y) &#123;</span><br><span class="line">    return (x%y);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">char *get_time()</span><br><span class="line">&#123;</span><br><span class="line">    time_t ltime;</span><br><span class="line">    time(&amp;ltime);</span><br><span class="line">    return ctime(&amp;ltime);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编译它<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unix % swig -python example.i</span><br><span class="line">unix % gcc -c example.c example_wrap.c \</span><br><span class="line">    -I/usr/local/include/python2.1</span><br><span class="line">unix % ld -shared example.o example_wrap.o -o _example.so</span><br></pre></td></tr></table></figure></p>
<p>最后，Python的输出<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> example</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.fact(<span class="number">5</span>)</span><br><span class="line"><span class="number">120</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.my_mod(<span class="number">7</span>,<span class="number">3</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.get_time()</span><br><span class="line"><span class="string">'Sun Feb 11 23:01:07 1996'</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到，使用SWIG确实达到了同样的效果，虽然下了更多的工夫，但如果你的目标是多语言还是很值得的。</p>
<h1 id="Python-C-API"><a href="#Python-C-API" class="headerlink" title="Python/C API"></a>Python/C API</h1><p><a href="https://docs.python.org/2/c-api/" target="_blank" rel="noopener">Python/C API</a>可能是被最广泛使用的方法。它不仅简单，而且可以在C代码中操作你的Python对象。</p>
<p>这种方法需要以特定的方式来编写C代码以供Python去调用它。所有的Python对象都被表示为一种叫做PyObject的结构体，并且<code>Python.h</code>头文件中提供了各种操作它的函数。例如，如果PyObject表示为PyListType(列表类型)时，那么我们便可以使用<code>PyList_Size()</code>函数来获取该结构的长度，类似Python中的<code>len(list)</code>函数。大部分对Python原生对象的基础函数和操作在<code>Python.h</code>头文件中都能找到。</p>
<p>示例</p>
<p>编写一个C扩展，添加所有元素到一个Python列表(所有元素都是数字)</p>
<p>来看一下我们要实现的效果，这里演示了用Python调用C扩展的代码<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Though it looks like an ordinary python import, the addList module is implemented in C</span></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l))</span><br></pre></td></tr></table></figure></p>
<p>上面的代码和普通的Python文件并没有什么分别，导入并使用了另一个叫做<code>addList</code>的Python模块。唯一差别就是这个模块并不是用Python编写的，而是C。</p>
<p>接下来我们看看如何用C编写<code>addList</code>模块，这可能看起来有点让人难以接受，但是一旦你了解了这之中的各种组成，你就可以一往无前了。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Python.h has all the required function definitions to manipulate the Python objects</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Python.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//This is the function that is called from your python code</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> PyObject* <span class="title">addList_add</span><span class="params">(PyObject* self, PyObject* args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    PyObject * listObj;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//The input arguments come as a tuple, we parse the args to get the various variables</span></span><br><span class="line">    <span class="comment">//In this case it's only one list variable, which will now be referenced by listObj</span></span><br><span class="line">    <span class="keyword">if</span> (! PyArg_ParseTuple( args, <span class="string">"O"</span>, &amp;listObj ))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//length of the list</span></span><br><span class="line">    <span class="keyword">long</span> length = PyList_Size(listObj);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//iterate over all the elements</span></span><br><span class="line">    <span class="keyword">int</span> i, sum =<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">        <span class="comment">//get an element out of the list - the element is also a python objects</span></span><br><span class="line">        PyObject* temp = PyList_GetItem(listObj, i);</span><br><span class="line">        <span class="comment">//we know that object represents an integer - so convert it into C long</span></span><br><span class="line">        <span class="keyword">long</span> elem = PyInt_AsLong(temp);</span><br><span class="line">        sum += elem;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//value returned back to python code - another python object</span></span><br><span class="line">    <span class="comment">//build value here converts the C long to a python integer</span></span><br><span class="line">    <span class="keyword">return</span> Py_BuildValue(<span class="string">"i"</span>, sum);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//This is the docstring that corresponds to our 'add' function.</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> addList_docs[] =</span><br><span class="line"><span class="string">"add(  ): add all elements of the list\n"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* This table contains the relavent info mapping -</span></span><br><span class="line"><span class="comment">   &lt;function-name in python module&gt;, &lt;actual-function&gt;,</span></span><br><span class="line"><span class="comment">   &lt;type-of-args the function expects&gt;, &lt;docstring associated with the function&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> PyMethodDef addList_funcs[] = &#123;</span><br><span class="line">    &#123;<span class="string">"add"</span>, (PyCFunction)addList_add, METH_VARARGS, addList_docs&#125;,</span><br><span class="line">    &#123;<span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>&#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   addList is the module name, and this is the initialization block of the module.</span></span><br><span class="line"><span class="comment">   &lt;desired module name&gt;, &lt;the-info-table&gt;, &lt;module's-docstring&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">initaddList</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    Py_InitModule3(<span class="string">"addList"</span>, addList_funcs,</span><br><span class="line">            <span class="string">"Add all ze lists"</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>逐步解释</p>
<ul>
<li><code>Python.h</code>头文件中包含了所有需要的类型(Python对象类型的表示)和函数定义(对Python对象的操作)</li>
<li>接下来我们编写将要在Python调用的函数, 函数传统的命名方式由{模块名}_{函数名}组成，所以我们将其命名为<code>addList_add</code>   </li>
<li>然后填写想在模块内实现函数的相关信息表，每行一个函数，以空行作为结束</li>
<li>最后的模块初始化块签名为<code>PyMODINIT_FUNC init{模块名}</code>。</li>
</ul>
<p>函数<code>addList_add</code>接受的参数类型为PyObject类型结构(同时也表示为元组类型，因为Python中万物皆为对象，所以我们先用PyObject来定义)。传入的参数则通过<code>PyArg_ParseTuple()</code>来解析。第一个参数是被解析的参数变量。第二个参数是一个字符串，告诉我们如何去解析元组中每一个元素。字符串的第n个字母正是代表着元组中第n个参数的类型。例如，”i”代表整形，”s”代表字符串类型, “O”则代表一个Python对象。接下来的参数都是你想要通过<code>PyArg_ParseTuple()</code>函数解析并保存的元素。这样参数的数量和模块中函数期待得到的参数数量就可以保持一致，并保证了位置的完整性。例如，我们想传入一个字符串，一个整数和一个Python列表，可以这样去写<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">char</span> *s;</span><br><span class="line">PyObject* <span class="built_in">list</span>;</span><br><span class="line">PyArg_ParseTuple(args, <span class="string">"siO"</span>, &amp;n, &amp;s, &amp;<span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>在这种情况下，我们只需要提取一个列表对象，并将它存储在<code>listObj</code>变量中。然后用列表对象中的<code>PyList_Size()</code>函数来获取它的长度。就像Python中调用<code>len(list)</code>。</p>
<p>现在我们通过循环列表，使用<code>PyList_GetItem(list, index)</code>函数来获取每个元素。这将返回一个<code>PyObject*</code>对象。既然Python对象也能表示<code>PyIntType</code>，我们只要使用<code>PyInt_AsLong(PyObj *)</code>函数便可获得我们所需要的值。我们对每个元素都这样处理，最后再得到它们的总和。</p>
<p>总和将被转化为一个Python对象并通过<code>Py_BuildValue()</code>返回给Python代码，这里的i表示我们要返回一个Python整形对象。</p>
<p>现在我们已经编写完C模块了。将下列代码保存为<code>setup.py</code><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#build the modules</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">'addList'</span>, version=<span class="string">'1.0'</span>,  \</span><br><span class="line">      ext_modules=[Extension(<span class="string">'addList'</span>, [<span class="string">'adder.c'</span>])])</span><br></pre></td></tr></table></figure></p>
<p>并且运行<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p>
<p>现在应该已经将我们的C文件编译安装到我们的Python模块中了。</p>
<p>在一番辛苦后，让我们来验证下我们的模块是否有效<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#module that talks to the C code</span></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l))</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sum of List - [1, 2, 3, 4, 5] &#x3D; 15</span><br></pre></td></tr></table></figure></p>
<p>如你所见，我们已经使用Python.h API成功开发出了我们第一个Python C扩展。这种方法看似复杂，但你一旦习惯，它将变的非常有效。</p>
<p>Python调用C代码的另一种方式便是使用<a href="http://cython.org/" target="_blank" rel="noopener">Cython</a>让Python编译的更快。但是Cython和传统的Python比起来可以将它理解为另一种语言，所以我们就不在这里过多描述了。</p>
<h1 id="open函数"><a href="#open函数" class="headerlink" title="open函数"></a><code>open</code>函数</h1><p><a href="http://docs.python.org/dev/library/functions.html#open" target="_blank" rel="noopener">open</a> 函数可以打开一个文件。超级简单吧？大多数时候，我们看到它这样被使用：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = open(<span class="string">'photo.jpg'</span>, <span class="string">'r+'</span>)</span><br><span class="line">jpgdata = f.read()</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure></p>
<p>我现在写这篇文章的原因，是大部分时间我看到<code>open</code>被这样使用。有<strong>三个</strong>错误存在于上面的代码中。你能把它们全指出来吗？如不能，请读下去。在这篇文章的结尾，你会知道上面的代码错在哪里，而且，更重要的是，你能在自己的代码里避免这些错误。现在我们从基础开始：</p>
<p><code>open</code>的返回值是一个文件句柄，从操作系统托付给你的Python程序。一旦你处理完文件，你会想要归还这个文件句柄，只有这样你的程序不会超出一次能打开的文件句柄的数量上限。</p>
<p>显式地调用<code>close</code>关闭了这个文件句柄，但前提是只有在read成功的情况下。如果有任意异常正好在<code>f = open(...)</code>之后产生，<code>f.close()</code>将不会被调用（取决于Python解释器的做法，文件句柄可能还是会被归还，但那是另外的话题了）。为了确保不管异常是否触发，文件都能关闭，我们将其包裹成一个<code>with</code>语句:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'photo.jpg'</span>, <span class="string">'r+'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    jpgdata = f.read()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><figcaption><span>打开模式)决定了这个文件如何被打开。</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 如果你想读取文件，传入&#96;&#96;&#96;r</span><br></pre></td></tr></table></figure>
<ul>
<li>如果你想读取并写入文件，传入<code>r+</code></li>
<li>如果你想覆盖写入文件，传入<code>w</code></li>
<li>如果你想在文件末尾附加内容，传入<code>a</code></li>
</ul>
<p>虽然有若干个其他的有效的<code>mode</code>字符串，但有可能你将永远不会使用它们。<code>mode</code>很重要，不仅因为它改变了行为，而且它可能导致权限错误。举个例子，我们要是在一个写保护的目录里打开一个jpg文件， <code>open(.., &#39;r+&#39;)</code>就失败了。<code>mode</code>可能包含一个扩展字符；让我们还可以以二进制方式打开文件(你将得到字节串)或者文本模式(字符串)</p>
<p>一般来说，如果文件格式是由人写的，那么它更可能是文本模式。jpg图像文件一般不是人写的（而且其实不是人直接可读的），因此你应该以二进制模式来打开它们，方法是在<code>mode</code>字符串后加一个<code>b</code>(你可以看看开头的例子里，正确的方式应该是<code>rb</code>)。<br>如果你以文本模式打开一些东西（比如，加一个<code>t</code>,或者就用<code>r/r+/w/a</code>），你还必须知道要使用哪种编码。对于计算机来说，所有的文件都是字节，而不是字符。</p>
<p>可惜，在Pyhon 2.x版本里，<code>open</code>不支持显示地指定编码。然而，<a href="http://docs.python.org/2/library/io.html#io.open" target="_blank" rel="noopener">io.open</a>函数在Python 2.x中和3.x(其中它是<code>open</code>的别名)中都有提供，它能做正确的事。你可以传入<code>encoding</code>这个关键字参数来传入编码。<br>如果你不传入任意编码，一个系统 - 以及Python -指定的默认选项将被选中。你也许被诱惑去依赖这个默认选项，但这个默认选项经常是错误的，或者默认编码实际上不能表达文件里的所有字符（这将经常发生在Python 2.x和/或Windows）。<br>所以去挑选一个编码吧。<code>utf-8</code>是一个非常好的编码。当你写入一个文件，你可以选一个你喜欢的编码（或者最终读你文件的程序所喜欢的编码）。</p>
<p>那你怎么找出正在读的文件是用哪种编码写的呢？好吧，不幸的是，并没有一个十分简单的方式来检测编码。在不同的编码中，同样的字节可以表示不同，但同样有效的字符。因此，你必须依赖一个元数据（比如，在HTTP头信息里）来找出编码。越来越多的是，文件格式将编码定义成<code>UTF-8</code>。</p>
<p>有了这些基础知识，我们来写一个程序，读取一个文件，检测它是否是JPG（提示：这些文件头部以字节<code>FF D8</code>开始），把对输入文件的描述写入一个文本文件。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'photo.jpg'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> inf:</span><br><span class="line">    jpgdata = inf.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> jpgdata.startswith(<span class="string">b'\xff\xd8'</span>):</span><br><span class="line">    text = <span class="string">u'This is a JPEG file (%d bytes long)\n'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    text = <span class="string">u'This is a random file (%d bytes long)\n'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> io.open(<span class="string">'summary.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> outf:</span><br><span class="line">    outf.write(text % len(jpgdata))</span><br></pre></td></tr></table></figure><br>我敢肯定，现在你会正确地使用<code>open</code>啦！</p>
<h1 id="22-目标Python2-3"><a href="#22-目标Python2-3" class="headerlink" title="22. 目标Python2+3"></a>22. 目标Python2+3</h1><p>很多时候你可能希望你开发的程序能够同时兼容Python2+和Python3+。</p>
<p>试想你有一个非常出名的Python模块被很多开发者使用着，但并不是所有人都只使用Python2或者Python3。这时候你有两个办法。第一个办法是开发两个模块，针对Python2一个，针对Python3一个。还有一个办法就是调整你现在的代码使其同时兼容Python2和Python3。</p>
<p>本节中，我将介绍一些技巧，让你的脚本同时兼容Python2和Python3。</p>
<p><strong>Future模块导入</strong></p>
<p>第一种也是最重要的方法，就是导入<code>__future__</code>模块。它可以帮你在Python2中导入Python3的功能。这有一组例子：</p>
<p>上下文管理器是Python2.6+引入的新特性，如果你想在Python2.5中使用它可以这样做：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> with_statement</span><br></pre></td></tr></table></figure></p>
<p>在Python3中<code>print</code>已经变为一个函数。如果你想在Python2中使用它可以通过<code>__future__</code>导入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">print</span></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line">print(<span class="keyword">print</span>)</span><br><span class="line"><span class="comment"># Output: &lt;built-in function print&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>模块重命名</strong></p>
<p>首先，告诉我你是如何在你的脚本中导入模块的。大多时候我们会这样做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> foo </span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="keyword">from</span> foo <span class="keyword">import</span> bar</span><br></pre></td></tr></table></figure>
<p>你知道么，其实你也可以这样做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> foo <span class="keyword">as</span> foo</span><br></pre></td></tr></table></figure>
<p>这样做可以起到和上面代码同样的功能，但最重要的是它能让你的脚本同时兼容Python2和Python3。现在我们来看下面的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> urllib.request <span class="keyword">as</span> urllib_request  <span class="comment"># for Python 3</span></span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">import</span> urllib2 <span class="keyword">as</span> urllib_request  <span class="comment"># for Python 2</span></span><br></pre></td></tr></table></figure>
<p>让我来稍微解释一下上面的代码。<br>我们将模块导入代码包装在<code>try/except</code>语句中。我们是这样做是因为在Python 2中并没有<code>urllib.request</code>模块。这将引起一个<code>ImportError</code>异常。而在Python2中<code>urllib.request</code>的功能则是由<code>urllib2</code>提供的。所以,当我们试图在Python2中导入<code>urllib.request</code>模块的时候，一旦我们捕获到<code>ImportError</code>我们将通过导入<code>urllib2</code>模块来代替它。</p>
<p>最后，你要了解<code>as</code>关键字的作用。它将导入的模块映射到<code>urllib.request</code>，所以我们通过<code>urllib_request</code>这个别名就可以使用<code>urllib2</code>中的所有类和方法了。</p>
<p><strong>过期的Python2内置功能</strong></p>
<p>另一个需要了解的事情就是Python2中有12个内置功能在Python3中已经被移除了。要确保在Python2代码中不要出现这些功能来保证对Python3的兼容。这有一个强制让你放弃12内置功能的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> future.builtins.disabled <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<p>现在，只要你尝试在Python3中使用这些被遗弃的模块时，就会抛出一个<code>NameError</code>异常如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> future.builtins.disabled <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">apply()</span><br><span class="line"><span class="comment"># Output: NameError: obsolete Python 2 builtin apply is disabled</span></span><br></pre></td></tr></table></figure>
<p><strong>标准库向下兼容的外部支持</strong></p>
<p>有一些包在非官方的支持下为Python2提供了Python3的功能。例如，我们有：</p>
<ul>
<li>enum <code>pip install enum34</code></li>
<li>singledispatch <code>pip install singledispatch</code></li>
<li>pathlib <code>pip install pathlib</code></li>
</ul>
<p>想更多了解，在Python文档中有一个<a href="https://docs.python.org/3/howto/pyporting.html" target="_blank" rel="noopener">全面的指南</a>可以帮助你让你的代码同时兼容Python2和Python3。</p>
<h1 id="23-协程"><a href="#23-协程" class="headerlink" title="23. 协程"></a>23. 协程</h1><p>Python中的协程和生成器很相似但又稍有不同。主要区别在于：</p>
<ul>
<li>生成器是数据的生产者</li>
<li>协程则是数据的消费者</li>
</ul>
<p>首先我们先来回顾下生成器的创建过程。我们可以这样去创建一个生成器:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">()</span>:</span></span><br><span class="line">    a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line">        a, b = b, a+b</span><br></pre></td></tr></table></figure>
<p>然后我们经常在<code>for</code>循环中这样使用它:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> fib():</span><br><span class="line">    <span class="keyword">print</span> i</span><br></pre></td></tr></table></figure>
<p>这样做不仅快而且不会给内存带来压力，因为我们所需要的值都是动态生成的而不是将他们存储在一个列表中。更概括的说如果现在我们在上面的例子中使用<code>yield</code>便可获得了一个协程。协程会消费掉发送给它的值。Python实现的<code>grep</code>就是个很好的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grep</span><span class="params">(pattern)</span>:</span></span><br><span class="line">    print(<span class="string">"Searching for"</span>, pattern)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = (<span class="keyword">yield</span>)</span><br><span class="line">        <span class="keyword">if</span> pattern <span class="keyword">in</span> line:</span><br><span class="line">            print(line)</span><br></pre></td></tr></table></figure>
<p>等等！<code>yield</code>返回了什么？啊哈，我们已经把它变成了一个协程。它将不再包含任何初始值，相反要从外部传值给它。我们可以通过<code>send()</code>方法向它传值。这有个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search = grep(<span class="string">'coroutine'</span>)</span><br><span class="line">next(search)</span><br><span class="line"><span class="comment">#output: Searching for coroutine</span></span><br><span class="line">search.send(<span class="string">"I love you"</span>)</span><br><span class="line">search.send(<span class="string">"Don't you love me?"</span>)</span><br><span class="line">search.send(<span class="string">"I love coroutine instead!"</span>)</span><br><span class="line"><span class="comment">#output: I love coroutine instead!</span></span><br></pre></td></tr></table></figure>
<p>发送的值会被<code>yield</code>接收。我们为什么要运行<code>next()</code>方法呢？这样做正是为了启动一个协程。就像协程中包含的生成器并不是立刻执行，而是通过<code>next()</code>方法来响应<code>send()</code>方法。因此，你必须通过<code>next()</code>方法来执行<code>yield</code>表达式。</p>
<p>我们可以通过调用<code>close()</code>方法来关闭一个协程。像这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search = grep(<span class="string">'coroutine'</span>)</span><br><span class="line">search.close()</span><br></pre></td></tr></table></figure>
<p>更多协程相关知识的学习大家可以参考David Beazley的这份<a href="http://www.dabeaz.com/coroutines/Coroutines.pdf" target="_blank" rel="noopener">精彩演讲</a>。</p>
<h1 id="函数缓存-Function-caching"><a href="#函数缓存-Function-caching" class="headerlink" title="函数缓存 (Function caching)"></a>函数缓存 (Function caching)</h1><p>函数缓存允许我们将一个函数对于给定参数的返回值缓存起来。<br>当一个I/O密集的函数被频繁使用相同的参数调用的时候，函数缓存可以节约时间。<br>在Python 3.2版本以前我们只有写一个自定义的实现。在Python 3.2以后版本，有个<code>lru_cache</code>的装饰器，允许我们将一个函数的返回值快速地缓存或取消缓存。</p>
<p>我们来看看，Python 3.2前后的版本分别如何使用它。</p>
<h1 id="Python-3-2及以后版本"><a href="#Python-3-2及以后版本" class="headerlink" title="Python 3.2及以后版本"></a>Python 3.2及以后版本</h1><p>我们来实现一个斐波那契计算器，并使用<code>lru_cache</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache(maxsize=32)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fib(n<span class="number">-1</span>) + fib(n<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print([fib(n) <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line"><span class="comment"># Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</span></span><br></pre></td></tr></table></figure>
<p>那个<code>maxsize</code>参数是告诉<code>lru_cache</code>，最多缓存最近多少个返回值。</p>
<p>我们也可以轻松地对返回值清空缓存，通过这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fib.cache_clear()</span><br></pre></td></tr></table></figure>
<h1 id="Python-2系列版本"><a href="#Python-2系列版本" class="headerlink" title="Python 2系列版本"></a>Python 2系列版本</h1><p>你可以创建任意种类的缓存机制，有若干种方式来达到相同的效果，这完全取决于你的需要。<br>这里是一个一般的缓存：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">memoize</span><span class="params">(function)</span>:</span></span><br><span class="line">    memo = &#123;&#125;</span><br><span class="line"><span class="meta">    @wraps(function)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> args <span class="keyword">in</span> memo:</span><br><span class="line">            <span class="keyword">return</span> memo[args]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rv = function(*args)</span><br><span class="line">            memo[args] = rv</span><br><span class="line">            <span class="keyword">return</span> rv</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@memoize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>: <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fibonacci(n - <span class="number">1</span>) + fibonacci(n - <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fibonacci(<span class="number">25</span>)</span><br></pre></td></tr></table></figure><br>这里有一篇<a href="https://www.caktusgroup.com/blog/2015/06/08/testing-client-side-applications-django-post-mortem/" target="_blank" rel="noopener">Caktus Group的不错的文章</a>，在其中他们发现一个Django框架的由lru_cache导致的bug。读起来很有意思。一定要打开去看一下。</p>
<h1 id="上下文管理器-Context-managers"><a href="#上下文管理器-Context-managers" class="headerlink" title="上下文管理器(Context managers)"></a>上下文管理器(Context managers)</h1><p>上下文管理器允许你在有需要的时候，精确地分配和释放资源。  </p>
<p>使用上下文管理器最广泛的案例就是<code>with</code>语句了。<br>想象下你有两个需要结对执行的相关操作，然后还要在它们中间放置一段代码。<br>上下文管理器就是专门让你做这种事情的。举个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'some_file'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.write(<span class="string">'Hola!'</span>)</span><br></pre></td></tr></table></figure>
<p>上面这段代码打开了一个文件，往里面写入了一些数据，然后关闭该文件。如果在往文件写数据时发生异常，它也会尝试去关闭文件。上面那段代码与这一段是等价的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file = open(<span class="string">'some_file'</span>, <span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file.write(<span class="string">'Hola!'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    file.close()</span><br></pre></td></tr></table></figure>
<p>当与第一个例子对比时，我们可以看到，通过使用<code>with</code>，许多样板代码(boilerplate code)被消掉了。 这就是<code>with</code>语句的主要优势，它确保我们的文件会被关闭，而不用关注嵌套代码如何退出。</p>
<p>上下文管理器的一个常见用例，是资源的加锁和解锁，以及关闭已打开的文件（就像我已经展示给你看的）。</p>
<p>让我们看看如何来实现我们自己的上下文管理器。这会让我们更完全地理解在这些场景背后都发生着什么。</p>
<h1 id="基于类的实现"><a href="#基于类的实现" class="headerlink" title="基于类的实现"></a>基于类的实现</h1><p>一个上下文管理器的类，最起码要定义<code>__enter__</code>和<code>__exit__</code>方法。<br>让我们来构造我们自己的开启文件的上下文管理器，并学习下基础知识。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">File</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_name, method)</span>:</span></span><br><span class="line">        self.file_obj = open(file_name, method)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.file_obj</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, type, value, traceback)</span>:</span></span><br><span class="line">        self.file_obj.close()</span><br></pre></td></tr></table></figure>
<p>通过定义<code>__enter__</code>和<code>__exit__</code>方法，我们可以在<code>with</code>语句里使用它。我们来试试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> File(<span class="string">'demo.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.write(<span class="string">'Hola!'</span>)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">我们的```__exit__```函数接受三个参数。这些参数对于每个上下文管理器类中的```__exit__```方法都是必须的。我们来谈谈在底层都发生了什么。</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> ```<span class="keyword">with</span>```语句先暂存了```File```类的```__exit__```方法</span><br><span class="line"><span class="number">2.</span> 然后它调用```File```类的```__enter__```方法</span><br><span class="line"><span class="number">3.</span> ```__enter__```方法打开文件并返回给```<span class="keyword">with</span>```语句</span><br><span class="line"><span class="number">4.</span> 打开的文件句柄被传递给```opened_file```参数</span><br><span class="line"><span class="number">5.</span> 我们使用```.write()```来写文件</span><br><span class="line"><span class="number">6.</span> ```<span class="keyword">with</span>```语句调用之前暂存的```__exit__```方法</span><br><span class="line"><span class="number">7.</span> ```__exit__```方法关闭了文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理异常</span></span><br><span class="line"></span><br><span class="line">我们还没有谈到```__exit__```方法的这三个参数：```type```, ```value```和```traceback```。  </span><br><span class="line">在第<span class="number">4</span>步和第<span class="number">6</span>步之间，如果发生异常，Python会将异常的```type```,```value```和```traceback```传递给```__exit__```方法。  </span><br><span class="line">它让```__exit__```方法来决定如何关闭文件以及是否需要其他步骤。在我们的案例中，我们并没有注意它们。</span><br><span class="line"></span><br><span class="line">那如果我们的文件对象抛出一个异常呢？万一我们尝试访问文件对象的一个不支持的方法。举个例子：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">with</span> File(<span class="string">'demo.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.undefined_function(<span class="string">'Hola!'</span>)</span><br></pre></td></tr></table></figure>
<p>我们来列一下，当异常发生时，<code>with</code>语句会采取哪些步骤。</p>
<ol>
<li>它把异常的<code>type</code>,<code>value</code>和<code>traceback</code>传递给<code>__exit__</code>方法</li>
<li>它让<code>__exit__</code>方法来处理异常</li>
<li>如果<code>__exit__</code>返回的是True，那么这个异常就被优雅地处理了。</li>
<li>如果<code>__exit__</code>返回的是True以外的任何东西，那么这个异常将被<code>with</code>语句抛出。</li>
</ol>
<p>在我们的案例中，<code>__exit__</code>方法返回的是<code>None</code>(如果没有<code>return</code>语句那么方法会返回<code>None</code>)。因此，<code>with</code>语句抛出了那个异常。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">2</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AttributeError: <span class="string">'file'</span> object has no attribute <span class="string">'undefined_function'</span></span><br></pre></td></tr></table></figure>
<p>我们尝试下在<code>__exit__</code>方法中处理异常：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">File</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_name, method)</span>:</span></span><br><span class="line">        self.file_obj = open(file_name, method)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.file_obj</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, type, value, traceback)</span>:</span></span><br><span class="line">        print(<span class="string">"Exception has been handled"</span>)</span><br><span class="line">        self.file_obj.close()</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> File(<span class="string">'demo.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.undefined_function()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: Exception has been handled</span></span><br></pre></td></tr></table></figure></p>
<p>我们的<code>__exit__</code>方法返回了<code>True</code>,因此没有异常会被<code>with</code>语句抛出。</p>
<p>这还不是实现上下文管理器的唯一方式。还有一种方式，我们会在下一节中一起看看。</p>
<h1 id="基于生成器的实现"><a href="#基于生成器的实现" class="headerlink" title="基于生成器的实现"></a>基于生成器的实现</h1><p>我们还可以用装饰器(decorators)和生成器(generators)来实现上下文管理器。<br>Python有个<code>contextlib</code>模块专门用于这个目的。我们可以使用一个生成器函数来实现一个上下文管理器，而不是使用一个类。<br>让我们看看一个基本的，没用的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</span><br><span class="line"></span><br><span class="line"><span class="meta">@contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_file</span><span class="params">(name)</span>:</span></span><br><span class="line">    f = open(name, <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">yield</span> f</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<p>OK啦！这个实现方式看起来更加直观和简单。然而，这个方法需要关于生成器、<code>yield</code>和装饰器的一些知识。在这个例子中我们还没有捕捉可能产生的任何异常。它的工作方式和之前的方法大致相同。</p>
<p>让我们小小地剖析下这个方法。</p>
<ol>
<li>Python解释器遇到了<code>yield</code>关键字。因为这个缘故它创建了一个生成器而不是一个普通的函数。</li>
<li>因为这个装饰器，<code>contextmanager</code>会被调用并传入函数名（<code>open_file</code>）作为参数。</li>
<li><code>contextmanager</code>函数返回一个以<code>GeneratorContextManager</code>对象封装过的生成器。</li>
<li>这个<code>GeneratorContextManager</code>被赋值给<code>open_file</code>函数，我们实际上是在调用<code>GeneratorContextManager</code>对象。</li>
</ol>
<p>那现在我们既然知道了所有这些，我们可以用这个新生成的上下文管理器了，像这样：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open_file(<span class="string">'some_file'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'hola!'</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><h2 id="本书推送贴的留言和讨论"><a href="#本书推送贴的留言和讨论" class="headerlink" title="本书推送贴的留言和讨论"></a>本书推送贴的留言和讨论</h2><ul>
<li>v2ex: <a href="http://www.v2ex.com/t/267557" target="_blank" rel="noopener">http://www.v2ex.com/t/267557</a></li>
<li>微博长文: <a href="http://weibo.com/1054764633/DoN6Z5Haq?type=repost" target="_blank" rel="noopener">http://weibo.com/1054764633/DoN6Z5Haq?type=repost</a></li>
</ul>
<h2 id="v2ex网友florije推荐"><a href="#v2ex网友florije推荐" class="headerlink" title="v2ex网友florije推荐"></a>v2ex网友florije推荐</h2><ul>
<li>另外一本同名IntermediatePython的更新的书  <a href="https://leanpub.com/intermediatepython" target="_blank" rel="noopener">https://leanpub.com/intermediatepython</a></li>
</ul>
<h2 id="v2ex网友xiaket推荐"><a href="#v2ex网友xiaket推荐" class="headerlink" title="v2ex网友xiaket推荐"></a>v2ex网友xiaket推荐</h2><ul>
<li>对于Python提高类的书，推荐Fluent Python 或 Pro Python</li>
</ul>
<h2 id="v2ex网友shishen10-推荐"><a href="#v2ex网友shishen10-推荐" class="headerlink" title="v2ex网友shishen10 推荐"></a>v2ex网友shishen10 推荐</h2><ul>
<li>老齐的教程 <a href="https://github.com/qiwsir/StarterLearningPython" target="_blank" rel="noopener">https://github.com/qiwsir/StarterLearningPython</a></li>
<li>老齐还整理了很多精华 <a href="https://github.com/qiwsir/ITArticles" target="_blank" rel="noopener">https://github.com/qiwsir/ITArticles</a></li>
</ul>
<h2 id="v2ex网友xiaowangge推荐"><a href="#v2ex网友xiaowangge推荐" class="headerlink" title="v2ex网友xiaowangge推荐"></a>v2ex网友xiaowangge推荐</h2><p><a href="https://github.com/Yixiaohan" target="_blank" rel="noopener">Yixiaohan</a>整理了一个不错的推荐：<a href="https://github.com/Yixiaohan/codeparkshare" target="_blank" rel="noopener">Python初学者（零基础学习Python、Python入门）书籍、视频、资料、社区推荐</a>大家可以前去Fork。</p>
<h2 id="v2ex推荐学习书目"><a href="#v2ex推荐学习书目" class="headerlink" title="v2ex推荐学习书目"></a>v2ex推荐学习书目</h2><ul>
<li><a href="https://flyouting.gitbooks.io/learn-python-the-hard-way-cn/content/" target="_blank" rel="noopener">Learn Python the Hard Way</a></li>
<li><a href="https://www.gitbook.com/book/yulongjun/learning-python-in-chinese/details" target="_blank" rel="noopener">Python 学习手册-第五版中文版</a> </li>
<li><a href="http://python3-cookbook.readthedocs.org/zh_CN/latest/" target="_blank" rel="noopener">Python Cookbook</a></li>
<li><a href="https://book.douban.com/subject/4866934/" target="_blank" rel="noopener">Python 基础教程</a></li>
</ul>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>python书籍</tag>
      </tags>
  </entry>
  <entry>
    <title>学习观</title>
    <url>/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/</url>
    <content><![CDATA[<p>看了<a href="https://zhuanlan.zhihu.com/p/58768600" target="_blank" rel="noopener">Yjango</a>的学习观系列感触很深</p>
<p><a href="https://www.zhihu.com/people/YJango" target="_blank" rel="noopener">YJango 知乎</a><br><a href="https://space.bilibili.com/344849038/video" target="_blank" rel="noopener">YJango bilibili</a><br><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxNDI5MDk0MQ==&amp;action=getalbum&amp;album_id=1342746447506374657&amp;subscene=159&amp;subscene=21&amp;scenenote=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzIxNDI5MDk0MQ%3D%3D%26mid%3D2247483892%26idx%3D1%26sn%3Dbd451e1b54871bf10e45b79fcea930ef%26chksm%3D97a89966a0df107078f7602f9e4b5fe5f39b92dfecd1fa6a4fd6cff4fc7768f173b9125c9ab3%26scene%3D21%23wechat_redirect#wechat_redirect" target="_blank" rel="noopener">YJango 微信公共号</a><br><a id="more"></a><br>原文整理如下:</p>
<ul>
<li><a href="#header1">学习观01：好多人一辈子都没搞清什么是学习</a></li>
<li><a href="#header1.5">学习观01.5：为什么有人擅长文科，有人却擅长理科，各自又该如何学习呢？</a></li>
<li><a href="#header2">学习观02：什么叫“先把书读厚，再把书读薄”</a></li>
<li><a href="#header3">学习观03：其实你早就学会了”英语“</a></li>
<li><a href="#header4">学习观04：为什么会觉得编程、解题毫无头绪</a></li>
<li><a href="#header5">学习观05：思维导图原理：人生与高手之差</a></li>
<li><a href="#header6">学习观06：为什么费曼技巧被称为终极学习法</a></li>
<li><a href="#header7">学习观07：英语学习法的操作演示</a></li>
<li><a href="#header8">学习观08：我知道很多人容易迷失，所以来谈谈为什么要学习科学</a></li>
<li><a href="#header9">学习观09：以为只有你会用食物诱骗小猫，网上这些招数一直在把你当小猫诱骗和洗脑</a></li>
<li><a href="#header10">学习观10：老师，我没有传纸条作弊，我在学习信息论</a></li>
<li><a href="#header11">学习观11：信息为什么还有单位，熵为什么用 log 来计算</a></li>
<li><a href="#header12">学习观12：理论上人类可以永生，那导致现实死亡的原因是？</a></li>
<li><a href="#header13">学习观13：为什么知识不是信息，学习不是记忆</a></li>
<li><a href="#header14">学习观14：26亿年前灭了所有地球生命的家伙告诉你什么是学习</a></li>
<li><a href="#header15">学习观15：为什么人类要靠不断重复来记忆，为什么好不容易记住的信息又会被遗忘</a></li>
<li><a href="#header15.5">学习观15.5：你学到的哪些记忆方法是错把人脑当成了电脑</a></li>
<li><a href="#header15.6">学习观15.6：如何靠“作弊”来快速记住面试、发表、演讲的内容</a></li>
<li><a href="#header15.7">学习观15.7：​昨夜想到个绝妙点子，想明天试下，结果。。猜到了吗？</a></li>
<li><a href="#header15.8">学习观15.8：你们天天制造知识恐慌，应该自己吃掉自己的脑子</a></li>
<li><a href="#header16">学习观16：性与杀戮改变了所有生物的轨迹？六亿年前到底发生了什么？</a></li>
<li><a href="#header18.5">学习观18.5：高考导致了“高分低能”？素质教育改革可能出现什么问题？</a></li>
</ul>
<h1 id="学习观01：好多人一辈子都没搞清什么是学习"><a href="#学习观01：好多人一辈子都没搞清什么是学习" class="headerlink" title="学习观01：好多人一辈子都没搞清什么是学习"></a><span id="header1">学习观01：好多人一辈子都没搞清什么是学习</span></h1><p>老师和家长总是告诉我们要好好学习，可从没有人告诉过我们什么是学习，学习和记忆的区别又是什么。以至于很多人误以为记忆就是学习。</p>
<p>更讽刺的是，市面上有一大堆学习方法，学习这些“学习方法”比你要学习的内容还要难。</p>
<p>或许你曾有过疑问，为什么小学你经常拿满分，但到了初中后拿满分却越来越难，并非你没有努力，是也不是因为难度不同，而是因为前后所运用的是两种不同的能力。</p>
<p>小学二年级的乘法所运用的能力是记忆，考试题都在乘法表的 81 种情况之内，只要没有偷懒，就可以拿满分。</p>
<p>但记忆可以解决的是情况有限的问题。它两个弊端：</p>
<ul>
<li>容量限制：你我都知道乘法有无数种情况，我们没有办法记忆所有情况的答案。</li>
<li>难以集齐：就算想要记忆，也无法集齐所有的情况。设想有 108 种不同的卡片，每次随机给你 1 张，要集齐所有卡片的难度有多大？当年小浣熊干脆面就是用这种水浒卡的营销策略大赚了一笔。</li>
</ul>
<p>实际生活中，所要解决的问题都有无数种情况，但可记忆的例子十分有限，这也是为什么我们需要学习。</p>
<p>学习是从有限的例子中，找出问题和答案之间规律的一个过程，而所找出的规律叫做知识。</p>
<p>高中有了文科和理科，恰巧分别对应着记忆和学习。</p>
<p>文科：人文规定，社会文化等知识，相对而言难以找到规律来压缩信息，只能依靠大量的记忆。</p>
<p>理科：自然规律，可以像 F = ma 一样找到一个公式压缩无限情况，可以通过若干例子找出规则。</p>
<p>然而分科并不科学。很多女孩子正是因为“听话”，可又没人告诉过她们正确的学习方式，使得她们今后的“学习”走偏了方向，会非常认真的记忆所有信息。</p>
<p>然而如果“学习”后要记忆的内容比“学习”前还要多，那并没有真正学习。</p>
<p>我们常说学习要举一反三，但严格来说，只有举一反三才叫学习。</p>
<p>诺贝尔物理奖得主费曼说“如果你没有办法用简单的语言表述你所学的知识，你就没有真正学会它”。因为学习正是在用知识来压缩原本无限的信息。</p>
<p>学习英语时，我们多么希望动词的过去式没有特例，因为我们不想记忆各种不同的情况，也难以搜集这些不同的情况。我们在用规律来换取信息的压缩。</p>
<p>如果我们真的能获得所有的例子，那就压根不需要学习了。</p>
<h1 id="学习观1-5：为什么有人擅长文科，有人却擅长理科，各自又该如何学习呢？"><a href="#学习观1-5：为什么有人擅长文科，有人却擅长理科，各自又该如何学习呢？" class="headerlink" title="学习观1.5：为什么有人擅长文科，有人却擅长理科，各自又该如何学习呢？"></a><span id="header1.5">学习观1.5：为什么有人擅长文科，有人却擅长理科，各自又该如何学习呢？</span></h1><p>你身边可能有这样一类学生：他们平时非常努力，上课认真记笔记，按时完成作业，文科非常优秀，但理科却并不突出。同时还有另一类学生：他们平时并不努力，上课扰乱秩序，作业基本靠抄，文科成绩很差，但理科却莫名其妙的好。也就是老师口中的聪明却调皮的孩子。</p>
<p>可为什么会产生这种差异呢？难道真的是第二类学生比第一类学生的天赋要高吗？</p>
<p>很多人看我的视频会觉得脑仁疼，所以这个视频我们就比较轻松的来聊一聊“文科”和“理科”。</p>
<p>1.什么造成了文理现象？</p>
<p>解释文理现象之间，先解释学习观中关于知识和信息的疑惑。</p>
<p>1.1. 观后感现象</p>
<p>观感1：学习观01说「知识不是信息，学习不是记忆」。</p>
<p>反应1：理科生听完觉得，喔，原来是这样。但文科生听完慌了，为什么自己学习的内容看起来全是信息啊。难道说自己一直都没在学习，只是在记忆吗？</p>
<p>观感2：不仅如此，学习观02还说「学习的第一步是明确输入和输出」。</p>
<p>反应2：这时的文科生都快哭了，因为怎么感觉自己学习的内容并没有输入啊。</p>
<p>以上的疑惑都是由知识的特点所造成的。</p>
<p>1.2. 观后感现象解释</p>
<blockquote>
<p>知识：知识是什么？是两类事物之间的关系。用数学语言描述，它就是一种映射（如果你不懂映射是什么，换个近似的词来说，它就是函数）。</p>
<p>知识作用：能够允许你用一类事物的信息来预测另一类事物的信息。所以一定有输入和输出，也一定可以用来它预测新情况。不然你干嘛学它。</p>
</blockquote>
<p>情况1：比如你认为「太阳从东边升起」是知识还是信息？是知识。</p>
<p>分析1：这句话实际上省略了「每天」二字，「日期」就是这个知识的输入，而输出是东、南、西、北等「方位」。所以它是一个分类知识。</p>
<p>情况2：那你认为「太阳昨天从东边升起」是知识还是信息？是信息。</p>
<p>分析2：因为「昨天」是已经发生了的事情。这句话消除了「昨天太阳从哪个方向升起」的不确定性。</p>
<p>情况3：那「太阳明天从东边升起」呢？是预测（算是信息，但存在不确定性）。</p>
<p>分析3：这个信息是将「明天」这个输入，送入「太阳每天从东边升起」这个知识后，得到的对于明天的预测「东边」。那有没有可能不对呢？有，比如流浪地球里的太阳都没了。这牵扯到了知识的可证伪性，以后再说。</p>
<p>特点：如果我们把所有的「日期」与「方向」画出一个关系图后，你会发现，这个函数很特殊。它的输出并不随输入的变化而变化。这种函数也叫常量函数。</p>
<p>现象解释：正是由于这种特殊性，使得这个知识退化成了一种情况，所以才可以仅通过记忆这一种情况来掌握它。但它依然是知识，依然既有输入，又有输出，能够用输入来预测输出。</p>
<p>更多的例子还有：</p>
<p>「成人有 206 块骨头，而新生儿的有超过 270 块骨头」这个知识。输入是「人」，输出是「骨头数量」。你能用它来预测任何一个人骨头的大致数量。</p>
<p>「闰年」这个知识，输入是「年号」，输出为「是闰年 或 不是闰年」。你可以用它来预测 2020 年是不是闰年。</p>
<p>1.3. 文理现象解释</p>
<blockquote>
<p>变体：当我们总结上述知识时就会发现，前两个知识的输入和输出的关系比较简单，所包含的不同情况的数量很少。用专业些的词来说，它们的变体（Variation）很少，而第三个知识的变体就相对较多。</p>
<p>知识特点：当一个知识的变体非常少时，也就意味着可以通过记忆所有情况来掌握它。文科的很多知识都有这种特点，所以才给你一种始终都在记忆的感觉。</p>
<p>注意：但关键并不在于是文科还是理科，而在于变体的数量是多是少。</p>
<p>例子：比如说车祸的责任判定，嫌疑人是否有罪等法律知识的变体实际上就非常多。因为案件有无数情况，每个情况可能非常复杂。所以我们经常看到有些案件不知道怎么判，就只能去翻过去发生过的，类似案件的卷宗，寻找规律，用这个规律来判断当前案件。</p>
<p>现象：我想很多学生当初就是不想学习理科，才报了某个文科专业，希望通过努力取得好成绩。可后来发现，怎么选的专业一点都不像“文科”啊，不论怎么努力背诵还是学不好，马上就要自闭了。</p>
<p>解释：如果你恰好是这类学生，那我可以告诉你，这并非天赋问题，而是所使用的“努力记忆”的方法从底层上就无法掌握变体数量非常多的知识。</p>
</blockquote>
<p>2.如何学习文科和理科？</p>
<p>那该如何学习文科和理科知识呢？</p>
<p>2.1. 理科学习</p>
<blockquote>
<p>现象：让我们先回到第二类学生的例子中。这类学生可能由于某些巧合的原因，开始奋发图强。心想，我没努力都已经取得这么好的成绩了，如果努力的话那还了得。自己也坚信自己很聪明。然而努力一段时间后却发现文科成绩确实提升了，但理科成绩提升的却非常有限，可能还有所下降。这与自己当初所畅想的完全不一样。付出了很多，回报却很少，十分失落，甚至开始陷入自我怀疑，觉得还不如不努力呢。</p>
<p>现象解释：实际上第二类学生最初并非完全没有努力。他们上课有在跟着老师的引导思考问题，大声喧哗和打断老师是因为自己会这个问题，想要引起老师的注意。虽然课后并不复习，也不按时完成作业，但遇到不会的题时，由于好奇和不服输等心理的驱使总会让他们花费时间去思考和解答这个问题。<br>而使他们数学成绩很好的原因并非是他们聪明，正是这种不经意的习惯，让他们的大脑自动构造出了泛化能力很强的模型。所以他们理科很好，文科却不好。<br>不过当他们奋发图强时，由于他们并不知道该如何学习，很容易转向纯粹的记忆与背诵，可这反而抛弃了让他们理科优秀的习惯，陷入了与第一类学生同样的困境。所以我们经常看到这类学生的成绩基本上处于中游。</p>
</blockquote>
<p>如果你看到了学习观15期，想必你已经知道问题出在哪里了。正是学习与记忆之间的关系。</p>
<p>变体少的知识可以通过记忆来掌握，但变体多的知识就不得不通过多个例子，让大脑的神经网络自动构建出泛化能力强的模型，来解答根本没有办法记完的无数情况。</p>
<p>理科学习：所以，理科的学习方法就是尽可能多的让自己暴露在各式各样的问题和例子当中，并且多加总结。在这个过程中一定会产生遗忘，但遗忘恰恰是大脑在自动寻找规律时所产生的一种现象。不要有负担，也千万不要想着自己做过的题就一定要强行记忆下来。</p>
<p>理科学习表现：当大脑的神经网络从不同的问题中找到一个普遍规律后，你会发现没有刻意记忆的你却学会了这个知识，而且短期很难忘掉。</p>
<p>学习观作业：还有一个比较有意思的现象就是，第二类学生可能根本记不住数学公式，但却可以使用数学公式。而第一类学生记住了数学公式，却不知道如何使用公式。这个现象的原因就当作学习观的作业，留给大家自己思考了。</p>
<p>2.2. 文科学习</p>
<p>那么文科知识该如何学习呢？</p>
<blockquote>
<p>文科知识特点：文科知识的变体很少，算是一种优势，但也是一种劣势。因为它反而不符合大脑的学习特点。</p>
<p>大脑学习特点：演化出的大脑只是按规则处理信息，比如在睡眠中的大脑就无法区分现实与梦境。同样的，大脑并不知道你所接收到的内容到底是知识还是信息，是规律还是特例，所以文科知识很容易被大脑当成信息给遗忘掉。</p>
<p>对应学习方法：处理这个问题最好的办法就是在不同的场地或书本上去看同一个知识，让大脑认为这是一个随处可见的普遍规律，那么大脑就会降低遗忘的速度。这也是为什么很多学生平时并没有刻意记忆，只是随手翻一翻笔记却记住了。</p>
</blockquote>
<p>3.擅长文科理科是天生的吗？</p>
<p>过渡：让我们回到标题，为什么有的孩子擅长“文科”，有的却擅长“理科”？</p>
<p>其实这很大程度上取决于孩子的性格与最初的成长环境。</p>
<blockquote>
<p>现象1：听话的孩子，努力背诵后，获得了好成绩与老师称赞，他尝到了甜头，知道努力记忆很好用。以后便期望通过更努力的背诵获得更好的成绩与更多的称赞，使他们越来越偏向于用努力记忆来学习，甚至在他们的认知中，记忆就成了学习的全部。</p>
<p>现象2：而调皮的孩子，解出难题后，被老师夸奖聪明但不努力，他感到了优越。以后便希望通过解出更多的难题来证明自己真的聪明，使他们越来越偏向于做更多的问题来学习，甚至可能还会在同学面前隐藏任何自己努力的痕迹。</p>
<p>解释：但如果两类学生的成长环境对调，听话孩子的理科一样可以很好。关键在于孩子在成长过程中对“学习”本身所形成的认知。所以我们也可以看到第三类，聪明努力，文理双全的学生。他们所使用的学习方式就非常符合大脑的特性。</p>
</blockquote>
<p>上述内容只是对学习观01中「知识不是信息，学习不是记忆」的例子扩展，它无法解释所有文理现象。</p>
<p>至于我为什么会知道这些？</p>
<p>初中的时候，我曾是第二类学生。老师们夸我聪明。虽然我不完成作业，但遇到难题时，觉得一定可以解出来的我，花费在思考问题上的时间并不少。在家长与老师的失望中长大，流过无数次今后要奋发图强的眼泪。混到大学时我也以为我聪明，只是没有努力。</p>
<p>直到出国后，我真的开始努力学习的时候，却陷入了第一类学生的困境中，被现实打击到怀疑人生。我也终于认识到了我并不聪明，我只是一个平庸的人。在随后做机器学习相关研究的过程中，我重新回顾了自己的过去，原来我根本不会学习。我经常有看到评论说，真羡慕现在就能看到学习观的学生，我又何尝不希望当初能有人告诉我这些呢。</p>
<p>所以希望大家避开我所犯过的错误，能够「认清自我，扩展边界」。</p>
<p>「认清自我」的意思是要明白大脑的生理局限，合理使用大脑。</p>
<p>「扩展边界」的意思是要善用身边的一切工具，扩展生理局限。</p>
<p>4.选文理的真正依据是什么？</p>
<p>过渡：最后一个问题：选文还是选理？</p>
<blockquote>
<p>现象：恐怕所有老师和家长都推荐学生选择理科，总给人一种文科无用的感觉。</p>
<p>假依据：但任何经过验证的知识都是有用的，并没有理科知识比文科知识更优越一说。在古代如果一个人知道的很多，那他是学富五车，满腹经纶。所以选择文科还是理科的原因并非文科知识无用，而在于如今的我们多了另一个扩展自身边界的工具：计算机。</p>
<p>真依据：由于文科中很多知识的变体较少，依靠记忆便可以解决，而计算机最擅长的就是记忆。我们都是靠知识获得工资，如果你所学的知识可以被计算机毫无成本的复制，你也就不具备竞争力。所以选择的真正依据是在你学习速度最快的时期，尽可能多的去学习变体多，可替代性低的知识，不至于你在未来被别人拿个手机给替代掉。</p>
</blockquote>
<h1 id="学习观02：什么叫“先把书读厚，再把书读薄”"><a href="#学习观02：什么叫“先把书读厚，再把书读薄”" class="headerlink" title="学习观02：什么叫“先把书读厚，再把书读薄”"></a><span id="header2">学习观02：什么叫“先把书读厚，再把书读薄”</span></h1><h2 id="光看定义是学不会的"><a href="#光看定义是学不会的" class="headerlink" title="光看定义是学不会的"></a>光看定义是学不会的</h2><p>在人脑中，学习是通过例子找出问题和答案的规律，重塑大脑连接而完成。学习的第一步就是要明确什么是问题，什么是答案。很多人压根连要学的知识所描述的问题和答案都不清楚就“学习”，最后只是记住知识的描述而已。</p>
<p>例1：这就是为什么你查维基、百度百科，看完定义还是什么都不懂。因为定义是知识的描述。</p>
<p>例2：学习做菜，我们很清楚，问题（输入）食材，答案（输出）是的美食，要学习的是怎么把食材变成美食。通过的是一个又一个的例子让大脑的连接记住了什么样的食材应该怎么切，多重的食材应该放多少盐。</p>
<p>例3：某人想要学习计算机 C 语言，认认真真的记住了培训老师的每一句话，但老师从来没告诉过他 C 这个知识的问题（输入）和答案（输出）是什么，要用 C 做什么（输入怎么变成输出）。最终只是记忆住了知识的描述而已。给他新的任务（输入），他还是不知道怎么把任务变成计算机可执行的步骤（输出）。甚至给他任务后，他都不知道自己在哪，要做什么。</p>
<h2 id="信息与知识"><a href="#信息与知识" class="headerlink" title="信息与知识"></a>信息与知识</h2><p>然而知识不是信息，学习不是记忆，并无法仅凭记住知识来学会知识。这也是人们在教和学的过程中最容易犯的错误。以为将知识告诉了某人，对方就可以学会。以为听懂了知识，就表示自己学会了。</p>
<p>信息是具体的情况，知识不是单纯的信息，而是信息与信息之间的关系。</p>
<p>例1：你小时候养的狗有四条腿，隔壁邻居家的狗有四条腿，这里有两个具体的情况。但要学习的知识是全宇宙的狗与狗腿数的关系，即狗都有四条腿。</p>
<p>例2：狗剩花了15元买了10斤西瓜，赵四花了30元买了20斤西瓜，这里是两个具体情况。可获得的知识是，可能他们村的西瓜斤数（输入）与价格（输出）的关系是1.5元每斤。</p>
<p>例3：父母对孩子的照顾是爱，人们对花草的呵护是爱，少年对李小龙的崇拜是爱，男生付出了金钱要求对方一定要回报自己，不是爱，是交易。4个实例中，爱的共性是奉献。输入是事件，输出是判断是否为爱。这种判断类的问题叫做分类。可以是多个类别。</p>
<p>为什么一定学习？正如学习观01和上面的例子所展现的，这些实例我们一辈子都记不完，也穷举不出所有的不同情况。只能找出共性，来判断从来没见过的事件是不是爱。来判断一锅的食材该放多少盐合适。</p>
<h2 id="改变你的大脑连接才是学习"><a href="#改变你的大脑连接才是学习" class="headerlink" title="改变你的大脑连接才是学习"></a>改变你的大脑连接才是学习</h2><p>然而知识的描述只是对学习起到指引的作用，最终的学习一定要通过例子理清问题和答案的关系来重塑大脑连接。</p>
<p>这就是为什么在讨论、写文章、以及辩论时一定会伴随着例子，而不是单纯的对知识进行描述。因此在看书时，也要注意区分，哪些是例子，哪些是对知识的描述。我个人在写文章时，甚至常常会将例子和知识描述分开来写。</p>
<p>例1：游泳并不是看了游泳书就能学会的，而是需要通过无数个例子来重塑大脑的神经连接。所以需要时间，需要睡眠，反复几天的重塑后，你会发现自己并没有看过什么新的游泳书，但就会游泳了，因为大脑连接重塑了。</p>
<p>例2：真正学会的知识最难忘记，而且是越复杂的知识越难忘，这也是为什么就算流落荒岛10年的人后回家后照样可以听懂母语，毕竟大脑连接已经构造出这种信息与信息的关系。</p>
<h2 id="先把书读厚，再把书读薄"><a href="#先把书读厚，再把书读薄" class="headerlink" title="先把书读厚，再把书读薄"></a>先把书读厚，再把书读薄</h2><p>华罗庚的先把书读厚，再把书读薄的学习方法就是指：先尽可能的搜集更多的例子帮助你体会问题和答案之间的关系，而当你真正学会的时候，这些例子就被压缩成知识。</p>
<p>例1：单看我的视频并且自己不去联想例子、也没有看这个文章，只是看完就完了的人，只会觉得他懂了。但实际没有。1分钟的视频是装不下那么多信息的，《学习观》视频最大的作用在于调动注意力和帮助梳理关系，但若看完了梳理的关系而不去联想身边的无数个例子从而改变自己大脑连接的话，也就没有做到先把书读厚，再把书读薄。</p>
<h2 id="学得的知识不一定正确"><a href="#学得的知识不一定正确" class="headerlink" title="学得的知识不一定正确"></a>学得的知识不一定正确</h2><p>最后，由于学习是为了解决新问题，所以需要验证从现有例子中所提炼的知识，是否能描述问题和答案的真正规律，而不是仅仅记忆了现有的例子。</p>
<p>例1：这就是为什么需要考试。考试的目的正是为了验证你构建的关系是否正确，通过分数来回过头调节自己的学习。不幸的由于成长环境的因素，让学生以为学习就是为了考试得高分。</p>
<p>例2：因为学习的核心是从少数例子推出规律的归纳法，永远无法保证正确，只能保证符合现有的例子中。所以需要来不断通过验证的方式调整自己归纳的规律。比如牛顿力学在宏观例子中好用，但在微观例子中就出现误差了。</p>
<p>例3：强烈建议看完<a href="http://mp.weixin.qq.com/s?__biz=MzIxNDI5MDk0MQ==&amp;mid=2247483794&amp;idx=1&amp;sn=54660d27b0cd39b51375113d2d8ae2ef&amp;chksm=97a89900a0df1016c0546b3160b94401ded269e7df8a3a8f1495434306adcbe7fdb6c846997d&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">超智能体01：什么是机器学习</a>，这里的例子说明了高考到底在做什么。</p>
<h1 id="学习观03：其实你早就学会了”英语“"><a href="#学习观03：其实你早就学会了”英语“" class="headerlink" title="学习观03：其实你早就学会了”英语“"></a><span id="header3">学习观03：其实你早就学会了”英语“</span></h1><p>学习最重要的两步：</p>
<ul>
<li>1.明确问题（输入）和答案（输出）</li>
<li>2.用例子构建知识：知识的构建分为两大类型：运动类和思考类 </li>
</ul>
<p>区别在于是否依靠意识，因为意识是后进化出来的，擅长解决的是推断问题， 但速度缓慢，无法应对多因素任务。</p>
<p>学不会的原因都出在：</p>
<ul>
<li>1.错误的输入和输出，甚至都不知道输入和输出是什么。</li>
<li>2.错误的知识构建方式，很多人一生都在用思考类方式来学习所有知识。比如看书学游泳。</li>
<li>3.不通过例子仅记忆知识。</li>
</ul>
<p>以语言为例，四项能力的正确的输入，输出，类型分别是：</p>
<ul>
<li>听：声音 -&gt; 意思，运动类</li>
<li>说：想法 -&gt; 发声，运动类</li>
<li>读：文字 -&gt; 意思，运动类</li>
<li>写：想法 -&gt; 打字，运动类</li>
</ul>
<p>我当年接受的应试英语的普遍误区是：</p>
<p>输入和输出是完形填空，阅读选择等这类从一些列文字到另一系列文字的思考类问题</p>
<ul>
<li>听：文字 -&gt; 文字，思考类</li>
<li>说：文字 -&gt; 文字，思考类</li>
<li>读：文字 -&gt; 文字，思考类</li>
<li>写：文字 -&gt; 文字，思考类</li>
</ul>
<p>而当你出国后，就会发现你所学“英语”的输入和真实英语输入是不同的。</p>
<p>那既然都是文字，是否意味着我们练就了很好的阅读能力呢？也并非如此， 因为多数时候学生只是记住了单词的中文描述。 </p>
<p>然而语言的输入从来就不是一个单词，而是一个句子，同一单词在不同句子中有不同的意思。 要想把握句子中某个单词的真正含义，需要通过体会大量不同的例句的意思。 所犯的正是没有例子这个错误。很多英语“名师”，不管他们总结的再好，若仅仅是告诉你语法，你永远都是在记忆信息，而不是学习知识。</p>
<p>在用百词斩背单词时还容易产生另一种偏差，输入输出变成了：图片 -&gt; 中文，运动类。</p>
<p>而学英语时最为致命的误区是插入了中文思考这个中间输出，把单一知识变成了两套思考类问题。</p>
<ul>
<li>听：声音 -&gt; 中文 -&gt; 意思，思考类</li>
<li>说：想法 -&gt; 中文 -&gt; 发声，思考类</li>
<li>读：文字 -&gt; 中文 -&gt; 意思，思考类</li>
<li>写：想法 -&gt; 中文 -&gt; 打字，思考类</li>
</ul>
<p>所以掺入意识思考中文后说英语会异常慢。 很多人可以听得懂意思，但让他同时给你翻译成中文的话，他会反应不过来， 就是因为他的大脑在这个过程中其实并不思考中文。</p>
<p>很多英语方法正是在解决这三个误区：</p>
<p>原理一：矫正输入输出的偏差： </p>
<ul>
<li>看电影学英语</li>
<li>练习说外语：选个话题，表达看法，不思考中文。</li>
<li>只看英语解释，是避免加入中文思考这个中间输出。<br>原理二：不经过意识，而使用运动类构建知识： </li>
<li>泛听，泛读</li>
<li>看电影学英语<br>原理三：通过例子避免仅把知识当信息记忆： </li>
<li>理解单词时看很多个例句，重构你的大脑对理解该意思的连接。</li>
<li>针对听、说、读、写，四项能力分别用大量例子练习。</li>
</ul>
<p>十年学不会英语的真相很残酷，并不是因为没有努力，你确实学到了知识，但却是完全不同的知识。</p>
<h1 id="学习观04：为什么会觉得编程、解题毫无头绪"><a href="#学习观04：为什么会觉得编程、解题毫无头绪" class="headerlink" title="学习观04：为什么会觉得编程、解题毫无头绪"></a><span id="header4">学习观04：为什么会觉得编程、解题毫无头绪</span></h1><p>我们可以同时应用多个运动类知识，譬如某人可以边走路边聊天。 但意识在某一刻只能专注一个思考类问题，因此，在某一刻若无法从脑中搜索到能直接从输入得到输出的知识，问题就无法解决。</p>
<p>然而我们所掌握的知识又是有限的，这种直接从输入得到输出的知识往往会超过我们的知识范围。应对办法可以是学习更多的知识。 但还有一个简单，却又无比强大的办法就是分而治之。 </p>
<p>将问题拆分成，脑中存有的，能直接从输入得到输出的小问题来解决。<br>你一定听过这个方法，但恐怕你并不知道它为什么如此强大。 </p>
<p>原因在于，它可以将原本需要你掌握指数级知识量才能解决的问题，变成用线性级知识量就可以解决。</p>
<p>假设你只掌握了3 种知识，不考虑顺序，单从组合来看，就可以形成 7 种新知识。 如果不拆分问题，你需要掌握 7 种知识才能做解决这些问题。 而有 20 种知识的时候，二者的差别就是 20 对 1048575。</p>
<p>在编程时，所用到的知识远不止 20 个。 但其实人类天生习惯于搜索能直接从输入得到输出的知识，这也是新人在面对编程问题常常束手无策的原因，因为根本搜索不到。</p>
<p>该原则适用于所有思考类知识，不管是写文章，答数学题，还是解决工作中所面临的困难。它直接决定了一个人解决问题的能力。</p>
<h2 id="学习观4-5：这些学习方法从何而来"><a href="#学习观4-5：这些学习方法从何而来" class="headerlink" title="学习观4.5：这些学习方法从何而来"></a>学习观4.5：这些学习方法从何而来</h2><p>没有免费午餐定理 早就告诉了我们，没有万能学习法，必须依照具体任务而选择学习方法。所以你会发现，至今所有的视频里，我只对原则提出了问题，但其实并未给出任何学习方法。</p>
<p>然而实际生活的情况并不像定理中所假设的那样，虽然没有万能学习法，但却有一些原则。在 《学习观》05 中便会提到第一个真正跟学习方法相关的构建知识的原则。</p>
<p>也就是说，我依旧在讲机器学习。我希望人们不再觉得机器学习与自己毫不相干，机器学习的知识还有另一种用法。</p>
<p>《学习观》并非独立的视频系列，从开始就是《超智能体》一部分，你若喜欢可叫它前传。不同之处就是同样的机器学习知识应用在了人脑上。</p>
<p>以后我也会慢慢揭开为什么这个专栏叫做《超智能体》，它到底指的是什么意思。到时候你会明白它既不与机器学习相关，也不与学习相关，而是我曾经未完成的作品。</p>
<p>关于深度学习和浅层学习与人脑学习的类比我没有详解解释。有两点：</p>
<ul>
<li>错误学习算法：不是所有任务都适合用深度学习来建模。该用浅层学习的时候用深度学习，该用深度学习的时候却用了浅层学习造成的不良表现。类比的是该用运动类学习方式，却用了思考类学习方式。</li>
<li>错误迁移方式：我们的学习大多是知识的迁移，因为发现知识的人并非我们自己，而是前人。浅层学习算法可以通过描述关系方便的将知识从一个系统迁移到另一个系统。然而深度学习却完全不能这么迁移。类比的是某人会英语，会游泳，但这个人无法仅通过语言让另一个人学会游泳。很多知识并不可以仅仅通过描述就能达到迁移的效果。</li>
</ul>
<h1 id="学习观05：思维导图原理：人生与高手之差"><a href="#学习观05：思维导图原理：人生与高手之差" class="headerlink" title="学习观05：思维导图原理：人生与高手之差"></a><span id="header5">学习观05：思维导图原理：人生与高手之差</span></h1><p>市面上有太多的学习方法，几乎都是作者的经验总结，背后没有原理，更像是信仰一样去完成某些步骤，坚信神奇会发生。很多人都知道思维导图简单又强大，但却很少有人能说出来为什么，往往也不知道该如何正确使用。</p>
<p>你一定想过，为什么有人可以快速学会新知识，有人可以迅速找到解决办法。YJango 下面要传达的内容，是创造知识的知识，是卖你知识的人不会告诉你的知识。</p>
<p>学习观前面的视频只提到了学习的定义和原则，但并没有涉及任何具体的学习方法。而思维导图就是我想介绍的第一个学习方法。</p>
<p>它的强大不在于帮助你记忆，而在于帮助你克服学习和应用中的误区。</p>
<h2 id="从没有人教过我们怎么学习"><a href="#从没有人教过我们怎么学习" class="headerlink" title="从没有人教过我们怎么学习"></a>从没有人教过我们怎么学习</h2><p>多数人的学习方法往往是阅读某个知识的描述，搞清意思，不断重复，并希望记住它。但这一过程只是在应用人脑的阅读和记忆能力，产生的学会了的错觉，实际是能看懂的感觉，并没有在学习。尤其是在读理科书的时候，你会觉得，每个词你都认识，就是看不懂，因为理科书在描述的关系。看小说容易是因为小说传达的并不是知识，而是具体的例子，需要你自己从中提取道理。</p>
<h2 id="如果能记忆，那就不需要学习了"><a href="#如果能记忆，那就不需要学习了" class="headerlink" title="如果能记忆，那就不需要学习了"></a>如果能记忆，那就不需要学习了</h2><p>而使用思维导图时，你不再只是记忆知识的描述和无限的例子。是先写出一个关键词。然后你会问自己它是什么，从而不断的联想起很多例子。你会强迫自己用这一个关键词描述所有的例子。也就是说，你在压缩信息。进而不得不分析这些例子的共同输入和输出，找出规律。</p>
<p>而当问自己它的作用或目的时，你实际上在思考输入是怎样变成输出的。因为输入和输出可代表一类事物中任何一个情况，因此被称为变量。而这里的变成指的是从输入到输出的一种变换关系，也就是函数。从输入变成输出的情况是无限的，但都可以被这一个函数所描述。要学习的知识正是这种不变的关系，而不是可变的具体情况。因为你永远都没办法见到所有的情况。只能从有限的例子中找出规律，用来预测你从未见过的情况。毕竟若要解决的问题是已知问题，直接上网搜索就可以了，也就不需要学习了。人生需要你解决就是那些谁都没见过的情况。</p>
<p>在思考这些问题的过程中，你的大脑连接被这些例子所改变了，最终关联到一个关键词上，便完成了一次学习过程。</p>
<h2 id="名字根本不重要"><a href="#名字根本不重要" class="headerlink" title="名字根本不重要"></a>名字根本不重要</h2><p>随后你会用一个更好的关键词来代表你找出的关系，一种是动宾结构，因为它描述了输入和函数，输出也随之确定了。</p>
<p>不过当人们开始传播该知识时，动宾结构会名词化，譬如以发现者的名字命名物理定律。一个知识可以有很多名字，但名字并不重要，重要的是从输入到输出的这种关系。不幸的是很多人只是记住了最不重要的名字。</p>
<p>还有一种知识本身就是名词，也会让你觉得它并没输出，但这种知识的输出是分类任务中的类型，描述它的句子是主谓结构，谓语被导图的连线所代替了。</p>
<h2 id="知识与智慧"><a href="#知识与智慧" class="headerlink" title="知识与智慧"></a>知识与智慧</h2><p>然而这并非思维导图的全部，它最强大的地方在于对知识的拆分。</p>
<p>而若将要学习的知识拆分小知识的组合的话，你需要的例子就从指数量级变成了线性量级。并且所拆分出的小知识中，很多都是你已经学过的，可直接用于该次知识的构建。知识网络的好处就在于知识的重用性上。这正是为什么有人可以快速掌握一项新知识的秘密。</p>
<p>思维导图的核心在于知识的拆分上，因此它既可以在学习未掌握的知识时用来拆分，也可以在应用已掌握的知识时用来拆分，也就是分而治之的具体操作方法。</p>
<p>但拆分知识的能力也是一种需要学习的知识，并且是一种比较特殊的二阶知识。不同点在于：一般的知识是描述信息与信息之间的关系，而二阶知识是描述知识与知识之间的关系。比如，多次重复一个知识，组合某些知识来产生新知识。</p>
<p>在拆分一个大知识时，若从大知识向小知识来拆分，则是正向思维；若从小知识向大知识来拆分，则是逆向思维。</p>
<p>知识的拆分只有一项终极原则，所拆分的知识之间要尽可能的彼此独立，这样才可以减少学习所需要的例子的数量。</p>
<h2 id="可以把思维导图扔了"><a href="#可以把思维导图扔了" class="headerlink" title="可以把思维导图扔了"></a>可以把思维导图扔了</h2><p>而当你回顾我所提到的，学习</p>
<ul>
<li><ol>
<li>要明确输入输出</li>
</ol>
</li>
<li><ol>
<li>要将信息压缩成知识</li>
</ol>
</li>
<li><ol>
<li>要用例子重塑大脑连接</li>
</ol>
</li>
<li><ol>
<li>要用二阶知识来分而治之</li>
</ol>
</li>
</ul>
<p>你会发现当正确思维导图时，你在不经意间做到了以上的所有要素。</p>
<p>然而它之所以好用是在于满足了这些要素，不在于思维导图这个名字。若没能在使用中满足要素，即使用了思维导图也没用。但若能满足这些要素，即使什么都不用，也能让你真正学习。</p>
<h2 id="学习观5-5：怎么设计你自己的思维导图画法"><a href="#学习观5-5：怎么设计你自己的思维导图画法" class="headerlink" title="学习观5.5：怎么设计你自己的思维导图画法"></a>学习观5.5：怎么设计你自己的思维导图画法</h2><h3 id="没有什么是固定的"><a href="#没有什么是固定的" class="headerlink" title="没有什么是固定的"></a>没有什么是固定的</h3><p>在解释完思维导图如何能够满足学习的要素后。就来思考该怎么设计画思维导图的步骤。</p>
<p>依据是如何能满足这四个要素，而不是对思维导图的信仰。你可以根据不同的需求来设计。</p>
<ol>
<li><p>要明确输入输出和任务</p>
</li>
<li><p>要将信息压缩成知识</p>
</li>
<li><p>要用例子重塑大脑连接</p>
</li>
<li><p>要用二阶知识来分而治之</p>
</li>
</ol>
<p>在设计步骤的过程中，我也会使用这些步骤来画步骤的导图。</p>
<h3 id="无米之炊"><a href="#无米之炊" class="headerlink" title="无米之炊"></a>无米之炊</h3><p>先看第3条，要用例子重塑大脑连接。</p>
<p>所以，第一步就是搜集例子。可以把搜集例子按渠道来拆分成脑中现有的和借助外力的。头脑风暴和上网调查。</p>
<p>头脑风暴最主要的作用就是搜集所有相关例子。而上网搜索，其实就是研究生们每天做研究的做法。</p>
<h3 id="自我提问与质疑"><a href="#自我提问与质疑" class="headerlink" title="自我提问与质疑"></a>自我提问与质疑</h3><p>再看第1条，需要明确输入输出和任务。</p>
<p>所以，第二步就是自我提问。</p>
<p>问列出的关键词是什么，比如问思维导图是什么。这实际上是在学习分类，输入是一个事物，输出是它是否是思维导图。可以上网搜到无数个例子，一辈子都没有办法记住这么多例子。</p>
<p>今天的任务清单：是</p>
<p>明天的任务清单：是</p>
<p>三角函数的知识体系：是</p>
<p>天龙八部的人物关系：是</p>
<p>而且，我大脑中可以回想起的不重复的例子也是有限的，无法穷举所有不同的情况。就是学习观 01 提到的，记忆的容量有限和难以集齐。所以需要找出这些例子的共性，就是第2条，找出规律，将信息压缩成知识。</p>
<p>我会不断的问为什么这个例子是思维导图，也在不断的质疑自己，反驳自己，也就是在寻找分类的边界，上个视频我把边界定成了能把杂乱信息拆分成知识网络的工具。这个知识是判断一个事物是不是思维导图。</p>
<h3 id="不要忘记目的"><a href="#不要忘记目的" class="headerlink" title="不要忘记目的"></a>不要忘记目的</h3><p>但我的目的是什么？是为了以后自己可以画出任何情况的思维导图，包括我从来没见过，这些例子中没有的情况。</p>
<p>这时的输入和输出就不一样了，输入是杂乱信息，输出是知识网络，要学习的知识是输入是怎样变成输出的。</p>
<p>这就是为什么一定要明确要学习的规律是什么，以免学错规律，或半天都不知道自己在干什么。</p>
<p>问：它是什么，为什么是，  是分类的自我提问。问：它的目的，如何达成，  是回归的自我提问。</p>
<h3 id="分而治之不是鸡汤"><a href="#分而治之不是鸡汤" class="headerlink" title="分而治之不是鸡汤"></a>分而治之不是鸡汤</h3><p>还有第4条，拆分任务，也是自我提问。问的是子知识与子知识的关系。</p>
<p>拆分成组合关系时，会问，它有什么。</p>
<p>拆分成执行步骤时，会问先后顺序，还有条件循环。</p>
<p>这个自我提问便是循环，而循环的停止条件是把大知识拆分成自己已知的知识为止。</p>
<h3 id="视角创造不同"><a href="#视角创造不同" class="headerlink" title="视角创造不同"></a>视角创造不同</h3><p>还有一点是视角。比如人可以拆分成 男女，再拆分成老少，还可以先老少，后男女。也就是视角不同了。一个是侧重性别，一个是侧重年龄。</p>
<p>上个视频中思维导图其实是在侧重原理，问的问题是它有什么属性特点来考虑组合关系。而这里我问的是先后顺序，来考虑画导图的步骤。</p>
<p>你会发现我的所有知识都是已有的知识，最大的不同就是视角。</p>
<h3 id="利用身边的一切资源"><a href="#利用身边的一切资源" class="headerlink" title="利用身边的一切资源"></a>利用身边的一切资源</h3><p>说完了步骤，可以想想该怎么利用网上有的思维导图工具。Xmind 有外框，联系，还有笔记，概要。</p>
<ul>
<li>外框：不太常用。可用于临时提醒自己在用怎么视角，以及注意力该放在哪里。</li>
<li>联系：可以画夸层级知识之间的关系，因为同层知识的关系已经是并列或顺序了。</li>
<li>笔记：因为每一个关键词都是一个知识，如果我想对知识进行描述，可以写在笔记里。</li>
<li>概要：笔记不显示知识描述，如果想显示的话，可以放在概要上。一般我会在把自己的导图发给别人时使用。</li>
</ul>
<h3 id="习惯也是在压缩信息"><a href="#习惯也是在压缩信息" class="headerlink" title="习惯也是在压缩信息"></a>习惯也是在压缩信息</h3><p>这里可以再把提问细分一下，分成问变量与变量之间关系的一阶知识，和问知识与知识之间关系二阶知识。</p>
<p>之前提到了怎么命名，如果是分类的话，句式是什么是什么的主语谓语结构，但谓语被连线代替了。所以就只有主语。</p>
<p>比如，“分类属于一阶知识“这个知识是名词。</p>
<p>如果是回归的话，就是像搜集例子这样的动词和宾语的结构。</p>
<h3 id="勿忘初衷"><a href="#勿忘初衷" class="headerlink" title="勿忘初衷"></a>勿忘初衷</h3><p>最后列出一些误区来提醒自己。</p>
<ul>
<li><ol>
<li>因为是压缩信息，我如果把所有东西都写上去的话，就失去意义了。</li>
</ol>
</li>
<li><ol>
<li>拆分的信息之间要独立。你可以看到独立与不独立对于信息的压缩能力的区别。</li>
</ol>
</li>
<li><ol>
<li>又因为重要的是整个过程对我大脑的重塑，所以看别人的导图也没意义。</li>
</ol>
</li>
<li><ol>
<li>最后是画完导图也要不断的验证自己构建的知识网，当有了新例子和体会后要更新导图。</li>
</ol>
</li>
<li><ol>
<li>还有一点是不要去画完全没有规律的信息的导图</li>
</ol>
</li>
</ul>
<h1 id="学习观06：为什么费曼技巧被称为终极学习法"><a href="#学习观06：为什么费曼技巧被称为终极学习法" class="headerlink" title="学习观06：为什么费曼技巧被称为终极学习法"></a><span id="header6">学习观06：为什么费曼技巧被称为终极学习法</span></h1><p>可网上对费曼技巧原理的解释却是玄之又玄，大量不知道从哪里得来的“心理学”理论强行解释。和推崇思维导图时一样，把工具当成了某种仪式步骤来执行，希望可以产生魔力。</p>
<h2 id="手段可以选择，目的才是核心"><a href="#手段可以选择，目的才是核心" class="headerlink" title="手段可以选择，目的才是核心"></a>手段可以选择，目的才是核心</h2><p>费曼技巧只有两步：第一步是学习，第二步是解释。学习时可以使用思维导图的所有技巧。可为什么只多了一步解释却有如此大的作用？在解释的过程中到底达成了哪些目的？</p>
<h2 id="条条大路通罗马"><a href="#条条大路通罗马" class="headerlink" title="条条大路通罗马"></a>条条大路通罗马</h2><p>想想一下我们是如何向某人解释一个知识的？可能我们看了很多的例子与知识描述，但我们不能将这些重复给对方听，只能浓缩信息，挑核心的规律来解释。浓缩后的规律会被转化成一个个句子，每个句子都包含主语谓语和关系。当将概念转化为语言时，同时做到了三件事：明确了复合知识的输入和输出，描述了所拆分出的子知识的输入输出及关系，最终理清了所要解释的知识。这些目的在使用思维导图时就可以达成，费曼技巧也和思维导图一样，只是工具。</p>
<h2 id="有效验证"><a href="#有效验证" class="headerlink" title="有效验证"></a>有效验证</h2><p>要说解释最关键的作用，就在于：验证二字。</p>
<p>使用思维导图时，可能你画完导图，却仍感觉什么都没学会。该如何验证自己已学会某项知识呢？</p>
<p>学习是用有限的例子归纳知识来压缩信息，所压缩的信息包括从来都没见过的情况。因为我们正是要用知识来解决未见过的问题，对于已见过的例子，记住答案或上网搜索就好了。</p>
<p>然而记住某些内容，只能重述这些内容，却无法解释新情况。与学习时不同，解释时所用的不再是学习时的例子，是对方所面对情况的输入和输出，要帮助对方来理清关系。</p>
<p>如果对方无法理解，则需要举新的例子，若还无法理解，则需要换新的视角。这些都是当初你在学习时从未遇到的情况，而当你可以生成并解释这些新情况时，也就验证了你的学习。</p>
<p>下面 6 个词的关系是：</p>
<ul>
<li>学习 —&gt; 理解 —&gt; 解释</li>
<li>记忆 —&gt; 记住 —&gt; 重述</li>
</ul>
<h2 id="思维导图与费曼技巧"><a href="#思维导图与费曼技巧" class="headerlink" title="思维导图与费曼技巧"></a>思维导图与费曼技巧</h2><p>思维导图是以过程为主导的学习方法，核心在于拆分知识，而费曼技巧是以目的为导向的学习方法，核心在于验证知识。但不要认为你只能使用一个。</p>
<p>对于拆分知识，费曼技巧并不如思维导图清晰。</p>
<p>对于验证知识，思维导图在新例子与视角上不如费曼技巧。</p>
<p>可以在费曼技巧的学习步骤时使用思维导图，也可以在思维导图的复习时用新例子验证。</p>
<h2 id="温故而知新，可以为师矣"><a href="#温故而知新，可以为师矣" class="headerlink" title="温故而知新，可以为师矣"></a>温故而知新，可以为师矣</h2><p>所有无限推崇费曼技巧的人真的理解费曼技巧？我称其为费曼技巧源于我对费曼先生的喜爱和敬佩，我有去了解了先生的生平，可我在很多人的文字里却看不到这份喜爱。所有人也都没有提到孔子。</p>
<p>但如果真的将该技巧追溯于某人，恐怕至少要到春秋末期。我国的教育家思想家孔子在两千年前就将该技巧应用在了教学中。我们初一（上）的论语十则中的「温故而知新，可以为师矣」记载的就是费曼技巧。</p>
<p>这里的「故」是学习时的例子，而「新」是未见过的例子当可以从有限的例子中提取知识来解释新的情况时，就意味着你真正的学会了它，便可将知识教授他人。</p>
<p>稍微有些讽刺不是吗？人们苦苦追寻的终极方法却每天清晨都传荡在中学的走廊。</p>
<h1 id="补-我们教学英语的方式几乎全错了-中英双文"><a href="#补-我们教学英语的方式几乎全错了-中英双文" class="headerlink" title="补:我们教学英语的方式几乎全错了(中英双文)"></a>补:我们教学英语的方式几乎全错了(中英双文)</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我本身是语音识别专业，尝试过用教计算机学习英语的方式来教自己，非常有效。虽然有很多自己的独门偏方，但我其实只想分享学习原则，非常抵触分享具体的学习方法。破例一次，作为例子给大家参考吧。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>A lot of people asked me to talk about English learning.</p>
<p>很多人要我谈谈英语学习</p>
<p>Since I’m not an expert on it</p>
<p>因为我不是这方面的专家</p>
<p>so this <a href="https://v.qq.com/x/page/e08080tndv1.html" target="_blank" rel="noopener">video</a> is just an example of </p>
<p>所以这个<a href="https://v.qq.com/x/page/e08080tndv1.html" target="_blank" rel="noopener">视频</a>仅是一个例子</p>
<p>how to apply the learning principles I’ve mentioned so far. </p>
<p>关于如何应用我目前提到的学习原则</p>
<p>But first I want you to watch several video clips.</p>
<p>但在首先我想让你看几个剪辑</p>
<p>If someone explained everything about this trick and I got it</p>
<p>如果某人解释了这个技巧的一切， 我也听懂了</p>
<p>can I do this as well? </p>
<p> 我也能做到这个吗</p>
<p>Again, can I do this and this? </p>
<p>还有这个也可以吗</p>
<p>I don’t think so. </p>
<p>不觉得我能</p>
<p>And languages are just like sports.</p>
<p>语言就和运动一样</p>
<p>We can understand everything the teacher explains</p>
<p> 我们可以理解老师讲解的一切</p>
<p>but still cannot speak it without practicing. </p>
<p>但如果不练习，就说不了</p>
<p>Knowledge like sports, instruments is called unconscious (Implicit) knowledge.</p>
<p>像运动、乐器这样的知识叫做隐式知识</p>
<p>We only need two principles for this kind of knowledge.</p>
<p> 对这种知识，只需要两个原则</p>
<p>First, define the direct input and output.</p>
<p>首先定义直接的输入和输出</p>
<p>There are four English skills</p>
<p>英语有四种能力</p>
<p>so you need to decide which one you wanna enhance. </p>
<p>所以你需要决定要加强哪一个</p>
<p>Then, learn it from a lot of different examples to reshape the connections in your brain.</p>
<p>然后通过很多例子来重塑大脑连接</p>
<p>Each example is a pair of the input and the corresponding output.</p>
<p>每个例子是输入与对应输出的配对。</p>
<p>Only two principles and we are doing it all wrong.</p>
<p>只有两个原则，我们全做错了</p>
<p>For listening skill, a common mistake to read the subtitles.</p>
<p>对于听力，一个常见的错误就是读（英语的）字幕</p>
<p>Because if you read the subtitles while listening to English,</p>
<p>因为如何你在听的过程中看字幕</p>
<p>you are going to learn a different kind of knowledge </p>
<p>你将学到一个不同的知识</p>
<p>whose input is the combination of voice and subtitles,</p>
<p>它的输入是声音与字幕的组合。</p>
<p>which cannot be used when the subtitles are missing.</p>
<p>当字幕缺失时就用不了了</p>
<p>This mistake is caused by the wrong input. </p>
<p>这是因错误输入造成的误区</p>
<p>Another one caused by the wrong output is only listening to a paragraph, </p>
<p>因错误输出造成的误区是只听英语</p>
<p>but do not know the meaning of it,</p>
<p>但却不知道意思</p>
<p>which means you don’t even have the corresponding output.</p>
<p>也就是说，你压根就没有输出</p>
<p>No matter how hard you practice, you’ll learn nothing. </p>
<p>不管怎么多努力的练习，将一无所获</p>
<p>So, the question most people are interested in would be how to learn English from scratch.</p>
<p>多数人感兴趣的问题可能是如何从零学英语</p>
<p>When working out in the gym, we will have several stages</p>
<p>健身时我们有不同的阶段</p>
<p>and we also have different stages for learning English</p>
<p>学习英语也有几个阶段</p>
<p>because some type of knowledge like grammar is built on the top of phrases.</p>
<p>因为有的知识，例如语法，是建立在短语知识之上的</p>
<p>I don’t want to make it too complex,</p>
<p>我不想把它搞得太复杂</p>
<p>so, for now, I will tell you the easiest way to implement.</p>
<p>所以现阶段我就告诉你最简单的实践方式</p>
<h1 id="学习观07：-英语学习法的操作演示"><a href="#学习观07：-英语学习法的操作演示" class="headerlink" title="学习观07： 英语学习法的操作演示"></a><span id="header7">学习观07： 英语学习法的操作演示</span></h1><h2 id="学习运动类知识的感觉"><a href="#学习运动类知识的感觉" class="headerlink" title="学习运动类知识的感觉"></a>学习运动类知识的感觉</h2><p>我提到过思考类与运动类知识。你认为学习观分享的知识是什么类型呢？对，无一例外，都是运动类。听懂运动类知识并不能让你马上就做到，就和学游泳一样，需要在实践中练习，让大量的例子对你的大脑连接进行重塑。如果你听完就能做到，要么是你在投机取巧式的应用已有的知识（比如用汉语的发音来理解英语发音），要么就是你的大脑原本就有这种知识。</p>
<h2 id="运动类学习方法与思考类几乎完全相反"><a href="#运动类学习方法与思考类几乎完全相反" class="headerlink" title="运动类学习方法与思考类几乎完全相反"></a>运动类学习方法与思考类几乎完全相反</h2><p>大家可以看出来，我非常善于使用思维导图，毕竟从初中就开始使用。所以，其实我最初是用过思考类知识的学习方式来学习日语的。每次说日语之前，我都在脑中用语法构造出一个符合语法规则的句子。然而，结果却是我比一般人开口要晚得多。越认真，越思考，反而越慢。</p>
<p>日本人做事都是很认真的，他们的英语读和写的能力其实比中国人好。但是听和说的能力反而是“因为”认真思考，变得比平均水平差。</p>
<p>在日本料理店的前台（厨师、调酒师等）的英语都很好。从来没有刻意学过，但听力和发音的平均水平是高过很多日本大学生的。</p>
<p>还有，你们知道哪些职业学外语非常快吗？ 厨师、歌手、运动员。没错就是所有擅长运动类知识的职业。而且，在外国最先开口说外语的恰恰不是认真的孩子。学习观0-6谈的都是思考类知识的学习，然而运动类知识几乎是和思考类完全对立。随着以后在学习心理部分讲到生物知识时，大家就会明白人类为什么会进化出两种完全不同的系统，这两个系统并没有优劣，而是各有利弊。相辅相成的。</p>
<ul>
<li>在学习思考类知识时：需要避免端到端的思考（意为：直接从输入到输出），还要尽可能的将大知识拆分成小知识来学习。</li>
<li>在学习运动类知识时：恰恰需要端到端的学习方式，还要尽量避免使用意识以及去掉所有中间过程。</li>
</ul>
<h2 id="用感觉从输入关联到输出"><a href="#用感觉从输入关联到输出" class="headerlink" title="用感觉从输入关联到输出"></a>用感觉从输入关联到输出</h2><p>在练习听、说、读、写四种能力的过程前与过程后，可以想任何东西，中文，日文，whatever you like。但是，在练习的过程中，只用感觉来从输入关联到输出。</p>
<h2 id="输出始终一致"><a href="#输出始终一致" class="headerlink" title="输出始终一致"></a>输出始终一致</h2><p>这个技巧和训练宠物狗一样。有人不想让宠物做什么事情的话，一定要始终要给宠物一致的信号，对应的行为（输入）会得到对应的惩罚（输出）。不要有时给奖励，有时不管，有时给惩罚。这就是学习观07中提到的「输出始终一致」。不然宠物就会迷惑，学不到你想要教给它的事情。（教宠物所用的方法是强化学习 reinforcement learning，我目前提到的都是监督学习 supervised learning，非机器学习专业的人不用在意。）</p>
<h2 id="如何克服中文思考"><a href="#如何克服中文思考" class="headerlink" title="如何克服中文思考"></a>如何克服中文思考</h2><p>克服中文思考最好的办法就是所有材料都是英文。但如果你是新手，那确实很难不看中文来确定意思。一旦涉及了中文，大脑就很容易偷懒的去依靠中文解释。有一个可以减缓的小方法就是，不要查看一一对应的中文意思。每次中文解释都不同。就是「输出始终一致」的逆向应用，让你无法形成关联。</p>
<p>例子：I’m afraid I succumbed to temptation and had a piece of cheesecake. 与这个英语句子一一对应的中文意思是：我恐怕我屈服于诱惑，吃了一块芝士蛋糕。</p>
<p>那就可以避开这种一一对应的解释，改成简单的：没忍住偷吃了。</p>
<p>也可以改成复杂的：看着眼前的芝士蛋糕，我最终还是没能控制住我自己，第二天胖了2斤。</p>
<p>注意：确定好了意思之后，还是要克制自己不要去思考，不要以为有了上面的方法就可以随心所欲的依赖中文。</p>
<h2 id="如何克服默读"><a href="#如何克服默读" class="headerlink" title="如何克服默读"></a>如何克服默读</h2><p>默读并不是必须的，人们默读的原因是为了提醒自己读了什么，就如同记一串数字一样，不断的重复，这是由于意识的 working memory 十分有限，我们难以靠意识记住毫无规律的 7 个数字。</p>
<p>chunking 就是一种可以代替默读的提醒功能的技巧。很多人以为阅读快是由于读的熟练，大脑处理速度就增快了。实际上不是，而是因为大脑把独立的信息组成一体（比如原本是一个单词一个单词处理，变成谓语+主语块进行处理），利用神经元的并行功能后才提高了速度。毕竟大脑也是要遵守自然法则的。这个问题会在以后对应的部分里详细解释。</p>
<p>另外，这里的默读指的是：虽然没有读出声，但实际上是以读出声的速度进行阅读的一种行为。</p>
<p>判断自己是否有这种行为的方式就是看看自己的阅读速度是否可以大于实际读出声的阅读速度。如果持平，那就是这里所指的默读。如果可以大于两倍以上的速度，则不用担心。</p>
<p>对于有这种行为的人，有一个小办法是，边嚼口香糖边练习阅读，让自己的嘴巴被另一个任务所占据，无法使用。</p>
<h1 id="学习观08：我知道很多人容易迷失，所以来谈谈为什么要学习科学"><a href="#学习观08：我知道很多人容易迷失，所以来谈谈为什么要学习科学" class="headerlink" title="学习观08：我知道很多人容易迷失，所以来谈谈为什么要学习科学"></a><span id="header8">学习观08：我知道很多人容易迷失，所以来谈谈为什么要学习科学</span></h1><h2 id="感知源于模型，尽管不是真实，却是生存唯一"><a href="#感知源于模型，尽管不是真实，却是生存唯一" class="headerlink" title="感知源于模型，尽管不是真实，却是生存唯一"></a>感知源于模型，尽管不是真实，却是生存唯一</h2><p>随着科学的发展，我们会看到更多生理和物理现象，对于自身和世界的认识会不断被颠覆，很容易产生经常悲观和虚无的情绪。</p>
<p>而最容易产生的情绪就是：“如果感知不是真的，那还有什么意义”</p>
<p>但当你学会思考 “为什么会产生这种现象” 时，你就会发现，不是不想求真，是因为从原理上就做不到。而且，允许我们存活到现在的恰恰是 “非真” 的模型，而不是真实。</p>
<p>如果我们真的进化出感知真实的生理器官，恐怕我们早就不存在了。比如下面的图片里， A 和 B 看似不同,<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/1.png" alt><br>实际上 A 和 B 是相同颜色<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/2.png" alt><br>因为周边的颜色环境造成 “相对的” 感知到 B 亮于 A。</p>
<p>为什么会这样？我们假设 B 是藏在阴影里的猎豹，如果真的进化出感知真实的视觉的话，那很多情况下 “该看不到的内容” 就是看不到，而我们存活到今天的原因正是因为演化出的感知系统不是求真，而是求存。所以进化出的感知系统一定是“相对感知”而不是“绝对感知”。</p>
<p>关于很多人觉得 “不真实就没有意义了” 的问题，实际上，我们脑中的 “求真” 想法并不是追求 “真实”，而是追求 “可坚信感”，因为我们依赖模型而活，如果不坚信自己的模型，就无法做出决策，无法生存。</p>
<p>也就解释了为什么网上的戾气那么重，评论的人那么不实事求是，固执己见。因为他们根本不是来 “求真” 的，所有人都是希望得到 “我的模型是可以坚信的” 感觉。我们是进化成的“本能”的偏袒自己的模型。</p>
<p>我们的大脑其实完全有能力意识到我们与其他生物感知不同的现象，也完全有能力意识到眼镜可以把视力调节成无数种形态。</p>
<h2 id="为什么要学习"><a href="#为什么要学习" class="headerlink" title="为什么要学习"></a>为什么要学习</h2><p>那是不是固执己见，坚信自己的模型就可以了呢？当然不是。自然选择已经告诉我们，有些模型是无法支持生物生存，直接被淘汰掉了。因此我们才要构建可以依赖的、可以维持生存的模型。因为我们的生存就是构建在这个规则之下的。</p>
<p>而学习正是为了构建这些模型（模型和知识在这里相同），从而让我们更好的看清世界，更好在自己所处环境里内生存。</p>
<h2 id="判断模型好坏"><a href="#判断模型好坏" class="headerlink" title="判断模型好坏"></a>判断模型好坏</h2><p>虽然科学也不能感知真实，但是它却可以让我们依赖，可以让我们生存。所以科学包揽了感知和认知模型的研究，是我们应该提倡的唯一选择。</p>
<p>但对于观念层面，我们没有唯一的评估标准。这里的模型是无法依靠科学来选择的。因此高等科学教育并不能保证人品。网上才有那么多打着科学旗号却怀揣其他目的的观念。他们直接把科学与真理划等号，这很危险，科学不是真理，是相对真理，或者说是生存真理。</p>
<p>而在知识付费领域里，观念层面的知识也最多，因为同一个问题可以被构建出无数个模型，但很多人并没有判断模型好坏的能力。</p>
<p>所以我其实对于知识付费又赞同又反对，赞同在于它提高了国民水平，反对在于它也可以被用于圈钱，降低人们的信任。因此我希望每个人都可以有判断模型好坏的基本能力后才去知识付费。</p>
<h2 id="了解生理模型"><a href="#了解生理模型" class="headerlink" title="了解生理模型"></a>了解生理模型</h2><p>一半以上的生理特性是演化出来为了更好的服务于构建认知和观念模型的。然而只要是人类，生理特性就可以被他人利用。</p>
<p>下一个视频你将会明白这个视频末尾的用意。你将会看清分析身边的观念植入行为。比如</p>
<ul>
<li>为什么你会被特定的标题吸引</li>
<li>为什么人们喜欢故事不是道理</li>
<li>为什么辩论根本不是在讲道理</li>
<li>软件如何让你不断查看手机的</li>
</ul>
<p>但是同样的，这些特性也可以被你自己所利用，用来学习新知识，这也是为什么学习观会分享这部分的内容的原因。</p>
<blockquote>
<p>所有的价值、原则、决策都源于模型。<br>谁塑造了你的模型，谁就塑造了你。</p>
</blockquote>
<h1 id="学习观09：以为只有你会用食物诱骗小猫，网上这些招数一直在把你当小猫诱骗和洗脑"><a href="#学习观09：以为只有你会用食物诱骗小猫，网上这些招数一直在把你当小猫诱骗和洗脑" class="headerlink" title="学习观09：以为只有你会用食物诱骗小猫，网上这些招数一直在把你当小猫诱骗和洗脑"></a><span id="header9">学习观09：以为只有你会用食物诱骗小猫，网上这些招数一直在把你当小猫诱骗和洗脑</span></h1><p>我想大多数人在听到“了解自我”时都会以为我指的是什么精神上的了解，然而我是一个纯粹的务实主义者。我所指的实际上是生理上的了解。请记住学习观 09 里的这个生理模型，因为以后将会提到，它可以被你用来压制其他欲望，克服拖延症。</p>
<p>此文还有另一个标题：我们亲手毁掉了孩子与生俱来的强大学习能力，却反过来责问孩子：你为什么不喜欢学习。</p>
<p>正如我们会用食物引诱宠物一样，网络也一直在用人性来引诱大众。权利、情色、金钱等这类引诱，我们早已习以为常。可为什么我们对有些并非这类低俗引诱的内容仍然难以抗拒？这里到底利用了人类的哪个生理模型？</p>
<h2 id="好奇心的作用"><a href="#好奇心的作用" class="headerlink" title="好奇心的作用"></a>好奇心的作用</h2><p>人们现在都意识到好奇心的重要性，但却发现，随着年龄的增长，好奇心会慢慢消退，不仅如此，学习能力也不如以前。</p>
<p>于是人们得出一个结论：好奇心和学习能力会随年龄增长而下降，很多人因此觉得自己已经来不及学习，这辈子就认命了。这是多数人对好奇心的认识。</p>
<p>然而这个认识却完全错了，因为不管年龄是否与好奇心相关，都会随着时间而增长，无法认为年龄就是影响好奇心的因素。</p>
<p>事实上，好奇心和食欲、性欲一样，是人类的基本欲望。和好奇心相关的大脑区域就是负责奖励机制的伏膈核，也正是对成瘾行为起重要作用的区域。因为我们不得不依赖自己的模型而存活，所以一定会想尽办法的去建立模型（学习），而好奇心便是演化出来促进学习的奖励机制，每个人生下来就是一部终身好奇的学习机器。</p>
<p>但这却和我们的普遍认知不符。因为很多家长费劲了脑汁也无法让孩子喜欢“学习”，有些人也觉得自己根本不爱“学习”。怎么能说好奇心和食欲一样是人类的基本欲望呢？</p>
<p>事实上，我们不是不喜欢学习，而是不喜欢不符合大脑特点的记忆。当我们对食物饥饿时，大脑会产生进食欲，会无法抑制的想要进食。学习和进食一样，当我们对知识饥饿时，大脑会产生求知欲，也会无法抑制的想要学习。</p>
<p>但多数人的“学习”并不是这样。他只是知道自己需要学习，却没有产生过好奇心，最终变成了强迫自己去记忆知识，又怎么可能会喜欢呢？</p>
<h2 id="好奇心的对象"><a href="#好奇心的对象" class="headerlink" title="好奇心的对象"></a>好奇心的对象</h2><p>既然好奇心是促进学习的关键，那么如何调动自己的好奇心呢？回答这个问题首先要搞清楚的就是：我们到底对什么好奇？</p>
<p>好奇心的目的是为了构建知识，让我们可以适应当前环境，得以生存，因此我们对知识好奇。然而最为关键的一点是，我们无法通过记忆知识来学习知识，只能通过归纳例子来构建知识，因此我们更为好奇的内容其实是例子。而在生活中的例子正是故事。</p>
<p>这就解释了为什么我们如此喜欢故事，为什么大多数的演讲都是以一个故事作为开始，为什么春秋战国时期的纵横家都是以讲故事的方式来游说君主。因为一个故事可以瞬间让大脑清楚知识的输入和输出。</p>
<p>而例子是从输入到输出的一个具体变换，缺一不可，所以当只有输入或输出时，好奇心会使我们的大脑急切的想要知道另一个，帮助自己搜集完整的例子。</p>
<p>现在我们来回答网络标题是如何利用好奇心来引诱大众的。大致可分为例子残缺和知识白送两大类。</p>
<ol>
<li>例子残缺：</li>
</ol>
<p>例子残缺的特点是：标题只给你输入或输出，但绝不完整的提供二者，让你的好奇心来点击内容。</p>
<pre><code>1.1. 不提供输入（“普及类”居多）

    例1：网上这些招数一直在把你当小猫诱骗

    （招数X-&gt;诱惑的）

    例2：这个东西不能乱玩，否则后悔来不及

    （东西X-&gt;危险的）

    例3：男人不育居然是这些习惯导致的

    （习惯X-&gt;使不育）

    例4：长寿秘诀破解，最关键的因素居然是这个…

    （因素X-&gt;使长寿）

    例5：赵丽颖和冯绍峰结婚背后的真相竟然是

    （原因X-&gt;使结婚）

1.2. 不提供输出（新闻类居多）：

    例1：新手司机误把油门当刹车 结果…

    （错踩油门-&gt;后果Y）

    例2：世界上最不合拍的两个星座在一起了

    （不合拍在一起-&gt;后果Y）

    例3：那些辞职去考研的人，后来怎么样了？

    （辞职考研-&gt;结果Y）

    例4：如果世界上只剩一个女人，将会发生什么？

    (极端情况-&gt;结果Y)

    例5：面试官问期望的薪酬是在问什么？

    (表面问题-&gt;实际问题Y)
</code></pre><ol>
<li>知识白送：</li>
</ol>
<p>而白送的知识分为两种，正是《知识的分类（下）》里介绍的回归知识（程序性知识）与分类知识（陈述性知识）。</p>
<pre><code>2.1. 对分类知识，我们好奇的是“判断依据”

    例1：开车遇到这种情况，赶紧加速离开

    （你-&gt;赶紧离开，情况依据B）

    例2：一个男生，真正爱你的八种表现

    （男生-&gt;爱你，表现依据B）

    例3：HR教你如何判断一个公司的好坏

    （公司-&gt;好or坏，判断依据B）

    例4：没有感冒却长期咳嗽有这些症状快就医

    （你-&gt;有病，症状依据B）

    例5：越没本事的男人，爱说下面四句话

    （男人-&gt;没本事，言语依据B）

2.2. 对回归知识，我们好奇的是“如何做到”

    例1：6 分钟视频教你如何成为爱情高手

    （方法P）

    例2：教你怎么涂粉底液三个小诀窍摆脱墙壁脸

    （诀窍P）

    例3：学会这个技巧，关键时刻能救你一命

    （技巧P）

    例4：像有钱人一样思考15个秘诀让你不知不觉发财

    （秘诀P）

    例5：学会这一招，葱蒜姜放半年都不会坏

    （花招P）
</code></pre><p>从上面的例子你会发现，只要产生了好奇心，都难以抵挡这种诱惑，也就是说没有一个人是不喜欢学习的。所以家长们在教育孩子的时候请不要对他的好奇心不耐烦，因为我们当代的教育系统已经是好奇心的最强杀手了。</p>
<ol>
<li>自我应用</li>
</ol>
<p>然而我们最感兴趣的是如何将好奇心应用到我们日常学习之中。关于这一点，我其实在《学习观5.5》中已经分享出来了。正是画思维导图时的自我提问。</p>
<p>有些人看完《学习观5.5》后觉得还是不会画思维导图。然而思维导图是用来拆分要学习的新知识。每次我们都需要重新思考新知识与子知识之间的关系，这些人真的有思考过吗？真的有进行那些自我提问吗？那些自我提问不仅可以帮助我们拆分知识，更会调动我们的好奇心来促进我们的学习。</p>
<h2 id="好奇心的敌人"><a href="#好奇心的敌人" class="headerlink" title="好奇心的敌人"></a>好奇心的敌人</h2><p>但知道了如何应用好奇心后还有一个关键问题没有回答那就是为什么随着年龄的增长自己的好奇心会下降呢？实际上，阻碍我们好奇心的根本不是年龄，恰恰是我们已有的知识。</p>
<ol>
<li>固执心理</li>
</ol>
<p>因为我们依赖模型而活，因此不仅进化出了用于构建模型的好奇心，也同时进化出了用于保护已有模型的固执心。 倘若什么都信，那就会像受骗的老人一样把钱财交出去。 而固执心有两个常见的表现：</p>
<p>1.1. 拒绝新知识</p>
<p>看到新知识后，会说服自己，这个没用，然后心安理得的不学习它。</p>
<p>1.2. 归为旧已知</p>
<p>看到新知识后，会说，这个不就是那个啥吗？我知道。然后忽略了细小差异。比如用中文发音系统来学习英语。</p>
<p>小孩子由于没有任何知识，固执心弱，所以好奇心强，学习能力强；但抗欺骗力弱。 而成人由于已经构建知识，固执心强，所以好奇心弱，学习能力差；更新能力也弱。 </p>
<p>固执心是进化出的保护机制，没什么不对的，这样反而不容易陷入知识恐慌。 但你又想要让自己有创造力，就唯有保持初学者那种一无所知的心态，才会激起好奇心，才会保持对知识的饥渴。</p>
<p>这正是乔布斯留给新生的 stay hungry, stay foolish（乔布斯更可能传达的是另一种意思）。</p>
<ol>
<li>权威思维</li>
</ol>
<p>除了固执心理外，好奇心还有另一个敌人，是权威思维。这种思维是不独立思考，只相信权威，认为任何问题都有一个官方的、唯一的知识。看到新知识后，就相信一定有一本教科书罗列了所有内容。而这是当代教育环境最容易让人们形成的一种学生思维。</p>
<p>有些人对于只要加上科学二字的内容就无条件相信。当意识到自己被骗后，就得出了科学不可相信的结论。但上面所指的“科学”是该人通过网络所形成的对科学的认识，而真正的科学是对知识不断验证和更新的一个过程。</p>
<p>地心说在当时可以满足所有的观测，是最科学的知识，而观测到更多新数据后，科学又将知识更新为了日心说，但当发现太阳也不是宇宙中心后，就又再次更新了知识。正是科学这种以验证为标准，又不断完善的精神才被人们所推崇。</p>
<p>所以我并不想要教任何现有知识，因为知识是无限的，当下的知识在未来一定会更新,我根本教不完，而当我死了之后呢？我做的所有努力都不会有任何意义。所以我想传达的是如何学习知识，如何判断知识。</p>
<p>以上是我个人对我所看到的数据所总结的知识，那是否就一定是完善的呢？当然不是，上面就没有提影响好奇心的另一个重要因素：要与自己相关，毕竟我们只会去构建与自己相关的知识。然而即使加入了“与自己相关的”这个要素，我们对好奇心的认识仍然需要不断更新完善的。</p>
<h2 id="广告观念植入"><a href="#广告观念植入" class="headerlink" title="广告观念植入"></a>广告观念植入</h2><p>了解了这些之后，我们再来分析，现在的广告是如何进行观念植入的。</p>
<ul>
<li>钻石：钻石恒久远，一颗永流传：钻石是保值的， 结婚应该送钻戒</li>
<li>哈根达斯：爱她，就请她吃哈根达斯：请她吃哈根达斯是爱它的表现</li>
<li>脑白金：今年过节不收礼，收礼只收脑白金：送礼应该送脑白金</li>
<li>王老吉：怕上火喝王老吉：王老吉防止上火</li>
</ul>
<p>没错，不管多么可笑，都是包装成观念（知识）来进行宣传。因为大脑所饥渴的是知识，而不是信息。</p>
<p>真的以为现在很多公众号是免费给你科普知识用的吗？10个知识里，9个真知识用来打开你的好奇心和戒备心，随后只需要掺入一个假观念就可以达成商业目的。 </p>
<h2 id="洗脑传销"><a href="#洗脑传销" class="headerlink" title="洗脑传销"></a>洗脑传销</h2><p>我们再来说说洗脑，还记得上个视频末尾盗梦空间的片段吗？</p>
<p>洗脑的基本操作就是利用人性的弱点（好奇、亲情、恐惧、贪念等）在目标的脑中通过例子构建一个观念模型。由于决策是自己的模型所产生的，所以不会怀疑自己被欺骗。不仅如此，一旦观念建成后，不管其多么荒谬，固执心就会保护所构建的观念。所以越是告诉被洗脑的人传销是骗人的，他越是加强保护心理。</p>
<h2 id="权健帝国"><a href="#权健帝国" class="headerlink" title="权健帝国"></a>权健帝国</h2><p>最近刚爆出来的权健就利用了多种手段。</p>
<ol>
<li>权威思维：</li>
</ol>
<p>号称权健是传承和弘扬中医文化的自然医学。</p>
<ol>
<li>获新生针对与癌症作斗争的人们（利用救人心切）：</li>
</ol>
<p>通过虚假案例：一个信息挽救一个鲜活的生命！内蒙古4岁女孩小周洋患癌症在权健自然医学重获新生，来构建权健可以治癌的观念。</p>
<ol>
<li>针对想一夜暴富的人们（利用贪念）：</li>
</ol>
<p>通过特别案例，来灌输暴富理论，构建无关因果，比如说，可以把马云的案例包装成“马云就是因为相信别人不相信的才取得成功”，然后让其相信组织编的故事。</p>
<h2 id="……"><a href="#……" class="headerlink" title="……"></a>……</h2><p>我是21日发布的学习观08，就是为警告人们观念植入问题而做的铺垫，希望不要再发生魏则西事件，而丁香医生25日就爆出了权健的黑幕《百亿保健帝国权健，和它阴影下的中国家庭》。你可能没有办法体会我看到文章后的心情。</p>
<p>我们从小接受的就是科学教育，本该最有能力辨别知识的好坏，本该用知识保护那些人们，可他们宁可相信谣言也不选择科学。难道我们自身不应该反思吗。我们总是把科学知识包装成高不可攀的样子。连传销都知道通过例子来构建模型，我们对学生的教育却变成了机械记忆。</p>
<p>还有人说，这就是现在的自然选择，淘汰掉一些蠢人。然而淘汰掉的是什么？仅仅是信息受限的朴实人民。留下来的是什么？那些快被涨破了的水蛭们吗？</p>
<p>蚁力神，三鹿奶粉、魏则西、权健，这些事件都让人义愤填膺，然而每个事件的背后都不是一个人，而是一个多方利益纠缠的共生体。权健事件的调查接下来一定会受到各种阻碍，一瓶果汁卖一千块，想象权健多有钱，有多少人在权健工作，又有多少人吃着沾血的馒头。</p>
<p>即使权健倒台了，未来也会有无数个这样的共生体。当利益链条错综复杂时，就再也无法期望政策可以保护我们。</p>
<p>但这样的共生体却是由我们的无知喂养起来的，能够保护我们的，唯有提升我们每个人的认知、学习和独立思考能力。</p>
<h1 id="学习观10：老师，我没有传纸条作弊，我在学习信息论"><a href="#学习观10：老师，我没有传纸条作弊，我在学习信息论" class="headerlink" title="学习观10：老师，我没有传纸条作弊，我在学习信息论"></a><span id="header10">学习观10：老师，我没有传纸条作弊，我在学习信息论</span></h1><p>这个<a href="https://v.qq.com/x/page/b083052co7d.html" target="_blank" rel="noopener">视频</a>只定性的讲解了什么是信息：用一句话来说：</p>
<p>信息是从多个可能状态中确定实际状态的代价</p>
<p>由于熵和信息的概念极其的重要，所以我很罗嗦的加了很多例子。</p>
<p>下面是接下来的视预告：</p>
<ul>
<li>如何量化信息的，熵公式是怎么来的</li>
<li>热力学熵与信息熵的差异</li>
<li>怎么获得信息？难道只有观察这一项吗？</li>
<li>区分数据、信息、知识</li>
<li>知识的代价</li>
</ul>
<p>很多的教程，甚至教材都把熵介绍为：熵是描述无序或混乱的。</p>
<p>这种描述对于没有物理背景和数学背景的人极其具有误导性。因为日常生活中的无序（disorder）和混乱（chaos）两词多用于描述一个事物（微观态）的具体排列方式，会错误的让新人认为排列的越乱，熵就大，偏离了这个伟大的概念的精髓。现在科学界都开始抛弃这种描述。</p>
<p>这也就是为什么下图在解释熵时并不妥的原因。当然已经明白熵的人，可以强行的增加额外信息来解释这张图，</p>
<p><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/3.png" alt></p>
<p>但对于新人而言，下图图来理解热力学熵的效果更好：灰色表示宏观态（共五种），带数字的球表示微观态（每种宏观态对应不同数量的微观态）。中间这个两球都在中间的宏观态的熵最大（因为这个宏观态是最难确定对应 6 个微观态中哪一个的）。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/4.png" alt></p>
<p>这段内容不做要求，不用掌握</p>
<p>在物理层面，我们观察到的物体都是大量无规则运动粒子在宏观层面的表现，比如温度。由于中间这种宏观态的微观态个数最多，所以从概率上会更有可能向这种状态发展，而由熵推出的这种发展趋势便是热力学第二定律。</p>
<p>当一个物体的宏观态改变时，对我们而言才算是状态的改变，而物体的能量分布当达到中间的宏观态时，就很难再改变它的宏观状态，它自发做功的能力就最弱。</p>
<p>这里直接以 4 格视频讲解的方式放出视频，作为参考而用。</p>
<p>如果要看仔细看观看这个<a href="https://v.qq.com/x/page/l083028dbgl.html" target="_blank" rel="noopener">视频</a>的话，请观看4遍：</p>
<p>第一遍：注意力集中在动画版块</p>
<p>第二遍：注意力集中在例子板块</p>
<p>第三遍：注意力集中在知识板块</p>
<p>第四遍：注意力集中在导图板块</p>
<h2 id="为什么要讲信息"><a href="#为什么要讲信息" class="headerlink" title="为什么要讲信息"></a>为什么要讲信息</h2><p>很多人问我，学习观的理论是从哪里来的，有什么书籍可以参考吗？有意思的是，当我从底层开始讲解时，他们又觉得太难了。</p>
<p>学习观的核心就是信息。如果想要深刻理解，就不得不更正自己对信息的错误认识。理解信息不单是有益于学习这么简单，生命本身以及整个人类社会的发展的底层都离不开两个概念：信息与能量。这两个概念可以让个体看清发展的走向。</p>
<p>当某人能够区分信息和知识，能够从数据中认得信息时，该人看待问题的方式会完全不一样。不管是在数学、编程、写作、演讲、听讲上都可以发生质变。</p>
<p>我知道很多人都是渴望知识的，经常去听课，花钱知识付费。但就拿罗振宇的 4 小时跨年演讲来说，有多少人听完后觉得好有道理，可事后又觉得什么都没学会。拿我的视频来说，有多少人听完后觉得好有意思，可事后又觉得抓不住核心。但如果理解了信息和知识后，在听讲的过程中就可以在脑中梳理出思维导图，明白哪些话是知识，哪些话是信息，哪些话是说话技巧，哪些话是废话，不必再漫无目的的记笔记，也不必再辛苦的记忆所有内容。</p>
<p>学习观10里所讲解的信息是以定性的方式来讲述的，恐怕很多人会有不少的疑惑。这些疑惑会在下一个定量讲解信息的视频里得以解答。</p>
<p>新模式对应着《学习观5》中所讲的学习方法： </p>
<ol>
<li><p>明确任务输入输出 </p>
</li>
<li><p>将信息压缩成知识 </p>
</li>
<li><p>例子重塑大脑连接 </p>
</li>
<li><p>拆分知识分而治之</p>
</li>
</ol>
<ul>
<li>动画区：辅助文字来理清事物关系</li>
<li>知识区：标明输入(变量)、输出(变量)、知识描述</li>
<li>例子区：标明输入(常量)、输出(常量)、应用知识 。不同例子用于理解知识区里所写的知识。需要将例子与知识描述对照着思考。</li>
<li>导图区：用于帮助定位当前时刻所讲解内容。展示拆分知识，当通过例子理解知识后，就变成关键词。</li>
</ul>
<p>此刻的你，接受着我所产生的声音、文字、画面，但你知道我所传递的内容不是这些媒介本身，而是信息。我们时时刻刻都在获取信息，讨论信息。然而到底什么是信息？</p>
<h2 id="熵与信息的定义"><a href="#熵与信息的定义" class="headerlink" title="熵与信息的定义"></a>熵与信息的定义</h2><p>定义： 当一件事情有多种可能情况时，这件事情对某人而言具体是哪种情况的不确定性叫做熵，而能够消除该人对这件事情不确定性的事物叫做信息。</p>
<p>熵和信息的关系：数量相等，意义相反。获取信息 = 消除熵。 </p>
<blockquote>
<p>例1：当小明不会某道数学选择题时（正确答案是C）。<br>    正确答案（宏观态）是 A，B，C，D 哪个选项（4 个微观态）的不确定性就是熵。 </p>
</blockquote>
<p>宏观态与微观态：这里，正确答案也叫宏观态，而每个可能选项叫做微观态。宏观态是不考虑内部细节的状态，而微观态是考虑具体细节的状态。</p>
<blockquote>
<p>例1：生物是宏观态，动物和植物都是生物这个宏观态的一种微观态。</p>
</blockquote>
<p>熵与信息的大小： 熵在 A，B，C，D 所有可能情况（宏观态）都是等概率（1/4）时最大，在确定 C 了（实际情况）是 100% 后最小。 </p>
<p>信息的种类：能够消除不确定性的信息有三种类型，它们本质都是正确的调整每个可能情况（微观态）的概率。</p>
<p>第一种：能正确的调整某件事情的可能情况（微观态）的概率。</p>
<blockquote>
<p>例1：小红告诉小明 “有一半可能性是 C 选项”。<br>    这句话帮助小明将 C 选项的概率调整到了 50%，就提供了信息（0.21 bits）。</p>
</blockquote>
<pre><code>括号里是提供的信息量，将在下个视频中讲解如何计算，这里只要能定性的判断什么是信息即可。
</code></pre><p>第二种：能正确的排除某件事情的干扰情况。 </p>
<blockquote>
<p>例1：小红告诉小明 “D 选项是错的”。<br>    这句话帮助小明将 D 选项的概率调整到了 0%（0.415 bits），这时小明只需要从 3 种情况里确定实际情况即可。 </p>
<p>例2：在此基础上，再告诉小明 “A选项是错的”。<br>    这句话帮助小明将 A 选项的概率调整到了 0%（0.585 bits），这时小明只需要从 2 种情况里确定实际情况即可。 </p>
<p>例3：在此基础上，再告诉小明 “B选项是错的”。<br>    这句话帮助小明将 B 选项的概率调整到了 0%（1 bits），只剩下一种情况了，不确定性（熵）完全消除。</p>
</blockquote>
<p>第三种：能够直接确定某件事情的实际情况。</p>
<blockquote>
<p>例1：小红告诉小明 “正确答案是 C”。<br>是将 C 选项的概率调整到了100%（2 bits），这句话帮助小明从 4 种等概率情况里确定了实际情况。 </p>
</blockquote>
<p>非信息：未能消除不确定性不叫信息。</p>
<blockquote>
<p>例1：但小红告诉小明 “肯定是ABCD里的一项”（0 bit）。</p>
</blockquote>
<p>这句话就没有帮助小明消除任何不确定性，信息为 0 。 </p>
<p>在这种定义下，并没有假信息一说。 因为只有能够消除某人对某件事情的不确定性的事物才是信息，因此小红告诉小明“正确答案是 D”提供的信息是 0。</p>
<p>那些不能够消除某人对某件事情不确定性的事物被称为数据或噪音。</p>
<p>噪音是干扰某人获得信息的事物。而数据是噪音与信息的混合，需要用知识将其分离。</p>
<h2 id="熵与信息的性质"><a href="#熵与信息的性质" class="headerlink" title="熵与信息的性质"></a>熵与信息的性质</h2><p>媒介无关：同一个观察者，对同一件事情接受到的信息与用于传递信息的信号形式无关。 </p>
<blockquote>
<p>例1：<br>视觉信号：小红将写有 C 的 传纸条递给小明，提供 2 bits。<br>听觉信号：小红告诉小明答案是 C，提供 2 bits。<br>触觉信号：小红蹬小明椅子 3 次，提供 2 bits。</p>
</blockquote>
<p>相对观察者：接收到的信息是相对于观察者已经对该件事情的实际了解程度而言的。</p>
<blockquote>
<p>例1：“明天的太阳东边升起” 这句话，<br>对知道的人而言，提供了 0 bit 信息。<br>对知道东或西升起的人而言，提供了1 bit 信息。<br>对觉得东南西北都有可能的人而言，提供了 2 bits 信息。</p>
<p>例2：<br>小红：会这道题，不管告不告诉小红正确答案是 C，小红对这道题的熵都为 0 bit。 因为观察者已经拥有这件事情的所有信息，不确定性从最初就不存在。<br>小明：不会这道题，熵为 2 bits 因为观察者没有关于这件事情的任何信息，不确定性最大，他需要从 4 种等概率情况里确定实际情况。<br>小虎：知道 D 是错的，熵为 1.58 bits 因为观察者拥有关于这件事情的部分信息，不确定性略小，他需要从 3 种等概率情况里确定实际情况。</p>
</blockquote>
<p>客观物理量：虽然信息是相对于观察者而言的，但信息是客观的物理量，不随观察者的主观意识改变。只有确定了真正的实际情况时才是信息。</p>
<blockquote>
<p>例1：小虎认为 C 是错的，熵就不会降低。<br>因为这个“主观认为 C 是错的”并没有实际帮助小虎消除事情的不确定性。</p>
</blockquote>
<p>相对于事件：信息（熵）还是相对于某件事情而言的。</p>
<blockquote>
<p>例1：<br>小明对“正确答案是ABCD 哪个选项”的熵是 2 bits。<br>小明对“正确答案属于 AB，还是属于DC ”的熵是 1 bit。</p>
<p>例2：<br>小红告诉小明“正确答案是 C”为小明确定正确答案提供了 2 bits 信息。<br>若想告诉别人，小红对小明说了什么，这件事情的熵不再是 2 bits，观察者也不再是小明，而是除小明和小红之外的观察者。</p>
<p>例3：<br>很多人在思考问题的时候，会不经意间切换所要思考的事情， 或者根本不知道自己要思考什么事情，这会造成怎么想都想不明白。</p>
</blockquote>
<p>概率和熵的区别：概率是某件事情（宏观态）某个可能情况（微观态）的确定性，而熵是某件事情（宏观态）到底是哪个情况（微观态）的不确定性。 </p>
<p>概率的输入是常量（微观态），熵的输入是变量（宏观态）。</p>
<p>用一句话来说：</p>
<p>信息是从多个可能状态中确定实际状态所需的物理量</p>
<h1 id="学习观11：信息为什么还有单位，熵为什么用-log-来计算"><a href="#学习观11：信息为什么还有单位，熵为什么用-log-来计算" class="headerlink" title="学习观11：信息为什么还有单位，熵为什么用 log 来计算"></a><span id="header11">学习观11：信息为什么还有单位，熵为什么用 log 来计算</span></h1><p>学习观10 里大家一定会有不少疑惑，其中之一就是那些信息到底是怎么计算出来的。在该视频中得以解答。</p>
<p>不过最少还仍然有两个问题：<br>为什么网上有那么多说”熵是描述混乱或无序的？</p>
<p>为什么做题消耗了那么多能量，小明最后只获得了 2 bits 的信息?</p>
<p>第一个问题：牵扯到 热力学熵的一种应用，然而不管考虑的是不是热力学熵，这种描述都是非常具有误导性的。因为热力学熵就是信息熵的特例，如果不能想明白二者的关系，意味着还没搞明白。接下来的视频会详细解释。</p>
<p>题外话，很多人会觉得这个概念非常难的原因是因为它们反常识，违背你日常生活经验所构建出的模型。多数人都会根据自己已有的经验进行判断，从而产生抵触。但是不要认为自己很笨，因为信息和热力学熵的关系困扰科学家们都足足一百年之久。</p>
<p>第二个问题：牵扯到 信息与知识的关系。是最主要想讲的内容</p>
<h2 id="不科学啊"><a href="#不科学啊" class="headerlink" title="不科学啊"></a>不科学啊</h2><p>上个<a href="https://v.qq.com/x/page/r0831tx2ow4.html" target="_blank" rel="noopener">视频</a>学习了如何定性的判断什么是熵和信息，其中有个例子：</p>
<blockquote>
<p>当小明不知道选择题是 ABCD 哪个选项时：<br>小红告小明 “D 选项是错的”，提供了 0.415 bits 的信息<br>再告诉小明 “A选项是错的”，提供了 0.585 bits 的信息<br>再告诉小明 “B选项是错的”，提供了 1 bit 的信息</p>
</blockquote>
<p>可明明每次都是告诉他一个错误选项，为什么三次提供给小明的信息量却都不相同？ 信息量到底是怎么计算的？信息为什么还有单位？</p>
<h2 id="以此类推"><a href="#以此类推" class="headerlink" title="以此类推"></a>以此类推</h2><p>回想一下，什么东西有单位？质量，温度等物理量。</p>
<p>没错，信息也是一个物理量。 要测量这个物理量，不妨回想一下我们是怎么测量质量的，“千克”最初又是怎么被定义出来的？</p>
<p>其实最初我们并不知道千克的质量，而是选择了一个参照物，把这个物体的质量就称为千克。当想要测量其他物体的质量时，就看这个物体的质量相当于多少个参照物体的质量。这里的”多少个“便是千克。如果换另一个参照物体，那么单位就会变化，比如斤。 </p>
<p>测量信息是也是一样，既然信息消除的是不确定性，那么就选择另一个事件的不确定性作为参照事件。 当想要测量其他事件的信息时，就看看待测事件的不确定性相当于 多少个 参照事件的不确定性。这里的”多少个“便是信息量。</p>
<p>当选择的参照事物是像抛硬币这样，只有 2 种等概率情况的事件时，测得的信息量的单位就被称为比特。</p>
<blockquote>
<p>有 e (自然底数) 种等概率情况的事件时，测得的信息量的单位就被称为 nat。<br>有 10 种等概率情况的事件时，测得的信息量的单位就被称为 ban。</p>
</blockquote>
<h2 id="哪里不对"><a href="#哪里不对" class="headerlink" title="哪里不对"></a>哪里不对</h2><p>然而测量质量时，我们是用 待测物体的质量 除以 参照物体的质量。 因为 待测物体的质量 m 等于 参照物体的质量 K 和 乘以 参照物体个数 n ，所以当知道 m 要求 n 时，我们用乘法的反函数，即除法来计算。 </p>
<p>可是测量信息时，却不能用除法，因为 抛掷 3 个硬币能够产生的等可能结果并非 3*2=6，而是 2^3=8 种。也就是说 待测不确定情况的个数 m 是由 参照不确定情况的个数 K 的参照物个数 n 是指数关系进行累积的。</p>
<p>所以当知道可能情况的个数 m，想求这些情况相当于多少个 n 参照事件所产生的时，我们用指数运算的反函数，即对数运算来计算。</p>
<p>这样 8 个不确定情况就相当于 3 个硬币抛 出的结果， 4 个不确定情况就相当于 抛 2 个硬币得出的，故小明对答案是ABCD里哪一选项的不确定性： $log_2 4 = 2 bits$</p>
<h2 id="更普遍性"><a href="#更普遍性" class="headerlink" title="更普遍性"></a>更普遍性</h2><p>但这里有个前提，那就是被测事件的所有可能情况都必须是等概率才行，因为参照事件本身的两种情况就是等概率。 </p>
<p>可是当不知道答案的小明被告知 “有一半可能性是 C 选项” 时，各个情况的概率就不一样了，这时该如何计算熵呢？</p>
<p>答案是分别测量待测事件每种可能情况的信息量后，乘以它们各自的发生概率再相加即可。</p>
<p>不过，怎么测量每种情况的信息量呢？ 怎么知道概率为 1/6 的情况的不确定性相当于抛掷多少次硬币所产生的不确定性呢？ </p>
<p>我们确实没法再用 log m 这个公式了，但我们知道 1% 会发生的情况，相当于从100个等概率情况中确定实际情况， 概率的倒数等于等概率情况的个数，m = 1/p。 </p>
<p>用 1/p 替换 m 后，我们就可以计算每种情况的信息量了， 再用每个情况的信息量乘以对应发生的概率，再相加后，就能算总信息量了。 </p>
<p><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/5.png" alt></p>
<blockquote>
<p>i: 用于指定第几个具体情况<br>$p_i$：第 i 个具体情况的概率<br>Σ：表示 i 个情况计算出的 p log p 累加起来<br>之所以前面有负号是因为 </p>
<p>log 1/p = log p^-1 = -log p<br>概率的倒数变成了负号被拿到前面了</p>
</blockquote>
<p>回到例子中，不知道任何答案的小明对 答案的不确定性是：</p>
<p>$log_2 \frac{1}{p} = log_2 \frac{1}{\frac{1}{4}} = 2$</p>
<p>当被告知 C 有 50%概率是正确答案时，小明对答案的不确定性是：</p>
<blockquote>
<script type="math/tex; mode=display">\frac{1}{6} * log_2 (\frac{1}{6})^{-1} + \frac{1}{6} * log_2 (\frac{1}{6})^{-1} + \frac{1}{2} * log_2 (\frac{1}{2})^{-1} + \frac{1}{6} * log_2 (\frac{1}{6})^{-1}</script><script type="math/tex; mode=display">= \frac{1}{6} * log_2 6 + \frac{1}{6} * log_2 6 + \frac{1}{2} * log_2 2 + \frac{1}{6} * log_2 6</script><script type="math/tex; mode=display">=1.79</script></blockquote>
<p>2-1.79=0.21 bits， 也就是小红告诉小明”C 的概率是 50%“时提供的信息。</p>
<p>可以思考一下：</p>
<ul>
<li>为什么三次提供给小明的信息量却都不相同？</li>
<li>学习观10 里的那些信息量是怎么计算的，都正确吗？</li>
</ul>
<p>再次描述一遍信息与熵的关系：</p>
<p>某人对某物的熵（不确定性）相当于水杯，信息相当于水，新填充的水（新信息）要看杯子原来有多少水（已有信息），但水无法溢过杯子（能接收到的最大信息就是该人目前的剩余不确定性）。</p>
<h2 id="我不相信天才，可我找不出其他词来形容他们"><a href="#我不相信天才，可我找不出其他词来形容他们" class="headerlink" title="我不相信天才，可我找不出其他词来形容他们"></a>我不相信天才，可我找不出其他词来形容他们</h2><p>作者有很多偶像，以往的视频里多次提到过理查德·费曼（Richard Phillips Feynman），而这几期信息的视频，希望大家记住这位伟大的数学家：克劳德·香农（Claude Elwood Shannon），信息公式的发明者。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/6.png" alt></p>
<h1 id="学习观12：理论上人类可以永生，那导致现实死亡的原因是？"><a href="#学习观12：理论上人类可以永生，那导致现实死亡的原因是？" class="headerlink" title="学习观12：理论上人类可以永生，那导致现实死亡的原因是？"></a><span id="header12">学习观12：理论上人类可以永生，那导致现实死亡的原因是？</span></h1><p>生命是什么？生命的敌人是什么？人类的未来会怎样？为什么时间有方向？若想造出人工智能生命，需要具备什么能力？信息究竟有多重要？</p>
<p>标题有点噱头，但请仔细体会“信息”在其中充当的角色。</p>
<p>若一个理论没有用处，不管它多么高雅，我也不会去讲它。而我也不想剧透，不想苍白的告诉大家某个东西有多么重要。但随着视频的制作，但随着视频的制作，我会慢慢给你很多这些内容在生活中的应用，相信你一定会惊呼。</p>
<p>末尾的问题在下一期讨论，将会更加深刻的体会到“信息是什么”。</p>
<p>其实你现在就可以重新思考一下，为什么信息这么重要：联想彩票，联想战争，联想经济</p>
<p>我们知道能量是守恒的，不能被创造或摧毁。可既然这样，为什么我们不能像能量一样永恒，反而注定要迈向死亡呢？</p>
<h2 id="时间之矢"><a href="#时间之矢" class="headerlink" title="时间之矢"></a>时间之矢</h2><p>想象一堆积木，每个积木相当于一个粒子，而由积木搭成的各种形状就是我们，纵然每个积木都可以永恒，可积木所搭建的形状一旦改变，也就意味着我们的死亡。 </p>
<p>而这些积木又在时刻运动着，假设 3 个积木在左，1 个积木在右的形状（只考虑积木在做在右的个数）就是我们，积木的无规则运动会使形状如何改变呢？ </p>
<p>这堆积木一共可以形成 5 种宏观态（形状），每种宏观态对应的微观态的数量并不相同。 这里宏观态是指不考虑积木的差异，只考虑整体的状态，而微观态则是考虑积木差异的状态。 </p>
<p>可以看出左右积木数量相等的宏观态的可能性最大，而我们自身宏观形态被维持的概率只有 1/4，不过当积木的数量足够多时，我们自身形状被改变的趋势几乎就是必然的了。 </p>
<p>这种，自发向对应微观态数量最多的宏观态改变的现象就是热力学第二定律。又因为这种改变无法自行逆转，故又称时间之矢。而这种自发改变的趋势，就是我们无法像能量一样永恒的原因。</p>
<h2 id="熵增定律"><a href="#熵增定律" class="headerlink" title="熵增定律"></a>熵增定律</h2><p>明白学习观11所介绍的信息熵之后，就可以用熵来理解这种现象。</p>
<p>所有积木都在左侧的宏观态只对应 1 种微观态，没有任何不确定性，我们称这个宏观态的熵为 0；而对应微观态数量越多，宏观态具体是哪一种微状态的不确定性越大，我们就称这个宏观态的熵越高。因此热力学第二定律也叫熵增定律，意为：孤立系统的熵只增不减 宏观态达到熵最大状态（两边积木数量相等）时也叫热平衡。</p>
<p>熵增定律常被描述为：事物会从有序状态向无序状态发展。</p>
<p>但这并非由什么外力所引起的，房间自发越变越乱的原因单纯只是因为我们把它弄乱（宏观态）的方式比把它弄整齐（宏观态）的方式要多得多。</p>
<p>注意，这里的无序并不是我们日常生活中所理解的房间的排列方式无序，而是房间乱（宏观态）的可能方式有很多（对应的可能微观态数量很多）。</p>
<p>因为积木每种排列方式（微观态）的概率都是相同的，并不会说哪个杂乱的方式更有可能发生；是不同数量的微观态共同定义的宏观态们之间的概率不同。所以学界慢慢抛弃了“熵是描述无序”的这种解释。因为它极具误导性。</p>
<h2 id="能量耗散"><a href="#能量耗散" class="headerlink" title="能量耗散"></a>能量耗散</h2><p>熵增定律同样适用于能量。</p>
<p>虽然能量不会消失，但能量的转移才可以造成改变（做功），而转移需要有能量差。可当一个系统的能量分布像积木分布一样两边都平衡时，也就没有能量可以转移。因此根据做功能力，能量也有高低品质之分，而热力学熵就可用于描述系统自发做功的能力（能量的品质）。</p>
<p>于是就有了克劳修斯（对热力学第二定律）的表述：不可能把热量（能量的一种）从低温物体（低能量）传向高温物体（高能量）而不引起其它变化（能量和物质交换）。</p>
<p>用沙丘来类比的话，沙子（能量）会摊成平缓的沙丘，而不会自己聚成一个高高的砂塔，除非我们消耗能量把它堆砌成沙塔（有外界对它做功），可不久它又会摊成沙丘。</p>
<p>由于对我们而言，宏观态的改变才算改变，因此若把整个宇宙视为一个宏观态，它也会无法避免的朝熵最大的状态发展，当它达到熵最大的状态时，将不会再有任何能量的转移，也不会有任何宏观态的改变。而宇宙的这种状态就被叫做热寂，这是对我们而言，宇宙终结的诅咒。面对我们的难道真的只有终结这一条路吗？</p>
<h2 id="麦克斯韦妖"><a href="#麦克斯韦妖" class="headerlink" title="麦克斯韦妖"></a>麦克斯韦妖</h2><p>为打破这个诅咒，麦克斯韦提出了一种假想：</p>
<p>既然分子在做无规则运动，那想象有一个小妖（麦克斯韦妖）可以感知分子的运动，控制着门的开关，当带有更多能量的分子要运动到左边时就开门，带有较少能量的分子要运动到右边时也开门，其他情况都关门。这样就可以自发的让系统（宏观态）的熵降低，左右两边会产生可用于做功的能量差，那岂不是能造出永动机？</p>
<p>这个假想困扰了科学家一百年，然而麦克斯韦妖最终还是未能打破诅咒，因为人们忽略了一点：感知到的分子运动就是信息。</p>
<p>学习观10中讲过信息和熵的关系，当观察者对某事件一无所知时，信息熵和信息，数量相等，意义相反，消除信息熵等于获取信息；</p>
<p>同时，信息熵和热力学熵二者可以相互转换，1 bit 的信息熵就等于 k ln 2 焦耳/温度的热力学熵。麦克斯韦妖其实是利用信息来降低信息熵从而降低热力学熵。然而信息的获取不是免费的，想要重置（获取或删除） 1 bit 的信息，理论上至少需要 k T ln 2 焦耳的能量（证明很简单，在辅助材料 [1] 中）。</p>
<p>注意到这其中的关系了吗？ 能量差需要低熵，而低熵需要信息，可信息又需要能量差。也就是说，麦克斯韦妖若想用信息获取能量，就不得不先用外界的能量来获得信息。也就不再是孤立系统，依然没有违背热力学第二定律。</p>
<p>科学家一百年都没有想明白其中道理的原因之一是因为我们自身的思考一直在向系统中传递信息，而你就是这个麦克斯韦妖。</p>
<h2 id="什么是生命"><a href="#什么是生命" class="headerlink" title="什么是生命"></a>什么是生命</h2><p>麦克斯韦妖没有打破诅咒，但它却揭示生命的本质。随后由薛定谔在 1943 年提出：生命以负熵为食。</p>
<p>一直以来， 不管从组成，还是从功能的角度，人们都无法找出完美划分生命与非生命的边界。因为组成生命的物质都是非生命，非生命也可以进行物质能量交换，而生命死亡后又会被归为非生命，不过个体虽然会死亡，其遗传物质却能延续，但要说生命就是遗传物质，这好比是说电脑就是硬盘，而要说生命就是遗传物质上的信息，虽然正确，却又没什么用，因为信息可以是任何事情，说生命是信息好比在说生命是任何事情。 </p>
<blockquote>
<p>组成：是否具有有机物，遗传物质等<br>功能：是否可以移动，呼吸，进食，新陈代谢、生长，变异，繁殖等</p>
</blockquote>
<p>然而薛定谔的定义却能将一切都解释的通，生命是一种很特殊的状态维持方式，它并不像金刚石这种本身结构就很稳定，可以被动的维持自身状态，而是主动的在用信息来减熵，从而维持自身状态。</p>
<p>这也就是病毒有意思的地方，在未入侵能为其提供能量支持的宿主细胞之前，它以非生命的形式来维持自身状态，而入侵之后，它又是以生命的形式维持状态。</p>
<p>依照这种定义，不管是否是碳基，是否是天然形成，只要满足用信息来抗衡熵增从而维持自身状态的系统就可被视为生命。</p>
<p>你一定会想，我们岂不是可以用专门处理信息的计算机造出人工智能生命？ 然而关键的问题就在这里，计算机的信息是由人类提供的。如果撤掉人类的所有支持，就好像回到宇宙生命形成之前，计算机又要怎样才能自己获得信息呢？</p>
<h2 id="辅助材料"><a href="#辅助材料" class="headerlink" title="辅助材料"></a>辅助材料</h2><p>想要了解更多，或细节不清楚的可参考这些</p>
<blockquote>
<p>热力学熵<br>Entropy is not disorder by Khutoryansky<br>Why Information is Entropy by Khutoryansky<br>What is entropy? by Jeff Phillips<br>热力学第二定律是什么 by 李永乐<br>什么是热力学第二定律 by 柴知道</p>
<p>信息熵<br>Information entropy by Khan Academy<br>Information Theory A Tutorial Introduction by Stone James [1]<br>Information theory: Entropy by Zinnia Jones<br>How Much Information? by Veritasium</p>
<p>生命是什么<br>《生命的奇迹》BBC 纪录片 Brian Edward Cox<br>《生命的跃升》Nick Lane<br>《自私的基因》Richard Dawkins<br>《生命是什么》Schrödinger</p>
<p>代码<br>github.com/YJango/Learning/blob/master/e12.ipynb</p>
</blockquote>
<h1 id="学习观13：为什么知识不是信息，学习不是记忆"><a href="#学习观13：为什么知识不是信息，学习不是记忆" class="headerlink" title="学习观13：为什么知识不是信息，学习不是记忆"></a><span id="header13">学习观13：为什么知识不是信息，学习不是记忆</span></h1><p>《学习观》的真正内容正式开始</p>
<blockquote>
<p>Stay with me<br>Don’t get lost</p>
</blockquote>
<p>想准确理解其中的内容，建议复习：<br>学习观01<br>学习观02<br>学习观08<br>学习观10<br>学习观11<br>学习观12</p>
<blockquote>
<p>需理解信息是一个不随主观意识所改变的客观物理量。<br>还要区分永恒（不灭）和永生（不会自然死亡）。</p>
</blockquote>
<p>我们知道能量是守恒的，不能被创造或摧毁。可既然这样，为什么我们不能像能量一样永恒，反而注定要迈向死亡呢？</p>
<h2 id="全知便可永恒"><a href="#全知便可永恒" class="headerlink" title="全知便可永恒"></a>全知便可永恒</h2><p>很多人认为信息是虚幻的，不真实的。</p>
<p>因为他知道自己获得的体验是由特定事物所提供的，所以由另一种事物所提供的体验就一定是虚假的。</p>
<p>但同一首乐曲可在不同的手机上播放，并没有人认为另个手机所播放的乐曲就是虚假的。表达信息的媒介是可以变的，不变的却是信息本身，而靠信息来减熵的生命能接收的也只有信息。那么究竟什么该被定义为真实呢？</p>
<p>有一点是无法否认的：我们正处在信息时代。</p>
<ul>
<li>多数人每天起床的第一件事就是看手机。</li>
<li>去新餐厅之前都要查用户评价。</li>
<li>去外国之前都要查交通和酒店。</li>
</ul>
<blockquote>
<p>我们时刻都在为信息付费。<br>因为在生活中，允许我们躲避危险的是信息。<br>在经济中，允许我们赚取差价的是信息。<br>在战争中，真正允许我们获得胜利的还是信息。</p>
</blockquote>
<p>我们以信息为生。</p>
<p>想象一个可以预测未来的人能获得多少资源？我们又要如何才能杀死可以预测未来的人？这就是为什么：</p>
<p>拥有所有信息便可永恒。 </p>
<p>但问题是，生命又要如何获取信息呢？</p>
<h2 id="信息源于知识"><a href="#信息源于知识" class="headerlink" title="信息源于知识"></a>信息源于知识</h2><p>拿麦克斯韦妖来说，它怎么知道该开门还是关门？ </p>
<p>虽然该信息量只有 1 bit，可它没有直接获取该信息的途径 (正是学习观08的主题，也是为什么虚拟显示可以欺骗人类)。 只能根据分子运动信息来间接的获取开关信息。</p>
<p>从一种事件的信息获得所需事件的信息</p>
<p>当高能分子向左运动或低能分子向右运动时就开门，否则关门。</p>
<p>输入事件：分子运动 -&gt; 输出事件：开关 </p>
<ul>
<li>当分子运动信息是高能分子向左运动时，开关信息便是开门， </li>
<li>当分子运动信息是低能分子向右运动时，开关信息便是开门，</li>
<li>当分子运动信息是高能分子向右运动时，开关信息便是关门，</li>
<li>当分子运动信息是低能分子向左运动时，开关信息便是关门，</li>
</ul>
<p>这些确定了事件（宏观态）具体是哪个情况（微状态）的事物就是学习观10中所介绍的信息。</p>
<blockquote>
<p>信息：确定事件具体情况的物理量</p>
</blockquote>
<p>从输入信息到输出信息的对应变化叫做例子（样本）。</p>
<blockquote>
<p>例子：从输入信息到输出信息的变化</p>
</blockquote>
<p>而压缩了两个事件之间所有信息对应变化（例子）的关系便是知识（函数、映射、模型、变换）。</p>
<blockquote>
<p>知识：从输入事件到输出事件的关系（所有例子的压缩）</p>
</blockquote>
<p>这就是为什么学习观02说：</p>
<blockquote>
<p>知识不是信息</p>
</blockquote>
<p>生命是靠知识获得信息。<br>因为系统永远会自发的从低熵宏观态向高熵宏观态变化，即使系统的熵被降低了仍会再次变化。所以生命需要找出能描述这种变化的知识（模型），反复利用该知识获得信息来维持自身状态不变。</p>
<blockquote>
<p>知识需可重复利用</p>
</blockquote>
<p>可生命又要如何找出知识呢？</p>
<h2 id="知识需要学习"><a href="#知识需要学习" class="headerlink" title="知识需要学习"></a>知识需要学习</h2><p>上面的情况一共只有 4 个例子，是不是直接记住就可以了？ </p>
<p>实际上，不只有4个例子。</p>
<p>因为“稍微偏上一点的向左”和“稍微偏下一点的向左”是两种不同的情况。全部例子的个数多到近乎无限。</p>
<p>而信息与熵是硬币的正反面，所以生命根本无法靠记忆信息来对抗熵增。 </p>
<p>生命依靠方式是学习，靠学习来找出能压缩所有例子的知识，这才是生命减熵的方式。 </p>
<p>也是为什么学习观01说：</p>
<blockquote>
<p>学习不是记忆</p>
</blockquote>
<h2 id="学习需要智能"><a href="#学习需要智能" class="headerlink" title="学习需要智能"></a>学习需要智能</h2><p>然而学习能力并不是凭空产生的。</p>
<p>在北欧神话中，主神奥丁为了获得知识，曾以一只眼睛换得一口智慧泉水。</p>
<p>你不用牺牲一只眼睛，因为你已经具备了奥丁所获得的能力。这也是为什么你没有意识到分子的运动情况多到无限，因为你早已瞬间将近乎无限的情况分成了四类。</p>
<p>而你所拥有的学习知识和使用知识的能力就是：智能。</p>
<blockquote>
<p>智能：学习知识和使用知识的能力</p>
</blockquote>
<p>爱因斯坦和霍金对智能都做过描述： </p>
<p>爱因斯坦 </p>
<blockquote>
<p>The measure of intelligence is the ability to change<br>智力的衡量标准是改变的能力</p>
</blockquote>
<p>霍金</p>
<blockquote>
<p>Intelligence is the ability to adapt to change<br>智力是适应改变的能力</p>
</blockquote>
<p>两位伟人都提到了“改变”，霍金的描述更为具体：若想在时刻改变的世界中生存就需要“根据改变做出对应的改变”的能力。而这与麦克斯韦妖的功能完全一致。 </p>
<p>等等，智能不应该是可以思考，能计划，有自我意识吗？ </p>
<p>这些能力实际上是智能在高级层面上的表现，但智能核心就是“根据改变做出对应改变的能力”。</p>
<h2 id="智能体的条件"><a href="#智能体的条件" class="headerlink" title="智能体的条件"></a>智能体的条件</h2><p>可想要拥有智能，需要四个能力的支持。 </p>
<ul>
<li>学习：构建模型的能力。</li>
<li>存储：存储模型的能力。</li>
<li>预测：使用模型的能力。</li>
<li>获能：获得能量的能力。</li>
</ul>
<p>这里我将“知识”替换成“模型”，二者意思相同，只不过“知识”人类在意识层面上所构建的“模型”。 </p>
<p>其中三个能力与计算机完全一致， 毕竟最初计算机的诞生正是为了更高速运用我们已学到的知识。</p>
<p>而若要形成生命，则需要考虑第五条，即：</p>
<p>熵差：自身减熵速率需大于减增速率。</p>
<p>一直以来大家都是从化学、物理、生物的角度来思考生命，我们不妨忽略这些限制，仅从信息减熵的角度来思考生命。</p>
<p>而为了与真正的生命相区分，我们给它一个不同的名字：智能体 (Agent)。</p>
<p>那理解智能体要做的第一件事就是回顾它在自然生命中是如何产生的。</p>
<p>一、熵差</p>
<p>对于生命的诞生而言，环境是一个非常重要的因素。</p>
<p>气体变化速率太快，固体又太慢，只有液体适中，所以自然选择了在液体中的固体。自身的固体零件相对稳定，而液体又支持了熵减系统的形成。于是人们才相信水是生命之源。</p>
<p>二、供能</p>
<p>一般认为生命必须依靠阳光，因为太阳是我们的低熵能量来源。</p>
<p>然而海底热泉却让我们看到了另一种可能，这里生命的能量来源不是阳光，而是海底沉积物的化能合成 (Chemosynthesis)。地球上居然同时有光明与黑暗两种生态系统。</p>
<p>三、存储</p>
<p>模型的存在需要存储媒介。</p>
<p>而 DNA 就是这个支持了地球上所有生命的存储媒介。DNA 神奇到让人很难相信它是自然的产物，以至于让 DNA 双螺旋结构发现者之一的克里克在 1973 年提出定向胚种论，认为地球上的生命是由外星文明播种所产生的。电影《普罗米修斯》恐怕也是受此启发才将“火种”描述为外星人留给地球的 DNA。</p>
<p>虽然模型是以信息的形式存储在 DNA 上的，但绝非单纯的信息，而是代码。随后会被编译成能实现模型功能的工具，用于减熵。</p>
<p>所以现在虽然能够测出 DNA 上的这些模型的代码，却无法使用和理解上面的模型。 因为我们需要搞清这些代码所编译的模型功能。 这也是为什么学习观02说：</p>
<blockquote>
<p>记住知识的描述无法使用知识</p>
</blockquote>
<p>这里所指的“知识的描述”就是“模型的代码”。</p>
<p>四、预测</p>
<p>而那些被 DNA 代码所编译成的工具就是蛋白质。</p>
<p>DNA 有 4 种碱基：A、C、G、T。 每三个碱基对应一个氨基酸。</p>
<p>大量 DNA 序列会让氨基酸形成不同功能的蛋白质，就可以完成麦克斯韦妖的开关门功能。</p>
<p>但三个碱基一共对应的氨基酸种类不是 64 种，而只有 20 种。 对应的氨基酸很多都是重复的，这种冗余似乎很浪费，让人非常不解。 </p>
<p>然而这种对应方式是万里挑一。 因为从信息论的角度，它的这种冗余使得遗传代码具备了极强的抗噪能力，把点突变（Point mutation）带来的灾难降到了最低。其原理类似于为什么 CD 盘被刮花后一样可以播放（因为里面加入的冗余是为了 error correction）。</p>
<p>五、学习</p>
<p>而这里面最为关键的就是学习能力。DNA 上所存储的能够减熵的模型究竟是怎么被学习到的？ </p>
<p>接下来的几期视频，我们将从学习的视角来观察自然生命历史（故叫学习观）。</p>
<p>而生命史上出现的第一个学习方法就是：演化（genetic algorithm）：智能体并不以个体来学习模型，而以群体来学习模型。学习过程分为三个部分：</p>
<blockquote>
<p>A.数据准备<br>虽然无法直接获得开关信息（麦克斯韦妖的例子），但有一个事实： 若系统猜对了信息就会存活，否则就会无法维持状态，从而间接的提供了开关信息。</p>
<p>B.模型候选<br>为了提供候选模型，智能体以 DNA 为模版，大量的自我繁殖，并且在繁殖中产生基因突变，使得每个克隆体都有不同的模型。</p>
<p>C.模型筛选<br>而判断模型是否正确的方式就靠自然选择：只有合适的模型才能够预测到信息。但自然选择并非直接判断模型好坏，而是一次次判断模型所预测的信息是否能够维持自身状态，从而淘汰无法预测到信息的模型，所以很多劣质模型依然有机会遗传给后代。</p>
</blockquote>
<p>当候选和筛选反复循环进行后，种群基因库就会整体形成一个能稳定减熵的模型。</p>
<p>演化: 智能体让自己的突变克隆体去问环境，突变所产生的模型是否能预测信息，以克隆体的死亡为代价获得能稳定减熵的模型的一种学习方法。</p>
<p>这就是为什么越低等的智能体所繁衍后代的数量越多，因为演化是这个智能体的唯一学习方法。</p>
<h2 id="什么限制永生"><a href="#什么限制永生" class="headerlink" title="什么限制永生"></a>什么限制永生</h2><p>然而这并没有解释人类个体的死亡。因为癌细胞可以永生，一直活到自然将其筛选为止，但为什么人类不可以。</p>
<p>虽然我们体内每天都有细胞死亡，但新细胞会接替其位置，这种更替明明可以永远持续下去。 可我们的自然死亡却被有意写到了基因代码当中，并非技术上无法实现永生。</p>
<p>人类这个智能体这么做的原因一定是因为信息的获取受到了某种限制，而这个限制就与动物史上所产生的第二个学习方法有关。</p>
<p>若想知道关于信息和知识那里定量的描述，请查：条件熵，互信息，信息增益</p>
<h1 id="学习观14：26亿年前灭了所有地球生命的家伙告诉你什么是学习"><a href="#学习观14：26亿年前灭了所有地球生命的家伙告诉你什么是学习" class="headerlink" title="学习观14：26亿年前灭了所有地球生命的家伙告诉你什么是学习"></a><span id="header14">学习观14：26亿年前灭了所有地球生命的家伙告诉你什么是学习</span></h1><h2 id="本期涉及众多学界难题，仅为信息角度的假说"><a href="#本期涉及众多学界难题，仅为信息角度的假说" class="headerlink" title="本期涉及众多学界难题，仅为信息角度的假说"></a>本期涉及众多学界难题，仅为信息角度的假说</h2><p>上期视频提到的“全知便可永恒”并不是忽略物理减熵，而是由于热力学熵和信息熵二者可相互转换。 这也是为什么软件等“虚拟经济”可以创造价值，因为允许我们生存（减熵）的不是能量，而是能量的转移（做功），所以可制造能量差的信息就能够创造价值（负熵）。</p>
<h2 id="RNA世界假说"><a href="#RNA世界假说" class="headerlink" title="RNA世界假说"></a>RNA世界假说</h2><p>虽然上期视频从信息减熵的角度重新理解了生命演化，但有一个问题： </p>
<p>蛋白质的功能需要 DNA 代码来控制，而 DNA 代码的复制又需要蛋白质来实现。 若两者都必不可少，那先有鸡还是先有蛋？</p>
<p>以人们猜测地球上现存所有生命的始祖可能是同时具有二者功能的 RNA 聚合体， 并给了它一个名字：LUCA（Last Universal Common Ancestor）</p>
<p>LUCA 应该是从能累积高浓度核苷酸的海底热泉喷发口处形成， 最初和病毒一样只会自我复制，而能量由喷发口所提供。 慢慢演化成由 DNA 存储模型，由蛋白质实现模型功能，且可独立获取能量的原核细菌和古细菌后，逃离喷发口。 </p>
<p>但既然 LUCA 已经可以不断复制自我，又为什么会牺牲复制速度，转而向更复杂的 DNA + 蛋白质的形式演化呢？</p>
<h2 id="演化之矢"><a href="#演化之矢" class="headerlink" title="演化之矢"></a>演化之矢</h2><p>环境的熵或许就可以告诉我们答案。 </p>
<p>熵增定律表明了，若把整个宇宙视为一个孤立系统，熵会不断增大。 而生命靠减熵来维持状态，但代价却是不断增加环境的总熵。</p>
<p>比如我们收拾完房间后反而会排除更多的废物+热量。 若想在加速熵增的环境中继续维持状态，就要更强的减熵能力，也就无法原地踏步。 </p>
<p>所以局部的生命演化可能没有方向，但若把所有生命整体视为一个复杂系统，它也会向不断增大智能的方向演化。</p>
<p>而增大智能的方式除了增强每个个体外，还有将若干个体组织成一个新个体的方式。 实现模型功能的最小单位是蛋白质，虽然单个蛋白质能实现的功能只相当于开关门那么简单， 但大量蛋白质有组织并行工作就有了质变（相变 phase transition），</p>
<p>由量变引起质变的现象就是：涌现 emergence。</p>
<p>涌现可以跃进式的增强智能，但涌现的形成是有条件的。 只有稳定的智能体之间有规则的相互作用才能形成更高层级的智能体。 </p>
<p>类比：想象用砖块来筑墙，若每块砖都可随时变形，墙很容易坍塌，而没有规则的堆砌，墙也会坍塌。 </p>
<p>DNA 的优势就在于它的稳定性。</p>
<p>若 RNA 上的代码受损，则会发生突变，虽然增加了个体的演化速度，但这种不稳定性却无法允许更高层级智能体的形成。 而 DNA 拥有两条链，即使单链受损也可用另一条来修复（编译出错时也有源代码来纠错），相对稳定的 DNA 允许了更高层级智能体的形成。 这也是为什么我们只发现低等病毒用 RNA 存储模型，而所有的高等生命都是用 DNA 来存储模型。</p>
<h2 id="反馈循环"><a href="#反馈循环" class="headerlink" title="反馈循环"></a>反馈循环</h2><p>LUCA 的后代在很长一段时期内都是厌氧菌，因为当时大气成分的氧含量非常稀少。 </p>
<p>但约在 26 亿年前，有一类叫做蓝绿菌的分支演化出了光合作用 Photosynthesis。 可用太阳能将二氧化碳和水转化成养料的同时排除氧气。</p>
<p>1.正反馈</p>
<p>这种等同于有无限能源的优势使得蓝绿菌的数量爆炸式增长，地球大气氧含量迅速增加，被地质学家称为大氧化事件 Great Oxygenation Event 。 </p>
<p>这些额外的氧气不断消耗给地球保温的甲烷，发生了最严重且持续时间最长的一次冰河时期 (休伦冰期)，而氧气对当时的生命属毒气，这使得地球上的生命包括蓝绿菌近乎全部灭绝。相当于被排泄物淹没致死。 </p>
<p>可见若一个物种的个体可以永生，一旦演化出某个优势模型时，无限加速繁殖的个体会让环境也加速熵增，该物种反而会因为演化的减熵速度无法跟进环境的熵增速度而灭绝。</p>
<p>2.负反馈</p>
<p>这就需要某种平衡机制，在控制论中被叫做负反馈。 </p>
<p>刚好在休伦冰期的 3 亿年后，部分幸存的细菌演化出了与光合作用反全相反的呼吸作用，彼此制约的双方最终将大气氧含量平衡到了 21% 左右。</p>
<p>随后某些好氧菌被大细菌整体吞噬后，形成内共生关系 endosymbiotic，变为了专门提供能量的线粒体。 而蓝绿菌被大细胞吞噬后变成了叶绿体。这些大细胞又慢慢演化成了真核细胞。 </p>
<p>但在这之后的很长一段时间内，不知道为什么，生命几乎没什么变化，因此也被称为“无聊的十亿年”。</p>
<h2 id="交配移动困境"><a href="#交配移动困境" class="headerlink" title="交配移动困境"></a>交配移动困境</h2><p>一直到距今 5.4 亿年前的寒武纪，生命就像触发了某个按钮似的，在短时期内迅速演化，被称为寒武纪生命大爆发 Cambrian Explosion。 </p>
<p>这让达尔文非常困惑，因为达尔文主张的演化是渐进式的，为什么会有大爆发现象。 然而从减熵的角度来看，维持生命的关键并非演化本身，而是演化所提供的信息。 </p>
<p>演化是史上第一个允许生命构建模型来获取信息的机制，但却不是唯一的机制，寒武爆发的关键就在于第二套建模系统的完善。</p>
<ol>
<li>整体稳定困境：</li>
</ol>
<p>在那无聊的十亿年中，虽然低等生物有涌现成高等生物的趋势，但却都遇到了一个瓶颈： 那就是随机突变的单细胞无法达到涌现成复杂多细胞生物所需要的稳定性。 </p>
<p>高层级的差异性机制 需要一个能将多细胞生物作为整体来增加差异性的机制，而有性生殖刚好最满足这个特性。这也解释了为什么高等生物都采用有性生殖，同时抑制单个细胞变异， 并非仅仅为了增加演化的差异性，而是在稳定整体的基础上增加差异性。</p>
<ol>
<li>移动信息困境：</li>
</ol>
<p>但是不能大范围移动的多细胞生物只能和相近的个体交配，会因无法提供足够的差异性导致演化速率过低而灭绝。 </p>
<p>可当多细胞生物大范围移动后，环境也会随移动而改变，又会因之前演化出模型无法预测新环境的信息而死亡。 </p>
<p>处于这种两难困境的低等生物始终无法形成更复杂的生命系统。</p>
<h2 id="一线曙光"><a href="#一线曙光" class="headerlink" title="一线曙光"></a>一线曙光</h2><p>直到寒武纪之前的埃迪卡拉纪，演化出了神经细胞。 </p>
<p>1.神经演化</p>
<p>众多神经细胞形成的神经网 nerve net 就可以配合演化机制 Neuro-evolution 加快学习环境模型的速率， 出现了不少如水母一样的刺胞动物 Cnidaria。</p>
<p>2.实时感官</p>
<p>而到了寒武纪初期，眼睛等高级感官系统相继产生，比如当时的顶级捕食者奇虾 Anomalocaris 就有一对很大的复眼，这使得实时观测信息成为了可能。</p>
<p>增加了实时感官能力的神经网络就允许个体在新环境中，学习用上一时刻信息（输入）来预测下一时刻信息（输出）的模型， 比如我们可以躲避车辆就是因为曾经看过上下时刻都发生了什么，可用神经网络构建出模型。 </p>
<p>当再看车辆迎面而来（输入信息）时便可及时预测车辆下一时刻的位置（输出信息）进而躲避。 </p>
<p>3.反馈循环</p>
<p>这种实时预测信息的能力也让捕食者与被捕食者之间形成了更激烈的军备竞赛关系， 比如没有演化出眼睛的被捕食者就靠甲壳和伪装来增加存活率，而这就让捕食者演化出了更坚硬的利牙与更高分辨率的眼睛。 但不管任何一方都会受到制约。靠太阳获能的细菌会被其他小生物吃掉，小生物又会被大生物吃掉， 而到了食物链顶端的捕食者又会因为吃光小生物而饿死。</p>
<p>这种制约关系抑制了某优势物种过分加快环境变化导致像当初大氧化一样的生物灭绝事件。 不过支持这种高强度军备竞赛的恰好是当初大氧化生物灭绝事件所带来的氧气。 </p>
<p>4.新层级演化</p>
<p>而这一切都是由于第二套建模系统的不断完善（至少经历了两个阶段），个体才能更容易在被危险杀死之前找到配偶，解决了多细胞生物大范围移动后的信息问题，生命才能以高级形式继续演化。</p>
<h2 id="第二套建模系统"><a href="#第二套建模系统" class="headerlink" title="第二套建模系统"></a>第二套建模系统</h2><p>以上仅是从减熵的角度对寒武爆发的猜测，我们无法知道当时究竟发生了什么。 但可以体会到，学习能力来的并不容易。从演化这个第一套学习系统开始至少经历了 20 亿年的不断试错才出现了神经网络这个第二套学习系统。</p>
<p>而神经网络在此之后依然不断完善，如今已经发展成人类大脑，可以允许我们在个体生命周期内学习知识来预测信息。 近几年非常火的人工智能也正是受神经网络的启发，用深度学习算法使语音、图像等识别达到了人类水平。 </p>
<p>这也给人类的未来工作带来了巨大的挑战，因为知识是我们获得信息的源泉。 计算机已经可以根据信息来归纳知识，但很多人却还像计算机一样去记忆信息，天真的认为人脑的容量是无限的。 </p>
<p>不过讽刺的是，我们教会了计算机如何学习，却从来没有教过学生如何使用演化出的大脑。 </p>
<p>下期视频我们将深入到细胞层级来探究生物神经网络究竟是如何允许个体学习的，以及人工神经网络和它有什么关系。 随后你将明白为什么人脑的记忆需要不断重复，而好不容易记住后却又为什么会遗忘。</p>
<h1 id="学习观15：为什么人类要靠不断重复来记忆，为什么好不容易记住的信息又会被遗忘"><a href="#学习观15：为什么人类要靠不断重复来记忆，为什么好不容易记住的信息又会被遗忘" class="headerlink" title="学习观15：为什么人类要靠不断重复来记忆，为什么好不容易记住的信息又会被遗忘"></a><span id="header15">学习观15：为什么人类要靠不断重复来记忆，为什么好不容易记住的信息又会被遗忘</span></h1><h2 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h2><p>1.1.现象</p>
<p>现在的学生至少要在学校接受十多年的教育，</p>
<p>每天记忆和学习，多么希望自己可以过目不忘。</p>
<p>1.2. 问题</p>
<p>可既然记忆如此重要，为什么人类没有演化成过目不忘？</p>
<ul>
<li>为什么我们要靠不断重复来记忆？</li>
<li>为什么好不容易记住的信息又会被遗忘？</li>
<li>为什么我们的记忆还会混淆信息？</li>
</ul>
<p>1.3. 假说</p>
<p>传统观点对此的解释有消退说和干扰说，</p>
<p>认为遗忘是信息在脑中不可避免的消退结果，而混淆是不同信息干扰记忆所造成的。</p>
<p>1.4. 疑点</p>
<p>可是计算机就能瞬间记忆，长期保存，再相似的信息也不会混淆。</p>
<p>加之患有超忆症的人群也可以过目不忘，说明演化了数亿年的生命完全有条件产生像计算机这样既没有消退也没有记忆干扰的大脑。</p>
<p>同时越来越多的研究也表明，遗忘恐怕并不是被动的，而是大脑的一种主动行为。</p>
<p>1.4. 疑点</p>
<p>可是计算机就能瞬间记忆，长期保存，再相似的信息也不会混淆。</p>
<p>加之患有超忆症的人群也可以过目不忘，说明演化了数亿年的生命完全有条件产生像计算机这样既没有消退也没有记忆干扰的大脑。</p>
<p>同时越来越多的研究也表明，遗忘恐怕并不是被动的，而是大脑的一种主动行为。</p>
<h2 id="生命稳态维持"><a href="#生命稳态维持" class="headerlink" title="生命稳态维持"></a>生命稳态维持</h2><p>这个问题看似很奇怪，但生命是一种特殊的状态维持方式，能被留下来的原因只有一个：</p>
<p>那就是通过预测信息，在不断变化的环境中维持了自身的状态。</p>
<p>而单靠应激反应就可以达成这一目的，并不需要一个大脑来体验世界。</p>
<p>2.1. 应激物理视角</p>
<p>比如草履虫（单细胞生物）细胞膜上的每个蛋白质（离子通道），</p>
<p>一般状态：在一般状态时，会消耗能量，通过控制离子的进出，让细胞膜内外保持 -40 mV浓度差。</p>
<p>预测危险：当触碰（以触觉来感知外界信息）到障碍物时，细胞膜的变形（物理输入信息）会让这些蛋白质（离子通道）开门，允许特定离子通过，这些离子就形成能让草履虫向反方向游动的电信号避开危险。</p>
<p>2.2. 应激数学视角</p>
<p>从数学视角来理解单个蛋白质的功能：</p>
<blockquote>
<p>输入：则外界的物理挤压相当于输入，<br>输出：蛋白质对应的开关门操作相当于输出，<br>模型：而决定了什么样的输入该对应哪一种输出的蛋白质相当于模型（函数）。</p>
</blockquote>
<p>2.3. 应激性的作用</p>
<p>别看单个蛋白质仅有开关门的功能，但它实际上把无数种情况都压缩进了一个函数（模型）。</p>
<p>类比：如果要靠计算机来记忆每一种物理挤压对应的开关门情况，就相当于让计算机去记忆 π 小数点后的每个数字。</p>
<p>2.4. 应激性的实现</p>
<p>而应激反应这是这些蛋白质并行工作后，涌现出的结果，可允许草履虫在状态被破坏之前躲避危险。</p>
<p>任何生命都会感知外界信息并做出维持自身状态的对应行为。</p>
<p>能体验这个世界的个体并不会比不能体验世界的个体更容易被留下来。</p>
<p>那么大脑究竟是干什么用的呢？</p>
<h2 id="移动演化危机"><a href="#移动演化危机" class="headerlink" title="移动演化危机"></a>移动演化危机</h2><p>这个问题的答案，在我们顾及应激反应的代价后，便能显现。</p>
<p>虽然草履虫单靠应激反应就能存活，</p>
<p>不过草履虫怎么知道该生成什么样的蛋白质才能产生可躲避危险的应激反应的？</p>
<p>3.1. 应激性的学习</p>
<p>它需要靠演化来学习应激模型：</p>
<p>通过大量克隆带有不同 DNA 的自己来生成不同的蛋白质，增加备选模型。</p>
<p>由自然选择筛选掉那些不能躲避危险的模型。</p>
<p>在筛选后的模型的基础上再不断重复上述过程。</p>
<p>只要种群基因库的更新速度快过环境的变化速度，整个种群就能相对稳定的形成可躲避危险的模型。</p>
<p>也就是为什么生物要一代一代的繁衍，又并非完美复制自己。</p>
<p>虽然生物想要（拟人化）一直延续下去，但又只能通过构建模型来预测危险，而繁衍和变异就是构建模型的过程。</p>
<p>然而多细胞生物的演化却面临一个两难问题。</p>
<p>3.2. 稳定性的保证</p>
<p>因为多细胞生物是由大量细胞所组成的，如果这些细胞都可随时变异，那就无法形成一个稳定的整体。</p>
<p>3.3. 差异性的需求</p>
<p>虽然自然产生了很多能保证稳定性的机制，可演化所需要的差异性又成了问题。</p>
<p>而有性生殖的出现允许了个体在保证稳定性的基础上增加差异性，但条件却是需要充分移动来洗牌。</p>
<p>3.4. 学习速度需求</p>
<p>然而一旦大范围移动后，演化的学习速度又跟不上移动所造成的环境变化速度。</p>
<p>类比：这就好比怀胎三年的哪吒出生后还没等产生后代，就因为试错而死亡了，两三胎下来李家就灭绝了。</p>
<p>多细胞生物想要演化就不得不让个体在生命周期内拥有学习能力。而大脑最初的作用就在于此。</p>
<h2 id="高等生命挑战"><a href="#高等生命挑战" class="headerlink" title="高等生命挑战"></a>高等生命挑战</h2><p>想要明白为什么大脑会演化出遗忘，必须先要搞清个体学习所面临的困难。</p>
<blockquote>
<p>以蟾蜍吃虫子为例：<br>模型：为了获得能量，蟾蜍必须要构建一个模型，当看到虫子就捕捉，否则不动。<br>输入：这时，输入为蟾蜍接收到的反光信号，<br>输出：输出为是否伸舌头捕捉，</p>
</blockquote>
<p>4.1. 影响输出因素</p>
<p>而蟾蜍看到了什么不仅取决于反光这个输入信号。</p>
<blockquote>
<p>模型<br>同时还取决于蟾蜍自身的模型是如何将输入计算成输出的。<br>例子：因此不同的生物看同一幅画面会有不同的感知，而即便是同一物种之间也会有差异。<br>比如，对这张图片，有的人看到的是灰色和绿色；有的人看到的却是粉色和白色。</p>
<p>输入<br>同样的，单有模型也不能决定输出。<br>例子：比如，即使某人带有致病基因，若没有触发该基因环境输入，则该人并不会得此病。<br>例子：也是为什么不要看字幕来练习听力。因为当没有字幕这个输入信号时，所构建的模型就无法使用了。</p>
<p>真实<br>由于生物所看到的事物会随自身的模型而改变，所以人类看到的也并不是世界的真实样貌。</p>
</blockquote>
<p>4.2. 模型泛化能力</p>
<p>任何感知都是通过学习所构建的，也都有它的缺陷。</p>
<p>比如蟾蜍会把所有移动的横条都识别为虫子，而即使是真的虫子，只要不动，或者竖立起来，就无法识别。</p>
<p>这是因为虫子的大小、形态、颜色以及太阳光照强度等不同，使得输入信号是虫子的情况有无数种，个体一生都无法见到所有情况。</p>
<p>但个体想要存活就不得不从”有限的例子“中，构建一个也能识别从未见过的情况的模型。而这就是学习。</p>
<p>比如，高考实际上就是 学生在从有限的练习题中构建可解出从未见过的高考题的模型，每个模型就是考生要学习的一个知识。</p>
<p>模型可识别从未见过的情况的能力叫做泛化能力，也就是“举一反三”。</p>
<p>例子：所以验证学习的方式是考从未见过的问题。</p>
<p>4.3. 过拟合欠拟合</p>
<p>但有时考生只会做学校出的模拟题，而不会做高考真题；</p>
<p>有时我们只能看懂自己的字，却看不懂别人写的字。</p>
<p>这种仅仅记住了学习时所见过的情况，却无法解决未见过情况（泛化能力不足）的现象被称作过拟合；</p>
<p>相对的，学习时所见过的例子也无法记住的现象被称作欠拟合。</p>
<p>4.4. 泛化能力需求</p>
<p>多细胞生物想要生存就不能仅记忆所见过的个别情况。</p>
<p>比如，蟾蜍视觉模型的泛化能力就无法顾及到不动的虫子，如果把它关进有大量死虫子的罐中，它会活活饿死。</p>
<p>又如，不能根据乔布斯和比尔盖茨都辍学了，就构建一个认为辍学就能成功的模型，然后辍学回家。</p>
<p>那就必须要抑制过拟合，提高泛化能力。</p>
<p>但问题是，草履虫是靠自然选择来筛选模型的，</p>
<p>可自然选择无法干预生命周期内的学习，这时多细胞生物又该如何筛选模型，如何提高模型的泛化能力呢？</p>
<p>不仅如此，生命周期内的学习还需要新的记忆能力。</p>
<p>4.5. 个体记忆需求</p>
<p>单靠演化来学习的生命并不需要生命周期内的记忆能力。</p>
<p>拿草履虫来说，它就好比一个老式钟表，并不能像手机闹钟一样记忆起床时间。</p>
<p>然而个体学习就需要有把曾经见过的例子记忆下来。</p>
<p>等搜集到足够的数据时，用于学习。</p>
<p>所以记忆最初的产生并不是为了让生物怀旧过去，而是允许生物从历史经验中学习。</p>
<p>4.6. 高等生命目标</p>
<p>多细胞生物只要确保自己在产生后代之前不死，就可以让演化机制在该层级上继续工作。</p>
<p>这就好比在游戏中，一旦到了存档点，即使死了也可以让后代继续冒险，但前提是要有允许个体走到存档点的学习能力。</p>
<p>而面对这些新挑战，我们的祖先又靠什么保证繁衍前不死的呢？</p>
<h2 id="网络记忆学习"><a href="#网络记忆学习" class="headerlink" title="网络记忆学习"></a>网络记忆学习</h2><p>当我们思考生命该如何对抗未知的时候，便能体会到，为什么当初生物的底层并没有选择像计算机一样的过目不忘。</p>
<p>这里将通过一个简单的例子，来比较两种不同的记忆方式。</p>
<p>输入：假设有两个输入，都可为0或1。</p>
<p>输出：当两个输入不一样时，就输出1，否则输出0。</p>
<p>一共有四种情况。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/7.png" alt></p>
<p>5.1. 查找记忆</p>
<p>第一种记忆方式与九九乘法表一样，就是将所有情况都记录下来，随后根据输入去查找对应的输出。</p>
<p>特点是：</p>
<ul>
<li>记忆迅速</li>
<li>保存稳定</li>
<li>不会混淆</li>
</ul>
<p>这是我们意识层面比较熟悉的记忆。恐怕不少人也认为人脑的长期记忆就是这种方式。</p>
<p>5.2. 网络记忆</p>
<p>而第二种记忆方式是构建一个网络，根据输入直接计算出对应的输出。</p>
<p>这里的输入和输出都是一种状态。</p>
<p>输入状态由两个因素（dimension）所表达（represent），好比一个物体的长和宽，因此也叫二维向量。</p>
<p>而输出状态是一维向量。</p>
<p>如果只记忆这种情况：<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/8.png" alt><br>那可以忽略第一个维度，只取第二个维度的原值。</p>
<p>圆圈中的数值表示每个维度的状态，这些控制着取多少状态的连接表示权重（weights）。</p>
<p>而若是只记忆这种情况：<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/9.png" alt><br>那可以让第一个维度乘以负一，与第二个维度的原值相加。</p>
<p>5.2.1. 网络记忆的位置</p>
<p>不同于第一种记忆，网络的记忆并不是存储在某个特定位置，而是由所有权重共同存储的，无法直接查看，只能根据输入计算输出。</p>
<p>不过若要同时记忆这四种情况：<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/10.png" alt><br>那之前任何一种直接从输入状态变换到输出状态的方式都不行。</p>
<p>但可以先变换到一个非线性（意为变化速率是常量）的中间状态，再从中间状态变到输出状态。</p>
<p>5.2.2. 网络记忆的保证</p>
<p>但如果要记忆的情况特别多怎么办？</p>
<p>实际上，只要中间状态的维度足够大，就可以记忆任意函数。因为大不了给每个情况都在中间状态中分配一个维度。这也叫做 通用近似定理（Universalapproximation theorem）。</p>
<p>不过通用近似定理仅仅保证了网络可以记忆，并不保证网络一定可以学习。因为如果需要见到所有情况的话，那就与第一种记忆方式没有什么区别了。</p>
<p>所以将这四个情况存储到网络中的行为依然被称为记忆，但如果通过这三种情况可推测第四种情况的话，便称为学习。</p>
<p>也就是说，在网络中，记忆可被视为过拟合的学习（比如，只学习这一种情况），而学习可被视为泛化的记忆（比如，同时记忆这三种情况来推出第四种情况）。</p>
<p>5.2.3. 这种记忆的特点</p>
<ul>
<li>记忆耗时：这种记忆方式因为需要不断调整网络的权值，直到能纳入所有情况为止，所以会比较耗时。</li>
<li>会有干扰：而这种调整也会影响先前情况的记忆。比如记忆完这种情况，再记忆剩下三种情况时，就会影响第一种情况的记忆。</li>
<li>会有混淆：同时，网络的记忆就会出现混淆两个相似情况的现象。就是我们俗称的“模糊记忆”。</li>
<li>记忆可稳定：不过，只要保证权值的大小不变，这种记忆一样可以稳定存在，并不会出现快速遗忘的现象。</li>
</ul>
<p>5.3. 生物记忆</p>
<p>根据我们平时的记忆特点，能够感觉出来，生物底层所采用的是网络记忆方式。</p>
<p>可网络记忆明明有那么多弊端，为什么不采用查找记忆呢？</p>
<p>的确，查找记忆可更快捷的记忆信息，比如计算机的记忆就几乎是瞬间的，然而前提却是要有人提供给它所有情况的信息。</p>
<p>可在自然中，谁来提供给生物这些情况的信息呢？查找记忆完全没有解决我们在上一节中所描述的生存困难，并不能从有限的例子中学习模型，来对抗生命最大的敌人：未知。<br>不仅如此，对于在信息不断增加的环境中所生存的生物而言，查找记忆也意味着需要近乎无限的存储空间。</p>
<p>而网络记忆虽然慢，还会混淆，但它实际上是在寻找所有见到（相似）情况的共同规律，将它们都压缩进一个网络中，无需记住所有情况，而是根据输入得到输出，节省了存储空间。</p>
<p>更重要的是，所找到的共同规律就可以用来预测从未见到的情况。</p>
<p>5.4. 网络学习</p>
<p>但既然网络的记忆也可以稳定存在，为什么会出现遗忘呢？</p>
<p>问题就在于个体该如何筛选模型，如何提高模型的泛化能力。</p>
<p>因为网络学习会从所见到的情况中寻找相同的规律，<br>但生物每次见到的情况都是随机的，部分随机情况之间也会有特殊规律。</p>
<p>比如，连续学习乔布斯和比尔盖茨的情况，就会找到辍学的规律。<br>而连续学习这两种情况，就会找到忽略第一个维度，只取第二个维度的规律。<br>但这种局部规律仅记住了部分情况，却失去了预测其他情况的能力。</p>
<p>不过根据概率：</p>
<blockquote>
<p>若有一种规律只出现过一次，那么该规律是普遍规律的概率就很低，而学到这个规律的个体很难存活。</p>
<p>但若有一种规律反复出现，那么该规律是普遍规律的概率就比较大，而学到这个规律的个体就更有可能存活。</p>
</blockquote>
<p>于是在众多个体当中，部分个体产生了一种基于概率的模型筛选机制：</p>
<blockquote>
<p>当网络的某个连接被高频率使用时，就强化该连接的形成（回答了：为什么需要不断重复来记忆）；</p>
<p>但当网络的某个连接被低频率使用的话，就弱化该连接的形成（回答了：为什么好不容易记住的信息会被遗忘）。</p>
</blockquote>
<p>这种筛选机制会使得特殊规律的模型难以存留，只有那些所有情况都有的规律才会被生物学习。</p>
<p>随后拥有该机制的个体在残酷的自然选择中存活了下来，继续繁衍，成了我们的祖先。</p>
<p>而这，便是为什么我们要靠不断重复来记忆，为什么好不容易记住的信息又会被遗忘。</p>
<h1 id="学习观15-5：你学到的哪些记忆方法是错把人脑当成了电脑"><a href="#学习观15-5：你学到的哪些记忆方法是错把人脑当成了电脑" class="headerlink" title="学习观15.5：你学到的哪些记忆方法是错把人脑当成了电脑"></a><span id="header15.5">学习观15.5：你学到的哪些记忆方法是错把人脑当成了电脑</span></h1><p>该期属于学习观15的应用篇，这里我会非常啰嗦的再讲一遍网络记忆，因为它是生物与机械在记忆上的底层区别，是《学习观》中最重要基础知识。随后会用英语学习作为实际例子来分析。搞清楚网络记忆后，你可以知道：</p>
<ul>
<li><ol>
<li>哪些记忆方法是错把电脑记忆方式当成人脑记忆方式。</li>
</ol>
</li>
<li><ol>
<li>为什么不要看字幕来练习听力。</li>
</ol>
</li>
<li><ol>
<li>如果从零学习一门外语，最符合大脑原理的第一步是什么。</li>
</ol>
</li>
</ul>
<h2 id="异或门例子"><a href="#异或门例子" class="headerlink" title="异或门例子"></a>异或门例子</h2><p>我们还是先从最简单的异或门例子开始，</p>
<p>虽然涉及很多数字，但请耐心体会。</p>
<p>1.1. 异或门描述</p>
<p>假设需要这样一个功能：根据两个输入，要求得到对应的输出，一共有4种情况：<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/11.png" alt></p>
<ul>
<li>当第一个输入是1，第二个输入是1时，就输出0。</li>
<li>当第一个输入是1，第二个输入是0时，就输出1。</li>
<li>当第一个输入是0，第二个输入是1时，就输出1。</li>
<li>当第一个输入是0，第二个输入是0时，就输出0。</li>
</ul>
<p>1.2. 查找记忆</p>
<p>若要记住这四种情况。</p>
<p>描述：一个种方法是像九九乘法表一样把所有情况都记录下来，然后根据输入来查找对应的输出。</p>
<p>例子：比如，当第一个输入是 1，第二个输入是 0 时，去表中查找，便可得到输出为1。</p>
<p>名称：可以把这种方式叫查找记忆（状态记忆），这个表格也叫查找表。</p>
<p>1.3. 查找记忆特点</p>
<ul>
<li>记忆迅速</li>
<li>长期保存</li>
<li>不会混淆</li>
<li>没有干扰</li>
</ul>
<p>1.4. 网络记忆</p>
<p>描述：还有一种方式是网络记忆：通过构建一个函数（映射），根据输入来直接计算出对应的输出。</p>
<p>一种情况例子：比如，想要记忆 [1,1] —&gt; 0时，可以让第一个输入乘以-0.5，让第二个输入乘以0.5，二者相加可得到0，便记住了这个情况。</p>
<p>1.4.1. 网络记忆位置</p>
<p>记忆在网络中并不是查找表的 [1,1] —&gt; 0，而是乘号右边的数字[-0.5,0.5]，这些数字也叫权重。</p>
<p>权重改变，则记忆就改变。</p>
<p>两种情况例子：但如果记忆完了 [1,1] —&gt; 0，想同时记忆 [1,0] —&gt; 1，若还用 [-0.5,0.5] 的权重，那 [1,0] 算出的就是错误的数值，[1,0] —&gt; 1 并没有被记忆。</p>
<p>1.4.2. 记忆覆盖现象</p>
<p>因此需要重新调整权重。这时可以让第一个输入乘以1，让第二个输入乘以-1，再二者相加，便可同时记忆这两种情况。</p>
<p>但之前的记住了[1,1]得0的[-0.5,0.5]权重就会被新记忆 [1, -1 ] 所覆盖掉。<br>1.4.3. 人脑长期记忆</p>
<p>人脑长期记忆的底层机制就是网络记忆，所以我们的记忆并不能像电脑那样直接查看，只能根据输入得到对应的输出。</p>
<p>四种情况例子：而如果要记忆四种情况，由于权重 [1, -1 ]并不满足四种情况，所以也会被更新掉。</p>
<p>1.4.4. 网络记忆本质</p>
<p>也就是说，网络记忆实际上是在寻找所有情况的共同规律。</p>
<p>1.5. 网络记忆特点</p>
<p>记忆缓慢：因为需要调整权重来寻找共同规律，所以网络记忆要比查找记忆慢的多。</p>
<p>记忆干扰：同时，共同规律往往会随新情况的到来而改变，所以我们已经形成的记忆会受新信息的干扰，而使用查找的电脑则不会。</p>
<p>泛化能力：但网络记忆最大的好处就是：它所找到的规律可以用来预测从来都没有见过的情况。</p>
<p>两种记忆对比：而查找记忆则办不到这一点，只能解决你告诉过它的情况，无法解决从未被记录的情况。</p>
<p>这就是为什么我们比一般的机器更有创造力。</p>
<p>1.5.1. 学习的定义</p>
<p>学习：从已知情况中寻找可预测未知情况的规律</p>
<p>知识：这个规律在意识层面上叫做知识，但更普遍的名字叫做模型。</p>
<p>1.5.2. 模型泛化能力</p>
<p>模型可以解决从未见过情况的能力叫做泛化能力。</p>
<p>1.5.3. 模型过拟合</p>
<p>而模型只能解决学习时见过的情况，却无法解决未见过情况的现象叫做过拟合。</p>
<p>练习例子验证：人类之所以需要练习的原因就是让大脑自动从各种不同的情况中寻找规律，以便应对从来没见过的情况。</p>
<p>工作例子验证：而在日语中有一个词，「指示待ち人間」，指的就是一类人：他们和机械一样，只能解决别人告诉过他的情况，却无法解决未被告知的情况，也就是泛化能力不足。 </p>
<h2 id="外语学习应用"><a href="#外语学习应用" class="headerlink" title="外语学习应用"></a>外语学习应用</h2><p>2.1. 英语功能描述</p>
<p>若把输入改为声波，而把输出改为英语， 那完成的功能便是英语的听力。</p>
<p>这时的输入和输出情况有多少种呢？无数种。口音，音调，发音长短的任何些许不同都是不同的情况。</p>
<p>2.2. 功能实现方式</p>
<p>这就意味着根本无法通过查找记忆来实现这样一个功能，只能通过学习来找到共同规律。因此英语听力需要练习。</p>
<p>2.3. 多例子学习</p>
<p>原因：但如果你只用一种口音来练习， 那么学到的规律就和只记忆 [1, 1] —&gt; 0 一样，仅仅是记住了这一种口音，并没有形成具有普遍性的规律，当来个新老外（无污蔑的意思）说话，你就听不懂了。</p>
<p>解决方法：所以需要的并不是一个口音反复听，而是听不同的口音，让大脑的自动从不同的口音中寻找规律。</p>
<p>2.4. 莫看字幕练习</p>
<p>原因：而当你看字幕来练习听力的时候，输入就既有语音，又有字幕。但大脑并不知道你要怎么学，它会自动的构建一个既使用语音输入，又使用字幕输入，来识别语言的网络。一旦撤掉字幕输入，那辛苦学到的网络就无法得到正确的输出结果了。</p>
<p>解决方法：所以，如果要练习听力，就将自己大脑接受到的输入信号控制为只有语音。</p>
<p>2.5. 音素输出扩展</p>
<p>原因：我们可以听懂外国人说中文是要归功于模型的泛化能力，但我们的中文泛化能力，由于会尽可能的识别从未见过的情况，反而在初学英语时，也会把英语识别成中文。</p>
<p>例子：比如，shazam 听成 傻蛋。</p>
<p>解决方法：所以，初学外语的第一步，就是扩展自己的语音的输出种类。一定不要用中文来标注英语音标，否则会一直停留在把英语泛化成中文的状态。</p>
<p>更正过程：而更正的过程就像左撇子改成右撇子一样，绝不是别人教你一遍就可以做到的事情。需要时间和练习来改变大脑的神经网络。</p>
<h2 id="网络中的遗忘"><a href="#网络中的遗忘" class="headerlink" title="网络中的遗忘"></a>网络中的遗忘</h2><p>不要用查找记忆来理解网络记忆的遗忘。</p>
<p>3.1. 两种遗忘对比</p>
<p>描述：查找记忆的删除是一条一条的删，而网络记忆的遗忘是连接的断开。</p>
<p>例子：比如，之前的例子，如果记忆完了 [1,1] —&gt; 0 和 [1,0] —&gt; 1 的 [1,-1] 权重中的 1 连接断开了，那么两条信息都会丢失，但如果 -1 的连接断开了，那么只有  [1,1] —&gt; 0 的信息会丢失。</p>
<p>3.2. 学习中的遗忘</p>
<p>因为网络是在寻找能符合所有情况的规律。</p>
<p>那么在这个过程中一定会出现有时断开连接，有时又重塑连接的现象。</p>
<p>假设猜想：如果一个人的大脑失去了断开连接的能力，那么他就会像记忆所有信息（就是网络的绝对过拟合），但寻找共同规律的学习能力却会严重受损。</p>
<p>假设佐证：比如现实中的雨人Kim Peek 就明显具有这种症状。他可以快速的看完一本书并且记住所有内容，但他的日常生活能力却异常低下，直到四岁才会走路，不会系扣子，智力测试也只有87分。</p>
<p>3.3. 学习后的遗忘</p>
<p>提问：上面的遗忘是发生在学习过程中的权重调整。但为什么已经记住了的内容也会被遗忘？</p>
<p>生存困难：原因就在于大脑找到的规律不一定是具有普遍性的规律，难免有局部规律。</p>
<p>例子：如果连续记忆这 [1,0] —&gt; 1， [1,1] —&gt; 0 两种情况，大脑就会找到 [1,-1]  规律。但该规律只是记住了这两种情况，无法预测其他情况。</p>
<p>演化结果：所以大脑演化出了根据使用的频率来强化和弱化连接的一种机制。它既不是为了消除痛苦回忆，也不是为了节省能量，就是为了抑制局部规律形成的过拟合。</p>
<p>预告：但是大脑对于不同的内容会有不同的态度，所以下一期我们会讲该如何有效的重复。</p>
<p>3.4.  你忘记所有的招式，就练成太极拳了</p>
<p>原文：“太极拳只重其义，不重其招。你忘记所有的招式，就练成太极拳了。”</p>
<p>解释：“忘记”并非字面意思，而是意为不要仅记住个别招式，而失去了应对无限情况的能力。这里的招式是“已知”，“情况”是指任何情况，包括已知，也包括未知。由于人脑的长期记忆也不是查找记忆，并不会去记忆每个情况，所以练太极拳的过程并不是记忆的过程，而是学习的过程。让大脑的神经网络从这些有限的招式中泛化出普遍规律，便可自动应对无限情况。</p>
<p>翻译：”情况无限，招式有限，需泛化有限招式，来应对无限情况。”</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>4.1. 说明</p>
<p>这些知识是机器学习中最基本的概念。</p>
<p>但机器学习本来就是要让机器来解决学习问题，因此完全适用于日常学习。</p>
<p>希望大家可以从生活中寻找更多的例子来反复体会这些概念。</p>
<p>也建议重新看一遍学习观1-7，相信你现在能够明白以前不懂的地方。</p>
<p>4.2. 概念</p>
<p>记忆：记住具体情况。 </p>
<p>模型的泛化能力：模型可预测未见过情况的能力。 </p>
<p>模型的过拟合：模型只记住了学习时见过的情况，却无法预测未见过情况的现象。 </p>
<p>学习：从已知情况中寻找可预测未知情况的规律的过程。 </p>
<p>查找记忆：将信息作为状态，存储到不同的地址中的记忆方式。</p>
<p>网络记忆：构建一个模型来根据输入计算出对应的输出的记忆方式，可被视为过拟合的学习。 </p>
<p>网络学习：可被视为泛化的记忆。 </p>
<p>网络记忆位置：由所有连接共同存储。 </p>
<p>学习中的遗忘：寻找共同规律（学习）过程中由于调整权重所造成的连接断开。 </p>
<p>学习后的遗忘：为提高泛化能力，根据连接被使用的频率强化或弱化连接的一种模型筛选机制。 </p>
<p>4.3. 应用</p>
<p>多例子学习：体验多个例子，让神经元更容易找到普遍的规律。 </p>
<p>音素的扩展：初学外语的第一步，先扩展音素，把每个音素都作为不同的发音。 </p>
<p>输入的控制：如果只练听力，就要控制涉及的输入类型，不要让额外输入参与到网络的构建中。</p>
<h1 id="如何靠“作弊”来快速记住面试、发表、演讲的内容"><a href="#如何靠“作弊”来快速记住面试、发表、演讲的内容" class="headerlink" title="如何靠“作弊”来快速记住面试、发表、演讲的内容"></a><span id="header15.6">如何靠“作弊”来快速记住面试、发表、演讲的内容</span></h1><p>1.生活出处需要表达</p>
<p>课堂的背诵，公众的演讲，论文的发表、工作的面试。再优秀的人也需要通过表达才能让他人知道自己的才能。</p>
<p>而为了展示自己最好的一面，往往会先写一个文稿。希望自己可以像文稿规划的一样来表达自我。</p>
<p>那该如何记住文稿的内容呢？</p>
<p>1.1. 常见背稿误区</p>
<p>很多人的方式是反复读稿，熟练到能够背诵为止，再上台。</p>
<p>但这种方式的效率实际上非常低下。一半以上的行为都属于无用功。</p>
<p>1.2. 反复读稿低效的原因</p>
<p>因为我们的记忆是以神经网络的形式存储在大脑当中的。输出是我们要演讲的信息，而输入是我们用于提取信息的前提。若没有对应的输入，即使记住了信息，也无法提取。</p>
<p>反复读稿之所以低效的原因就在于选择了错误的输入。</p>
<ol>
<li>如何有效背稿</li>
</ol>
<p>那读稿的输入究竟哪里有问题呢？</p>
<p>2.1. 输入分析</p>
<p>当反复读稿时，要构建的记忆的输入就会是「文稿的反光」和「脑中的声音」（或叫内部语言）。</p>
<p>2.1.1. 输入缺失</p>
<p>但在演讲时往往是不能拿文稿上台的，</p>
<p>这意味着你靠「文稿的反光」辛苦构建的记忆却无法再用「文稿的反光」来提取。</p>
<p>那就只能依赖「脑中的声音」这一个输入。</p>
<p>2.1.2. 输入不稳定</p>
<p>然而「脑中的声音」却很容易受自身和外界因素的干扰。比如周围的噪音，自身的情绪等。</p>
<p>最常见的就是当思考“自己忘词了怎么办？”“说不好会不会被嘲笑”“如果搞砸了会怎样”之类的问题时，就会因大脑转去预测这些问题，结果无法切换到原来的输入。</p>
<p>回过神来后，大脑基本就是一片空白。</p>
<p>2.2. 解决思路</p>
<p>所以对于专注力不是那么强的人，就不能仅依靠「脑中的声音」，还需要选择稳定的，固定不变的信息，作为记忆的输入。</p>
<p>2.2.1. 解决方案</p>
<p>那么哪些信息是演讲时稳定不变的呢？</p>
<p>如果你使用PPT来演讲，那么最稳定的信息就是当页 PPT 的对应图片。</p>
<p>如果没有PPT，也没有关系，自身的某个肢体动作，台下的某个坐席，麦克风的触觉，提眼镜等信息也都较为稳定，可以在你忘词的时候帮助你回到原来的输入上。</p>
<p>佐证：这也是触景生情的原理。人们在过去无意中将回忆关联到了某个场景；当再次看到同样的场景时，便会想起一连串的回忆。</p>
<p>2.2.2. 方案目的</p>
<p>那么我们也可以利用该原理，把演讲时一定会有的“场景”作为输入，把要演讲的句子作为输出，来构建二者的关联。这样即使没有文稿，也可以顺利地提取记忆。</p>
<p>2.3. 操作步骤</p>
<p>具体操作步骤如下：</p>
<ol>
<li><p>先看自己的文稿，暂时记住一句话或一小段话；</p>
</li>
<li><p>然后看着对应的 PPT 图片，或者做着某种肢体动作的同时，说出那句话；</p>
</li>
<li><p>对要记住的每一个句子都重复上述两步，直到记住所有演讲内容为止。</p>
</li>
</ol>
<p>注意1：不要一直边看文稿边读</p>
<p>这里最核心的关键就是，不要一直边看文稿边读。</p>
<p>因为你是要构建 「PPT 对应图片」与这句话的关联，而不是「文稿的反光」与这句话的关联。</p>
<p>毕竟在演讲时，文稿无法再成为你提取记忆的输入（也是为什么反复读稿很大程度上是在做无用功）。</p>
<p>而最有效的准备方式就是在你要演讲的实际场地进行练习。</p>
<p>因为实际场地的每一个细节信息都有可能成为你回忆起演讲内容的输入。</p>
<p>这也是乔布斯在准备演讲时所使用的方式。</p>
<p>注意2：两种记忆方式的优劣</p>
<p>虽然都使用 PPT 等稳定信息，但使用方式有两种：</p>
<p>1.将 PPT 信息作为「演讲内容」的主要输入</p>
<p>优点：使用该方式记忆较为轻松，因为 PPT 就相当于自己的稿子。</p>
<p>缺点：当 PPT 缺失的时候，就等同于稿子丢失，造成不知道说什么。同时一直盯着 PPT 并不适合要笑对观众的演讲场合。</p>
<p>2.将 PPT 信息作为「脑中的声音」的输入，作为提醒</p>
<p>优点：该方式是把 PPT 信息作为「提示词」，仅在想不起来内容时使用，可以表达自如。而且由于不会一直使用，所以「自身微动作」比 PPT 更合适作为输入。</p>
<p>缺点：不如第一种方式对「脑中的声音」的依赖性小。练习时间比第一种方式长。</p>
<p>附加：这并不是严格意义上的循环网络</p>
<p>机器学习背景的人可能觉得这很像循环神经网络。</p>
<p>但二者是有区别的：循环神经网路对每个时刻的处理都使用相同的神经网络参数；而在视频中的背诵演示中，不同时刻的处理并不一定使用相同的神经网络参数。</p>
<h1 id="学习观15-7：​昨夜想到个绝妙点子，想明天试下，结果。。猜到了吗？"><a href="#学习观15-7：​昨夜想到个绝妙点子，想明天试下，结果。。猜到了吗？" class="headerlink" title="学习观15.7：​昨夜想到个绝妙点子，想明天试下，结果。。猜到了吗？"></a><span id="15.7">学习观15.7：​昨夜想到个绝妙点子，想明天试下，结果。。猜到了吗？</span></h1><p>15.6中对步骤的描述没有提到是一句话或一段话的记忆。在这里更改一下。</p>
<p>步骤：</p>
<p>step1. 看自己的稿子的一句话或一段话，记住意思。</p>
<p>step2. 看ppt的图片（台下的坐席，灯光等），直接讲出刚才记住的意思。</p>
<p>重复上述过程直到记住所有演讲内容。</p>
<p>下面是一个图示：「演讲的第二句话」的输入是「演讲的第一句话」 + 「ppt图2」。</p>
<p>这个就是最基本的循环神经网络图。我们在用AI生成语言或声音时，多数都使用这类网络。</p>
<p><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/12.png" alt></p>
<p>剩下的还是一个很小的应用，大家都知道。</p>
<p>涉及的知识还是：</p>
<ol>
<li><p>网络记忆</p>
</li>
<li><p>影响输出的因素</p>
</li>
</ol>
<div style="text-align:center;font-size:30px">应用</div>

<p>1.现象<br>由于这几天不断做实验和整理论文，昨晚突然有了一个非常好的主意，想着明天编写程序尝试一下，便安心的睡了。结果第二天起来就想不起来了。 我想大家一定有类似的经历，尤其是做科研和艺术创作类工作的人。</p>
<p>2.疑问<br>但明明是自己想出来的主意，应该在自己脑中才对。</p>
<p>3.思路<br>依然是大脑对记忆的编码方式。我们得到的好主意是一条信息，而这条信息是由我们脑中的模型，也就是神经网络根据特定输入以及一些逻辑操作所产生的输出。</p>
<p>4.分析</p>
<p>睡一觉后会产生两个变化：</p>
<p>4.1. 相同模型，不同输入：一觉起来后，往往很难再获得同样的输入信号，所以即使拥有同样模型，也依然无法再产生那个好主意。</p>
<blockquote>
<p>验证例子：这也是为什么当人们想回忆起某事时，就会一步一步重现自己刚才做了什么，直到得到那个特定的输入为止。</p>
<p>验证例子：也是为什么自己可能很擅长某件事，但让自己回想自己究竟能干什么时，啥都想不起来，只有实际去做，获得对应的输出时，才能得到对应的输出。</p>
</blockquote>
<p>4.2. 相同输入，不同模型：脑中的神经网络在睡眠过程中会根据白天接收到的信息进行更新。很多人以为睡觉是在浪费时间，但睡觉相当于操作系统的更新，无法再运行中更新，需要重启。那么一觉起来后很多神经连接就发生了变化。所以即使有同样的输入信号，也很有可能会因为模型的改变而无法得到相同的输出。</p>
<p>5.解决</p>
<p>这也就是为什么「有了好主意，一定要记下来」。千万不要认为是你的主意，你就可以随时随地想出来，大脑并非像你想的那样自由。</p>
<p>关于网络记忆和查找记忆，有人在学习观15中问过，为什么不同时采用两种记忆方式呢。实际上，我们手中那个会发光的东西就是对人脑网络记忆的扩展。当年的诺基亚哪里会想到，手机不单单是通讯设备。</p>
<h1 id="学习观15-8：你们天天制造知识恐慌，应该自己吃掉自己的脑子"><a href="#学习观15-8：你们天天制造知识恐慌，应该自己吃掉自己的脑子" class="headerlink" title="学习观15.8：你们天天制造知识恐慌，应该自己吃掉自己的脑子"></a><span id="header15.8">学习观15.8：你们天天制造知识恐慌，应该自己吃掉自己的脑子</span></h1><div style="text-align:center;font-size:30px">四个应用</div>

<p>由于这四个小应用是一环扣一环的，因此请不要跳着阅读。</p>
<p>一、为什么用熟悉、掌握、精通来描述技能</p>
<blockquote>
<p>现象<br>相同的app安在不同的手机上会有一模一样的功能。<br>如果知识真的是信息，<br>那么不同的人学习相同的知识应该会有相同的能力。<br>可实际却是，相同的知识在被不同的人学习后，<br>会有熟练、掌握、精通之类的等级。</p>
<p>解释<br>学习知识的过程是用大量已知的情况（例子）来构建可预测未知情况的模型的过程。<br>除非要学习的知识是像“太阳从东边升起”这类输出不随输入而改变的“文科”知识，<br>否则根据人们所使用样本（例子）的数量和质量，所构建的模型会表现出不同的泛化能力。<br>熟练、掌握、精通实际上是在描述每个人所构建出的模型的泛化能力。</p>
</blockquote>
<p>二、为什么你打三年的游戏都没成为职业选手？</p>
<blockquote>
<p>现象<br>为什么打了十年的篮球/扑克/麻将/游戏也没有成为职业？<br>为什么看了十年的美剧也听不懂外语？</p>
</blockquote>
<p>标题是指我大学打了三年 Dota，还是菜的可以。<br>《刻意练习》书中也描述过相同的问题（我还没有看过）。<br>但若用机器学习来解释这些现象会非常简洁。</p>
<blockquote>
<p>解释<br>在机器学习中，任何模型（以神经网络为例）有两种模式：<br>一个是更新模式，也就是学习。<br>一个是预测模式，也就是应用。<br>我脑中的神经网络只有头两个月是处在学习模式，<br>随后都处在应用模式，一直用两个月学到的模型打 Dota。<br>因此不管我再打多少年的 Dota，还是头两个月的水平。</p>
<p>延伸<br>那么大脑什么时候会处在更新模式？多数是发生在睡眠时。<br>因为清醒状态时，我们需要应用模型来躲避危险，<br>这个时候不能随便更改模型。<br>类比：好比你在使用 word软件 写文档，若每时每刻都更新 word软件，定会影响文档的书写。合理的做法是等文档写完，关闭软件后再更新。这个关闭再更新的过程就发生在睡眠中。</p>
<p>因此不睡眠会失去更新的机会，从而丧失学习能力。<br>而睡眠时具体如何更新神经网络取决于白天搜集的信息。<br>信息的搜集还取决于好奇心（也就是学习观09中的内容，强烈建议你看完学习观15后，回过头再看一遍09）。<br>题外：我在知乎上看到高中生睡5个小时写作业的提问时，心里的状态是 “孩子，这你白天不是白学了吗”</p>
</blockquote>
<p>三、为什么听了那么多道理，依然过不好这一生？</p>
<p>这个问题在<a href="http://mp.weixin.qq.com/s?__biz=MzIxNDI5MDk0MQ==&amp;mid=2247484001&amp;idx=1&amp;sn=dfd88829ca2fe7a397ca7802f10a6ec4&amp;chksm=97a89af3a0df13e5b54304bf0075e72ebd3b954fc64ec7de89c6a6f66772d780951a3e73b462&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《毁掉一个人很容易：消其目的》</a>中的“丢弃这三类模型”下讲过三个原因。而这里补充的是第四个：</p>
<blockquote>
<p>现象<br>为什么很多人花了那么多钱，听了那么多的课，<br>全都听懂了，却没有任何长进？</p>
<p>解释<br>若能毫无障碍听明白每个细节，这基本上是在使用预测模式。<br>但在预测模式下能被识别出的信息并不会引起意识的注意。<br>例子：比如，大脑会时时刻刻预测你行走的每个楼梯阶梯的高度，若与预测相符，则根本不会发信号给丘脑（丘脑决定了意识会注意到什么），不会将这些信息搜集起来，用于在晚上睡眠时更新大脑的神经网络。</p>
<p>简言之，你是全都听懂了，但你就和看了一部喜剧片一样。<br>学习本来就是从“不能”到“能”的一个过程。<br>如果你能够毫无障碍，不加思索的预测每个细节，<br>那你根本就不用学。</p>
</blockquote>
<p>四、如何克服知识恐慌</p>
<p>现在到处有人制造知识恐慌，<br>看个动画片都能给你弹出来一个培训广告。<br>下面就告诉你怎么克服知识恐慌。</p>
<blockquote>
<p>1.知识不是信息。<br>卖课的再怎么制造恐慌，再怎么限时免费，<br>即使你可以把他的课倒背如流，信息还是信息啊。<br>而知识的描述更是无法使用的信息，很快就被忘记。</p>
<p>2.知识的学习是模型的构建。<br>学习需要更新大脑的神经网络，<br>若只处在预测模式，就等同于看喜剧片，<br>那为什么不看真的喜剧片呢？何苦折磨自己。</p>
<p>3.知识是筛选出来的。<br>对同一个任务，可以有无数个模型。<br>那么模型的筛选对学习就起着至关重要的作用。<br>不管是在演化中的自然选择，<br>还是在神经网络中的遗忘机制都是为了筛选模型。<br>科学可以在众多哲学中脱颖而出正是因为它有严格验证标准。<br>可目前的知识付费，并没有筛选环节。</p>
<p>4.科学家才是知识的制造者。<br>一个博士花数年的时间也只能在某个特定的领域将人类的知识扩展那么一点点。硕士与本科不一样的地方就在于硕士开始用科学制造知识（这里的科学不是一个形容词）。<br>人们总是妄想着某个“得道高僧”拥有什么“武林秘籍”，<br>学会了就能“号令武林”。<br>然而市面上所卖的真正知识都是从科学家那里来的，<br>始终可以从文献中找到。<br>所以不用担心自己错过“一统天下”的机会。<br>学习观中的知识也都是机器学习中的知识。<br>简言之，如果未达成条件，你教了我也学不了，恐慌个啥。<br>生活真不容易，还要防着僵尸们吃掉自己的脑子。</p>
</blockquote>
<h1 id="性与杀戮改变了所有生物的轨迹？六亿年前到底发生了什么？"><a href="#性与杀戮改变了所有生物的轨迹？六亿年前到底发生了什么？" class="headerlink" title="性与杀戮改变了所有生物的轨迹？六亿年前到底发生了什么？"></a><span id="header16">性与杀戮改变了所有生物的轨迹？六亿年前到底发生了什么？</span></h1><p>不少人认为繁衍和死亡是生命必不可少的部分。然而事实并非如此，因为生命的目的（被动）恰恰就是不惜一切代价地永远生存。但想要生存就需要预测信息（以躲避危险）；可想要预测信息就需要学习模型。而个体的死亡正是“想要”永远生存的种群为了学习模型所付出的代价。</p>
<p>违背多数人认知的是，遗忘非但不是缺陷，还是曾允许生命可以不再以个体的死亡为代价来学习的关键。因为在第二套学习系统中，遗忘代替了个体的死亡，起到了筛选模型的作用。这也是为什么遗忘被称为神经细胞群的自然选择。</p>
<p>那么 5.7 亿年前的生命是如何构建出这样一套网络学习系统的呢？这套系统又是如何改变整个生物界的呢？</p>
<ol>
<li>远古的尝试</li>
</ol>
<p>相变增强：可以明确的是，生命无法一下子形成特别复杂的功能，它们都是在原有功能的基础上涌现的。</p>
<p>例子：比如，草履虫的应激反应就是蛋白质间的涌现。</p>
<p>1.1.无聊的十亿年</p>
<p>演化停滞原因：而演化出真核细胞后的生命一直企图做到的就是细胞间的涌现。那么随后演化停滞长达十亿年之久的的原因，恐怕正是由极其苛刻的涌现条件所造成的。</p>
<p>涌现条件苛刻：因为要想涌现成新的学习系统，细胞之间不仅要形成稳定的连接，还要有统一的信息交流语言。</p>
<p>1.2.埃迪卡拉尝试</p>
<p>事件：而在 6.4 亿年前的埃迪卡拉纪，生命曾有过一种尝试，也被叫做阿瓦隆大爆发（Avalon explosion）。</p>
<p>诡异形态：当时的很多生物如狄更逊水母（Dickinsonia）既没有嘴巴也没有肠道，而是被动的等着海底养分流过，完全不同于今日的动物，不过却像极了我们的居民楼。</p>
<p>原因分析：这类生物更像是众多细胞的线性聚集，细胞之间相当于独立生存，所以才不得不长成让所有细胞都能充分吸收养分的形状。这与居民都想要采光和通风好的房间是同一个道理。</p>
<p>失败原因：然而这类生物虽然形成了稳定连接，却未能达成总体大于个体之和的涌现，在寒武纪大爆发之前就突然集体消失了。</p>
<p>1.3.寒武纪尝试</p>
<p>事件：而几乎就在同一时期，出现了专门用于信息交流的神经细胞和弥散的神经网。随之而来的就是寒武纪时期的动物集中大爆发。</p>
<p>类比：人类个体也曾经历过类似的模式，花费了漫长的时间才形成了稳定的连接，有了统一的语言，实现了多次涌现，并仅在过去的 250 年间（0.01％的时间），集中创造出了人类 97% 的财富。</p>
<p>提问：那么当初这些神经细胞究竟是如何实现稳定连接和信息交流的呢？</p>
<ol>
<li>神经元交流</li>
</ol>
<p>神经细胞虽种类多样，但基本原理相同。</p>
<p>2.1.物理连接</p>
<p>轴突：经历了一代又一代的变异与筛选，神经细胞演化出一条长长的尾巴，叫轴突（Axon），</p>
<p>树突：能与其他神经细胞的胞体（Cell body）或树突（Dendrites）连接在一起。</p>
<p>胶质细胞：同时还有一类叫做胶质细胞（Glial cells）的帮凶支撑和隔离神经细胞并提供养分，也就解决了网络搭建问题。</p>
<p>2.2.统一语言</p>
<p>语言需求：然而为了形成减熵系统，神经细胞还需要统一的交流语言。</p>
<p>明星创业：不过之前蛋白质们所达成的涌现，使细胞已经拥有了制造电脉冲的能力。而神经细胞所使用的语言，也正是这种电脉冲的发送频率。</p>
<p>语言类比：此时每个神经细胞就相当于一个钢琴键。当众多钢琴键以不同的频率共同被弹奏时，就产生了你我所看到、听到、感觉到的各种不同体验。而神经细胞的工作机制也与钢琴键非常类似。</p>
<p>2.3.起始阶段</p>
<p>作用类比：钢琴键被按下之前，都会处于初始状态；而神经细胞被激活之前，也会处于膜电位（Membrane potential）约在 -70 mV 的初始状态。这相当于琴键悬空的那一小段距离。</p>
<p>静息电位：这种初始的膜电位状态叫做静息电位（Resting Potential）。</p>
<p>极化：使神经细胞处于这种初始状态的过程叫做极化（Polarization）。</p>
<p>工种名称：促使这一过程的关键是一类叫做钠钾离子泵（Sodium–potassium pumps）的蛋白质“工人”们。</p>
<p>工种功能：这些蛋白质会消耗能量（ATP），通过控制钠离子和钾离子的进出，让膜电位始终保持在 -70mV。</p>
<p>工种恰饭：大脑一半的能量都消耗在了这一过程。</p>
<p>也就是说，即使什么都不想，大脑也是个“饭桶”：它仅占我们身体总重的 2%，却消耗 20% 的总能量；新生儿大脑的消耗更是多达 60%。</p>
<p>2.4.发送阶段</p>
<p>作用类比：钢琴键被按下之后，会带动连锁反应，发出声音。而当神经细胞被激活后，也会发出一个电脉冲。</p>
<p>激活条件：被激活的条件是轴突与胞体相连处，也叫轴丘（Axon hillock），的膜电位升到 -55mV 。如果没有达到这个阈值，则不会产生任何反应。这与琴键需要按下一定的距离才能发声的道理一样。</p>
<p>去极化：发送电脉冲的过程（Depolarization）。</p>
<p>工种名称：促使神经细胞产生电脉冲的是一类叫做电压门控钠离子通道（Voltage-gated sodium channels）的蛋白质“工人”们。</p>
<p>工种功能：这些蛋白质会在膜电位升到 -55mV 时开门，允许钠离子涌入，使膜电位飙升到 +40mV，带动其他部位连锁的去极化，形成动作电位（Action potential），一直传导到与其他神经细胞连接处，也叫突触（Synapse）。</p>
<p>又一帮凶：而为了加快传导速度，又有一类叫做施旺细胞（Schwann cells）的帮凶们从中作梗，一层一层的包裹轴突形成髓鞘（Myelin sheaths），使动作电位只能在中断的部分，又叫兰氏结（Nodes of Ranvier），之间跳跃式传导（Saltatory conduction），把传导速度从10 m/s 增至到了 150 m/s。</p>
<p>2.5.复位阶段</p>
<p>作用类比：钢琴键在发出声音后，需要复位到初始状态，以待下次敲击；同样的，神经细胞被激活后也需要重新回到初始状态。</p>
<p>再极化：这一复位过程也叫再极化（Repolarization）。</p>
<p>工种名称：帮助神经细胞复位的是一类叫做电压门控钾离子通道（Voltage-gated potassium Ion channels）的蛋白质“工人”们。</p>
<p>工种功能：这些蛋白质会在膜电位高于 +40mV 左右时开门，允许钾离子涌出，使膜电位迅速下降到低于 -70 mV 。随后再由钠钾离子泵调整到 -70 mV 的初始状态，等待下次激活。</p>
<p>阶段小结：初始，发送，复位，这三个阶段构成了一个完整的“按键”功能，使神经细胞之间有了彼此发送信号的能力。</p>
<p>不足之处：但这一过程却存在一个问题：“按键”发送的信号都是一样的。这就好比所有钢琴键都只能发出一种声音。</p>
<p>2.6.变频阶段</p>
<p>需求分析：可要想形成语言，神经细胞必须要有改变信号频率的能力。</p>
<p>调整场所：而这一调整就发生在神经细胞之间的突触上。</p>
<p>工种名称：该过程同时涉及到了：</p>
<p>运输车，又叫突触囊泡（Synaptic vesicles），</p>
<p>送信员，又叫神经递质（Neurotransmitters），</p>
<p>和收信员，又叫配体门控离子通道（Ligand-gated ion channels）。</p>
<p>工种功能：当动作电位传导到轴突的末梢时，会让突触囊泡（运输车）穿过细胞膜，释放出神经递质（送信员）。</p>
<p>而在接受信息的神经细胞的细胞膜上，有很多不同的受体（收信员），在与特定的神经递质结合时会开门，允许对应离子通过（或造成其他影响）。</p>
<p>有的会增高膜电位（激发），有的则会降低膜电位（抑制），影响膜电位累积到 -55mV 的速度，从而调整了电脉冲被触发的频率。</p>
<p>共同决定：由于调整膜电位的神经递质来自所有与它连接的神经细胞，所以该神经细胞的“按键”是否被按下，是由这些神经细胞所共同决定的。</p>
<p>语言诞生：「完整的按键功能」加上「可调的发送频率」，使神经细胞就可以靠频率来发送不同信息。好比钢琴键可以发出不同的音调。</p>
<p>意义：这便是首次在前寒武纪所出现的系统。然而它的出现却彻底打破了整个生物界的平衡。</p>
<ol>
<li>神经网功能</li>
</ol>
<p>神经计算：当众多神经细胞一起工作时，它们不仅可以传递信息，还可以通过计算，实现任何功能。</p>
<p>例子：比如按这个比例改变频率就可以实现异或门功能，而按其他频率就可以识别数字。</p>
<p>3.1.感知系统</p>
<p>神经系统前：神经系统出现之前，即使个体演化出了眼睛也无法使用。因为细胞们根本来不及搞清探测到的光子信息意味着什么，信息就变了。</p>
<p>神经系统后：但有了神经系统后，想演化出眼睛，只要感光细胞（Photoreceptor cell）聚成一排,把扫过的光学信号即时的翻译成电脉冲频率，再经神经细胞群的整合计算，就可以瞬间得出形状和方向等信息。</p>
<p>多维感知：不仅是反光，任何物理信息都可以被对应感知细胞转换成电脉冲频率，形成能感知外界环境的多维系统（Multi-sensory system）。</p>
<p>3.2.运动系统</p>
<p>超智能体：而感知系统侦测到的信息，又可以经过另一组神经细胞群的计算，得到实时控制心跳、血压、肌肉等各个“部门”的工作指令，让所有细胞涌现成一种全新的超智能体：动物。</p>
<p>共同协作：动物完全可以被想象成是一群细胞所驾驶的航空母舰，而促使这些细胞成为一体的就是神经系统所带来的高速信息交流能力（想象5G普及后会怎样）。</p>
<p>共同协作：如果向航空母舰里的某个细胞以相同的语言发送信息，就可以获得它的控制权；而如果能阻断细胞之间的信息交流，就可以彻底杀死整个超智能体。</p>
<p>例子：比如，河鲀毒素就能和钠离子通道结合，阻断发给肌肉细胞的指令，0.5 mg 便可让人死于麻痹（暂无有效的解毒剂）。</p>
<p>这也是为什么英美这么爱干预中国的内政，因为他们根本撼动不了由所有中国人所构成的超智能体。</p>
<p>3.3.性与食</p>
<p>感知运动：不管怎样，神经系统的出现，使生命首次拥有了实时感知环境并立刻做出运动反应的能力。</p>
<p>有性生殖：这种感知运动能力使不同个体可以充分洗牌后交配，不必担心近亲交配导致备选模型（个体）差异性不足，而被自然一次性全部筛选掉。</p>
<p>捕食者：同时也造就了生命史上从未有过的角色：捕食者（Predator）。</p>
<p>无一幸免：捕食者的出现给演化算法引入了巨大的模型筛选压力。这等于给埃迪卡拉纪那些没有保护的软体生物全都判了死刑，存活了下来的多是不小心变异出甲壳的个体。</p>
<p>协同演化：又由于捕食者也会因没有捕食利器而被饿死，造成捕食者与被捕食者之间相互的筛选，形成了正反馈循环（Positive feedback），从此再也停不下来了。</p>
<p>3.4.寒武纪大爆发</p>
<p>物种爆发：</p>
<ul>
<li>由“氧”所支撑的高额耗能</li>
<li>由神经系统所实现的感知运动</li>
<li>由“性”所增多的备选模型</li>
<li>由“食”所带来的筛选压力</li>
</ul>
<p>这一切彻底地引爆了寒武纪的物种演化。</p>
<p>生命形态转变：起初生命是对抗熵增的一个奇迹。自此生命是比拼策略的一场游戏。</p>
<p>这恐怕就是寒武纪时期究竟发生了什么的回答。</p>
<p>然而这里却存在一个关键的问题：神经系统确实可以靠调整频率实现任何功能，但神经细胞们是怎么知道该如何调整频率才能产生想要的感知和运动的呢？</p>
<ol>
<li>神经网学习</li>
</ol>
<p>学习模型：答案是学习。</p>
<p>如果没有学习，上述的一切都不可能发生。</p>
<p>神经演化：最初的感知与运动，如视觉与心跳，依然是由演化算法，通过个体的死亡，筛选随机变异出的突触而获得的，被称为神经演化（Neuroevolution）。</p>
<p>神经网络：然而随着运动能力的增强，环境变化速度越来越快，个体生长周期也越来越长，如果还靠个体的死亡来筛选合适的突触，那没等筛选出来，就都死光了。</p>
<p>新的演化：而打破这一僵局的是一个细微的演化产物：突触可塑性（Synaptic plasticity）。</p>
<p>4.1.突触可塑性</p>
<p>突触可塑性：突触上的受体不再固定不变，而是可以根据经验自动调节。</p>
<p>比如，长期增强作用（Long-term potentiation）可以通过增多受体数量，加快接受神经递质的速度。</p>
<p>又如，长期抑制作用（Long-term depression）可以通过减少受体数量，降低接受神经递质的速度（甚至屏蔽）。</p>
<p>常见误区：但二者都对学习有重要作用，不要认为长期抑制作用是抑制学习。</p>
<p>因为学习是寻找合适频率的过程，而不是一直增大频率的过程。</p>
<p>新的适应力：这种调节能力就好比按键手机变成了智能手机，可以安装新的 App，无需返场重造。这样即使个体移动到了全新的环境，也可以通过重塑突触而适应，无需死亡。也正是这一特性，使动物个体首次在生命周期内拥有了学习能力。</p>
<p>4.2.个体学习</p>
<ul>
<li>新的缺点：然而这种学习能力看似美好，却有致命的缺点，也为随后的生命演化埋下了巨大伏笔。</li>
<li>原因对比：由于个体学习并不像演化算法那样，依靠试错，找出规律，而是总结经验，找出规律。</li>
<li>经验需求：那么就必须要有经验（也就是例子）；这也是从此个体需要记忆能力的原因。好在神经系统既可以学习又可以记忆。</li>
<li>泛化需求：但是总结经验所找出的规律必须要能预测这些经验之外的情况，否则个体依然会死。也就是说，需要泛化能力（Generalization）。</li>
<li>尝试与筛选：而泛化能力强的规律是不可能一下子就找到的，跟演化一样，需要不断尝试，不断筛选。</li>
</ul>
<p>而在神经网络中，</p>
<ul>
<li>不断尝试表现出的现象就是：重复（强化使用频率高的连接）。</li>
<li>不断筛选表现出的现象就是：遗忘（弱化使用频率低的连接）。</li>
</ul>
<p>遗忘是学习：所以遗忘并不是大脑的缺陷，而是筛选规律的过程，是学习的关键部分。</p>
<p>类比：就如同科学家排除不符合实验结果的那些猜想一样。<br>遗忘重要性：没有遗忘，个体就不得不靠以死亡为代价的试错来筛选模型。</p>
<p>4.3.正确学习</p>
<p>知道这些后，便可知为什么《学习观》第一季中反复说：</p>
<ul>
<li>“学习是重塑大脑连接”</li>
<li>“学习要明确输入输出”</li>
<li>“学习要通过多个例子”</li>
</ul>
<p>重塑大脑连接：如果你只使用现有连接，那你大脑的功能根本不会改变。</p>
<blockquote>
<p>例子：这也是为什么玩了十年游戏，还是个菜鸡的原因。</p>
</blockquote>
<p>明确输入输出：如果你选错了输入输出，那你所改变的就是不同感知细胞之间的连接。</p>
<blockquote>
<p>例子：这也是为什么若边看字幕边练听力，撤掉字幕就听不懂的原因。换句话说，想在什么情况下使用，就在什么情况下学习。</p>
<p>例子：同时这还是为什么背诵时要打乱顺序的原因，因为你不想让上一个信息作为下一个信息的输入。</p>
</blockquote>
<p>注意：输入并不是一个神经细胞频率，而是一群神经细胞的频率所共同表达的状态。比如画面的输入就是整个平面的感光细胞的共同状态。</p>
<p>多例子学习：而如果你没通过输入多个例子学习，那你的神经系统就找不到共性，会把知识作为信息遗忘掉。</p>
<blockquote>
<p>例子：这也是为什么反复把一个例子背无数遍，还是很快就遗忘的原因。</p>
</blockquote>
<p>情况1：当上一个信息在使用中无法获得时，需要打乱顺序。如每个单词的使用都是独立的，若不打乱顺序就需要回忆起上一个单词才能回忆起下一个单词。</p>
<p>情况2：当上一个信息在使用中可以获得时，无需打乱顺序。如背诵演讲稿的每句话都对上一句话有依赖，不要打乱顺序。</p>
<p>记忆技巧：不过我们可以利用大脑的特性来欺骗大脑。</p>
<blockquote>
<p>例子：比如说，虽然用一个例子学习很容易遗忘，但如果你在家里和学校等多个场地，或者课本和科普等多个教材上看到同一个例子，那大脑就会把它作为不同例子来对待。</p>
</blockquote>
<p>因为不同场所会触发不同的感知细胞，而大脑很容易找到这些感知细胞所接收到的信息的共性。</p>
<p>这种技巧还有很多，会当更新到演化出的对应功能的位置时详细讲解。</p>
<p>此时生命还没有演化出意识，但我想提一个心理学中经常说的概念：表征。</p>
<p>4.4.英语与表征</p>
<p>表征：你肯定听过表征（Representation），但你一定不知道它是什么意思，又为什么需要表征。</p>
<p>原因是，这个世界上的猫有无数种形态，你怎么知道你见到了主子？</p>
<p>但大脑却能识别出来的原因是不管什么形态的猫都会被神经网络转化成统一的判断结果。</p>
<blockquote>
<p>例子：比如，偶数有无数个，但判断一个数是否为偶数时，都会转化成该数除以 2 再取余，这就是表征。我们总是爱问“什么的本质是什么”的原因也在于此。</p>
</blockquote>
<p>英语的难点：这同时也是外语（英语）难学的地方。</p>
<p>因为所学习的单词的意思不是图片，不是文字（它们都是感光信号），而是大脑内部对该单词的表征。</p>
<p>4.5.人工神经网络</p>
<p>深度学习：这种神经系统随后也被我们借鉴到人工智能的研究中，产生了深度学习技术。</p>
<blockquote>
<p>相同点：<br>「细胞群频率」对应着「向量的数值」<br>「激活的条件」对应着「非线性函数」<br>「可变频突触」对应着「网络的权重」</p>
</blockquote>
<p>人工遗忘：而为了增加泛化能力，也人为地加入了 Dropout 这种遗忘机制。</p>
<p>不同点：和生物神经网络不一样的是，深度学习是靠反响传播算法调整权重的，因为我们暂时还不知道大脑在分子层级上具体是基于什么原则调整频率的。</p>
<p>AI vs 生物：2012 ~ 2017 年就相当于人工智能的寒武纪大爆发。计算机首次获得了感知等级（Perception）的智能；语音，画面等感知都不再是问题了。</p>
<p>AI少了什么：但如果你认为这样我们就会被机器所统治，那你也太小瞧生命在随后 5 亿年中的演化了。因为在寒武纪，生命除了感知，还演化出了一个更重要的功能…</p>
<h1 id="学习观18-5：高考导致了“高分低能”？素质教育改革可能出现什么问题？"><a href="#学习观18-5：高考导致了“高分低能”？素质教育改革可能出现什么问题？" class="headerlink" title="学习观18.5：高考导致了“高分低能”？素质教育改革可能出现什么问题？"></a><span id="header18.5">学习观18.5：高考导致了“高分低能”？素质教育改革可能出现什么问题？</span></h1><p>1.聪明的马</p>
<p>为了解释为什么，让我们回到1900年初的德国。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/13.png" alt><br>照片里的这个人叫做 威廉 奥斯汀，曾是一名数学老师。一般大胡子的数学老师们都不太安分，而奥斯汀就觉得动物的智力被低估了，于是就开始教名为汉斯的一匹马学算数。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/14.png" alt><br>经过一段时间的训练后，奥斯汀便把汉斯带到众人面前来展示。当问到二加二等于几时，汉斯踏了四下蹄子。而问到“若今天是星期一，再过多少天是星期六”时，汉斯竟仍然可以正确的踏五下蹄子。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/15.png" alt><br>德国人可是一个很严谨的民族，专门成立了一个委员会来调查此事，带头的是另一个大胡子，叫做卡尔 斯坦普夫。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/16.png" alt><br>经过严格的调查后，他们并没有在奥斯汀身上发现任何欺诈或作弊行为。甚至用别人替代奥斯汀来提问，汉斯仍然可以正确回答。<br><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/17.png" alt><br>可问题出现在，当提问者远离汉斯的视线，或提问者自己也不知道答案时，汉斯便无法回答了。</p>
<ol>
<li>建模偏差</li>
</ol>
<p>结论：事实上，汉斯并没有真正学会的算数，而是构建了另一种模型：根据提问者在无意识间流露的微表情和肢体动作来判断什么时候该停止踏蹄。</p>
<p>PS：这可是所有女生都默认自己男朋友有，但实际男朋友根本没有的能力。</p>
<p>特点：虽然汉斯构建的微表情模型能够在提问者在场的条件下，完成同算数模型一样的功能，但二者并不是一个模型。</p>
<p>启示：后来这个故事就被叫做“聪明的汉斯”。在心理学（观察者期望效应）和机器学习领域都给了人们启示。 </p>
<p>2.1.机器学习启示</p>
<p>聪明汉斯效应：机器学习中就以此来说明：人工智能看似能完成的一些智力任务，其实是在依赖所找到的某些微妙特征。一旦条件出现哪怕些许的变动，人工智能就很容易失效。</p>
<p>A.I.例子：比如，某些识别坦克和飞机的模型，其实是在识别背景是否存在蓝天色。因此人们称机器根本没有理解知识。</p>
<p>2.2.教育启示</p>
<blockquote>
<p>人类例子：然而较为讽刺的是，人类自身也存在着同样的问题；学生构建的模型能够在卷面上，实现与教育想教的模型一样的功能，但二者往往也不是同一个模型。</p>
<p>建模模式：因为在这种条件下，不管是汉斯，还是升学的学生并不是你想教什么，他们就学什么，而是通过最大化某一奖励（reward）来构建模型的（强化学习模式），但能最大化奖励的模型并不一定是教育目标的模型。</p>
<p>宠物例子：对于汉斯或宠物们而言，它们的目的是最大化主人给的食物（奖励）。至于自己是否真的学会了算数或握手对最大化食物并没有影响。宠物学到的“握手”很可能只是想要把你手中的食物“按”出来。</p>
<p>学生例子：对于需要升学的学生而言，他们的目的是最大化自己的分数（奖励）。虽然用真正的知识也可以得到正确答案。但若有另一个模型可以用更快捷的方式得到正确答案的话，哪怕是“三短一长选一长，三长一短选一短”这种无法解决实际问题的模型，学生们依然会选择后者。</p>
</blockquote>
<p>2.3.教育偏差现象</p>
<blockquote>
<p>应试系统：实际上，扮演汉斯角色的并不仅仅是学生。初高中的老师、家长以及培训机构也都是这个系统下的参与者，其目的同样是最大化学生的分数，而这逐渐衍生出来了另一套看似在教实用知识，其实是在最大化分数的教育系统（并没有对错）。</p>
<p>辅导书现象：你是否曾好奇过：为什么初高中的英语教材在每一课的后面都有对话和听力等专项练习？毕竟老师上课的时候根本不会用到那些部分啊。为什么不直接选择一套好的辅导书作为教材，这样不就不用额外花钱买辅导书了吗。</p>
<p>奖励差异：这是因为编写教材的专家们想要最大化的奖励与编写辅导书的专家们想要最大化的奖励并不相同。</p>
<p>辅导书目的：辅导书是升学专家为了让学生学会能最大化得分的知识而编写的材料；</p>
<p>教材目的：教材是教育专家为了让学生学会能解决实际问题的知识而编写的材料。</p>
<p>英语例子：比如《新概念英语》的设计就非常科学，完全对应《学习观07》中提到的听、说、读、写四种模型，并为每一种模型都专门准备了输入和输出的练习材料，对应的正是课后的对话和听力。</p>
<p>实用vs应试：然而，对于最大化分数这个目的而言，教材的知识并不是最有效的模型，因此往往被取代。</p>
<p>例子：不少人在《学习观07》下留言，说视频中介绍的单词学习法太慢。他们其实是以“最大化分数“为目的而做的评价，事实上也确实如他们所说。但若以“有效的交流”为目的时，学习观07中介绍的就是最有效原则。</p>
</blockquote>
<ol>
<li>建模偏差现象</li>
</ol>
<p>回到开篇的“两种声音”。</p>
<p>3.1.知识无用论</p>
<p>第一种声音：第一种声音的拥护者所说的“知识无用”，其实是指那些为最大化分数而构建的知识。</p>
<p>类比：那么就会像汉斯离开提问者便无法答题一样，当这些学生离开学校后，当初在学校构建的模型往往也不能被用于解决实际问题。</p>
<p>高分低能原因：这也是为什么“高分低能”会出现的原因之一，不仅仅是人们所说的“智商高，情商低”。</p>
<p>第二种声音：而第二种声音的拥护者所说的“知识有用”，指的是可解决实际问题的知识。这些知识都是人类最聪明的那些大脑所沉积下来的模型，是人类繁荣的根基，其有效性自然无需质疑。</p>
<p>结论：两种声音之所以都对，是因为它们最初描述的就不是一个事物。</p>
<p>3.2.教育错配就业</p>
<blockquote>
<p>起因：正是由于 “能最大化分数的模型” 并不一定是 “教育目标的模型”，就让学生们在就业时处在一个很尴尬的局面。</p>
<p>筛选方式：企业（尤其小企业没有资源，不得不利用升学系统）一方面靠分数和学校出身来筛选（应届生）人才，</p>
<p>筛选目标：另一方面，又希望筛选出的人才可以解决工作中的实际问题。</p>
<p>尴尬局面：这种筛选方式与筛选目标的不匹配，使得千辛万苦通过考试的学生们（应届生）又不得不在进入职场后，重新参加培训来学习实用知识。</p>
</blockquote>
<p>4.建模偏差消除</p>
<p>4.1.更改奖励函数</p>
<blockquote>
<p>素质教育改革：听到这里一定会有抨击高考的声音，希望可以推行素质教育改革。其实教育部早在2001年就提出了教育改革的口号，并试图推行素质教育，然而却并未见效，为什么？</p>
<p>系统演化方向：先前提到过，升学的学生是通过最大化某种奖励来构建模型的，不管怎样限制，随着时间的推移，升学教育系统最终都会演化成学生在学习“能最大化奖励的模型”，而不是在学习“教育目标的模型”。</p>
<p>类比：这就如同任何网络游戏最终都会演变成单一的最优玩法而变得枯燥一样（必然趋势）。</p>
<p>机器学习视角：只要升学教育系统主要的目的是为了筛选人才，那么素质教育改革的问题其实就可以被视为：能否通过改变奖励机制，使“能最大化奖励的模型”就等于“教育目标的模型”，或者说让“得分目标”完全等价于“能力目标”。</p>
<p>可行性：关于这个问题，其实并不是绝对不可能，但我们可以参考机器学习中的一个笑话。</p>
<p>AI类比：如果你把吸尘器的奖励函数设计成“捡垃圾越多奖励越多”，那么很可能产生的后果是，吸尘器学会了捡垃圾，又学会了放下，然后捡起和放下无限循环。</p>
<p>结论：所以在设计奖励机制时，难以避免会出现这种意外结果。</p>
</blockquote>
<p>4.2.如何教育改革</p>
<blockquote>
<p>假设：倘若推行素质教育后，其奖励机制依然不能使“得分目标”等价于“能力目标”的话，</p>
<p>预测：那么系统最终也一定会根据素质教育的奖励机制演化成另一套偏离教育目标的应试教育系统。</p>
<p>应试成本：唯一不同的是，这套新的教育系统会使学生获得分数的成本变高，让人才筛选变得不再公平。</p>
</blockquote>
<p><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/18.png" alt></p>
<blockquote>
<p>例子：比如富人的得分上限是900，而穷人的上限只能是600（分数只是升学条件的数字化，不一定真的是数）。</p>
<p>类比：这种“素质教育”就如同一款宣称大家可以打神装的网络游戏。但实际上必须要先充钱才有可能打得过BOSS，那么充不起钱的玩家和充的起钱的玩家的差距将会越来越大。</p>
</blockquote>
<p>美国素质教育：最好的例子就在美国。美国素质教育的评分机制涉及各种才艺特长（主观评判），很多只有富人的子女才能享受到的提分渠道。</p>
<p><img src="/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/19.png" alt></p>
<p>例子：在2019年爆出的“美国高校招生丑闻”，更是揭露了美国富人是如何利用“素质教育”的评分机制，让自己的子女获得更高的分数，从而进入名牌大学。</p>
<blockquote>
<p>应对思路：如果当前无法保证能设计出使“得分目标”等价于“能力目标”的奖励机制，那么最合理的解决方案恰恰不是推行德、体、艺等提高评分条件的“素质教育”，反而是降低学生得分成本的“应试教育”，让每个学生都有机会仅靠历年真题便能获得所有分数。而这个“应试教育”就是高考。</p>
<p>高考制度：它主要解决的并不是人才教育的质量问题，而是人才选拔的公平问题。大家其实也都清楚这一点，正如大家喜欢的网络游戏都是没有充值系统的网络游戏。</p>
</blockquote>
<p>4.3.明确输入输出</p>
<p>那么建模偏差难道就没有办法消除了吗？</p>
<blockquote>
<p>教育问题：说到底，教育体制之所以这么难改革，是因为它主要就不是教育学问题，而是社会学问题。</p>
<p>选拔目的：只要一个系统起到的是筛选（升学）作用，那么系统内的个体就无法跳出“最大化奖励”的行为模式。</p>
<p>非选拔目的：但只要这个系统脱离筛选目的，学生就可以自己通过明确输入输出，来抑制建模偏差。</p>
<p>例子：比如，当输入也同时固定时，汉斯就无法用微表情，只能用算数来得出结果了。这便是为什么明确输入输出是《学习观》中的第一条学习原则。</p>
<p>实际解决方案：可是社会又需要人才筛选的功能，那么就只能暂时把教育系统进行分段：一个是负责筛选目的的义务教育阶段。一个是负责实用目的的高等教育阶段。</p>
</blockquote>
<ol>
<li>被禁止的高三</li>
</ol>
<p>现在我们就可以回答《学习观》的一个奇怪规定了。</p>
<p>5.1.高三教育阶段</p>
<p>问：为什么初中生，高一高二可以看学习观，唯独高三不可以看，可一旦高考结束，就又可以看了。</p>
<p>答：因为高三学生所处的正是起到筛选目的的义务教育阶段，无法避免（也应该这么做）的要靠学习应试知识来最大化自己的分数。</p>
<p>5.2. 学习观偏实用</p>
<p>适用范围：虽然《学习观》中介绍的学习方法既适用于应试知识的学习，又适用于实用知识的学习，</p>
<p>当前价值偏向：但由于目前的《学习观》主要强调的是负责实用目的的高等教育，比如学习观07就不是为应试生而准备的，所以我很担心会有高三学生看完目前的学习观后，产生“应试知识没用”的心理而耽误自己的人生，因此才禁止高三学生观看。</p>
<p>跳节更新原因：我上一篇文章中也提到过，《学习观》以后会多出一个部分，来专门针对应试知识的学习。这也是为什么突然从16期，临时更新了一个18.5期来介绍应试知识和实用知识的区别。</p>
<p><div style="text-align:center;font-size:30px;">结束语</div><br>其实在《学习观》中，</p>
<p>我始终都在避免谈论天赋、制度等问题。</p>
<p>尽管我们都知道存在天赋异禀的能人，</p>
<p>尽管我们都疑惑高考究竟筛选了什么。</p>
<p>但这些都不是我们能改变的因素。</p>
<p>毕竟我们无法像玩网络游戏那样，</p>
<p>在资质不满意或制度不合理时换游戏。</p>
<p>唯一能做的只有集结所有自己能改变的因素，</p>
<p>选出最合理的每一步。</p>
]]></content>
      <categories>
        <category>YJango</category>
      </categories>
      <tags>
        <tag>学习观</tag>
        <tag>YJango</tag>
      </tags>
  </entry>
  <entry>
    <title>YJango其他文章汇总</title>
    <url>/2020/08/11/YJango%E5%85%B6%E4%BB%96%E6%96%87%E7%AB%A0%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p><a href="https://githubzhangshuai.github.io/2020/08/10/%E5%AD%A6%E4%B9%A0%E8%A7%82/#more" target="_blank" rel="noopener">YJango学习观系列</a><br><a id="more"></a></p>
<ul>
<li><a href="#header1">函数、概率、信息的基础串讲</a></li>
<li><a href="#header2">如何生动有趣的入门线性代数</a></li>
<li><a href="#header3">概率simple入门</a></li>
<li><a href="#header4">很多人都不知道自动化与机器学习的区别</a></li>
<li><a href="#header5">机器学习理论引入的知识分类</a></li>
<li><a href="#header6">英语总结</a></li>
<li><a href="#header7">深层学习为何要“Deep”</a></li>
<li><a href="#header8">YJango的TensorFlow1.x整体把握</a></li>
<li><a href="#header9">YJango的循环神经网络——介绍</a></li>
<li><a href="#header10">YJango的循环神经网络——实现LSTM</a></li>
<li><a href="#header11">YJango的循环神经网络——scan实现LSTM</a></li>
<li><a href="#header12">YJango的循环神经网络——双向LSTM&amp;GRU</a></li>
<li><a href="#header13">YJango的卷积神经网络——介绍</a></li>
<li><a href="#header14">YJango的Word Embedding—介绍</a></li>
<li><a href="#header15">YJango的Batch Normalization—介绍</a></li>
</ul>
<h1 id="函数、概率、信息的基础串讲"><a href="#函数、概率、信息的基础串讲" class="headerlink" title="函数、概率、信息的基础串讲"></a><span id="header1">函数、概率、信息的基础串讲</span></h1><p><img src="https://pic1.zhimg.com/v2-0cd32763d1067e4fa6ae63e494302e07_1440w.jpg?source=172ae18b" alt></p>
<p>一、函数（映射）<br>这是一个发生在信使、农民、猎人、学者、商人之间的故事。<br>为了在恶劣环境下增加生存几率，信使编造了一个故事：<br>大家都是象神的子民，需一起生活，用自己的劳动所得去换取别人的劳动所得。</p>
<p>一次，猎人用一块猪肉与农民交换了一个土豆。<br>但第二次交换时，肉块比较大，猎人觉得自己换的不如之前的多。<br>为确保每次交换的公平，他们向学者求助。</p>
<p>学者需要确保的不是具体一次交换的公平，而是以后每次交换的公平。<br>但每次交换时，肉块重量都可以变化，并不确定。</p>
<p>所以学者把肉块每次交换的重量叫做变量 (variable) x ，<br>具体一次交换的重量叫做常量 (constant) $x_2$ ，下角标的 2 表示第 2 次。<br>同理土豆每次交换的重量叫做变量 (variable) y 。</p>
<p>接下来学者需要确定的是重量（变量）可以有哪些值。</p>
<p>于是他把所有可能猪肉重量想象成一个整体，叫做集合 (set) X= ｛所有可能猪肉重量｝。<br>所有土豆重量的整体叫做集合 Y= {所有可能土豆重量}。<br>集合内所包含的每个重量都叫做元素 (element)。</p>
<p>由于变量 x 与长宽高一样，所描述的重量是一种状态 (state)，<br>因此包含状态这种物理意义的集合 X 也叫空间 (space) X 。</p>
<p>那么学者需要确保的就是：<br>猪肉空间 X 里任意重量 x 都可以换得土豆空间 Y 里一个公平重量 y ，<br>也就是确定从 x 到 y 的变换关系 。</p>
<p>他把输入 x 这种变换前的状态叫做自变量 (independent variable)，<br>把输出 y 这种变换后的状态叫做因变量 (dependent variables)。<br>把从 x 到 y 这种单向变换的关系叫做函数或映射 (transformation or function or map) f:x-&gt;y （二者不一定非是因果关系，只是想根据自变量获得因变量的值而已）。</p>
<p>由于肉的获取较为困难，所以大家同意每次交换的土豆重量 y 是猪肉重量 x 的 2 倍，也就是 y=f(x)=2x 。</p>
<blockquote>
<p>集合 set（空间 space）：个体组成的整体。<br>元素 element（常量 constant）：整体内部的个体 。<br>变量 variable（宏观态 macro state）：集合的任意（某个）元素。<br>自变量：变化前的任意状态。<br>因变量：变化后的任意状态。<br>常量 constant（微观态 micro state）：集合的具体（一个）元素。<br>函数 transformation（变换）：某集合任意元素变成另个集合对应元素的方式（关系）。</p>
</blockquote>
<p>但此时学者遇到了一个困难，怎么测量重量呢？</p>
<p>学者想到的办法是用其它物体的重量作为参照重量进行对比。<br>这个物体需要易保存，且尽量不随时间变化而改变，因此肉和土豆都不行。</p>
<p>最后学者选择了一块金属，恰好与我们今日所选择的千克等重，并把它的重量称为常量k 。 他又选了一块小金属，重量是千克的一半，并把它的重量称为常量 b。</p>
<p>学者做了一个天枰。每次测量时就去看最少用多少 ( n ) 个参照重量 k 可以翘起 m 重量的被测物。<br>n=m/k ， n 便是重量，单位是千克。若选择小金属， n=m/b ，则单位是斤。</p>
<blockquote>
<p>测量：用选定事物的属性来 衡量 要测量事物的属性。</p>
</blockquote>
<p>可算出肉的重量后，很难找到相同重量的土豆，且土豆又不易分割。<br>所以学者又发明了一个中间交换物：钱（货币）。</p>
<p>钱，这个中间交换物可以是任何东西，甚至可以是并非实体的口头约定，<br>关键是如何才能让大家都认可。</p>
<p>聪明的学者从海边找了一堆他们的住处无法见到的贝壳，<br>又让信使告诉大家：象神托梦说“大家死后可用贝壳跟象神换取任何物品。”<br>于是大家欣然接受用自己劳动的所得交换贝壳。</p>
<blockquote>
<p>货币：构建在公众信用基础上的中间交换量 信用：可由礼德、宗教、法律等提供</p>
</blockquote>
<p>二、概率<br>农民和学者住的近，但和其他人住的都比较远，不过商人的倒卖填补了交换的距离障碍。</p>
<p>根据大家的喜好，鸡、鸭、猪、羊、牛、鹿的肉价分别如下：</p>
<p>鸡肉：2 贝壳/千克<br>鸭肉：3 贝壳/千克<br>猪肉：4 贝壳/千克<br>羊肉：5 贝壳/千克<br>牛肉：6 贝壳/千克<br>鹿肉：7 贝壳/千克</p>
<p>可有一次商人要卖肉给农民时，却忘了自己手里的这块肉是哪种肉类。<br>对肉类有不确定性的商人就问住在隔壁的学者。<br>可学者也不确定这是什么肉类，<br>但学者知道猎人每个月都会猎到 5 只鸡、4 只鸭、3 头猪、2 头羊、1 头牛、1 头鹿。</p>
<p>学者把所有可能的猎物想象成另一个集合，称其为样本空间 S ，<br>把 3 头猪想象成一个小集合 C= {猪阿强,猪不屈,猪富贵}，<br>而小集合是由 S 里的部分元素组成的 $C \subseteq S$ ，<br>因此学者在数学上称它为 S 的 子集 (subset)，<br>在物理上称它为事件 (event)，<br>一共有 6 个不相交的事件。</p>
<p>A= {鸟愤怒，鸟闪电，鸟炸弹，鸟大嘴，鸟抑郁}<br>B= {可达鸭、哥达鸭、大葱鸭、小黄鸭}<br>C= {猪阿强,猪不屈,猪富贵}<br>D= {喜羊羊,懒羊羊}<br>G= {撼地牛}<br>K= {鹿大仙}</p>
<p>学者把这块肉具体是哪个猎物的肉叫做结果 s (outcome)。<br>若结果是事件 A 里的元素 $s \in A$ ，那这块肉就是鸟肉 A （事件 A 发生）。</p>
<p>学者把事件作为输入，把自己对事件会发生的确信度作为输出，<br>抽象出了一个叫做概率的函数 P() 。</p>
<p>当样本空间内的样本发生几率相同时，确信度与事件的函数关系是：<br>确信度等于事件元素个数占有样本空间元素个数的比例 P(A)=|A|/|S| ，<br>因此确信度所处的区域是 [0,1] ，而 1 表示必然事件，0 表示不可能事件。</p>
<p>于是学者对于 6 个事件的确信度分别是：<br><img src="https://www.zhihu.com/equation?tex=P%28A%29+%3D+5%2F16%5C%5C+P%28B%29+%3D+4%2F16%5C%5C+P%28C%29+%3D+3%2F16%5C%5C+P%28D%29+%3D+2%2F16%5C%5C+P%28G%29+%3D+1%2F16%5C%5C+P%28K%29+%3D+1%2F16%5C%5C" alt></p>
<blockquote>
<p>结果：可能发生的状态。<br>样本空间：所有可能发生的结果所组成的集合。<br>事件：样本空间的子集。<br>发生：当实际结果$s_{outcome} \in A$ 时，表示 A 事件发生。<br>概率：将事件空间变换到概率空间的函数。<br>朴素概率计算： $P_{nvative}(A)=|A|/|S|$ （A 表示集合中元素的个数）。</p>
</blockquote>
<p>但每次都输入一个事件非常不方便，而且事件本身无法进行运算。<br>当某个事件发生后，需要用该类肉的价格来替换事件进行运算。</p>
<p>所以学者干脆想出了另一个函数，叫做随机变量 X （随机变量不是变量，是函数），<br>输入是事件，而输出就是对应肉类的价格。</p>
<p>这样 6 个事件经过随机变量 X() 后就被变换成了可运算的数字。<br><img src="https://www.zhihu.com/equation?tex=X%28A%29+%3D+2%5C%5CX%28B%29+%3D+3%5C%5CX%28C%29+%3D+4%5C%5CX%28D%29+%3D+5%5C%5CX%28G%29+%3D+6%5C%5CX%28K%29+%3D+7" alt></p>
<blockquote>
<p>随机变量(r.v.)：将事件空间变换到实数空间的函数（映射更准确）。</p>
</blockquote>
<p>随机变量 X 有 6 可能状态种，<br>但不管是哪种肉类（鸡肉、鸭肉、猪肉、羊肉、牛肉、鹿肉），它们都是肉，<br>就如同男人和女人都叫人一样。</p>
<p>学者把 随机变量 X 所代表的肉 叫做宏观态 (macrostate)，<br>把随机变量 X 的每个具体的可能状态叫做微观态 (microstate)。</p>
<blockquote>
<p>宏观态：不考虑内部差异的状态统称（就是变量） 微观态：考虑内部差异的具体状态（就是常量）</p>
</blockquote>
<p>学者想知道肉类这个宏观态的各个微观态的概率分布（distribution）情况，便将事件随机变量值 X 作为输入，将事件的确信度值 P 作为输出，构造出了概率质量函数 (PMF)。</p>
<blockquote>
<p>概率质量函数 (PMF)：将离散随机变量的实数空间变换到概率空间的函数。</p>
</blockquote>
<p>虽然学者对此肉的种类依然有不确定性，<br>但他根据概率和对应的随机变量值算出了平均价格，<br>称之为随机变量 [公式] 的期望 (expected value)。</p>
<p>$E(X)=\sum_i^6p_ix_i=5/162+4/163+3/164+2/165+1/166+1/167 \approx 3.56$</p>
<blockquote>
<p>期望：随机变量 [公式] 所有可能状态的加权平均值。</p>
</blockquote>
<p>学者提议商人用这个价格卖给农民，但农民只想用正确的价格购买。<br>由于这是商人自己忘记肉类而造成的，<br>因此商人不得不求助信使到猎人那里问一下自己上次买的是什么肉，<br>再用点火把的方式来来通知商人。</p>
<p>他们约定用 3 个火把的状态来通知肉的种类：</p>
<p>不点、不点、不点<br>不点、不点、点燃：对应鸡<br>不点、点燃、不点：对应鸭<br>不点、点燃、点燃：对应猪<br>点燃、不点、不点：对应羊<br>点燃、不点、点燃：对应牛<br>点燃、点燃、不点：对应鹿<br>点燃、点燃、点燃</p>
<p>信息</p>
<p>可问题来了，商人该付给信使多少个贝壳呢？<br>信使通知商人肉的种类消除的是不确定性，该如何量化不确定性呢？</p>
<p>学者最初想沿用测量重量的方法：<br>看看最少用多少 ( n ) 个参照宏观态（另一个随机变量）的不确定性，<br>可以消除随机变量 $X$ 的不确定性。</p>
<p>学者选择火把的状态做为参照宏观态（也就是抛硬币会产生了两种状态），<br>用侧重量的公式来测量不确定性，分母是参照宏观态的微观态数量，<br>点燃或不点燃两种状态的数量，即 2，<br>分子是随机变量 X 的微观态数量，即 6 种肉类。</p>
<p>n=m/k=6/2 ，得出 3</p>
<p>然而 3 个火把一共可以产生 8 种不同状态，而不是 6 种。</p>
<p>学者很快意识到这种方法 n=m/k=6/2=3 ，<br>测量不确定性是错误的沿用了测量重量时的假设。</p>
<p>测量重量时用除法是由于重量 m 是以线性方式进行累积的，<br>n 是累积次数， k 是单位的重量（单次累积量），<br>累积 n 次后获得了 m=n*k 千克的肉。<br>反过来，要求累积了多少次，就用反函数 n=m/k 。</p>
<p>可状态却不是以线性方式进行累积的，<br>每加一个火把时，它的两种状态会与之前所有状态共同产生新状态 n<em>n</em>n ，共 k 次。</p>
<p>即状态是以指数方式累积的，<br>n 是累积次数， k 是参照宏观态的微观态数量，<br>累积完 n 次后获得了 $m=k^n$ 个不同的状态。<br>若反过来求 m 个状态是由参照宏观态多少次累积而成的，<br>就用指数的反函数，即对数 $n=log_km$ 进行测量。</p>
<p>于是学者构建了一个新的函数：信息熵 (information entropy)，<br>输入是随机变量 X ，输出是不确定性。<br>而用于消除不确定性的事物叫做信息。</p>
<p>熵相当于一个没有光的黑屋子，信息相当于把黑屋子照亮的光。<br>同一个随机变量 X 的信息与熵数量相等，意义相反。消除不确定性意味着获取信息。</p>
<p>学者又把由 2 种微观态的参照宏观态所测出信息熵的单位叫做 bit 。<br>把由 e 种微观态的参照宏观态所测出信息熵的单位叫做 nat 。<br>把由 10 种微观态的参照宏观态所测出信息/熵的单位叫做 ban。</p>
<p><img src="https://www.zhihu.com/equation?tex=log_2+6+%5Capprox+2.58+%5C++bits%5C%5C+log_e+6+%5Capprox+1.79+%5C++nats%5C%5C+log_%7B10%7D+6+%5Capprox+0.78++%5C++bans%5C%5C" alt></p>
<p>然而这个公式只在待测宏观态的所有微观态都是等概率时才有效，<br>因为选定的参照宏观态本身的 2 种微观态就是等概率 50% 与 50%。<br>H(X) bit 的信息可以从 $2^{H(X)}$ 个等概率的微状态里确定对应宏观态具体是哪个微观态。</p>
<p>可学者已经通过统计观察，获得了一些这块肉是哪种肉的信息，<br>他知道这块肉更有可能是鸡肉，6 种微观态不再是等概率。<br>即使信使什么消息都不传递，他也有更高的可能性猜中鸡肉。<br>若这时再让信使告诉学者肉类是鸡肉，那么他消除的不确定性就不足 &lt; log_26 bits</p>
<p>若想算出分布不是均等的宏观态 X 的信息，他需要调整公式，<br>分别测量宏观态所对应每个微观态的信息后，再平均起来。</p>
<p>事实上，上面的公式本身就可视为 6 个微观态的信息平 $p_i$ 均后的结果：<br>$H(X)=\sum_i^6log_2m_i=1/6log_26+1/6log_26+1/6log_26+1/6log_26+1/6log_26+1/6log_26 \approx 2.58bits$</p>
<p>这时只需将 $p_i$ 替换成对应微状态的比例即可，</p>
<p>$H(X)=\sum_i^6log_2m_i=5/16<em>m1+4/16</em>m2+3/16<em>m3+2/16</em>m4+1/16<em>m5+1/16</em>m6$</p>
<p>然而学者不知道 $m_i$ ，他不知道确认每个微观态相当于从多少个等概率的状态里确定一个状态，但他知道，概率的倒数 $1/P_i$ 就等于 $m_i$ 个等概率状态的个数。<br>当 $P_i$ 是 1%，那么确定 1% 可能发生的事件，相当于从 100 个等概率的状态里确定一个状态，相当于要用 $6.64 \approx log_2100$ 个火把的组合才能让某人从 100 个等概率状态里确定一个状态。所以，学者对肉类的实际不确定性是：</p>
<p>$H(X)=\sum_i^6log_21/p_i=5/16log_216/5+4/16log_216/4+3/16log_216/3+2/16log_216/2+1/16log_216/1+1/16log_216/1 \approx 2.35bits $</p>
<p>经过整理后，学者就得到了最后的通用熵公式： $H(X)=\sum p_ilog_2{p_i}^{-1}=-\sum p_ilog_2{p_i}$</p>
<p>这个公式不再假设随机变量 X 是均匀分布，而是 X 的每个取值的信息都可以变化。<br>当 X 是连续随机变量时，就正是要用超智能体02视频里所讲的积分，即累积速率可变的“乘法”来求得。</p>
<p>通用熵公式：每个微观态的信息量（又叫自信息）的加权平均值。</p>
<p>那么学者最后让商人付给信使 2.35 个贝壳了吗？<br>没有，他让商人付了 2.58 个贝壳。</p>
<p>因为对学者而言，学者知道肉类的部分信息，6 种状态不再是等概率；<br>可对商人而言，商人没有任何关于该肉类的信息，6 种状态都是等概率的，<br>而这是商人与信使之间的交换。<br>也就是说，信息是相对于观察者（接受者）而言的。</p>
<p>对猎人而言，肉类的熵为 0，他没有不确定性。<br>对学者而言，肉类的熵为 2.35 bits ，他的不确定性被统计到的信息减少了。<br>对商人而言，肉类的熵为 2.58 bits ，他的不确定性最大。</p>
<p>“太阳从东边升起”这句话， 对已经知道这件事情的人而言，没有提供任何信息。<br>对知道或东或西升起的人而言，因为从 2 个等可能状态中确定了一个状态，提供了 1 bit 的信息。<br>而对于觉得东南西北 4 个方向都有可能的人而言，提供了 2 bits 的信息。</p>
<blockquote>
<p>熵的相对性：相对于观察者</p>
</blockquote>
<p>教育</p>
<p>学者知道每个人的存活质量是由大家的效率而共同决定的。<br>如果猎人打猎的技术好，大家就有更多的肉可以交换，<br>于是学者把大家聚在一起，教授函数、概率、熵的概念。</p>
<p>农民对于熵和概率的区别很迷惑，于是学者解释道：</p>
<p>概率的输入是事件（一个具体的微观态），<br>输出是该事件会发生的确信度，是微观态这个常量的属性。<br>而熵的输入是随机变量 X（一个宏观态），更准确的说是该宏观态的分布，<br>输出是该随机变量 X 会是哪个微观态的不确定性，是宏观态的属性。</p>
<blockquote>
<p>概率：微观态的确定性。<br>熵：宏观态的不确定性。</p>
</blockquote>
<p>猎人问道，可不可以说熵是描述无序或混乱的。学者并不建议这样理解信息熵。</p>
<p>因为日常生活中的无序（disorder）和混乱（chaos）两词多用于描述一个具体微观态的排列方式，所以没有物理背景的初学者听到这种描述时，会错误的认为熵是描述一个具体的微观态如何排列的。</p>
<p>于是就出现了大量的人说：这种排列方式的熵大，这种排列方式的熵小。<br><img src="https://pic1.zhimg.com/80/v2-be88c02bf2d6aa49530d07cced9a4632_720w.jpg" alt></p>
<p>可对你我而言，两种排列方式的熵是 0，<br>因为我们已经看到了它，获得了确定它当前状态的所有信息。</p>
<p>不仅如此，由于熵是描述宏观态的不确定性，<br>所以在讨论熵时一定要明确指定要描述的是哪个宏观态的熵。<br>因为即使是同样的场景，也会因选择不同的宏观态而产生不同的熵。</p>
<p>为体会这种相对性我们回到现代，看一下这个叫做俄罗斯赌盘的自杀游戏。</p>
<p>如果把随机变量（宏观态） X 定义为第几枪会打出子弹，<br>那么样本空间为 S {第1枪，第2枪，第3枪，第4枪，第5枪，第6枪}。</p>
<p>没观察前，6 枪都可能，并不确定会在哪一枪：不确定性是 $log_26$ 。</p>
<p>第一次开枪，消除掉了第一枪的不确定性，还有 5 种可能，不确定性是 $log_25$ ，<br>而这一次观察提供了 $log_26-log_25 bits$ 的关于随机变量 X 的信息。</p>
<p>如果第二次开枪打出了子弹，则消除了所有不确定性，获得 $log_25$ 的信息，<br>若没打出子弹，则消除掉了第二枪的不确定性，还有 4 种可能，不确定性是 $log_24$ ，<br>而这一次观察提供了 $log_25-log_24 bits$ 的关于随机变量 X 的信息。</p>
<p>以此类推开 5 枪后若都没打出子弹，我们的不确定性也消除了，<br>全部的信息加起来也正好是 log_26 bits 。</p>
<p>然而如果把随机变量（宏观态） X 定义为开枪后的人是死（微观态）是活（微观态），<br>那样本空间就为 S= {死，活}。</p>
<p>开枪前，熵是 $H(X)=-\sum p_ilog_2p_i=-1/6log_2(1/6)-5/6log_2(5/6) \approx 0.65bits$</p>
<p>概率质量函数是 ｛ 5/6,1/6 ｝</p>
<p>开完一枪后，熵为 0， 我们观察到这个人要么死了，要么还活着。</p>
<p>熵的相对性：相对宏观态（随机变量）</p>
<h1 id="如何生动有趣的入门线性代数"><a href="#如何生动有趣的入门线性代数" class="headerlink" title="如何生动有趣的入门线性代数"></a><span id="header2">如何生动有趣的入门线性代数</span></h1><p>什么是线性代数？<br>不断变化的世界使我们产生时间观念。正确描述事物状态及其不同时间下的变化至关重要。我们知道在三维空间下如何描述物体的位置。然而除了长宽高，世界上还有很多决定事物状态的因素。如决定股票价钱的因素、决定天气的因素。这些因素又该如何合理的描述？线性代数给了我们答案。</p>
<p>线性代数是有关任意维度空间下事物状态和状态变化的规则。</p>
<p><img src="https://pic1.zhimg.com/80/v2-7c00f984b625f66240f47d092ba14fe1_720w.jpg" alt></p>
<p>向量点乘</p>
<p>先让我们来看一段<a href="https://www.bilibili.com/video/av6467776/" target="_blank" rel="noopener">视频</a>，但我希望你只看一遍！PPAP洗脑全球<br>如果你继续读到了这句话，那么恭喜你，你抵抗住了病毒的洗脑。同时你听到了3个向量点乘。</p>
<p>1、I have a pen, I have an apple—-&gt;apple pen<br>[applepen] = [1 1r]·[pen r apple r] （eq.1）<br>2、I have a pen, I have a pineapple—-&gt;pineapple pen<br>[pine apple pen] = [1 1]·$\left[ \begin{array}{cc} pen \\ pineapple \end{array} \right]$ （eq.2）<br>3、apple pen, pineapple pen—-&gt;pen pineapple apple pen<br>[penpineapple applepen] = [1 1]·$\left[ \begin{array}{cc} applepen \\ pineapplepen \end{array} \right]$ （eq.3）<br>以（eq.1）举例。等式右边的第二个向量表示你有什么，右边的第一个向量表示你各拿几个，而等式的左边表示你获得了什么。从中你可以看出来：</p>
<p>向量点乘(dot product)是一种组合(combination)<br>矩阵乘向量<br>我们也可以把（eq.1）（eq.2）合二为一表示为（eq.4）：</p>
<p>I have a pen, I have an apple—-&gt;apple pen，<br>I have a pen, I have a pineapple—-&gt;pineapple pen<br>$\left [ \begin{array}{c} applepen \\ pineapplepen \end{array} \right] = \left [ \begin{array}{ccc} 1&amp;0&amp;1 \\ 0&amp;1&amp;1 \end{array} \right ]· \left [<br>    \begin{array}{c} apple \\ pineapple \\ pen \end{array} \right] $（eq.4）</p>
<p>这时，表示你各拿几个的向量变成了两行（两组），也就成了矩阵（向量是只有一行或一列的矩阵）。<br>表示你各拿几个的一个向量也叫一组权重(weights)。<br>在 [1 0 1]中，第一个1对应着apple，第二个0对应着pineapple，第三个1对应着pen，我们不可以随意调换位置。所以，</p>
<blockquote>
<p>向量是有顺序的一组数字，每个数字都是该向量的一个因素(element) 因素横着排列的向量叫做行向量(row vector)，因素竖着排列的向量叫做列向量(column vector)</p>
</blockquote>
<p>到这里我们需要更具体的描述一下第一个结论。向量点乘是一种组合，但</p>
<blockquote>
<p>向量点乘向量可以是列向量中各个因素的一个组合</p>
</blockquote>
<p>上式（eq.4）可分两步计算：</p>
<p>计算第一行权重$\left [ \begin{array}{ccc} 1&amp;0&amp;1  \end{array} \right ]· \left [<br>    \begin{array}{c} apple \\ pineapple \\ pen \end{array} \right]$得到的组合apple pen后，放到了第一行$\left [ \begin{array}{c} applepen \\  \end{array} \right]$<br>计算第二行权重$\left [ \begin{array}{ccc}  0&amp;1&amp;1 \end{array} \right ]· \left [<br>    \begin{array}{c} apple \\ pineapple \\ pen \end{array} \right] $得到的组合pineapple pen后，放到了第二行$\left [ \begin{array}{c}  \\ pineapplepen \end{array} \right]$<br>行成的$\left [ \begin{array}{c} applepen \\ pineapplepen \end{array} \right]$依然有顺序，仍然是一个向量。比较向量点乘向量，我们可以看出</p>
<blockquote>
<p>矩阵乘向量可以是列向量中各个因素的多个有顺序的组合</p>
</blockquote>
<p>向量乘矩阵<br>然而形成组合的成分并不一定非要是向量中的各个元素，也可以是不同向量之间的组合。我们可以把（eq.1）（eq.2）（eq.3）改写成（eq.5）（eq.6）：<br>$\left[ \begin{array}{c} apple &amp; pineapplepen \end{array} \right]=[1·1]· \left [ \begin{array}{rr} pen&amp;pen\\ apple&amp;pineapple \end{array} \right ]$（eq.5）<br>$\left[ \begin{array}{c} penpineappleapplepen \end{array} \right]=\left[ \begin{array}{c} pineapplepen&amp;applepen \end{array} \right]·\left [ \begin{array}{r} 1\\1 \end{array} \right ]$（eq.6）</p>
<p>在（eq.5）等式右侧的矩阵是由两个行向量组成的。矩阵中，第一个行向量表示怪蜀黍两次组合中分别先拿什么，第二个行向量表示两次组合中分别后拿什么。等式右侧的权重（行向量）的第一个因素对应着矩阵中第一个行向量的个数，第二个因素表示右侧第二个行向量的个数。这样保持矩阵中每个行向量内部因素的比例，完成矩阵内向量与向量之间的组合。</p>
<blockquote>
<p>向量乘矩阵可以是矩阵中各个行向量的多个有顺序的组合</p>
</blockquote>
<p>而向量中的每个因素都可以当成是因素个数为一个的向量，也再次解释了为什么，向量可以看成是矩阵。</p>
<p>在（eq.6）中，你会发现，要形成组合的向量被拿到了乘法点(dot)的左边，而权重被拿到了右边。因为当行向量的因素作为组合成分时，乘法点右侧的矩阵（向量）装有着权重信息。效果是拿一个penpineapple和一个applepen形成组合。<br>从中你可以看出矩阵乘法并不满足乘法交换律，因为交换了两个矩阵的位置，就交换了权重与要形成组合的向量的位置。</p>
<blockquote>
<p>矩阵乘法不满足乘法交换律：commutative law: AB =! BA</p>
</blockquote>
<p>矩阵乘矩阵<br>如果怪蜀黍跳了两遍舞蹈。第二遍跳舞时，他在两次组合时，后一次拿的东西都是都拿两个，那么我们就可以把等式右侧的行向量变成两个行向量，也就形成了一个矩阵。<br>$ \left[ \begin{array}{cc} applepen &amp; pineapplepen \\ 2<em>apple+pen &amp;  2</em>pineapple+pen \end{array} \right] $ = $ \left[ \begin{array}{cc} 1&amp;1 \\ 1&amp;2  \end{array}  \right] · \left[ \begin{array}{cc} pen &amp; pen \\ apple &amp; pineapple \end{array} \right] $</p>
<p>那怪蜀黍在唱第二遍时，就要唱：<br>I have a pen. I have two apples. 2-Apples-pen!<br>I have a pen. I have two pineapples. 2-Pineapples-pen!<br>那该蜀黍就有卖水果的嫌疑，每次都拿两个水果。<br>至此你看到了我用的是2*pineapple +pen方式去形成组合。也就是只有乘法来控制数量，加法来组合不同向量。这样的组合方式才是线性代数讨论的组合，即线性组合。所以我们所有概括的结论中，所有组合前面都要加上线性二字。同时乘法所乘的数属于什么数要事先规定好（经常被规定为是实数$ \subset R$，也有虚数域）。不过这还没有结束，严谨性是数学的特点。我上文所说的“加法”“乘法”也只不过是一个名字而已。它们到底指的是什么运算，遵循什么样的规则。然后当你看线性代数教材的时候，你就会发现这8条规则。</p>
<p>$x+y=y+x$.<br>$x+(y+z)=(x+y)+z$.<br>There is a unique “zero vector” such that $x+0=x$ for all x.<br>For each x there is a unique vector $-x$ such that $x+(-x)=0$.<br>$1x=x$.<br>$(c_1c_2)x=c_1(c_2)x$.<br>$c(x+y)=cx+cy$.<br>$(c_1+c_2)x=c_1x+c_2x$.</p>
<p>然而你不需要去记它们。你只需要知道，他们是用于描述和约束在线性代数中的加法，乘法的运算。特别要注意的是，这些运算都有一个原点（0），为了允许正负的出现。</p>
<blockquote>
<p>线性组合：一组向量乘上各自对应的一个标量后再相加所形成的组合。（满足上述对乘法、加法的规则）</p>
</blockquote>
<p>当我们再用(m by n)，即m行n列的方式去描述一个矩阵的形状(shape)时，你就得到了矩阵的第一种描述：</p>
<p>矩阵的静态信息<br>坐标值与坐标系：</p>
<p>矩阵所包含的信息从来都是成对出现，拿向量$\left[ \begin{array}{c} apple\\pen \end{array} \right]$举例来说，这个向量并没有被赋予任何数值。但你已经确定了你要在apple的数量和pen的数量的两个因素（两个维度）下描述你的数据。换句话说，你已规定好你的坐标系。所以当你写出任何具有实际数值的向量，例如$\left[ \begin{array}{c} 2\\1 \end{array} \right]$<br>时，他们的坐标系（二维向量空间）和坐标值就同时被确定了。它实际上是$\left[ \begin{array}{c} apple\\pen \end{array} \right]$和$\left[ \begin{array}{c} 2\\1 \end{array} \right]$的缩写。二者无法分割。即使是$\left[ \begin{array}{c} apple\\pen \end{array} \right]$，虽然我没有再pen，apple前面写具体数字。但依然包含所有因素间的比例相同的隐含信息。而调换2和1的顺序同时也表示坐标轴之间的调换。</p>
<p>坐标值的两种看法：</p>
<p>单单考虑坐标值时，有两种角度去理解矩阵所包含的静态信息。</p>
<p>矩阵的静态坐标值信息：</p>
<blockquote>
<p>（1）若干个维度相同的要形成组合的向量信息<br>（2）若干组维度相同的权重信息</p>
</blockquote>
<p>他们本质都是向量，然而（2）中所指的向量（或叫权重）是用于控制每个向量的数量（scale），而（1）中的所指的向量是要通过乘法与加法的线性组合形成新向量的向量。</p>
<p>拿矩阵$\left[ \begin{array}{c} 1&amp;1\\2&amp;1 \end{array} \right]$来说，你可以理解成该矩阵包含了两个行向量，也可以理解为包含了两组权重；同时，用列向量的方式也同样可以理解成向量和权重。</p>
<p>矩阵的动态信息</p>
<p>在一个矩阵内，你把矩阵内的向量理解为向量或权重都可以。但是当两个矩阵进行矩阵乘法时，一旦选择以权重信息理解其中一个矩阵，另一个矩阵的信息就会被瞬间确定为要形成组合的向量（量子力学的味道）。</p>
<p>$\left[ \begin{array}{cc} applepen&amp;pineapplepen\\apple+2<em>pen&amp;pineapple+2</em>pen \end{array} \right]=\left[ \begin{array}{cc} 1&amp;2\\2&amp;1 \end{array} \right] · \left[ \begin{array}{cc} pen&amp;pen\\apple&amp;pineapple \end{array} \right]$<br>例来说，它的实际数学表达应该是：<br>$\left[ \begin{array}{cc} 1+1&amp;1+1\\1+2<em>1&amp;1+2</em>1 \end{array} \right]=\left[ \begin{array}{cc} 1&amp;2\\2&amp;1 \end{array} \right]·\left[ \begin{array}{cc} 1&amp;1\\2&amp;1 \end{array} \right]$ 即便是都换成了数字，其物理意义任然存在，始终并未丢失。但也可以被理解为其他的物理意义。我会在$\left[ \begin{array}{cc} 1&amp;1\\2&amp;1 \end{array} \right]$ 与 $\left[ \begin{array}{cc} pen&amp;pen\\apple&amp;pineapple \end{array} \right]$ 二者之间进行切换，他们表示同一个矩阵。</p>
<p>当我把矩阵$\left[ \begin{array}{cc} 1&amp;2\\2&amp;1 \end{array} \right]$看成是两组行向量的权重时，后一个矩阵的两个行向量$\left[ \begin{array}{cc} pen&amp;pen \end{array} \right]$和$\left[ \begin{array}{ccc} apple&amp;pineapple&amp;diag \end{array} \right]$就瞬间被赋予了要形成组合的向量的观察方式。<br>当我把矩阵$\left[ \begin{array}{cc} 1&amp;1\\1&amp;1 \end{array} \right]$看成是两组列向量的权重时，前一个矩阵的两个列向量$\left[ \begin{array}{ccc} 1&amp;r&amp;2 \end{array} \right]$和$\left[ \begin{array}{c} 1\\1 \end{array} \right]$就瞬间被赋予了要形成组合的向量的观察方式。</p>
<p>矩阵的动态信息，两个矩阵相乘A⋅B 时，</p>
<blockquote>
<p>当把前者矩阵(A)中行向量理解成若干组权重，后者矩阵(B)中的行向量就是要形成组合的成分。</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/80/v2-fe7c8ed324c9a2e5efdec392ee9a80c0_720w.jpg" alt><br>同样的两个矩阵相乘</p>
<blockquote>
<p>当把后者矩阵(B)中列向量理解成若干组权重，前者矩阵(A)中的列向量就是要形成组合的成分。</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-9f586226bbae39c9dabb28d8d4436928_720w.jpg" alt></p>
<p>注意对应行向量与列向量。</p>
<p>请回想线性组合的描述（一组向量乘上各自对应的一个标量后再相加所形成的组合），这是因为向量的维度和权重的维度要一一对应。所以，</p>
<p>矩阵A(m by n)和矩阵B(p by q)能够做乘法的条件是 n = p</p>
<p>向量空间<br>很多线性代数教材所引入的第一个概念就是线性空间（linear space）。可见它的地位。虽然它有些抽象，但是却是自然而然推演出来的一个概念。<br>空间的本质是集合。而且是一个能够容纳所有你要描述内容的集合。<br>在具体讨论之前先要对上句话中“你要描述的内容”进行进一步说明。<br>从如何理解线性代数这四个字开始。首先我们已经知道了什么是线性（那8个条件约束的加法和乘法）。那什么是代数？意思是指你可以把任何概念都代入其中。<br>可以怪蜀黍手中的水果和笔换成盆和<a href="https://link.zhihu.com/?target=http%3A//www.bilibili.com/video/av6470859/">大菠萝PPAP河南话版</a>。也可以换成任何宇宙上有的物体。然而不仅仅是物体，甚至可以是一个抽象的概念。我个人最喜欢的描述是：向量空间是描述状态(state)的线性空间。再加上之前的约束，于是我们就有了</p>
<blockquote>
<p>向量空间是能够容纳所有线性组合的状态空间</p>
</blockquote>
<p>那什么样的空间（所有状态的集合）能够容纳所有的线性组合？<br>如果说，我现在想要描述的你的两个状态（下图中的行向位置，和纵向位置），向量的维度就是二维。那么一个大圆盘够不够容纳所有的线性组合？答案是不够。<br><img src="https://picb.zhimg.com/80/v2-0bb7c531c8d518aa4af78f90752c3ee6_720w.jpg" alt></p>
<p>因为线性组合是一组向量乘上各自对应的一个标量后再相加所形成的组合，而这个标量是实数域的时候，由于实数域无线延伸，那么乘以标量后的状态也会无限延伸。所以向量空间一定是各个维度都像实数轴一样可以无线延伸。最终你得到的将不会是一维下的线段，二维下的圆盘。而一定是一维下的无限延伸的直线，二维下的无限延伸的平面。<br>向量空间的基本特点是各个维度都可以无限延伸。<br>我之所以用状态二字，是因为刚才的两个维度，我可以用于描述你的速度和体温。这时，这两个维度所展开的依然是一个平面，但却又不是描述位置的平面。</p>
<p>子空间</p>
<p>子空间（subspace）可以被想成是向量空间内的空间，同样要满足能够容纳线性组合的条件<br>那么最小的子空间是什么？只有一个状态的空间（集合）。而这个状态不是其他状态，就是0。只有这样才可以在乘以完一个标量后依然不会跑出空间外部。（因为跑出去了，我们就不得不扩大空间来容纳它）。其次空集可不可以是向量空间？不可以，空集是没有任何元素的集合，既然什么状态都没有，又怎么能够容纳线性组合。</p>
<blockquote>
<p>最小的向量空间是只包含零向量的空间</p>
</blockquote>
<p>假如上图的圆盘是一个无线延伸的平面，那么这个平面的子空间就是那个平面上所有直线吗？不是，8个运算规则中明确规定了，一定要有原点，这样才可以包含正负。所以这个平面的子空间是所有过原点的直线，并且包括中心的那个原点自己所组成的最小子空间，同时也包括这个平面自身（最大的子空间）</p>
<p>线性无关</p>
<p>s你会发现，在怪蜀黍的例子中，当要把可以把（eq.1）（eq.2）合二为一表示为（eq.4）时，是这个样子：</p>
<p>$\left [ \begin{array}{c} applepen \\ pineapplepen \end{array} \right] = \left [ \begin{array}{ccc} 1&amp;0&amp;1 \\ 0&amp;1&amp;1 \end{array} \right ]· \left [<br>    \begin{array}{c} apple \\ pineapple \\ pen \end{array} \right] $（eq.4）</p>
<p>eq.4）最右侧的向量并不是$\left[ \begin{array}{c} apple\\pen\\pineapple\\pen \end{array} \right]$4个维度。而是三个。因为pen 和pen是一个东西。我们想用的是若干个毫不相关的因素去描述状态。这里的毫不相关是在线性空间下的毫不相关，所以叫做线性无关。那么当我们要描述的状态是由向量来描述时怎么办？我们知道判断两个向量是否线性无关是，可以看他是否在空间下平行。但怎么判断几个向量之间（不一定是两个）是否线性无关？我们需要可靠的依据。这也是数学为什么要证明，它要让使用者知道某个性质在什么条件下适用，什么条件下又不适用。</p>
<blockquote>
<p>线性无关（linearly independent）: 当$c_i$表示权重，$v_i$表示向量时，<br>$c_1v_1+c_2v_2+…+c_kv_k=0$只发生在当 $c_1=c_2=…c_k=0$全都等于零时。<br>换句话说，这些向量不可以通过线性组合形成彼此。形成彼此的情况只能是他们都是零向量。</p>
</blockquote>
<p>张成</p>
<p>明白了线性无关后，张成（spanning）就十分容易了，接下来要注意的是词的属性和关联词。<br>张成（spanning）是一个动词，而动词的主语是一组向量（a set of vectors）。描述的是一组向量通过线性组合所能形成子空间。是个动词，描述的内容并不是形成的这个空间，而是形成的这个行为。</p>
<p>$\left[ \begin{array}{c} apple\\pen\\pineapple\\pen \end{array} \right]$，就可以看成是4个向量，这4个向量，可以张成一个三维空间。（因为有两维线性相关，所以并不能张成4维）</p>
<p>基（基底）</p>
<p>基底也是建立在张成的基础上理解的。</p>
<blockquote>
<p>一个向量空间的一个基底（A basis for a vector space V）是一串有顺序的向量（a sequence of vectors），满足：<br>A、向量之间彼此线性无关 （不可多余）<br>B、这些向量可以张成向量空间V （不可过少）<br>换句话说，刚刚好可以张成向量空间V的一串向量是该向量空间V的一个基底</p>
</blockquote>
<p>基底是一个类似people的复数名词，是从属于某个空间的，而不是矩阵，也不是向量。</p>
<p>维度</p>
<p>一个向量空间可以有无数个基底。但每个基底所包含的向量的个数（the number of vectors in every basis）是一个空间的维度。注意，维度是空间的概念，而不是描述一个具体的向量。人们常说的n维向量实际是指n维向量空间内的向量，由于在讨论时并未给向量指定任何实际的数值，所以可以是任何值，可以张成整个空间。所以其真正描述的依旧是一个空间。并且，选择的维度是一个站在观察者角度，希望在某个向量空间下可以尽可能的描述一个物体的状态而选择的，并不一定是被描述者真实处在的空间。数学就是这么“拐外抹角”的去描述一个概念，不过确实非常有必要。但若是你觉得理解起来有困难。就简单记住：</p>
<blockquote>
<p>互不相关的因素的个数是一个向量空间的维度。</p>
</blockquote>
<p>秩</p>
<p>秩（rank）是矩阵的概念。指的是一个矩阵的所有列向量所能张成的空间的维度。</p>
<blockquote>
<p>矩阵的所有列向量所张成的空间叫做列空间（column space）<br>矩阵的所有行向量所张成的空间叫做行空间（row space）<br>一个矩阵的列空间的维度是这个矩阵的秩，同时也等于该矩阵行空间的维度<br>秩是用于描述矩阵的包含的信息的<br>转置一个矩阵可以理解为调换一个矩阵的行空间与列空间。<br>单位矩阵可以被理解为行空间与列空间相同。</p>
</blockquote>
<p>线性变换</p>
<p>线性变换（linear transformation）可以说是最最重要的概念了。你可以忘记我上面描述的所有内容，但不可以不深刻理解线性变换。下面是关于什么叫变换。由于概念很重要，我先不用逗比例子来解释。而用比较抽象的描述。<br><img src="https://pic3.zhimg.com/80/v2-265f8d8b33abf1659a10ec3d23e97ff1_720w.jpg" alt><br>一个从n维实数域（$R^n$）到m维实数域（$R^m$）的变换（transformation or mapping or function）T是将n维实数域（$R^n$）空间下任意一个向量x转换成为在m维实数域（$R^m$）空间下对应向量T(x)<br>其中n维实数域（$R^m$）空间叫做变换T的domain，m维实数域（$R^m$）的空间叫做该变换的codomain。<br>向量T(x)叫做向量x的image（变换T行为下的）<br>所有image组成的集合叫做变换T的range</p>
<p>而线性变换是是指线性规则所造成的变换，T()是由一个矩阵A来实现的。此时你就会看到无处不在的式子：</p>
<p>y=Ax ：列向量x左乘一个矩阵A后得到列向量y</p>
<p>$\left [ \begin{array}{c} applepen \\ pineapplepen \end{array} \right] = \left [ \begin{array}{ccc} 1&amp;0&amp;1 \\ 0&amp;1&amp;1 \end{array} \right ]· \left [<br>    \begin{array}{c} apple \\ pineapple \\ pen \end{array} \right] $（eq.4）举例来说，<br>x是三维空间的向量（即A的domain是三维），而经过线性变换后，变成了二维空间的向量A（即A的codomain是二维）。</p>
<blockquote>
<p>矩阵A可以被理解成一个函数(function)，将三维空间下的每个向量投到二维空间下。<br>y=Ax也可以理解为x经由一个动因A，使其状态发生了改变。<br>Ax同时也是深层神经网络每层变换中的核心：y=a(Ax+b)</p>
</blockquote>
<p>在机器学习中你会你会需要构架一个虚拟的世界，并选择合适的、用于描述某个事物状态的各种因素。</p>
<blockquote>
<p>线性代数是有关如何构架“世界”的学问。矩阵又是存储着所架构的世界的信息的媒介。</p>
</blockquote>
<p>举一个小小的例子，比如你想通过温度，气候，湿度，当天时间，海拔，经度，纬度等信息来描述天气状况，从而进行预测是否会下雨。你如何合理的选择这些信息？你如何知道这些信息，海拔和气候如是否相关，是否重复？如果重复，那么你又是否可以减少某个信息？判断的准则又是什么？</p>
<p>数学讲的是我刚才所描述的内容的纯粹的结构关系。请你忘记我给你举得怪蜀黍例子，抓住“逻辑框架”。当你可以把这种关系应用在任何符合该结构关系的现实现象中时，你就算是精通了如何应用数学。</p>
<p>线性代数的内容十分庞大，行列式，特征向量，奇异值分解等你也会经常用到。然而我的描述就到此为止，我无法涵盖所有内容。写这篇文章只是希望能够用你脑中已有的概念帮助你构建一个对线性代数模糊的认识。当你今后用到线性代数时，再不断的加深和更正此刻的理解。</p>
<h1 id="概率simple入门"><a href="#概率simple入门" class="headerlink" title="概率simple入门"></a><span id="header3">概率simple入门</span></h1><p>什么是概率？<br>通过线性代数，我们知道了该如何描述事物状态及其变化。遗憾的是，对一个微小的生物而言，世界并非确定性（deterministic）的，由于信息量的限制，很多事物是无法确定其变化后会到达哪种状态。然而为了更好的生存，预测未来状态以决定下一刻的行为至关重要。而概率给我们的决策提供了依据。</p>
<blockquote>
<p>概率是用来衡量我们对事物在变化到不同状态的确信度。</p>
</blockquote>
<p><img src="https://pic3.zhimg.com/v2-82dd0737ac79a63d949e7c8ad1239ba3_1440w.jpg?source=172ae18b" alt></p>
<p>该篇内容是Joseph K. Blitzstein的<a href="https://link.zhihu.com/?target=http%3A//projects.iq.harvard.edu/stat110/about">《Introduction to Probability》</a>中的第一章，该概率书非常适合入门。文章是加入自己理解后的总结。以服务于机器学习的理解视角切入。</p>
<p>通过线性代数，我们知道了该如何描述事物状态及其变化。遗憾的是，对一个微小的生物而言，世界并非确定性（nondeterministic）的，由于感知限制，很多事物是无法确定其状态的。然而为了更好的生存，预测未来状态以决定下一刻的行为至关重要。而概率给我们的决策提供了依据。</p>
<p>一、什么是概率？</p>
<blockquote>
<p>概率是我们对事件处于哪个状态的确信度。</p>
</blockquote>
<p>情景：如何考虑转盘在未来停止后指针指向各个数字的可能性？为方便研究，需要总结出在任何情况都普遍适用的属性，并给予它们固定的名字。<br>1,2,3是可能被指到的三个结果（outcome）。在这里，这三个结果组成的集合也同时是样本空间（sample space），即无论事态如何发展，结果都不会出现在该集合之外（和向量空间一样）。样本空间的子集，如{1,2}叫做一个事件（event），表示指针指到1或2的情况。满足任何一个情况都算作该事件发生了（occurred）。所有事件发生的可能性都用值域为[0,1]间的实数表示，1表示必然发生，0表示不可能发生。{1}, {2,3}两个不相交的事件的概率和为1。[0,1]间的实数是概率得出的值，但并非概率的全部。概率是一个函数。</p>
<p><img src="https://picb.zhimg.com/80/v2-746ff9fde965cb61f90d83e712ba4c67_720w.jpg" alt></p>
<blockquote>
<p>概率：概率是将样本空间内的子集投向概率空间的函数。<br>概率P()将事件$A \subset S$作为输入，并输出[0,1]之间的实数表示其发生的可能性。该函数需要满足两个条件：<br>1.$ P(0)=0,P(S)=1 $，空集的概率为0，全集的概率为1。<br>2.$P(\bigcup_{j=1}^{\infty} A_j)= \sum_{j=1}^{\infty} P(A_j)$不相交事件之间的并集事件的概率等于各个事件概率之和。<br>结果：可能到达的状态<br>样本空间：所有可能发生的结果所组成的集合。<br>事件：样本空间的子集。</p>
</blockquote>
<p>当实际发生的结果$S_{outcome} \subset A$时，表示A事件发生。</p>
<p>二、朴素概率的计算以及和普遍概率的区别是什么？</p>
<p>人们在计算概率时常常犯的错误就是不假思索的假定所有结果所发生的可能性都相同。并用事件的结果个数比上样本空间的结果个数。<br>朴素概率：$P_{native}A=|A|/|S|$，|A|和|S|表示集合中元素的个数。<br>但是这种假设并不严谨。</p>
<p>实例：在上图原盘问题中，如果使用朴素概率来计算指针停止时指向2的概率，就会得到$P_{native}A=|A|/|S|=1/3$的概率。但很明显，指向3的结果就占有原盘一半的空间，指向3的概率更大。使得各个结果发生的可能性并不相同。不可以使用朴素概率算法。从图中可以看出答案是1/4。</p>
<p>样本空间好比是总价为1的一筐苹果，一个事件就是一堆苹果，概率是将这堆苹果转换成实际价钱的函数。但苹果有大有小，只有当所有苹果都一模一样时，这堆苹果的价钱才是 苹果数/总个数。空集，即一个苹果都没有的话，价格为0。整框苹果的话，价格自然为1。把整框苹果分成几堆（事件之间不相交），价格的总和为1。<br><img src="https://pic1.zhimg.com/80/v2-c61d2008f6fbdb7f298d1d3ad16c8bae_720w.jpg" alt></p>
<p>条件概率<br>当我们获得更多信息后，新信息会对原始样本空间产生更新。</p>
<p>三、条件概率又是什么？</p>
<p>条件概率是新信息对样本空间进行调整后的概率情况。<br>实例：从一副洗好的扑克里，不放回的依次抽两张卡片。事件A表示第一张卡片是心，事件B表示第二张卡片是红色。求事件B发生的条件下，事件A发生的概率P(A|B)。以及事件A发生的条件下，事件B发生的概率P(B|A)。</p>
<p>卡片都是均匀形状，可用朴素概率计算。最初的样本空间是 54∗53=2862 种。事件B发生后，样本空间被调整，所有第二张不是红色的结果都会从样本空间内去掉，变成 26∗53=1378种（可认为第二张先抓，顺序不影响组合结果）。其中第一张是心，且第二张是红色的结果有13∗25=325种。所以P(A|B)的概率为 325/1378≈0.236。</p>
<p>事件A发生后，所有第一张不是心的结果都会从样本空间内去掉，变成13∗53=689种。其中第一张是心，且第二张是红色的结果有 13∗25=325种。所以P(B|A)的概率为325/689≈0.472。</p>
<p>P(A|B)和P(B|A)二者的条件对原始样本空间的调整不同，所以并不相等。同时“|”右边的事件并不意味着先发生，也并不意味着是左边事件的起因。</p>
<p>实例：先后投两次硬币。原始样本空间是{正正，反反，正反，反正}。已知事件A是第一次投得正面，事件B是第二次投得正面。P(B|A)更新后的样本空间为{正正，正反}。但第二次投得正面的概率仍然是1/2。事件A和事件B彼此没有影响，叫做两个事件独立。</p>
<blockquote>
<p>条件概率：$P(A|B)=P(A \bigcap B)/P(B)$<br>P(A|B)表示B事件条件下，A发生的条件概率。<br>P(A)叫做先验概率（prior probability），即事态未更新时，A事件的概率。<br>P(A|B)也叫做后验概率（posterior probability），即事态更新后，A事件的概率。<br>$P(A \bigcap B)$是B发生后A的事件集合，而除以P(B)是在该基础上，将样本空间的总概率重新调整为1。<br>当事件A与B为独立事件时，其中一个事件的发生并不会对另一个事件的样本空间产生影响。即P(A|B)=P(A)，P(B|A)=P(B)。</p>
</blockquote>
<p>贝叶斯公式<br>人们经常将P(A|B)和P(B|A)搞混，把二者搞混的现象叫做检察官谬误（prosecutor’s fallacy）。</p>
<p>四、P(A|B)和P(B|A)两者之间的关系是什么？<br>实例：某机器对在所有人口中得病率为1%的癌症识别率为95%（有病的人被测出患病的概率和没病的人被测出健康的概率）。一个被测得有病的人真实患癌症的概率是多少？<br>得出答案是95%的人就是搞混了P(A|B)和P(B|A)。正确答案约等于16%。拿10000个人来思考。<br>真正的样本空间是由测得有病的癌症患者和测得有病的正常人组成，所以答案是95/(95+495)≈16%。</p>
<p>我们知道条件概率是新信息对样本空间进行调整后的概率情况，所以检察官谬误实际上是样本空间的更新产生了差错。不过我们可以从条件概率中寻找关系：通过变形条件概率的定义，就可以得出著名的贝叶斯公式和全概率公式。</p>
<blockquote>
<p>贝叶斯公式（Bayes’ theorem）：$P(A|B)=P(B|A)P(A)/P(B)$<br>全概率公式（Law of total probability）：$P(B)=\sum_i^n P(B|A_i)P(A_i)$<br>其中$A_i$是样本空间S的分割(partition)，即彼此不相交，并且组成的并集是样本空间。<br>如下图：<br><img src="https://pic3.zhimg.com/80/v2-4598c108df88f9f8119757eab0e318c8_720w.jpg" alt><br>用这两个公式，我们重新计算上面的癌症问题：</p>
</blockquote>
<p>例： 其中 P(A)是人口中患癌症的概率，为1%，P(B)是测得有病的概率。P(A|B)就是测得有病时，患癌症的概率。 P(B|A)是有患癌症时，测得有病的概率，为95%。P(B|AC)就是没病时却测得有癌症的概率，为5%。<br>想知道的是，当被测得有病时，真正患癌症的概率P(A|B)是多少。<br>由贝叶斯公式可以得到：$P(A|B) = P(B|A)P(A)/P(B)=0.95<em>0.01/P(B)$<br>由全概率公式可以得到：$P(B)=P(B|A)P(A)+P(B|A^C)P(A^C)$<br>全部代入就得到：$ 0.95</em>0.01/(0/95<em>0.01+0.05</em>0.99)\approx 16$%</p>
<p>这两个公式在机器学习中非常重要。贝叶斯公式告诉了我们P(A|B)和P(B|A)两者之间的关系。很多时候，我们难以得出其中一个的时候，可以改求另一个。</p>
<p>实例：语音识别中，听到某串声音的条件O下，该声音是某段语音[公式]的条件概率最大的arg maxP(s|o)为识别结果。然而P(s|o)并不好求。所以改求p(s|o)=P(o|s)P(s)/P(o)。P(o)对比较同一个P(s|o)时并没有影响，因为大家都有，则不需要考虑。剩下的P(o|s)叫做声学模型，描述该段语音会发出什么样的声音。而P(s)叫做语言模型，包含着语法规则信息。</p>
<p>而全概率公式又是连接条件概率与非条件概率的桥梁。</p>
<blockquote>
<p>全概率公式可以将非条件概率，分成若干块条件概率来计算。</p>
</blockquote>
<p>实例：三门问题。三扇门中有一扇门后是汽车，其余是羊。参赛者会先被要求选择一扇门。这时主持人会打开后面是羊的一扇门，并给参赛者换到另一扇门的机会。问题是参赛者该不该换？ 应该换门。换门后获得汽车的概率为2/3，不换门的概率为1/3。<br>用全概率公式来思考该问题就可以将问题拆分成若干个相对简单的条件概率。<br>实例：三门问题。三扇门中有一扇门后是汽车，其余是羊。参赛者会先被要求选择一扇门。这时主持人会打开后面是羊的一扇门，并给参赛者换到另一扇门的机会。问题是参赛者该不该换？ 应该换门。换门后获得汽车的概率为2/3，不换门的概率为1/3。</p>
<p>用全概率公式来思考该问题就可以将问题拆分成若干个相对简单的条件概率。<br>P(getcar)获得汽车的概率可以用拆分成选择各个门可得汽车的概率。$P(D_1)$为车在第一扇门的概率。<br>$P(getcar)=P(getcar|D_1)P(D_1)+P(getcar|D_2)P(D_2)+P(getcar|D_3)P(D_3)$<br>$P(getcar)=P(getcar|D_1)<em>1/3+P(getcar|D_2)</em>1/3+P(getcar|D_3)<em>1/3$<br>如果不换门，得车的概率就是$P(D_1)$，即1/3.<br>若换门。当车在第一扇门后时，$P(getcar|D_1)</em>1/3$由于换门的选择而变成了0。<br>但当车在第二或第三扇门后时，由于主持人去掉了一扇后面为羊的门，换门的选择会100%得到车。<br>所以，$P(getcar)=0<em>1/3+1</em>1/3+1*1/3=2/3$</p>
<p>随机变量<br>五、是否有更好的方式表达事件？</p>
<p>随机变量是一种非常方便的事件表达方式。虽然它的名字叫做随机变量，但它实际上是一个函数。 我们在“什么是概率”的例子中已经应用了随机变量的概念。我们用数字去表达事件。比较一下不用随机变量的方式。</p>
<p>实例：我们用文字去表达事件和概率。样本空间 $\Omega$ = { 橘黄色，绿色，蓝色 }。<br>情况1：若仅仅是问转盘停止后指针指到某个颜色的概率还可以接受。如P(指到橘黄色)。<br>情况2：如果是奖励游戏，转到橘黄、绿、蓝色分别奖励1、2、3元。转3次后，想知道奖励了多少钱的概率。3元的我们要写一次描述，4元的也要写一次描述。十分笨拙。如果想问的是美元呢？我们又没办法用事件去乘以汇率。</p>
<p>然而如果用随机变量，就变得非常方便。设$X_r$表示转r次后一共奖励了多少人民币。 c是人民币对美元汇率的话，c·$X_r$就表示表示转r次后一共奖励了多少美元。$X_{r+1}-X(r)$就表示了下一局赢得了多少人民币。</p>
<blockquote>
<p>随机变量：给定一个样本空间 $\Omega$一个随机变量(r.v.)是将样本空间投射到实数域的函数。</p>
</blockquote>
<p>一个样本空间可以有很多个随机变量。在最初的例子，我们就已经将样本空间$\Omega$={橘黄色，绿色，蓝色}对应到了实数域中的1,2,3。<br><img src="https://pic1.zhimg.com/80/v2-f328f131d915bd589d9c195bee4a3dc2_720w.jpg" alt></p>
<p>随机变量作为函数而言是确定的。输入事件橘黄色，一定会得到1这个输出，函数本身并没有什么“随机”。“随机”是由于函数的输入的发生概率。</p>
<p>X=3表达的是指针指到蓝色的事件。P(X=3)表达指针指到蓝色的事件的概率。</p>
<p>随机变量是认为事先选择的，非常灵活，好的随机变量会使问题简化许多。</p>
<p>根据随机变量投射后的值域是离散还是连续，随机变量可以分为离散随机变量和连续随机变量。</p>
<p>分布<br>随机变量中的“随机”来自事件发生的概率。分布（distribution）是描述随机变量所对应的所有事件的发生概率的情况。</p>
<p>实例：上例随机变量$X_1$（转1次奖励人民币数）的分布情况用概率质量函数（probability mass function，简写为PMF）表示就是：<br><img src="https://pic2.zhimg.com/80/v2-8d4508f43698f933a1c54f0347c1999d_720w.jpg" alt></p>
<p>概率五要件<br>样本空间：所有可能结果组成的集合。<br>随机变量：将事件投向实数的函数。用数字代表事件。<br>事件：样本空间的子集。<br>概率：将事件投向[公式]实数域的函数。用实数表示确信度。<br>分布：随机变量的取值情况。</p>
<p><img src="https://pic2.zhimg.com/80/v2-c4bf85890918ede7952697c4d5e88618_720w.jpg" alt></p>
<p>注意在应用中区分物理意义与数学定义。如随机变量虽然是以事件为输入，实数为输出。但是在用于表达概率P(X=3)是用3这个数字去表示事件，并得出该事件的概率，并不是将实数作为输入。又如概率的数学定义是事件投射到[0,1]的实数上，但在物理意义中，是样本空间的内在情况决定了事件。上图中：</p>
<p>蓝线：表示人们为了描述物理现象而定义的数学函数。箭头由输入空间指向输出空间。<br>概率函数：输入为事件，输出为[0,1]实数<br>随机变量函数：输入为事件，输出为实数（但使用时，用实数代表事件）<br>概率质量函数：输入为实数，输出为[0,1]实数</p>
<p>红线：表示真实的物理现象。箭头由因指向果。<br>由确信度所反映的内在分布情况决定了事件的发生。<br>事件的发生决定了随机变量的输出值。</p>
<h1 id="很多人都不知道自动化与机器学习的区别"><a href="#很多人都不知道自动化与机器学习的区别" class="headerlink" title="很多人都不知道自动化与机器学习的区别"></a><span id="header4">很多人都不知道自动化与机器学习的区别</span></h1><p>目录</p>
<ul>
<li>集合、变量、函数</li>
<li>自动化与机器学习的区别</li>
</ul>
<p>正文<br>这篇文章其实是给文科生补三个数学概念的。</p>
<p>一、集合（一类）、变量（任意）、函数（知识）</p>
<p>可能不少人有一种误解，认为输入和输出是一个具体的情况。又怎么能说知识在压缩信息呢？ 然而输入和输出的可能情况并非只有一个，往往是无数个。</p>
<p>例1：没有两片树叶是一模一样的，但我们却可以认识所有的树叶。<br><img src="https://pic4.zhimg.com/80/v2-c7f11100548239eb1228b3386bb32b48_720w.jpg" alt></p>
<p>例2：小猫的长相都不相同，也形态各异，但我们却可以认识所有的猫。<br><img src="https://picb.zhimg.com/80/v2-30bf9b80852d026da71460ee2d779486_720w.jpg" alt></p>
<p>例3：每次炒菜时的所用的食材量，火候，调味料也都千差万别，但我们却可以对应的进行调整，做出叫同个菜名的食物。</p>
<p>你可以感觉出这里想要描述的是一类事物，每一次的输入可以是这一类事物中的任意一种情况。我们的知识可以应对所有输入而得到对应的输出。</p>
<p>那么该如何描述这些概念呢？因为如果在讨论一个问题时没有办法准确描述所指的事物到底是什么，那就根本没有办法开展和交流。 这些看似微不足道的概念其实直到 19 世纪才第一次正式创立。而这个奠定了整个现代数学，乃至所有科学根基的工具就是集合。</p>
<p>若把所有的树叶（元素）归到一个集合中，用这个集合来表示一类事物，输入是这一类事物中的任意一个。也正是由于这种任意性，输入也被称为变量。输出同理也是变量。</p>
<p>我们在讨论输入和输出时，并不是说隔壁老王家那只叫翠花的二哈，而是说所有的哈士奇。<br><img src="https://picb.zhimg.com/80/v2-7610bd6fb437760a76b443fcad4a977b_720w.jpg" alt></p>
<p>但其我们早就掌握了这种概念，只是当初并未正式的总结。<a href="https://www.bilibili.com/video/av32026984" target="_blank" rel="noopener">《超智能体》02</a> 的开篇里提到的「智人为什么会从众多生物中脱颖而出的原因就是在于智人的语言出现了变量与函数的概念」。</p>
<p>例1：英语中的 the apple 指的是特定的一个苹果，是一个常量。而 an apple 指的却是任意一个苹果，也就是变量。</p>
<p>例2：中文里我们也会用“这个”和“那个”来特指某个事物，而没有这些特指时，我们往往是在讨论变量。</p>
<p>例3：“人啊，还是要看命”句话里的人就是变量。 这句话甚至包含了一种“知识”，其预测就是所有人都要认命。虽然这个“知识”不合理。但你可以感觉到它的作用。中医、玄学、科学也都在提取知识，只是科学是当中最可靠的一种提取知识的方法。</p>
<p>这种概念的产生让我们能够从经验中提取知识，并一代一代的传递下去。</p>
<p>二、自动化与当代 AI 的区别</p>
<p>如果把每个图像和这个图像是否是树叶记录下来，哪怕是最快的计算机也无法查找和存储这些多的信息。这恰恰是当初计算机所办不到的事情，没有办法仅通过演示几个例子就能让计算机搞清楚其他没有演示的情况，也就是学习能力。</p>
<p>曾经计算机所执行的指令都是人类所学到的知识。如今的人工智能所产生的突破并非有了意识，而是能够让计算机自己来从有限的例子中学到知识，然后将学到的知识用于今后的预测中。</p>
<p>这也是自动化与目前AI最大的区别，即知识是否是由机器自己发现的，这也正是很多张口闭口都是AI的人压根就没搞明白的事情。现在你再来看这个视频时，恐怕会有新的理解：</p>
<p>有了集合、变量、函数的概念后，下一篇我们再来谈知识的第二种分类：以任务类型为视角。</p>
<h1 id="机器学习理论引入的知识分类"><a href="#机器学习理论引入的知识分类" class="headerlink" title="机器学习理论引入的知识分类"></a><span id="header5">机器学习理论引入的知识分类</span></h1><p>前言<br>这种分类完全是从机器学习理论中引入到日常学习中的。明白了这种分类后，就可以轻松理解<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxNDI5MDk0MQ%3D%3D%26mid%3D2247483892%26idx%3D1%26sn%3Dbd451e1b54871bf10e45b79fcea930ef%26chksm%3D97a89966a0df107078f7602f9e4b5fe5f39b92dfecd1fa6a4fd6cff4fc7768f173b9125c9ab3%26scene%3D21%23wechat_redirect">《学习观》05</a>中提到的两个概念了。</p>
<p>目录</p>
<ul>
<li>分类</li>
<li>回归</li>
<li>数学的特别</li>
</ul>
<p>正文<br>若以任务类型为视角，则知识可划分为“分类（classification）”与 “回归 （regression）”。</p>
<p>一、君子和而不同，小人同而不和</p>
<p>分类知识是最常用的。它们是判断一个事物属于哪个类型，也出现在各式各样的决策中。</p>
<p>例1：判断一个数字是偶数还是奇数（二类）</p>
<ul>
<li>输入：任意数字</li>
<li>输出：是否为偶数</li>
</ul>
<p>例2：判断一个行为是不是爱情（二类）</p>
<ul>
<li>输入：任意事件</li>
<li>输出：是否为爱情</li>
</ul>
<p>例3：从外表判断一个人的年龄段（三类）</p>
<ul>
<li>输入：某人外表</li>
<li>输出：少年或青年或老年</li>
</ul>
<p>例4：判断一个生物属于植物、动物、昆虫、微生物等（超过三类）</p>
<ul>
<li>输入：某生物的特征</li>
<li>输出：哪种生物类别</li>
</ul>
<p>例5：判断一个发音属于哪个单词（数千类）</p>
<ul>
<li>输入：某段发音</li>
<li>输出：哪个单词</li>
</ul>
<p>例6：决定是否与某人结婚</p>
<ul>
<li>输入：相处时观察到的言行与家庭等综合因素</li>
<li>输出：结婚与否</li>
</ul>
<p>例7：决定是否买车</p>
<ul>
<li>输入：个人与环境等综合因素</li>
<li>输出：是否买车</li>
</ul>
<p>在没有计算机扩展我们自身能力之前，思考类知识中大部分都是分类知识。在中国古代的著作里，如《论语》、《孟子》等留下来的知识也都是分类知识。「君子和而不同，小人同而不和」可用于判断君子和小人。</p>
<p>分类知识可以说是最重要，可往往也是数学中最容易被轻视的知识。因为应用任何知识之前，都是先用分类知识来判断问题属于哪一类，然后才能够应用公式和性质来解决。由于很多学生缺少这类知识的训练，造成他们只会做逻辑题，却难以解决应用题，更无法用数学知识解决生活问题。让学生误以为数学只是一种智力游戏。</p>
<p>很多题解不出来是因为学生根本无法判断某个问题属于哪一类，连用公式和性质的机会都没有。遇到这种情况时，很多家长和老师都简单的归因为“不熟”，提升方式也基本靠悟。但是什么叫“不熟”？这种“不熟”又该如何提升。</p>
<p>很多人往往是工作了以后才提升了该能力，因为他们遇到了很多实际例子，基本等于重学了一遍。不觉得很奇怪吗？本该在学校学习的东西反而不得不在工作中才真正学会。可这并非没有办法的事情，因为只要在学生阶段针对性的用实际例子来训练这类知识，就不必在工作中来重新学习了。</p>
<p>比如学习三角函数时，第一个要学习的知识不是三角函数的公式，而是要学会判断哪些东西可被视为三角形的各个角和边。世界上没有“三角形”这种东西，是人们观察了很多个实例，总结共性后所抽象出来的知识。在学习时也需要通过很多个实例才能将这种知识迁移到学生的脑中。只有当学会了判断它后，才能够将三角函数的知识应用于机器人的控制，飞机的导航，傅里叶变换，周期性动画的制作等。</p>
<p>而在画思维导图时，这类知识对应的关键词一般是主谓结构。你会自我提问“它是什么”来明确输入输出，问“为什么是”来决定判断的边界（依据）。</p>
<p><img src="https://pic4.zhimg.com/80/v2-f1002fbea2dd7fe0855195a711bdd025_720w.jpg" alt></p>
<p>一个事物（输入）都有哪些类（输出）</p>
<p>例1：比如知识的分类（上）里的大脑模式视角这个关键词（输入）是主语，系动词被思维导图的连线代替了，谓语（输出）拥有两类：</p>
<ul>
<li>输入：大脑模式视角</li>
<li>输出：思考类或运动类</li>
<li>边界：是否依靠意识</li>
</ul>
<p>一个知识（输入）都有子知识（输出）</p>
<p>例2：思考类与运动类的速度特点这个关键词这个关键词（输入）是主语，谓语（输出）拥有4个子知识（由你自己总结）：</p>
<ul>
<li>输入：特点</li>
<li>输出：速度、精度、因素量、并行</li>
</ul>
<p>二、从定性到定量</p>
<p>另一种只是不再是判断类别，而涉及如何从一种状态变成另一种状态。大都数的运动类都是回归知识。一个问题既可以被当作回归来处理（定量），也可以当作分类来处理（定性）。 在判断完问题属于哪一类后，可精准预测的定量的知识就更有价值。</p>
<p>例1：走路</p>
<ul>
<li>输入：环境</li>
<li>输出：大脑送给肌肉的信号</li>
</ul>
<p>例2：炒菜的放盐量</p>
<ul>
<li>输入：食材数量</li>
<li>输出：该放多少盐</li>
</ul>
<p>例3：从外表判断一个人的实际年龄（分类时是判断年龄段）</p>
<ul>
<li>输入：某人外表</li>
<li>输出：年龄</li>
</ul>
<p>例4：射击游戏的鼠标移动</p>
<ul>
<li>输入：游戏画面情况</li>
<li>输出：手臂肌肉移动</li>
</ul>
<p>三、数学的特别</p>
<p>想特别说明的就是数学的知识，所有的数学的公式都是别人总结好的“知识的描述”，是纯粹的关系，而且是将同一类知识进行二次提取。比如 y=ax+b 这种关系可以表示所有符合线性关系的知识。</p>
<p>例1：西瓜斤数与总价的关系</p>
<ul>
<li>输入：斤数</li>
<li>输出：总价</li>
</ul>
<p>例2：光的传播时间与位移</p>
<ul>
<li>输入：时间</li>
<li>输出：位移</li>
</ul>
<p>例3：超过3公里时北京市出租汽的行驶距离与价格</p>
<ul>
<li>输入：距离</li>
<li>输出：价格</li>
</ul>
<p>当作为二阶知识时，线性关系这个分类知识的：</p>
<ul>
<li>输入：某个知识</li>
<li>输出：是否为线性</li>
</ul>
<p>这里我将问题留个读者：为什么大部分人都难以学好数学？数学明明被用于所有科学的现实问题，可为什么大部分人都觉得自己学不会。一遍一遍的记住了数学公式，甚至理清了这些公式是怎么来的，但在现实问题面前全面崩溃。</p>
<p>在画思维导图时，回归知识对应的关键词一般是动宾结构。你会自我提问“它的目的”来明确输入变成什么样的输出，问“如何达到”来明确具体的步骤。</p>
<p><img src="https://pic1.zhimg.com/80/v2-b4df6267baa84a74f9a919d1b34448a9_720w.jpg" alt></p>
<p>例1：比如<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxNDI5MDk0MQ%3D%3D%26mid%3D2247483892%26idx%3D1%26sn%3Dbd451e1b54871bf10e45b79fcea930ef%26chksm%3D97a89966a0df107078f7602f9e4b5fe5f39b92dfecd1fa6a4fd6cff4fc7768f173b9125c9ab3%26scene%3D21%23wechat_redirect">《学习观》05</a>里的拆分知识这个关键词的“拆分”是动词（函数），“知识”是宾语（输出）：</p>
<p>输入：要被拆分的复合知识<br>输出：众多子知识<br>函数：“拆分”这个动作行为<br>​</p>
<p>明白了这两种知识分类之后，请再看一遍 <a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxNDI5MDk0MQ%3D%3D%26mid%3D2247483892%26idx%3D1%26sn%3Dbd451e1b54871bf10e45b79fcea930ef%26chksm%3D97a89966a0df107078f7602f9e4b5fe5f39b92dfecd1fa6a4fd6cff4fc7768f173b9125c9ab3%26scene%3D21%23wechat_redirect">《学习观》05</a> 和<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxNDI5MDk0MQ%3D%3D%26mid%3D2247483899%26idx%3D1%26sn%3D07994dbdaec912616b07832972db94bf%26chksm%3D97a89969a0df107f0ab791abdf6b8bb6aeef77bfcc55b1ef397d6655eb00161c03089c8010d5%26scene%3D21%23wechat_redirect">《学习观》5.5</a>，希望你可以有新的理解。</p>
<h1 id="英语总结"><a href="#英语总结" class="headerlink" title="英语总结"></a><span id="header6">英语总结</span></h1><h2 id="英语学习关键点"><a href="#英语学习关键点" class="headerlink" title="英语学习关键点"></a>英语学习关键点</h2><p>发音：想摆脱口音，第一注重元音的发音。</p>
<p>元音：忘记中文的音，以每个音都是新音素去对待英语的音素。歌手唱歌时听不出口音是因为元音发的对。双元音ai、ei记得发到位，不要看做一个音，而是两个音圆滑的发出。<br>辅音：只要嘴型对，辅音的发音基本没问题。比如w是圆型口型，v是上齿触下唇。可快速发very well来体会不同。</p>
<h2 id="记音素要在单词中记忆"><a href="#记音素要在单词中记忆" class="headerlink" title="记音素要在单词中记忆"></a>记音素要在单词中记忆</h2><p>单词：记忆一定要有背景。<br>意思：新单词的意思理解要建立在已学英语单词的基础上，不要只看中文意思，要看英语解释。柯林斯英汉就很好。比较try和attempt：V-T/V-I If you try to do something, you want to do it, and you take action which you hope will help you to do it. ；V-T If you attempt to do something, especially something difficult, you try to do it. 试图 (尤指做困难的事)<br>记忆：音素放在单词中记忆，单词放在句子中记忆（最好可清晰展示用法的句子），句子放在段落中记忆。（学英语可以认为你就是在学英语句子）</p>
<p><img src="https://pic2.zhimg.com/80/v2-d544ab8761c043f3728bdf57b6b728ea_720w.jpg" alt><br><img src="https://pic2.zhimg.com/80/v2-491de634ad4b70b00dc0a3ebaa58d672_720w.jpg" alt><br><img src="https://pic1.zhimg.com/80/v2-92e1a99688d288711b704cb2f7fa1b67_720w.jpg" alt><br><img src="https://pic1.zhimg.com/80/v2-92e1a99688d288711b704cb2f7fa1b67_720w.jpg" alt><br><img src="https://pic2.zhimg.com/80/v2-fe46aca26f8e4625ea462c93fe6871b9_720w.jpg" alt></p>
<h1 id="深层学习为何要“Deep”"><a href="#深层学习为何要“Deep”" class="headerlink" title="深层学习为何要“Deep”"></a><span id="header7">深层学习为何要“Deep”</span></h1><p>一、基本变换：层<br>神经网络是由一层一层构建的，那么每层究竟在做什么？</p>
<p>数学式子：$\vec{y}=a(W\vec{x}+b)$，其中$\vec{x}$是输入向量，$\vec{y}$是输出向量，$\vec{b}$是偏移向量，W是权重矩阵，a()是激活函数。每一层仅仅是把输入$\vec{x}$经过如此简单的操作得到$\vec{y}$。<br>数学理解：通过如下5种对输入空间（输入向量的集合）的操作，完成 输入空间 —&gt; 输出空间 的变换 (矩阵的行空间到列空间)。<br>注：用“空间”二字的原因是被分类的并不是单个事物，而是一类事物。空间是指这类事物所有个体的集合。</p>
<ol>
<li>升维/降维</li>
<li>放大/缩小</li>
<li>旋转</li>
<li>平移</li>
<li>“弯曲”<br>这5种操作中，1,2,3的操作由$W\vec{x}$完成，4的操作是由$+\vec{b$完成，5的操作则是由a()来实现。</li>
</ol>
<p><img src="https://pic2.zhimg.com/v2-1ebee9a3fb36a6d1502d517b24bfb5c3_b.jpg" alt></p>
<p>物理理解：对 $W·\vec{x}$ 的理解就是通过组合形成新物质。$a()$又符合了我们所处的世界都是非线性的特点。<br>情景：$\vec{x}$是二维向量，维度是碳原子和氧原子的数量[C;O]，数值且定为[1;1]，若确定$\vec{y}$是三维向量，就会形成如下网络的形状 (神经网络的每个节点表示一个维度)。通过改变权重的值，可以获得若干个不同物质。右侧的节点数决定了想要获得多少种不同的新物质。（矩阵的行数）<br><img src="https://pic4.zhimg.com/80/v2-69d03cf2b3677ad7dc3b0d9af58841b4_720w.jpg" alt></p>
<ol>
<li>如果权重W的数值如（1），那么网络的输出 $\vec{y}$ 就会是三个新物质，[二氧化碳，臭氧，一氧化碳]。<br>$\left[ \begin{array}{c} CO_2\\O_3\\CO \end{array} \right]=$\left[ \begin{array}{cc} 1&amp;2\\0&amp;3\\1&amp;1 \end{array} \right]·\left[ \begin{array}{c} C\\O \end{array} \right]（1）</li>
<li>也可以减少右侧的一个节点，并改变权重W至（2），那么输出$\vec{y}$ 就会是两个新物质，[O_{0.3};CO_{1.5}]。<br>$\left[ \begin{array}{c} O_{0.3}\\CO_{1.5} \end{array} \right]=\left[ \begin{array}{cc} 0&amp;0.3\\1&amp;1.5 \end{array} \right]·\left[ \begin{array}{c} C\\O \end{array} \right]$（2）</li>
<li>如果希望通过层网络能够从[C, O]空间转变到[CO_2;O_3;CO]空间的话，那么网络的学习过程就是将W的数值变成尽可能接近(1)的过程 。如果再加一层，就是通过组合[CO_2;O_3;CO]这三种基础物质，形成若干更高层的物质。</li>
<li>重要的是这种组合思想，组合成的东西在神经网络中并不需要有物理意义。<blockquote>
<p>每层神经网络的物理理解：通过现有的不同物质的组合形成新物质。</p>
</blockquote>
</li>
</ol>
<p>二、理解视角：<br>现在我们知道了每一层的行为，但这种行为又是如何完成识别任务的呢？</p>
<p>数学视角：“线性可分”<br>*一维情景：以分类为例，当要分类正数、负数、零，三类的时候，一维空间的直线可以找到两个超平面（比当前空间低一维的子空间。当前空间是直线的话，超平面就是点）分割这三类。但面对像分类奇数和偶数无法找到可以区分它们的点的时候，我们借助 x % 2（取余）的转变，把x变换到另一个空间下来比较，从而分割。<br><img src="https://pic3.zhimg.com/80/v2-92ff4b847ac5fa41d91d1e76a910c483_720w.jpg" alt></p>
<ul>
<li>二维情景：平面的四个象限也是线性可分。但下图的红蓝两条线就无法找到一超平面去分割。<br><img src="https://picb.zhimg.com/80/v2-b1bd0f75b46ed27daf27910f2a6b6e3f_720w.jpg" alt><br>神经网络的解决方法依旧是转换到另外一个空间下，用的是所说的5种空间变换操作。比如下图就是经过放大、平移、旋转、扭曲原二维空间后，在三维空间下就可以成功找到一个超平面分割红蓝两线 (同SVM的思路一样)。<br><img src="https://pic4.zhimg.com/80/v2-8d7d1ef957ebbf8ba9bb9cf8ff2d87ff_720w.jpg" alt></li>
</ul>
<p>上面是一层神经网络可以做到的，如果把$\vec{y}$ 当做新的输入再次用这5种操作进行第二遍空间变换的话，网络也就变为了二层。最终输出是$\vec{y}=a_2(W_2·(a_1(W_1·\vec{x}+b_1))+b_2)$。</p>
<p>设想网络拥有很多层时，对原始输入空间的“扭曲力”会大幅增加，如下图，最终我们可以轻松找到一个超平面分割空间。<br><img src="https://picb.zhimg.com/v2-b7d47097d8f10e6baeb329e88e59b563_b.webp" alt></p>
<p>当然也有如下图失败的时候，关键在于“如何扭曲空间”。所谓监督学习就是给予神经网络网络大量的训练例子，让网络从训练例子中学会如何变换空间。每一层的权重W就控制着如何变换空间，我们最终需要的也就是训练好的神经网络的所有层的权重矩阵。</p>
<p>这里有非常棒的可视化空间变换<a href="https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" target="_blank" rel="noopener">demo</a>，一定要打开尝试并感受这种扭曲过程。更多内容请看<a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank" rel="noopener">Neural Networks, Manifolds, and Topology</a>。<br><img src="https://pic1.zhimg.com/v2-664c152ffc58a28a7f900f9a723cbb83_b.jpg" alt></p>
<blockquote>
<p>线性可分视角：神经网络的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。<br>增加节点数：增加维度，即增加线性转换能力。<br>增加层数：增加激活函数的次数，即增加非线性转换次数。</p>
</blockquote>
<p>物理视角：“物质组成”</p>
<ul>
<li><p>类比：回想上文由碳氧原子通过不同组合形成若干分子的例子。从分子层面继续迭代这种组合思想，可以形成DNA，细胞，组织，器官，最终可以形成一个完整的人。继续迭代还会有家庭，公司，国家等。这种现象在身边随处可见。并且原子的内部结构与太阳系又惊人的相似。不同层级之间都是以类似的几种规则再不断形成新物质。你也可能听过分形学这三个字。可通过观看从<a href="https://video.tudou.com/v/XMjAzNzM4MzcyOA==.html?__fr=oldtd" target="_blank" rel="noopener">1米到150亿光年</a>来感受自然界这种层级现象的普遍性。<br><img src="https://picb.zhimg.com/80/v2-3ec7216f7ab84dac089836b166c0ae28_720w.jpg" alt></p>
</li>
<li><p>人脸识别情景：我们可以模拟这种思想并应用在画面识别上。由像素组成菱角再组成五官最后到不同的人脸。每一层代表不同的不同的物质层面 (如分子层)。而每层的W存储着如何组合上一层的物质从而形成新物质。<br>如果我们完全掌握一架飞机是如何从分子开始一层一层形成的，拿到一堆分子后，我们就可以判断他们是否可以以此形成方式，形成一架飞机。<br>附：<a href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.72546&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" target="_blank" rel="noopener">Tensorflow playground</a>展示了数据是如何“流动”的。<br><img src="https://pic2.zhimg.com/80/v2-82f05552fd2ddde28a0ef20814d7acbb_720w.jpg" alt></p>
</li>
</ul>
<blockquote>
<p>物质组成视角：神经网络的学习过程就是学习物质组成方式的过程。<br>增加节点数：增加同一层物质的种类，比如118个元素的原子层就有118个节点。<br>增加层数：增加更多层级，比如分子层，原子层，器官层，并通过判断更抽象的概念来识别物体。</p>
</blockquote>
<p>三、神经网络的训练<br>知道了神经网络的学习过程就是学习控制着空间变换方式（物质组成方式）的权重矩阵后，接下来的问题就是如何学习每一层的权重矩阵 W 。</p>
<p>如何训练：</p>
<p>既然我们希望网络的输出尽可能的接近真正想要预测的值。那么就可以通过比较当前网络的预测值和我们真正想要的目标值，再根据两者的差异情况来更新每一层的权重矩阵（比如，如果网络的预测值高了，就调整权重让它预测低一些，不断调整，直到能够预测出目标值）。因此就需要先定义“如何比较预测值和目标值的差异”，这便是损失函数或目标函数（loss function or objective function），用于衡量预测值和目标值的差异的方程。loss function的输出值（loss）越高表示差异性越大。那神经网络的训练就变成了尽可能的缩小loss的过程。</p>
<p>所用的方法是梯度下降（Gradient descent）：通过使loss值向当前点对应梯度的反方向不断移动，来降低loss。一次移动多少是由学习速率（learning rate）来控制的。</p>
<p>梯度下降的问题：<br>然而使用梯度下降训练神经网络拥有两个主要难题。</p>
<p>1、局部极小值<br>梯度下降寻找的是loss function的局部极小值，而我们想要全局最小值。如下图所示，我们希望loss值可以降低到右侧深蓝色的最低点，但loss有可能“卡”在左侧的局部极小值中。<br><img src="https://pic2.zhimg.com/80/v2-00fe10bf8877137bc5957cf0cd7f9219_720w.jpg" alt></p>
<p>试图解决“卡在局部极小值”问题的方法分两大类：</p>
<ul>
<li>调节步伐：调节学习速率，使每一次的更新“步伐”不同。常用方法有：</li>
<li>随机梯度下降（Stochastic Gradient Descent (SGD)：每次只更新一个样本所计算*的梯度</li>
<li>小批量梯度下降（Mini-batch gradient descent）：每次更新若干样本所计算的梯度的平均值</li>
<li>动量（Momentum）：不仅仅考虑当前样本所计算的梯度；Nesterov动量（Nesterov Momentum）：Momentum的改进</li>
<li>Adagrad、RMSProp、Adadelta、Adam：这些方法都是训练过程中依照规则降低学习速率，部分也综合动量</li>
<li>优化起点：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）</li>
</ul>
<p>2、梯度的计算<br>机器学习所处理的数据都是高维数据，该如何快速计算梯度、而不是以年来计算。<br>其次如何更新隐藏层的权重？<br>解决方法是：计算图：反向传播算法<br>这里的解释留给非常棒的<a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Backprop/">Computational Graphs: Backpropagation</a><br>需要知道的是，反向传播算法是求梯度的一种方法。如同快速傅里叶变换（FFT）的贡献。<br>而计算图的概念又使梯度的计算更加合理方便。</p>
<p>基本流程图：<br>下面就结合图简单浏览一下训练和识别过程，并描述各个部分的作用。要结合图解阅读以下内容。但手机显示的图过小，最好用电脑打开。<br><img src="https://pic2.zhimg.com/80/v2-69c014a15afde18a4086950c30e97d1b_720w.jpg" alt></p>
<ul>
<li>收集训练集（train data）：也就是同时有input以及对应label的数据。每个数据叫做训练样本（sample）。label也叫target，也是机器学习中最贵的部分。上图表示的是我的数据库。假设input本别是x的维度是39，label的维度是48。</li>
<li><p>设计网络结构（architecture）：确定层数、每一隐藏层的节点数和激活函数，以及输出层的激活函数和损失函数。上图用的是两层隐藏层（最后一层是输出层）。隐藏层所用激活函数a( )是ReLu，输出层的激活函数是线性linear（也可看成是没有激活函数）。隐藏层都是1000节点。损失函数L( )是用于比较距离MSE：mean((output - target)^2)。MSE越小表示预测效果越好。训练过程就是不断减小MSE的过程。到此所有数据的维度都已确定：<br>训练数据：$input\in R_39;label \in R_48$<br>权重矩阵：$W_{h_1} \in R^{1000x39};W_{h2} \in R^{1000x1000};W_o \in R^{48x1000}$<br>偏移向量：$b_{h_1} \in R^1000;b_{h_2} \in R^1000; b_o \in R^48$<br>网络输出：$output \in R^{48}$</p>
</li>
<li><p>数据预处理（preprocessing）：将所有样本的input和label处理成能够使用神经网络的数据，label的值域符合激活函数的值域。并简单优化数据以便让训练易于收敛。比如中心化（mean subtraction）、归一化（normalization）、主成分分析（PCA）、白化（whitening）。假设上图的input和output全都经过了中心化和归一化。</p>
</li>
<li>权重初始化（weights initialization）：$W_{h_1},W_{h_2},W_0$在训练前不能为空，要初始化才能够计算loss从而来降低。$W_{h_1},W_{h_2},W_0$初始化决定了loss在loss function中从哪个点开始作为起点训练网络。上图用均匀分布初始权重（Uniform distribution）。</li>
<li>训练网络（training）：训练过程就是用训练数据的input经过网络计算出output，再和label计算出loss，再计算出gradients来更新weights的过程。<ul>
<li>正向传递：，算当前网络的预测值$output=linear(W_o·Relu(W_{h_2}·Relu(W_{h_1}·input+b_{h_1})+b_{h_2})+b_o)$</li>
<li>计算loss：$loss=mean((output-target)^2)$</li>
<li>计算梯度：从loss开始反向传播计算每个参数（parameters）对应的梯度（gradients）。这里用Stochastic Gradient Descent (SGD) 来计算梯度，即每次更新所计算的梯度都是从一个样本计算出来的。传统的方法Gradient Descent是正向传递所有样本来计算梯度。SGD的方法来计算梯度的话，loss function的形状如下图所示会有变化，这样在更新中就有可能“跳出”局部最小值。<br><img src="https://pic2.zhimg.com/80/v2-0c0e7f5ffa98c2c1eb87763dd5d1d9a3_720w.jpg" alt></li>
<li>更新权重：这里用最简单的方法来更新，即所有参数都 $W=W-learninggrate*gradient$</li>
<li>预测新值：训练过所有样本后，打乱样本顺序再次训练若干次。训练完毕后，当再来新的数据input，就可以利用训练的网络来预测了。这时的output就是效果很好的预测值了。下图是一张实际值和预测值的三组对比图。输出数据是48维，这里只取1个维度来画图。蓝色的是实际值，绿色的是预测值。最上方的是训练数据的对比图，而下方的两行是神经网络模型从未见过的数据预测对比图。（不过这里用的是RNN，主要是为了让大家感受一下效果）<br><img src="https://pic3.zhimg.com/80/v2-a675e692f7f7755d91bcdba5e988e910_720w.jpg" alt></li>
</ul>
</li>
</ul>
<p>注：此部分内容不是这篇文章的重点，但为了理解深层神经网络，需要明白最基本的训练过程。<br>若能理解训练过程是通过梯度下降尽可能缩小loss的过程即可。<br>若有理解障碍，可以用python实践一下<a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">从零开始训练一个神经网络</a>，体会整个训练过程。若有时间则可以再体会一下计算图自动求梯度的方便利用<a href="https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners" target="_blank" rel="noopener">TensorFlow</a>。</p>
<p>结合<a href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.50484&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" target="_blank" rel="noopener">Tensorflow playground</a>理解5种空间操作和物质组成视角<br>打开网页后，总体来说，蓝色代表正值，黄色代表负值。拿分类任务来分析。</p>
<ul>
<li>数据：在二维平面内，若干点被标记成了两种颜色。黄色，蓝色，表示想要区分的两类。你可以把平面内的任意点标记成任意颜色。网页给你提供了4种规律。神经网络会根据你给的数据训练，再分类相同规律的点。<br><img src="https://pic3.zhimg.com/80/v2-6d17d1fb77d3ae1838f5253193456317_720w.jpg" alt></li>
<li>输入：在二维平面内，你想给网络多少关于“点”的信息。从颜色就可以看出来，$x_1$左边是负，右边是正，$x_1$表示此点的横坐标值。同理，$x_2$表示此点的纵坐标值。$x_1^2$是关于横坐标值的“抛物线”信息。你也可以给更多关于这个点的信息。给的越多，越容易被分开。<br><img src="https://picb.zhimg.com/80/v2-111e37e8479aa57bedbfb2dbcd8e5b63_720w.jpg" alt></li>
<li>连接线：表示权重，蓝色表示用神经元的原始输出，黄色表示用负输出。深浅表示权重的绝对值大小。鼠标放在线上可以看到具体值。也可以更改。在（1）中，当把$x_2$输出的一个权重改为-1时，$x_2$的形状直接倒置了。不过还需要考虑激活函数。（1）中用的是linear。在（2）中，当换成sigmoid时，你会发现没有黄色区域了。因为sigmoid的值域是(0,1)<br><img src="https://pic4.zhimg.com/80/v2-2820b0562c8fcd7a49d57c4deb1e4f3c_720w.jpg" alt><br><img src="https://pic2.zhimg.com/80/v2-83fd9f01e2ea38c7f6b8aeaa308cf040_720w.jpg" alt></li>
<li>输出：黄色背景颜色都被归为黄点类，蓝色背景颜色都被归为蓝点类。深浅表示可能性的强弱。<br><img src="https://pic4.zhimg.com/80/v2-dff3f6e72881ebd222414eabb9504671_720w.jpg" alt><br>上图中所有在黄色背景颜色的点都会被分类为“黄点“，同理，蓝色区域被分成蓝点。在上面的分类分布图中你可以看到每一层通过上一层信息的组合所形成的。权重（那些连接线）控制了“如何组合”。神经网络的学习也就是从数据中学习那些权重。Tensorflow playground所表现出来的现象就是“在我文章里所写的“物质组成思想”，这也是为什么我把<a href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.27298&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" target="_blank" rel="noopener">Tensorflow playground</a>放在了那一部分。</li>
</ul>
<p>不过你要是把Tensorflow的个名字拆开来看的话，是tensor（张量）的flow（流动）。Tensorflow playground的作者想要阐述的侧重点是“张量如何流动”的。</p>
<p>5种空间变换的理解：Tensorflow playground下没有体现5种空间变换的理解。需要打开这个网站尝试：<a href="https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" target="_blank" rel="noopener">ConvNetJS demo: Classify toy 2D data</a><br><img src="https://pic4.zhimg.com/80/v2-55811ac3d91f56f19543714b1b5abe49_720w.jpg" alt><br>左侧是原始输入空间下的分类图，右侧是转换后的高维空间下的扭曲图。<br><img src="https://pic2.zhimg.com/80/v2-a81a10592b96a1d2b067e1d4ae3951e7_720w.jpg" alt><br>最终的扭曲效果是所有绿点都被扭曲到了一侧，而所有红点都被扭曲到了另一侧。这样就可以线性分割（用超平面（这里是一个平面）在中间分开两类）</p>
<p>四、表现原因<br>文章的最后稍微提一下深层神经网络。深层神经网络就是拥有更多层数的神经网络。</p>
<p>按照上文在理解视角中所述的观点，可以想出下面两条理由关于为什么更深的网络会更加容易识别，增加容纳变异体（variation）（红苹果、绿苹果）的能力、鲁棒性（robust）。</p>
<p>数学视角：变异体（variation）很多的分类的任务需要高度非线性的分割曲线。不断的利用那5种空间变换操作将原始输入空间像“捏橡皮泥一样”在高维空间下捏成更为线性可分/稀疏的形状。</p>
<p>物理视角：通过对“抽象概念”的判断来识别物体，而非细节。比如对“飞机”的判断，即便人类自己也无法用语言或者若干条规则来解释自己如何判断一个飞机。因为人脑中真正判断的不是是否“有机翼”、“能飞行”等细节现象，而是一个抽象概念。层数越深，这种概念就越抽象，所能涵盖的变异体就越多，就可以容纳战斗机，客机等很多种不同种类的飞机。</p>
<h1 id="lt-span-id-”header8-gt-YJango的TensorFlow1-x整体把握-lt-span-gt"><a href="#lt-span-id-”header8-gt-YJango的TensorFlow1-x整体把握-lt-span-gt" class="headerlink" title="&lt;span id=”header8&gt;YJango的TensorFlow1.x整体把握&lt;/span&gt;"></a>&lt;span id=”header8&gt;YJango的TensorFlow1.x整体把握&lt;/span&gt;</h1><p>目前主流的TensorFlow，用tensorflow这样工具的原因是：它允许我们用计算图（Computational Graphs）的方式建立网络。同时又可以非常方便的对网络进行操作。</p>
<p>下面就是对计算图的直观讲解</p>
<p>比喻说明：</p>
<ul>
<li><p>结构：计算图所建立的只是一个网络框架。在编程时，并不会有任何实际值出现在框架中。所有权重和偏移都是框架中的一部分，初始时至少给定初始值才能形成框架。因此需要initialization初始化。</p>
</li>
<li><p>比喻：计算图就是一个管道。编写网络就是搭建一个管道结构。在投入实际使用前，不会有任何液体进入管道。而神经网络中的权重和偏移就是管道中的阀门，可以控制液体的流动强弱和方向。在神经网络的训练中，阀门会根据数据进行自我调节、更新。但是使用之前至少要给所有阀门一个初始的状态才能形成结构。用计算图的好处是它允许我们可以从任意一个节点处取出液体。<br><img src="https://pic4.zhimg.com/80/v2-7ac033df767fcd722c88cda4829cb13b_720w.jpg" alt></p>
</li>
</ul>
<p>用法说明：<br>请类比管道构建来理解计算图的用法</p>
<p>构造阶段（construction phase）：组装计算图（管道）</p>
<ul>
<li>计算图（graph）：要组装的结构。由许多操作组成。</li>
<li>操作（ops）：接受（流入）零个或多个输入（液体），返回（流出）零个或多个输出。</li>
<li>数据类型：主要分为张量（tensor）、变量（variable）和常量（constant）<ul>
<li>张量：多维array或list（管道中的液体）<ul>
<li>创建语句：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor_name&#x3D;tf.placeholder(type, shape, name)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>变量：在同一时刻对图中所有其他操作都保持静态的数据（管道中的阀门）</p>
<ul>
<li>创建语句：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name_variable &#x3D; tf.Variable(value, name)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>初始化语句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#个别变量</span><br><span class="line">init_op&#x3D;variable.initializer()</span><br><span class="line">#所有变量</span><br><span class="line">init_op&#x3D;tf.initialize_all_variables()</span><br><span class="line">#注意：init_op的类型是操作（ops），加载之前并不执行</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新语句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update_op&#x3D;tf.assign(variable to be updated, new_value)</span><br></pre></td></tr></table></figure>
</li>
<li><p>常量：无需初始化的变量</p>
<ul>
<li>创建语句：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name_constant&#x3D;tf.constant(value)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>执行阶段（execution phase）：使用计算图（获取液体）</p>
<ul>
<li><p>会话：执行（launch）构建的计算图。可选择执行设备：单个电脑的CPU、GPU，或电脑分布式甚至手机。</p>
<ul>
<li>创建语句：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#常规</span><br><span class="line">sess &#x3D; tf.Session()</span><br><span class="line">#交互</span><br><span class="line">sess &#x3D; tf.InteractiveSession()</span><br><span class="line">#交互方式可用tensor.eval()获取值，ops.run()执行操作</span><br><span class="line">#关闭</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>执行操作：使用创建的会话执行操作</p>
<ul>
<li>执行语句：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sess.run(op)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>送值（feed）：输入操作的输入值（输入液体）</p>
<ul>
<li>语句：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sess.run([output], feed_dict&#x3D;&#123;input1:value1, input2:value1&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>取值（fetch）：获取操作的输出值（得到液体）</p>
<ul>
<li>语句：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#单值获取 </span><br><span class="line">sess.run(one op)</span><br><span class="line">#多值获取</span><br><span class="line">sess.run([a list of ops])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1 id="YJango的循环神经网络——介绍"><a href="#YJango的循环神经网络——介绍" class="headerlink" title="YJango的循环神经网络——介绍"></a><span id="header9">YJango的循环神经网络——介绍</span></h1><p>该文主要目的是让大家体会循环神经网络在与前馈神经网络的不同之处。</p>
<p>大家貌似都叫Recurrent Neural Networks为循环神经网络。</p>
<p>我之前是查维基百科的缘故，所以一直叫它递归网络。</p>
<p>下面我所提到的递归网络全部都是指Recurrent Neural Networks。</p>
<p>递归神经网络的讨论分为三部分</p>
<p>介绍：描述递归网络和前馈网络的差别和优劣<br>实现：梯度消失和梯度爆炸问题，及解决问题的LSTM和GRU<br>代码：用tensorflow1.x实际演示一个任务的训练和使用</p>
<p>时序预测问题<br>一、用前馈神经网络来做时序信号预测有什么问题？</p>
<ul>
<li>依赖受限：前馈网络是利用窗处理将不同时刻的向量并接成一个更大的向量。以此利用前后发生的事情预测当前所发生的情况。如下图所示：<br><img src="https://pic4.zhimg.com/v2-c984ea3ba696a0a786bb0bb52f0d772e_b.webp" alt><ul>
<li>但其所能考虑到的前后依赖受限于将多少个向量（window size）并接在一起。所能考虑的依赖始终是固定长度的。</li>
</ul>
</li>
<li>网络规格：想要更好的预测，需要让网络考虑更多的前后依赖。<br>例：若仅给“国（）”，让玩家猜括号中的字时，所能想到的可能性非常之多。但若给“中国（）”时，可能性范围降低。当给“我是中国（）”时，猜中的可能性会进一步增加。</li>
</ul>
<p>那么很自然的做法就是扩大并接向量的数量，但这样做的同时也会使输入向量的维度和神经网络第一层的权重矩阵的大小快速增加。如YJango的前馈神经网络—代码LV3中每个输入向量的维度是39，41帧的窗处理之后，维度变成了1599，并且神经网络第一层的权重矩阵也变成了1599 by n（n为下一层的节点数）。其中很多权重都是冗余的，但却不得不一直存在于每一次的预测之中。</p>
<ul>
<li>训练所需样本数：前两点可能无伤大雅，而真正使得递归神经网络（recurrent）在时序预测中击败前馈神经网络的关键在于训练网络所需要的数据量。</li>
</ul>
<p>网络差异之处<br>几乎所有的神经网络都可以看作为一种特殊制定的前馈神经网络，这里“特殊制定”的作用在于缩减寻找映射函数的搜索空间，也正是因为搜索空间的缩小，才使得网络可以用相对较少的数据量学习到更好的规律。</p>
<p>例：解一元二次方程y=ax+b。我们需要两组配对的(x,y)来解该方程。但是如果我们知道y=ax+b实际上是y=ax，这时就只需要一对(x,y)就可以确定x与y的关系。递归神经网络和卷积神经网络等神经网络的变体就具有类似的功效。</p>
<p>二、相比前馈神经网络，递归神经网络究竟有何不同之处？</p>
<p>需要注意的是递归网络并非只有一种结构，这里介绍一种最为常用和有效的递归网络结构。</p>
<p>数学视角<br>首先让我们用从输入层到隐藏层的空间变换视角来观察，不同的地方在于，这次将时间维度一起考虑在内。</p>
<p>注：这里的圆圈不再代表节点，而是状态，是输入向量和输入经过隐藏层后的向量。</p>
<p>例子：用目前已说的所有字预测下一个字。</p>
<p>前馈网络：window size为3帧的窗处理后的前馈网络</p>
<ul>
<li>动态图：左侧是时间维度展开前，右侧是展开后（单位时刻实际工作的只有灰色部分。）。前馈网络的特点使不同时刻的预测完全是独立的。我们只能通过窗处理的方式让其照顾到前后相关性。<br><img src="https://pic3.zhimg.com/v2-8cc0db5acd643dd7ca3668ad6e35aabb_b.webp" alt></li>
<li>数学式子：$h_t=\phi(W_{xh}·concat(x_{t-1},x_t,x_{t+1})+b)$，concat表示将向量并接成一个更大维度的向量。</li>
<li>学习参数：需要从大量的数据中学习$W_{xh}$和b。</li>
<li>要学习各个时刻（3个）下所有维度（39维）的关系（39*3个），就需要很多数据。</li>
</ul>
<p>递归网络：不再有window size的概念，而是time step</p>
<ul>
<li>动态图：左侧是时间维度展开前，回路方式的表达方式，其中黑方框表示时间延迟。右侧展开后，可以看到当前时刻的$h_t$并不仅仅取决于当前时刻的输入$x_t$，同时与上一时刻的$h_{t-1}$也相关。<br><img src="https://pic3.zhimg.com/v2-db52470df88f53271d8e06722da39122_b.jpg" alt><br>数学式子：$h_t=\phi(W_{xh}·x_t+W_{hh}·h_{t-1}+b)$。{h_t}同样也由x_t经$W_{zh}$的变化后的信息决定，<br>但这里多另一份信息：$W_{hh}·h_{t-1}$，而该信息是从上一时刻的隐藏状态$h_{t-1}$经过一个不同的$W_{hh}$变换后得出的。<br>注：$W_{xh}$的形状是行为dim_input，列为dim_hidden_state，而$W_{hh}$是一个行列都为dim_hidden_state的方阵。<br>学习参数：前馈网络需要3个时刻来帮助学习一次$W_{xh}$，而递归网络可以用3个时刻来帮助学习3次$W_{xh}$和$W_{hh}$。换句话说：所有时刻的权重矩阵都是共享的。这是递归网络相对于前馈网络而言最为突出的优势。</li>
</ul>
<blockquote>
<p>递归神经网络是在时间结构上存在共享特性的神经网络变体。</p>
</blockquote>
<p>时间结构共享是递归网络的核心中的核心。</p>
<p>物理视角<br>共享特性给网络带来了诸多好处，但也产生了另一个问题：</p>
<p>三、为什么可以共享？</p>
<p>在物理视角中，YJango想给大家展示的第一点就是为什么我们可以用这种共享不同时刻权重矩阵的网络进行时序预测。</p>
<p>下图可以从直觉上帮助大家感受日常生活中很多时序信号是如何产生的。</p>
<p>例1：轨迹的产生，如地球的轨迹有两条线索决定，其中一条是地球自转，另一条是地球围绕太阳的公转。下图是太阳和其他星球。自转相当于[公式]，而公转相当于[公式]。二者共同决定实际轨迹。<br><img src="https://pic1.zhimg.com/80/v2-8b7c5c5225b6a7337e23786ecb5b2fe3_720w.jpg" alt></p>
<p>例2：同理万花尺<br><img src="https://pic2.zhimg.com/v2-9e770433dc6a146e7625dd155ba00ec5_b.jpg" alt></p>
<p>例3：演奏音乐时，乐器将力转成相应的震动产生声音，而整个演奏拥有一个主旋律贯穿全曲。其中乐器的物理特性就相当于[公式]，同一乐器在各个时刻物理特性在各个时刻都是共享的。其内在也有一个隐藏的主旋律基准[公式]，旋律信息[公式]与音乐信息[公式]共同决定下一时刻的实际声音。<br>上述例子中所产生的轨迹、音乐都是我们所能观察到的observations，我们常常会利用这些observation作为依据来做出决策。</p>
<p>下面的例子可能更容易体会共享特性对于数据量的影响。</p>
<p>实例：捏陶瓷：不同角度相当于不同的时刻<br><img src="https://pic3.zhimg.com/v2-c84b8f3c65a62e4ceee81899c5126365_b.webp" alt></p>
<ul>
<li>若用前馈网络：网络训练过程相当于不用转盘，而是徒手将各个角度捏成想要的形状。不仅工作量大，效果也难以保证。</li>
<li>若用递归网络：网络训练过程相当于在不断旋转的转盘上，以一种手势捏造所有角度。工作量降低，效果也可保证。</li>
</ul>
<p>递归网络特点</p>
<ul>
<li>时序长短可变：只要知道上一时刻的隐藏状态$h_{t-1}$与当前时刻的输入$x_t$，就可以计算当前时刻的隐藏状态$h_t$。并且由于计算所用到的W_{xh}与W_{hh}在任意时刻都是共享的。递归网络可以处理任意长度的时间序列。</li>
<li>顾及时间依赖：若当前时刻是第5帧的时序信号，那计算当前的隐藏状态$h_5$就需要当前的输入$x_5$和第4帧的隐藏状态$h_4$，而计算$h_4$又需要$h_3$，这样不断逆推到初始时刻为止。意味着常规递归网络对过去所有状态都存在着依赖关系。<br>注：在计算$h_0$的值时，若没有特别指定初始隐藏状态，则会将$h_{t-1}$全部补零，表达式会变成前馈神经网络：$h_t=\phi (W_{xh}·x_t+0+b)$</li>
<li>未来信息依赖：前馈网络是通过并接未来时刻的向量来引入未来信息对当前内容判断的限制，但常规的递归网络只对所有过去状态存在依赖关系。所以递归网络的一个扩展就是双向（bidirectional）递归网络：两个不同方向的递归层叠加。</li>
<li><p>关系图：正向（forward）递归层是从最初时刻开始，而反向（backward）递归层是从最末时刻开始。<br><img src="https://picb.zhimg.com/80/v2-a9c81692067bca508bc88ba3f3ecb7b8_720w.jpg" alt></p>
</li>
<li><p>数学式子：</p>
<ul>
<li>正向递归层：$h_t^f=\phi (W_{xh}^f·x_t+W_{hh}^f·h_{t-1}+b^f)$</li>
<li>反向递归层：$h_t^b=\phi (W_{xh}^b·x_t+W_{hh}^b·h_{t+1}+b^b)$</li>
<li>双向递归层：$h_t=h_t^f+h_t^b$</li>
<li>注：还有并接的处理方式，即$h_t=concat(h_t^f,h_t^b)$，但反向递归层的作用是引入未来信息对当前预测判断的额外限制。并不是信息维度不够。至少在我所有的实验中，相加（sum）的方式往往优于并接。</li>
<li>注：也有人将正向递归层和反向递归层中的权重$W_{xh}^f$与$W_{xh}^b$共享，$W_{hh}^f$与$W_{xh}^b$共享。我没有做实验比较过。但直觉上$W_{hh}^f$与$W_{hh}^b$共享在某些任务中可能会有些许提升。$W_{hh}^f$与$W_{hh}^b$的共享恐怕并不会起到什么作用（要贴合任务而言）。</li>
<li>注：隐藏状态$h_t$通常不会是网络的最终结果，一般都会将$h_t$再接着另一个$\phi (W_{ho}·h_t+b_o)$将其投射到输出状态$o_t$。一个最基本的递归网络不会出现前馈神经网络那样从输入层直接到输出层的情况，而是至少会有一个隐藏层。</li>
<li>注：双向递归层可以提供更好的识别预测效果，但却不能实时预测，由于反向递归的计算需要从最末时刻开始，网络不得不等待着完整序列都产生后才可以开始预测。在对于实时识别有要求的线上语音识别，其应用受限。</li>
</ul>
</li>
<li><p>递归网络输出：递归网络的出现实际上是对前馈网络在时间维度上的扩展。</p>
<ul>
<li>关系图：常规网络可以将输入和输出以向量对向量（无时间维度）的方式进行关联。而递归层的引入将其扩展到了序列对序列的匹配。从而产生了one to one右侧的一系列关联方式。较为特殊的是最后一个many to many，发生在输入输出的序列长度不确定时，其实质两个递归网络的拼接使用，公共点在紫色的隐藏状态$h_{t+1}$。<br><img src="https://picb.zhimg.com/80/v2-78c48d9ca0b66d6b46202b019b5ecb66_720w.jpg" alt></li>
<li>many to one：常用在情感分析中，将一句话关联到一个情感向量上去。</li>
<li>many to many：第一个many to many在DNN-HMM语音识别框架中常有用到</li>
<li>many to many(variable length)：第二个many to many常用在机器翻译两个不同语言时。</li>
</ul>
</li>
<li><p>递归网络数据：递归网络由于引入time step的缘故，使得其训练数据与前馈网络有所不同</p>
<ul>
<li>前馈网络：输入和输出：矩阵<ul>
<li>输入矩阵形状：(n_samples, dim_input)</li>
<li>输出矩阵形状：(n_samples, dim_output)</li>
<li>注：真正测试/训练的时候，网络的输入和输出就是向量而已。加入n_samples这个维度是为了可以实现一次训练多个样本，求出平均梯度来更新权重，这个叫做Mini-batch gradient descent。</li>
<li>如果n_samples等于1，那么这种更新方式叫做Stochastic Gradient Descent (SGD)。</li>
</ul>
</li>
<li>递归网络：输入和输出：维度至少是3的张量，如果是图片等信息，则张量维度仍会增加。<ul>
<li>输入张量形状：(time_steps, n_samples, dim_input)</li>
<li>输出张量形状：(time_steps, n_samples, dim_output)</li>
<li>注：同样是保留了Mini-batch gradient descent的训练方式，但不同之处在于多了time step这个维度。</li>
<li>Recurrent 的任意时刻的输入的本质还是单个向量，只不过是将不同时刻的向量按顺序输入网络。你可能更愿意理解为一串向量 a sequence of vectors，或者是矩阵。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>网络对待<br>请以层的概念对待所有网络。递归神经网络是指拥有递归层的神经网络，其关键在于网络中存在递归层。</p>
<p>每一层的作用是将数据从一个空间变换到另一个空间下。可以视为特征抓取方式，也可以视为分类器。二者没有明显界限并彼此包含。关键在于使用者如何理解。</p>
<p>以层的概念理解网络的好处在于，今后的神经网络往往并不会仅用到一种处理手段。往往是前馈、递归、卷积混合使用。 这时就无法再以递归神经网络来命名该结构。</p>
<p>例：下图中就是在双向递归层的前后分别又加入了两个前馈隐藏层。也可以堆积更多的双向递归层，人们也会在其前面加入“深层”二字，提高逼格。<br><img src="https://pic3.zhimg.com/80/v2-1c85ff7e786721b62d2f9b15991a6871_720w.jpg" alt></p>
<p>注：层并不是图中所画的圆圈，而是连线。圆圈所表示的是穿过各层前后的状态。</p>
<p>归网络问题<br>常规递归网络从理论上应该可以顾及所有过去时刻的依赖，然而实际却无法按人们所想象工作。原因在于梯度消失（vanishinggradient）和梯度爆炸（exploding gradient）问题。下一节就是介绍Long Short Term Memory（LSTM）和Gated Recurrent Unit（GRU）：递归网络的特别实现算法。</p>
<p>最后点个题，重要的事情说一万遍</p>
<blockquote>
<p>Recurrent Layer在时间结构上存在共享特性。</p>
</blockquote>
<h1 id="YJango的循环神经网络——实现LSTM"><a href="#YJango的循环神经网络——实现LSTM" class="headerlink" title="YJango的循环神经网络——实现LSTM"></a><span id="10">YJango的循环神经网络——实现LSTM</span></h1><p><img src="https://pic2.zhimg.com/v2-b75451ec04c675d9ba7e3e1ba2971821_1440w.jpg?source=172ae18b" alt><br>介绍<br>描述最常用的RNN实现方式：Long-Short Term Memory（LSTM）</p>
<p>梯度消失和梯度爆炸<br>网络回忆：在《循环神经网络——介绍》中提到循环神经网络用相同的方式处理每个时刻的数据。</p>
<p>动态图：<br><img src="https://pic3.zhimg.com/v2-db52470df88f53271d8e06722da39122_b.jpg" alt></p>
<p>数学公式：$h_t=\phi(W_{xh}·x_t+W_{hh}·h_{t-1}+b)$</p>
<p>设计目的：我们希望循环神经网络可以将过去时刻发生的状态信息传递给当前时刻的计算中。</p>
<p>实际问题：但普通的RNN结构却难以传递相隔较远的信息。</p>
<p>考虑：若只看上图蓝色箭头线的、隐藏状态的传递过程，不考虑非线性部分，那么就会得到一个简化的式子(1)：<br>(1) $h_t=W_{hh}·h_{t-1}$<br>如果将起始时刻的隐藏状态信息$h_0$向第t时刻传递，会得到式子(2)<br>(2) $h_t=(W_{hh})^t·h_0$<br>$W_{hh}$会被乘以多次，若允许矩阵$W_{hh}$进行特征分解<br>(3) $h_t=(W_{hh})^t·h_0$<br>式子(2)会变成(4)<br>(4) $h_t=Q·\lambda ^t·Q^T·h_0 $<br>当特征值小于1时，不断相乘的结果是特征值的t次方向 0 衰减； 当特征值大于1时，不断相乘的结果是特征值的t次方向 $\infty$扩增。 这时想要传递的$h_0$中的信息会被掩盖掉，无法传递到$h_t$。</p>
<p><img src="https://pic2.zhimg.com/80/v2-f1f179149020fd496170de1bb25fcbee_720w.png" alt></p>
<p>类比：设想$y=a^t*x$，如果a等于0.1，x在被不断乘以0.1一百次后会变成多小？如果a等于5，x在被不断乘以5一百次后会变得多大？若想要x所包含的信息既不消失，又不爆炸，就需要尽可能的将a的值保持在1。<br>注：更多内容请参阅<a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank" rel="noopener">Deep Learning by Ian Goodfellow中第十章</a>。</p>
<p>Long Short Term Memory (LSTM)<br>上面的现象可能并不意味着无法学习，但是即便可以，也会非常非常的慢。为了有效的利用梯度下降法学习，我们希望使不断相乘的梯度的积(the product of derivatives)保持在接近1的数值。</p>
<p>一种实现方式是建立线性自连接单元(linear self-connections)和在自连接部分数值接近1的权重，叫做leaky units。但Leaky units的线性自连接权重是手动设置或设为参数，而目前最有效的方式gated RNNs是通过gates的调控，允许线性自连接的权重在每一步都可以自我变化调节。LSTM就是gated RNNs中的一个实现。</p>
<p>LSTM的初步理解<br>LSTM(或者其他gated RNNs)是在标准RNN （$h_t=\phi (W_{xh}·x_t+W_{hh}·h_{t-1}+b)$）的基础上装备了若干个控制数级(magnitude)的gates。可以理解成神经网络(RNN整体)中加入其他神经网络(gates)，而这些gates只是控制数级，控制信息的流动量。</p>
<p>数学公式：这里贴出基本LSTM的数学公式，看一眼就好，仅仅是为了让大家先留一个印象，不需要记住，不需要理解。</p>
<p><img src="https://pic4.zhimg.com/80/v2-eb4824dc842bb2f73be2b8d297c51944_720w.png" alt><br>尽管式子不算复杂，却包含很多知识，接下来就是逐步分析这些式子以及背后的道理。 比如[公式]的意义和使用原因，sigmoid的使用原因。</p>
<p>门(gate)的理解<br>理解Gated RNNs的第一步就是明白gate到底起到什么作用。</p>
<ul>
<li>物理意义：gate本身可看成是十分有物理意义的一个神经网络。<ul>
<li>输入：gate的输入是控制依据；</li>
<li>输出：gate的输出是值域为[公式]的数值，表示该如何调节其他数据的数级的控制方式。</li>
</ul>
</li>
<li>使用方式：gate所产生的输出会用于控制其他数据的数级，相当于过滤器的作用。<ul>
<li>类比图：可以把信息想象成水流，而gate就是控制多少水流可以流过。</li>
</ul>
</li>
</ul>
<p><img src="https://pic4.zhimg.com/80/v2-5b968059df4831d80b6c049579c5ea18_720w.jpg" alt></p>
<p>例如：当用gate来控制向量[20 5 7 8]时，<br>若gate的输出为[0.1 0.2 0.9 0.5]时，原来的向量就会被对应元素相乘(element-wise)后变成：<br>$\left[ \begin{array}{cccc}20&amp;5&amp;7&amp;8 \end{array} \right]\odot \left[ \begin{array}{cccc}0.1&amp;0.2&amp;0.9&amp;0.5 \end{array} \right]=\left[ \begin{array}{cccc}20<em>0.1&amp;5</em>0.2&amp;7<em>0.9&amp;8</em>0.5 \end{array}\right]=\left[ \begin{array}{cccc}2&amp;1&amp;6.3&amp;4 \end{array} \right]$</p>
<p>若gate的输出为$\left[ \begin{array}{cccc}0.5&amp;0.5&amp;0.5&amp;0.5 \end{array} \right]$时，原来的向量就会被对应元素相乘(element-wise)后变成：<br>$\left[ \begin{array}{cccc}20&amp;5&amp;7&amp;8r \end{array} \right]\odot \left[ \begin{array}{cccc}0.5&amp;0.5&amp;0.5&amp;0.5r \end{array} \right]=\left[ \begin{array}{cccc}10&amp;2.5&amp;3.5&amp;4r \end{array} \right]$</p>
<p>控制依据：明白了gate的输出后，剩下要确定以什么信息为控制依据，也就是什么是gate的输入。<br>例如：即便是LSTM也有很多个变种。一个变种方式是调控门的输入。例如下面两种gate：<br>$g=sigmoid(X_{xg}·x_t+W_{hg}·h_{t-1}+b)$：<br>这种gate的输入有当前的输入$x_t$和上一时刻的隐藏状态$h_{t-1}$， 表示gate是将这两个信息流作为控制依据而产生输出的。<br>$g=sigmoid(X_{xg}·x_t+W_{hg}·h_{t-1}+W_{cg}·c_{t-1}+b)$：<br>这种gate的输入有当前的输入$x_t$和上一时刻的隐藏状态$h_{t-1}$，以及上一时刻的cell状态$c_{t-1}$， 表示gate是将这三个信息流作为控制依据而产生输出的。这种方式的LSTM叫做peephole connections。</p>
<p>LSTM的再次理解<br>明白了gate之后再回过头来看LSTM的数学公式</p>
<p>数学公式：<br><img src="https://pic1.zhimg.com/80/v2-e45fc2149e49856df73c73fc9b635db3_720w.png" alt></p>
<p>gates：先将前半部分的三个式子$i_t,f_t,o_t$统一理解。在LSTM中，网络首先构建了3个gates来控制信息的流通量。<br>注： 虽然gates的式子构成方式一样，但是注意3个gates式子W和b的下角标并不相同。它们有各自的物理意义，在网络学习过程中会产生不同的权重。<br>有了这3个gates后，接下来要考虑的就是如何用它们装备在普通的RNN上来控制信息流，而根据它们所用于控制信息流通的地点不同，它们又被分为：</p>
<ul>
<li>输入门$i_t$：控制有多少信息可以流入memory cell（第四个式子$C_t$）。</li>
<li>遗忘门$f_t$：控制有多少上一时刻的memory cell中的信息可以累积到当前时刻的memory cell中。</li>
<li>输出门$o_t$：控制有多少当前时刻的memory cell中的信息可以流入当前隐藏状态$H_t$中。</li>
<li>注：gates并不提供额外信息，gates只是起到限制信息的量的作用。因为gates起到的是过滤器作用，所以所用的激活函数是sigmoid而不是tanh。</li>
</ul>
<p>信息流：信息流的来源只有三处，当前的输入$x_t$，上一时刻的隐藏状态$h_{t-1}$，上一时刻的cell状态$c_{t-1}$，其中$c_{t-1}是额外制造出来、可线性自连接的单元（请回想起leaky units）。真正的信息流来源可以说只有当前的输入$x_t$，上一时刻的隐藏状态$h_{t-1}$两处。三个gates的控制依据，以及数据的更新都是来源于这两处。<br>分析了gates和信息流后，再分析剩下的两个等式，来看LSTM是如何累积历史信息和计算隐藏状态h的。<br>历史信息累积：<br>式子：$c_t=f_t \odot c_{t-1}+i_t \odot tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c)$<br>其中$new=tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c)$是本次要累积的信息来源。<br>改写：$c_t=f_t \odot c_{t-1}+i_t \odot new$</p>
<p>所以历史信息的累积是并不是靠隐藏状态h自身，而是依靠memory cell这个自连接来累积。 在累积时，靠遗忘门来限制上一时刻的memory cell的信息，并靠输入门来限制新信息。并且真的达到了leaky units的思想，memory cell的自连接是线性的累积。</p>
<p>当前隐藏状态的计算：如此大费周章的最终任然是同普通RNN一样要计算当前隐藏状态。<br>式子：$h_t=o_t \odot tanh(c_t)$<br>当前隐藏状态$h_t$是从$c_t$计算得来的，因为$c_t$是以线性的方式自我更新的，所以先将其加入带有非线性功能的$tanh(c_t)$。 随后再靠输出门$o_t$的过滤来得到当前隐藏状态$h_t$。</p>
<p>普通RNN与LSTM的比较<br>下面为了加深理解循环神经网络的核心，再来和YJango一起比较一下普通RNN和LSTM的区别。</p>
<ul>
<li><p>比较公式：最大的区别是多了三个神经网络(gates)来控制数据的流通。</p>
<ul>
<li>普通RNN：$h_t=tanh(W_{xh}·x_t+W_{hh}·h_{t-1}+b)$</li>
<li>LSTM：$h_t=o_t \odot tanh(f_t \odot c_{t-1} + i_t \odot tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c))$</li>
<li>比较：二者的信息来源都是$h_t=tanh(W_{xh}·x_t+W_{hh}·h_{t-1}+b)$， 不同的是LSTM靠3个gates将信息的积累建立在线性自连接的memory cell之上，并靠其作为中间物来计算当前$h_t$。</li>
</ul>
</li>
<li><p>示图比较：图片来自<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM</a>，强烈建议一并阅读。</p>
<ul>
<li>普通RNN：<br><img src="https://picb.zhimg.com/80/v2-bce0d99e8bc969fa3c2ffa99f93935c5_720w.png" alt></li>
<li><p>加号圆圈表示线性相加，乘号圆圈表示用gate来过滤信息。<br><img src="https://picb.zhimg.com/80/v2-703dffbefaddcf8dcdb39ccf589312e9_720w.jpg" alt></p>
</li>
<li><p>比较：新信息从黄色的tanh处，线性累积到memory cell之中后，又从红色的tanh处加入非线性并返回到了隐藏状态$h_t$的计算中。</p>
<blockquote>
<p>LSTM靠3个gates将信息的积累建立在线性自连接的权重接近1的memory cell之上，并靠其作为中间物来计算当前$h_t$。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>LSTM的类比<br>对于用LSTM来实现RNN的记忆，可以类比我们所用的手机（仅仅是为了方便记忆，并非一一对应）。<br><img src="https://pic4.zhimg.com/80/v2-3cbda7436b42c7b05c8a11868794e2b2_720w.jpg" alt></p>
<p>普通RNN好比是手机屏幕，而LSTM-RNN好比是手机膜。</p>
<p>大量非线性累积历史信息会造成梯度消失(梯度爆炸)好比是不断使用后容易使屏幕刮花。</p>
<p>而LSTM将信息的积累建立在线性自连接的memory cell之上，并靠其作为中间物来计算当前$h_t好比是用手机屏幕膜作为中间物来观察手机屏幕。</p>
<p>输入门、遗忘门、输出门的过滤作用好比是手机屏幕膜的反射率、吸收率、透射率三种性质。</p>
<p>Gated RNNs的变种<br>需要再次明确的是，神经网络之所以被称之为网络是因为它可以非常自由的创建合理的连接。而上面所介绍的LSTM也只是最基本的LSTM。只要遵守几个关键点，读者可以根据需求设计自己的Gated RNNs，而至于在不同任务上的效果需要通过实验去验证。下面就简单介绍YJango所理解的几个Gated RNNs的变种的设计方向。</p>
<ul>
<li>信息流：标准的RNN的信息流有两处：input输入和hidden state隐藏状态。</li>
</ul>
<p>但往往信息流并非只有两处，即便是有两处，也可以拆分成多处，并通过明确多处信息流之间的结构关系来加入先验知识，减少训练所需数据量，从而提高网络效果。</p>
<p>例如：<a href="https://arxiv.org/pdf/1503.00075.pdf" target="_blank" rel="noopener">Tree-LSTM</a>在具有此种结构的自然语言处理任务中的应用。<br><img src="https://picb.zhimg.com/80/v2-b6cc00b99f001d39d97be0932bd52575_720w.jpg" alt></p>
<ul>
<li><p>gates的控制方式：与LSTM一样有名的是Gated Recurrent Unit (GRU)，而GRU使用gate的方式就与LSTM的不同，GRU只用了两个gates，将LSTM中的输入门和遗忘门合并成了更新门。并且并不把线性自更新建立在额外的memory cell上，而是直接线性累积建立在隐藏状态上，并靠gates来调控。<br><img src="https://pic3.zhimg.com/80/v2-eb21a394a6bc08394a26b60707471c98_720w.png" alt></p>
</li>
<li><p>gates的控制依据：上文所介绍的LSTM中的三个gates所使用的控制依据都是$Wx_{t}+Wh_{t-1}$，但是可以通过与memory cell的连接来增加控制依据或者删除某个gate的$Wx_t$或$Wh_{t-1}$来缩减控制依据。比如去掉上图中$z_t=sigmoid(W_z·[h_{t-1},x_t])$中的$h_{t-1}$从而变成$z_t=sigmoid(W_z·h_{t-1})$</p>
</li>
</ul>
<h1 id="YJango的循环神经网络——scan实现LSTM"><a href="#YJango的循环神经网络——scan实现LSTM" class="headerlink" title="YJango的循环神经网络——scan实现LSTM"></a><span id="header11">YJango的循环神经网络——scan实现LSTM</span></h1><h2 id="处理训练数据"><a href="#处理训练数据" class="headerlink" title="处理训练数据"></a>处理训练数据</h2><p>目的：减掉每句数据的平均值，除以每句数据的标准差，降低模型拟合难度。<br>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 所需库包</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import time</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 直接使用在代码演示LV3中定义的function</span><br><span class="line">def Standardize(seq):</span><br><span class="line">  #subtract mean</span><br><span class="line">  centerized&#x3D;seq-np.mean(seq, axis &#x3D; 0)</span><br><span class="line">  #divide standard deviation</span><br><span class="line">  normalized&#x3D;centerized&#x2F;np.std(centerized, axis &#x3D; 0)</span><br><span class="line">  return normalized</span><br><span class="line"># 读取输入和输出数据</span><br><span class="line">mfc&#x3D;np.load(&#39;X.npy&#39;)</span><br><span class="line">art&#x3D;np.load(&#39;Y.npy&#39;)</span><br><span class="line">totalsamples&#x3D;len(mfc)</span><br><span class="line"># 20%的数据作为validation set</span><br><span class="line">vali_size&#x3D;0.2</span><br><span class="line"># 将每个样本的输入和输出数据合成list，再将所有的样本合成list</span><br><span class="line"># 其中输入数据的形状是[n_samples, n_steps, D_input]</span><br><span class="line"># 其中输出数据的形状是[n_samples, D_output]</span><br><span class="line">def data_prer(X, Y):</span><br><span class="line">  D_input&#x3D;X[0].shape[1]</span><br><span class="line">  data&#x3D;[]</span><br><span class="line">  for x,y in zip(X,Y):</span><br><span class="line">      data.append([Standardize(x).reshape((1,-1,D_input)).astype(&quot;float32&quot;),</span><br><span class="line">                   Standardize(y).astype(&quot;float32&quot;)])</span><br><span class="line">  return data</span><br><span class="line"># 处理数据</span><br><span class="line">data&#x3D;data_prer(mfc, art)</span><br><span class="line"># 分训练集与验证集</span><br><span class="line">train&#x3D;data[int(totalsamples*vali_size):]</span><br><span class="line">test&#x3D;data[:int(totalsamples*vali_size)]</span><br></pre></td></tr></table></figure></p>
<p>示意图：1，2，3，4，5表示list中的每个元素，而每个元素又是一个长度为2的list。<br><img src="https://picb.zhimg.com/80/v2-2ffadd794aef7201d3a1a2e598a490ce_720w.jpg" alt></p>
<p>解释：比如全部数据有100个序列，如果设定每个input的形状就是[1, n_steps, D_input]，那么处理后的list的长度就是100，这样的数据使用的是SGD的更新方式。而如果想要使用mini-batch GD，将batch size(也就是n_samples)的个数为2，那么处理后的list的长度就会是50，每次网络训练时就会同时计算2个样本的梯度并用均值来更新权重。 因为每句语音数据的时间长短都不相同，如果使用3维tensor，需要大量的zero padding，所以将n_samples设成1。但是这样处理的缺点是：只能使用SGD，无法使用mini-batch GD。如果想使用mini-batch GD，需要几个n_steps长度相同的样本并在一起形成3维tensor（不等长时需要zero padding，如下图）。</p>
<p>演示图：v表示一个维度为39的向量，序列1的n_steps的长度为3，序列2的为7，如果想把这三个序列并成3维tensor，就需要选择最大的长度作为n_steps的长度，将不足该长度的序列补零（都是0的39维的向量）。最后会形成shape为[3,7,39]的一个3维tensor。<br><img src="https://picb.zhimg.com/80/v2-918f052df1c52710ee2d26ffe5af441a_720w.jpg" alt></p>
<h2 id="权重初始化方法"><a href="#权重初始化方法" class="headerlink" title="权重初始化方法"></a>权重初始化方法</h2><p>目的：合理的初始化权重，可以降低网络在学习时卡在鞍点或极小值的损害，增加学习速度和效果<br>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def weight_init(shape):</span><br><span class="line">  initial &#x3D; tf.random_uniform(shape,</span><br><span class="line">  minval&#x3D;-np.sqrt(5)*np.sqrt(1.0&#x2F;shape[0]), </span><br><span class="line">  maxval&#x3D;np.sqrt(5)*np.sqrt(1.0&#x2F;shape[0])</span><br><span class="line">  )</span><br><span class="line">  return tf.Variable(initial,trainable&#x3D;True)</span><br><span class="line"># 全部初始化成0</span><br><span class="line">def zero_init(shape):</span><br><span class="line">  initial &#x3D; tf.Variable(tf.zeros(shape))</span><br><span class="line">  return tf.Variable(initial,trainable&#x3D;True)</span><br><span class="line"># 正交矩阵初始化</span><br><span class="line">def orthogonal_initializer(shape,scale &#x3D; 1.0):</span><br><span class="line">  #https:&#x2F;&#x2F;github.com&#x2F;Lasagne&#x2F;Lasagne&#x2F;blob&#x2F;master&#x2F;lasagne&#x2F;init.py</span><br><span class="line">  scale &#x3D; 1.0</span><br><span class="line">  flat_shape &#x3D; (shape[0], np.prod(shape[1:]))</span><br><span class="line">  a &#x3D; np.random.normal(0.0, 1.0, flat_shape)</span><br><span class="line">  u, _, v &#x3D; np.linalg.svd(a, full_matrices&#x3D;False)</span><br><span class="line">  q &#x3D; u if u.shape &#x3D;&#x3D; flat_shape else v</span><br><span class="line">  q &#x3D; q.reshape(shape) #this needs to be corrected to float32</span><br><span class="line">  return tf.Variable(scale * q[:shape[0], :shape[1]],trainable&#x3D;True, dtype&#x3D;tf.float32)</span><br><span class="line">def bias_init(shape):</span><br><span class="line">  initial &#x3D; tf.constant(0.01, shape&#x3D;shape)</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line"># 洗牌</span><br><span class="line">def shufflelists(data):</span><br><span class="line">  ri&#x3D;np.random.permutation(len(data))</span><br><span class="line">  data&#x3D;[data[i] for i in ri]</span><br><span class="line">  return data</span><br></pre></td></tr></table></figure></p>
<p>解释：其中shufflelists是用于洗牌重新排序list的。正交矩阵初始化是有利于gated_rnn的学习的方法。</p>
<h2 id="定义LSTM类"><a href="#定义LSTM类" class="headerlink" title="定义LSTM类"></a>定义LSTM类</h2><p>属性：使用class类来定义是因为LSTM中有大量的参数，定义成属性方便管理。<br>代码：在init中就将所有需要学习的权重全部定义成属性<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class LSTMcell(object):</span><br><span class="line">  def __init__(self, incoming, D_input, D_cell, initializer, f_bias&#x3D;1.0):</span><br><span class="line"></span><br><span class="line">      # var</span><br><span class="line">      # incoming是用来接收输入数据的，其形状为[n_samples, n_steps, D_input]</span><br><span class="line">      self.incoming &#x3D; incoming</span><br><span class="line">      # 输入的维度</span><br><span class="line">      self.D_input &#x3D; D_input</span><br><span class="line">      # LSTM的hidden state的维度，同时也是memory cell的维度</span><br><span class="line">      self.D_cell &#x3D; D_cell</span><br><span class="line">      # parameters</span><br><span class="line">        # 输入门的 三个参数</span><br><span class="line">        # igate &#x3D; W_xi.* x + W_hi.* h + b_i</span><br><span class="line">      self.W_xi &#x3D; initializer([self.D_input, self.D_cell])</span><br><span class="line">      self.W_hi &#x3D; initializer([self.D_cell, self.D_cell])</span><br><span class="line">      self.b_i  &#x3D; tf.Variable(tf.zeros([self.D_cell])) </span><br><span class="line">        # 遗忘门的 三个参数 </span><br><span class="line">        # fgate &#x3D; W_xf.* x + W_hf.* h + b_f</span><br><span class="line">      self.W_xf &#x3D; initializer([self.D_input, self.D_cell])</span><br><span class="line">      self.W_hf &#x3D; initializer([self.D_cell, self.D_cell])</span><br><span class="line">      self.b_f  &#x3D; tf.Variable(tf.constant(f_bias, shape&#x3D;[self.D_cell])) </span><br><span class="line">        # 输出门的 三个参数</span><br><span class="line">        # ogate &#x3D; W_xo.* x + W_ho.* h + b_o</span><br><span class="line">      self.W_xo &#x3D; initializer([self.D_input, self.D_cell])</span><br><span class="line">      self.W_ho &#x3D; initializer([self.D_cell, self.D_cell])</span><br><span class="line">      self.b_o  &#x3D; tf.Variable(tf.zeros([self.D_cell])) </span><br><span class="line">        # 计算新信息的三个参数</span><br><span class="line">        # cell &#x3D; W_xc.* x + W_hc.* h + b_c</span><br><span class="line">      self.W_xc &#x3D; initializer([self.D_input, self.D_cell])</span><br><span class="line">      self.W_hc &#x3D; initializer([self.D_cell, self.D_cell])</span><br><span class="line">      self.b_c  &#x3D; tf.Variable(tf.zeros([self.D_cell]))  </span><br><span class="line"></span><br><span class="line">      # 最初时的hidden state和memory cell的值，二者的形状都是[n_samples, D_cell]</span><br><span class="line">      # 如果没有特殊指定，这里直接设成全部为0</span><br><span class="line">      init_for_both &#x3D; tf.matmul(self.incoming[:,0,:], tf.zeros([self.D_input, self.D_cell]))</span><br><span class="line">      self.hid_init &#x3D; init_for_both</span><br><span class="line">      self.cell_init &#x3D; init_for_both</span><br><span class="line">      # 所以要将hidden state和memory并在一起。</span><br><span class="line">      self.previous_h_c_tuple &#x3D; tf.stack([self.hid_init, self.cell_init])</span><br><span class="line">      # 需要将数据由[n_samples, n_steps, D_cell]的形状变成[n_steps, n_samples, D_cell]的形状</span><br><span class="line">      self.incoming &#x3D; tf.transpose(self.incoming, perm&#x3D;[1,0,2])</span><br></pre></td></tr></table></figure></p>
<p>解释：将hidden state和memory并在一起，以及将输入的形状变成[n_steps, n_samples, D_cell]是为了满足tensorflow中的scan的特点，后面会提到。<br>每步计算方法：定义一个function，用于制定每一个step的计算。<br>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def one_step(self, previous_h_c_tuple, current_x):</span><br><span class="line"></span><br><span class="line">    # 再将hidden state和memory cell拆分开</span><br><span class="line">    prev_h, prev_c &#x3D; tf.unstack(previous_h_c_tuple)</span><br><span class="line">    # 这时，current_x是当前的输入，</span><br><span class="line">    # prev_h是上一个时刻的hidden state</span><br><span class="line">    # prev_c是上一个时刻的memory cell</span><br><span class="line"></span><br><span class="line">    # 计算输入门</span><br><span class="line">    i &#x3D; tf.sigmoid(</span><br><span class="line">        tf.matmul(current_x, self.W_xi) + </span><br><span class="line">        tf.matmul(prev_h, self.W_hi) + </span><br><span class="line">        self.b_i)</span><br><span class="line">    # 计算遗忘门</span><br><span class="line">    f &#x3D; tf.sigmoid(</span><br><span class="line">        tf.matmul(current_x, self.W_xf) + </span><br><span class="line">        tf.matmul(prev_h, self.W_hf) + </span><br><span class="line">        self.b_f)</span><br><span class="line">    # 计算输出门</span><br><span class="line">    o &#x3D; tf.sigmoid(</span><br><span class="line">        tf.matmul(current_x, self.W_xo) + </span><br><span class="line">        tf.matmul(prev_h, self.W_ho) + </span><br><span class="line">        self.b_o)</span><br><span class="line">    # 计算新的数据来源</span><br><span class="line">    c &#x3D; tf.tanh(</span><br><span class="line">        tf.matmul(current_x, self.W_xc) + </span><br><span class="line">        tf.matmul(prev_h, self.W_hc) + </span><br><span class="line">        self.b_c)</span><br><span class="line">    # 计算当前时刻的memory cell </span><br><span class="line">    current_c &#x3D; f*prev_c + i*c</span><br><span class="line">    # 计算当前时刻的hidden state</span><br><span class="line">    current_h &#x3D; o*tf.tanh(current_c)</span><br><span class="line">    # 再次将当前的hidden state和memory cell并在一起返回</span><br><span class="line">    return tf.stack([current_h, current_c])</span><br></pre></td></tr></table></figure></p>
<p>解释：将上一时刻的hidden state和memory拆开，用于计算后，所出现的新的当前时刻的hidden state和memory会再次并在一起作为该function的返回值，同样是为了满足scan的特点。定义该function后，LSTM就已经完成了。one_step方法会使用LSTM类中所定义的parameters与当前时刻的输入和上一时刻的hidden state与memory cell计算当前时刻的hidden state和memory cell。<br>scan：使用scan逐次迭代计算所有timesteps，最后得出所有的hidden states进行后续的处理。<br>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def all_steps(self):</span><br><span class="line">    # 输出形状 : [n_steps, n_sample, D_cell]</span><br><span class="line">    hstates &#x3D; tf.scan(fn &#x3D; self.one_step,</span><br><span class="line">                      elems &#x3D; self.incoming, #形状为[n_steps, n_sample, D_input]</span><br><span class="line">                      initializer &#x3D; self.previous_h_c_tuple,</span><br><span class="line">                      name &#x3D; &#39;hstates&#39;)[:,0,:,:] </span><br><span class="line">    return hstates</span><br></pre></td></tr></table></figure></p>
<p>解释：scan接受的fn, elems, initializer有以下要求：<br>fn：第一个输入是上一时刻的输出（需要与fn的返回值保持一致），第二个输入是当前时刻的输入。<br>elems：scan方法每一步都会沿着所要处理的tensor的第一个维进行一次一次取值，所以要将数据由[n_samples, n_steps, D_cell]的形状变成[n_steps, n_samples, D_cell]的形状。<br>initializer：初始值，需要与fn的第一个输入和返回值保持一致。<br>scan的返回值在上例中是[n_steps, 2, n_samples, D_cell]，其中第二个维度的2是由hidden state和memory cell组成的。</p>
<h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><p>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">D_input &#x3D; 39</span><br><span class="line">D_label &#x3D; 24</span><br><span class="line">learning_rate &#x3D; 7e-5</span><br><span class="line">num_units&#x3D;1024</span><br><span class="line"># 样本的输入和标签</span><br><span class="line">inputs &#x3D; tf.placeholder(tf.float32, [None, None, D_input], name&#x3D;&quot;inputs&quot;)</span><br><span class="line">labels &#x3D; tf.placeholder(tf.float32, [None, D_label], name&#x3D;&quot;labels&quot;)</span><br><span class="line"># 实例LSTM类</span><br><span class="line">rnn_cell &#x3D; LSTMcell(inputs, D_input, num_units, orthogonal_initializer)</span><br><span class="line"># 调用scan计算所有hidden states</span><br><span class="line">rnn0 &#x3D; rnn_cell.all_steps()</span><br><span class="line"># 将3维tensor [n_steps, n_samples, D_cell]转成 矩阵[n_steps*n_samples, D_cell]</span><br><span class="line"># 用于计算outputs</span><br><span class="line">rnn &#x3D; tf.reshape(rnn0, [-1, num_units])</span><br><span class="line"># 输出层的学习参数</span><br><span class="line">W &#x3D; weight_init([num_units, D_label])</span><br><span class="line">b &#x3D; bias_init([D_label])</span><br><span class="line">output &#x3D; tf.matmul(rnn, W) + b</span><br><span class="line"># 损失</span><br><span class="line">loss&#x3D;tf.reduce_mean((output-labels)**2)</span><br><span class="line"># 训练所需</span><br><span class="line">train_step &#x3D; tf.train.AdamOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure></p>
<p>解释：以hard coding的方式直接构建一个网络，输入是39维，第一个隐藏层也就是RNN-LSTM，1024维，而输出层又将1024维的LSTM的输出变换到24维与label对应。<br>注： 这个网络并不仅仅取序列的最后一个值，而是要用所有timestep的值与实际轨迹进行比较计算loss</p>
<h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 建立session并实际初始化所有参数</span><br><span class="line">sess &#x3D; tf.InteractiveSession()</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"># 训练并记录</span><br><span class="line">def train_epoch(EPOCH):</span><br><span class="line">  for k in range(EPOCH):</span><br><span class="line">      train0&#x3D;shufflelists(train)</span><br><span class="line">      for i in range(len(train)):</span><br><span class="line">          sess.run(train_step,feed_dict&#x3D;&#123;inputs:train0[i][0],labels:train0[i][1]&#125;)</span><br><span class="line">      tl&#x3D;0</span><br><span class="line">      dl&#x3D;0</span><br><span class="line">      for i in range(len(test)):</span><br><span class="line">          dl+&#x3D;sess.run(loss,feed_dict&#x3D;&#123;inputs:test[i][0],labels:test[i][1]&#125;)</span><br><span class="line">      for i in range(len(train)):</span><br><span class="line">          tl+&#x3D;sess.run(loss,feed_dict&#x3D;&#123;inputs:train[i][0],labels:train[i][1]&#125;)</span><br><span class="line">      print(k,&#39;train:&#39;,round(tl&#x2F;83,3),&#39;test:&#39;,round(dl&#x2F;20,3))</span><br><span class="line">t0 &#x3D; time.time()</span><br><span class="line">train_epoch(10)</span><br><span class="line">t1 &#x3D; time.time()</span><br><span class="line">print(&quot; %f seconds&quot; % round((t1 - t0),2))</span><br><span class="line"># 训练10次后的输出和时间</span><br><span class="line">(0, &#39;train:&#39;, 0.662, &#39;test:&#39;, 0.691)</span><br><span class="line">(1, &#39;train:&#39;, 0.558, &#39;test:&#39;, 0.614)</span><br><span class="line">(2, &#39;train:&#39;, 0.473, &#39;test:&#39;, 0.557)</span><br><span class="line">(3, &#39;train:&#39;, 0.417, &#39;test:&#39;, 0.53)</span><br><span class="line">(4, &#39;train:&#39;, 0.361, &#39;test:&#39;, 0.504)</span><br><span class="line">(5, &#39;train:&#39;, 0.327, &#39;test:&#39;, 0.494)</span><br><span class="line">(6, &#39;train:&#39;, 0.294, &#39;test:&#39;, 0.476)</span><br><span class="line">(7, &#39;train:&#39;, 0.269, &#39;test:&#39;, 0.468)</span><br><span class="line">(8, &#39;train:&#39;, 0.244, &#39;test:&#39;, 0.452)</span><br><span class="line">(9, &#39;train:&#39;, 0.226, &#39;test:&#39;, 0.453)</span><br><span class="line">563.110000 seconds</span><br></pre></td></tr></table></figure></p>
<p>解释：由于上文的LSTM是非常直接的编写方式，并不高效，在实际使用中会花费较长时间。</p>
<h2 id="预测效果"><a href="#预测效果" class="headerlink" title="预测效果"></a>预测效果</h2><p>代码：pY=sess.run(output,feed_dict={inputs:test[10][0]})<br>plt.plot(pY[:,8])<br>plt.plot(test[10][1][:,8])<br>plt.title(‘test’)<br>plt.legend([‘predicted’,’real’])<br>解释：plot出一个样本中的维度的预测效果与真是轨迹进行对比<br>效果图：<br><img src="https://picb.zhimg.com/80/v2-6f015b68033e1158702857617109679d_720w.jpg" alt></p>
<h2 id="总结说明"><a href="#总结说明" class="headerlink" title="总结说明"></a>总结说明</h2><p>该文是尽可能只展示LSTM最核心的部分（只训练了10次，有兴趣的朋友可以自己多训练几次），帮助大家理解其工作方式而已，完整代码可以从我的github中LSTM_lV1中找到。<br>该LSTM由于运行效率并不高，下一篇会稍微进行改动加快运行速度，并整理结构方便使用GRU以及多层RNN的堆叠以及双向RNN，同时加入其他功能。</p>
<h1 id="YJango的循环神经网络——双向LSTM-amp-GRU"><a href="#YJango的循环神经网络——双向LSTM-amp-GRU" class="headerlink" title="YJango的循环神经网络——双向LSTM&amp;GRU"></a><span id="header12">YJango的循环神经网络——双向LSTM&amp;GRU</span></h1><p><img src="https://pic2.zhimg.com/v2-39576faf84513b6f607ee5bbbec2f175_1440w.jpg?source=172ae18b" alt></p>
<p>介绍<br>双向LSTM和GRU用scan的方式实现</p>
<p>任务描述：<br>本次的代码LV2是紧接着代码LV1的升级版，所学习的内容与先前的一样，不同的是：</p>
<p>简单梳理调整了代码结构，方便使用<br>将所有gate的计算并在一个大矩阵乘法下完成提高GPU的利用率<br>除了LSTM（Long-Short Term Memory）以外的cell，还提供了GRU（gate recurrent unit） cell模块<br>双向RNN（可选择任意cell组合）<br>该代码可被用于练习结构改造或实际建模任务</p>
<h2 id="定义LSTMcell类"><a href="#定义LSTMcell类" class="headerlink" title="定义LSTMcell类"></a>定义LSTMcell类</h2><p>目的：LSTMcell包含所有学习所需要的parameters以及每一时刻所要运行的step方法<br>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class LSTMcell(object):</span><br><span class="line">  def __init__(self, incoming, D_input, D_cell, initializer,</span><br><span class="line">               f_bias&#x3D;1.0, L2&#x3D;False, h_act&#x3D;tf.tanh, </span><br><span class="line">               init_h&#x3D;None, init_c&#x3D;None):</span><br><span class="line">      # 属性</span><br><span class="line">      self.incoming &#x3D; incoming # 输入数据</span><br><span class="line">      self.D_input &#x3D; D_input</span><br><span class="line">      self.D_cell &#x3D; D_cell</span><br><span class="line">      self.initializer &#x3D; initializer # 初始化方法</span><br><span class="line">      self.f_bias &#x3D; f_bias # 遗忘门的初始偏移量</span><br><span class="line">      self.h_act &#x3D; h_act # 这里可以选择LSTM的hidden state的激活函数</span><br><span class="line">      self.type &#x3D; &#39;lstm&#39; # 区分gru</span><br><span class="line">      # 如果没有提供最初的hidden state和memory cell，会全部初始为0</span><br><span class="line">      if init_h is None and init_c is None:</span><br><span class="line">          # If init_h and init_c are not provided, initialize them</span><br><span class="line">          # the shape of init_h and init_c is [n_samples, D_cell]</span><br><span class="line">          self.init_h &#x3D; tf.matmul(self.incoming[0,:,:], tf.zeros([self.D_input, self.D_cell]))</span><br><span class="line">          self.init_c &#x3D; self.init_h</span><br><span class="line">          self.previous &#x3D; tf.stack([self.init_h, self.init_c])</span><br><span class="line">      # LSTM所有需要学习的参数</span><br><span class="line">      # 每个都是[W_x, W_h, b_f]的tuple</span><br><span class="line">      self.igate &#x3D; self.Gate()</span><br><span class="line">      self.fgate &#x3D; self.Gate(bias &#x3D; f_bias)</span><br><span class="line">      self.ogate &#x3D; self.Gate()</span><br><span class="line">      self.cell &#x3D; self.Gate()</span><br><span class="line">      # 因为所有的gate都会乘以当前的输入和上一时刻的hidden state</span><br><span class="line">      # 将矩阵concat在一起，计算后再逐一分离，加快运行速度</span><br><span class="line">      # W_x的形状是[D_input, 4*D_cell]</span><br><span class="line">      self.W_x &#x3D; tf.concat(values&#x3D;[self.igate[0], self.fgate[0], self.ogate[0], self.cell[0]], axis&#x3D;1)</span><br><span class="line">      self.W_h &#x3D; tf.concat(values&#x3D;[self.igate[1], self.fgate[1], self.ogate[1], self.cell[1]], axis&#x3D;1)</span><br><span class="line">      self.b &#x3D; tf.concat(values&#x3D;[self.igate[2], self.fgate[2], self.ogate[2], self.cell[2]], axis&#x3D;0)</span><br><span class="line">      # 对LSTM的权重进行L2 regularization</span><br><span class="line">      if L2:</span><br><span class="line">          self.L2_loss &#x3D; tf.nn.l2_loss(self.W_x) + tf.nn.l2_loss(self.W_h)</span><br><span class="line">  # 初始化gate的函数        </span><br><span class="line">  def Gate(self, bias &#x3D; 0.001):</span><br><span class="line">      # Since we will use gate multiple times, let&#39;s code a class for reusing</span><br><span class="line">      Wx &#x3D; self.initializer([self.D_input, self.D_cell])</span><br><span class="line">      Wh &#x3D; self.initializer([self.D_cell, self.D_cell])</span><br><span class="line">      b  &#x3D; tf.Variable(tf.constant(bias, shape&#x3D;[self.D_cell]),trainable&#x3D;True) </span><br><span class="line">      return Wx, Wh, b</span><br><span class="line">  # 大矩阵乘法运算完毕后，方便用于分离各个gate</span><br><span class="line">  def Slice_W(self, x, n):</span><br><span class="line">      # split W&#39;s after computing</span><br><span class="line">      return x[:, n*self.D_cell:(n+1)*self.D_cell]  </span><br><span class="line">  # 每个time step需要运行的步骤</span><br><span class="line">  def Step(self, previous_h_c_tuple, current_x):</span><br><span class="line">      # 分离上一时刻的hidden state和memory cell</span><br><span class="line">      prev_h, prev_c &#x3D; tf.unstack(previous_h_c_tuple)</span><br><span class="line">      # 统一在concat成的大矩阵中一次完成所有的gates计算</span><br><span class="line">      gates &#x3D; tf.matmul(current_x, self.W_x) + tf.matmul(prev_h, self.W_h) + self.b</span><br><span class="line">      # 分离输入门</span><br><span class="line">      i &#x3D; tf.sigmoid(self.Slice_W(gates, 0))</span><br><span class="line">      # 分离遗忘门</span><br><span class="line">      f &#x3D; tf.sigmoid(self.Slice_W(gates, 1))</span><br><span class="line">      # 分离输出门</span><br><span class="line">      o &#x3D; tf.sigmoid(self.Slice_W(gates, 2))</span><br><span class="line">      # 分离新的更新信息</span><br><span class="line">      c &#x3D; tf.tanh(self.Slice_W(gates, 3))</span><br><span class="line">      # 利用gates进行当前memory cell的计算</span><br><span class="line">      current_c &#x3D; f*prev_c + i*c</span><br><span class="line">      # 利用gates进行当前hidden state的计算</span><br><span class="line">      current_h &#x3D; o*self.h_act(current_c)</span><br><span class="line">      return tf.stack([current_h, current_c])</span><br></pre></td></tr></table></figure></p>
<h2 id="定义GRUcell类"><a href="#定义GRUcell类" class="headerlink" title="定义GRUcell类"></a>定义GRUcell类</h2><p>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class GRUcell(object):</span><br><span class="line">  def __init__(self, incoming, D_input, D_cell, initializer, L2&#x3D;False, init_h&#x3D;None):</span><br><span class="line">      # 属性</span><br><span class="line">      self.incoming &#x3D; incoming</span><br><span class="line">      self.D_input &#x3D; D_input</span><br><span class="line">      self.D_cell &#x3D; D_cell</span><br><span class="line">      self.initializer &#x3D; initializer</span><br><span class="line">      self.type &#x3D; &#39;gru&#39;</span><br><span class="line">      # 如果没有提供最初的hidden state，会初始为0</span><br><span class="line">      # 注意GRU中并没有LSTM中的memory cell，其功能是由hidden state完成的</span><br><span class="line">      if init_h is None:</span><br><span class="line">          # If init_h is not provided, initialize it</span><br><span class="line">          # the shape of init_h is [n_samples, D_cell]</span><br><span class="line">          self.init_h &#x3D; tf.matmul(self.incoming[0,:,:], tf.zeros([self.D_input, self.D_cell]))</span><br><span class="line">          self.previous &#x3D; self.init_h</span><br><span class="line">      # 如果没有提供最初的hidden state，会初始为0</span><br><span class="line">      # 注意GRU中并没有LSTM中的memory cell，其功能是由hidden state完成的</span><br><span class="line">      self.rgate &#x3D; self.Gate()</span><br><span class="line">      self.ugate &#x3D; self.Gate()</span><br><span class="line">      self.cell &#x3D; self.Gate()</span><br><span class="line">      # 因为所有的gate都会乘以当前的输入和上一时刻的hidden state</span><br><span class="line">      # 将矩阵concat在一起，计算后再逐一分离，加快运行速度</span><br><span class="line">      # W_x的形状是[D_input, 3*D_cell]</span><br><span class="line">      self.W_x &#x3D; tf.concat(values&#x3D;[self.rgate[0], self.ugate[0], self.cell[0]], axis&#x3D;1)</span><br><span class="line">      self.W_h &#x3D; tf.concat(values&#x3D;[self.rgate[1], self.ugate[1], self.cell[1]], axis&#x3D;1)</span><br><span class="line">      self.b &#x3D; tf.concat(values&#x3D;[self.rgate[2], self.ugate[2], self.cell[2]], axis&#x3D;0)</span><br><span class="line">      # 对LSTM的权重进行L2 regularization</span><br><span class="line">      if L2:</span><br><span class="line">          self.L2_loss &#x3D; tf.nn.l2_loss(self.W_x) + tf.nn.l2_loss(self.W_h)</span><br><span class="line">  # 初始化gate的函数   </span><br><span class="line">  def Gate(self, bias &#x3D; 0.001):</span><br><span class="line">      # Since we will use gate multiple times, let&#39;s code a class for reusing</span><br><span class="line">      Wx &#x3D; self.initializer([self.D_input, self.D_cell])</span><br><span class="line">      Wh &#x3D; self.initializer([self.D_cell, self.D_cell])</span><br><span class="line">      b  &#x3D; tf.Variable(tf.constant(bias, shape&#x3D;[self.D_cell]),trainable&#x3D;True) </span><br><span class="line">      return Wx, Wh, b</span><br><span class="line">  # 大矩阵乘法运算完毕后，方便用于分离各个gate</span><br><span class="line">  def Slice_W(self, x, n):</span><br><span class="line">      # split W&#39;s after computing</span><br><span class="line">      return x[:, n*self.D_cell:(n+1)*self.D_cell]  </span><br><span class="line">  # 每个time step需要运行的步骤</span><br><span class="line">  def Step(self, prev_h, current_x):</span><br><span class="line">      # 分两次，统一在concat成的大矩阵中完成gates所需要的计算</span><br><span class="line">      Wx &#x3D; tf.matmul(current_x, self.W_x) + self.b</span><br><span class="line">      Wh &#x3D; tf.matmul(prev_h, self.W_h)</span><br><span class="line">      # 分离和组合reset gate</span><br><span class="line">      r &#x3D; tf.sigmoid(self.Slice_W(Wx, 0) + self.Slice_W(Wh, 0))</span><br><span class="line">      # 分离和组合update gate</span><br><span class="line">      u &#x3D; tf.sigmoid(self.Slice_W(Wx, 1) + self.Slice_W(Wh, 1))</span><br><span class="line">      # 分离和组合新的更新信息</span><br><span class="line">      # 注意GRU中，在这一步就已经有reset gate的干涉了</span><br><span class="line">      c &#x3D; tf.tanh(self.Slice_W(Wx, 2) + r*self.Slice_W(Wh, 2))</span><br><span class="line">      # 计算当前hidden state，GRU将LSTM中的input gate和output gate的合设成1，</span><br><span class="line">      # 用update gate完成两者的工作</span><br><span class="line">      current_h &#x3D; (1-u)*prev_h + u*c</span><br><span class="line">      return current_h</span><br></pre></td></tr></table></figure></p>
<h2 id="定义RNN函数"><a href="#定义RNN函数" class="headerlink" title="定义RNN函数"></a>定义RNN函数</h2><p>目的：用于接受cell的实例，并用scan计算所有time steps的hidden states<br>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def RNN(cell, cell_b&#x3D;None, merge&#x3D;&#39;sum&#39;):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  该函数接受的数据需要是[n_steps, n_sample, D_output],</span><br><span class="line">  函数的输出也是[n_steps, n_sample, D_output].</span><br><span class="line">  如果输入数据不是[n_steps, n_sample, D_input],</span><br><span class="line">  使用&#39;inputs_T &#x3D; tf.transpose(inputs, perm&#x3D;[1,0,2])&#39;.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # 正向rnn的计算</span><br><span class="line">  hstates &#x3D; tf.scan(fn &#x3D; cell.Step,</span><br><span class="line">                  elems &#x3D; cell.incoming,</span><br><span class="line">                  initializer &#x3D; cell.previous,</span><br><span class="line">                  name &#x3D; &#39;hstates&#39;)</span><br><span class="line">  # lstm的step经过scan计算后会返回4维tensor，</span><br><span class="line">  # 其中[:,0,:,:]表示hidden state，</span><br><span class="line">  # [:,1,:,:]表示memory cell，这里只需要hidden state</span><br><span class="line">  if cell.type &#x3D;&#x3D; &#39;lstm&#39;:</span><br><span class="line">      hstates &#x3D; hstates[:,0,:,:]</span><br><span class="line">  # 如果提供了第二个cell，将进行反向rnn的计算</span><br><span class="line">  if cell_b is not None:</span><br><span class="line">      # 将输入数据变为反向</span><br><span class="line">      incoming_b &#x3D; tf.reverse(cell.incoming, axis&#x3D;[0])</span><br><span class="line">      # scan计算反向rnn</span><br><span class="line">      b_hstates_rev &#x3D; tf.scan(fn &#x3D; cell_b.Step,</span><br><span class="line">                  elems &#x3D; incoming_b,</span><br><span class="line">                  initializer &#x3D; cell_b.previous, # 每个cell自带的初始值</span><br><span class="line">                  name &#x3D; &#39;b_hstates&#39;)</span><br><span class="line">      if cell_b.type &#x3D;&#x3D; &#39;lstm&#39;:</span><br><span class="line">          b_hstates_rev &#x3D; b_hstates_rev[:,0,:,:]</span><br><span class="line">      # 用scan计算好的反向rnn需要再反向回来与正向rnn所计算的数据进行合并</span><br><span class="line">      b_hstates &#x3D; tf.reverse(b_hstates_rev, axis&#x3D;[0])</span><br><span class="line">      # 合并方式可以选择直接相加，也可以选择concat</span><br><span class="line">      if merge &#x3D;&#x3D; &#39;sum&#39;:</span><br><span class="line">          hstates &#x3D; hstates + b_hstates</span><br><span class="line">      else:</span><br><span class="line">          hstates &#x3D; tf.concat(values&#x3D;[hstates, b_hstates], axis&#x3D;2)</span><br><span class="line">  return hstates</span><br></pre></td></tr></table></figure></p>
<p>解释：可以使用两个GRU cell进行双向rnn的就算，也可以混搭网络构建<br>目的：这里演示的是两层relu feedforward layers后，接一层双向GRU-RNN，最后再接两层relu feedforward layers。<br>效果图：<br><img src="https://pic2.zhimg.com/80/v2-338d014ff709c0f35ece69610a9b31fb_720w.jpg" alt><br>代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">D_input &#x3D; 39</span><br><span class="line">D_label &#x3D; 24</span><br><span class="line">learning_rate &#x3D; 7e-5</span><br><span class="line">num_units&#x3D;1024</span><br><span class="line">L2_penalty &#x3D; 1e-4</span><br><span class="line">inputs &#x3D; tf.placeholder(tf.float32, [None, None, D_input], name&#x3D;&quot;inputs&quot;)</span><br><span class="line">labels &#x3D; tf.placeholder(tf.float32, [None, D_label], name&#x3D;&quot;labels&quot;)</span><br><span class="line"># 保持多少节点不被dropout掉</span><br><span class="line">drop_keep_rate &#x3D; tf.placeholder(tf.float32, name&#x3D;&quot;dropout_keep&quot;)</span><br><span class="line"># 用于reshape</span><br><span class="line">n_steps &#x3D; tf.shape(inputs)[1]</span><br><span class="line">n_samples &#x3D; tf.shape(inputs)[0]</span><br><span class="line"># 将输入数据从[n_samples, n_steps, D_input]，reshape成[n_samples*n_steps, D_input]</span><br><span class="line"># 用于feedforward layer的使用</span><br><span class="line">re1 &#x3D; tf.reshape(inputs, [-1, D_input])</span><br><span class="line"># 第一层</span><br><span class="line">Wf0 &#x3D; weight_init([D_input, num_units])</span><br><span class="line">bf0 &#x3D; bias_init([num_units])</span><br><span class="line">h1 &#x3D; tf.nn.relu(tf.matmul(re1, Wf0) + bf0)</span><br><span class="line"># dropout</span><br><span class="line">h1d &#x3D; tf.nn.dropout(h1, drop_keep_rate)</span><br><span class="line"># 第二层</span><br><span class="line">Wf1 &#x3D; weight_init([num_units, num_units])</span><br><span class="line">bf1 &#x3D; bias_init([num_units])</span><br><span class="line">h2 &#x3D; tf.nn.relu(tf.matmul(h1d, Wf1) + bf1)</span><br><span class="line"># dropout</span><br><span class="line">h2d &#x3D; tf.nn.dropout(h2, drop_keep_rate)</span><br><span class="line"># 将输入数据从[n_samples*n_steps, D_input]，reshape成[n_samples, n_steps, D_input]</span><br><span class="line"># 用于双向rnn layer的使用</span><br><span class="line">re2 &#x3D; tf.reshape(h2d, [n_samples,n_steps, num_units])</span><br><span class="line"># 将数据从[n_samples, n_steps, D_input]，转换成[n_steps, n_samples, D_input]</span><br><span class="line">inputs_T &#x3D; tf.transpose(re2, perm&#x3D;[1,0,2])</span><br><span class="line"># 实例rnn的正向cell，这里使用的是GRUcell</span><br><span class="line">rnn_fcell &#x3D; GRUcell(inputs_T, num_units, num_units, orthogonal_initializer)</span><br><span class="line"># 实例rnn的反向cell</span><br><span class="line">rnn_bcell &#x3D; GRUcell(inputs_T, num_units, num_units, orthogonal_initializer)</span><br><span class="line"># 将两个cell送给scan里计算，并使用sum的方式合并两个方向所计算的数据</span><br><span class="line">rnn0 &#x3D; RNN(rnn_fcell, rnn_bcell)</span><br><span class="line"># 将输入数据从[n_samples, n_steps, D_input]，reshape成[n_samples*n_steps, D_input]</span><br><span class="line"># 用于feedforward layer的使用</span><br><span class="line">rnn1 &#x3D; tf.reshape(rnn0, [-1, num_units])</span><br><span class="line"># dropout</span><br><span class="line">rnn2 &#x3D; tf.nn.dropout(rnn1, drop_keep_rate)</span><br><span class="line"># 第三层</span><br><span class="line">W0 &#x3D; weight_init([num_units, num_units])</span><br><span class="line">b0 &#x3D; bias_init([num_units])</span><br><span class="line">rnn3 &#x3D; tf.nn.relu(tf.matmul(rnn2, W0) + b0)</span><br><span class="line">rnn4 &#x3D; tf.nn.dropout(rnn3, drop_keep_rate)</span><br><span class="line"># 第四层</span><br><span class="line">W1 &#x3D; weight_init([num_units, num_units])</span><br><span class="line">b1 &#x3D; bias_init([num_units])</span><br><span class="line">rnn5 &#x3D; tf.nn.relu(tf.matmul(rnn4, W1) + b1)</span><br><span class="line">rnn6 &#x3D; tf.nn.dropout(rnn5, drop_keep_rate)</span><br><span class="line"># 输出层</span><br><span class="line">W &#x3D; weight_init([num_units, D_label])</span><br><span class="line">b &#x3D; bias_init([D_label])</span><br><span class="line">output &#x3D; tf.matmul(rnn6, W) + b</span><br><span class="line"># loss</span><br><span class="line">loss&#x3D;tf.reduce_mean((output-labels)**2)</span><br><span class="line">L2_total &#x3D; tf.nn.l2_loss(Wf0) + tf.nn.l2_loss(Wf1)+ tf.nn.l2_loss(W0) + tf.nn.l2_loss(W1) + tf.nn.l2_loss(W)#+ rnn_fcell.L2_loss + rnn_bcell.L2_loss </span><br><span class="line"># 训练所需的</span><br><span class="line">train_step &#x3D; tf.train.AdamOptimizer(learning_rate).minimize(loss + L2_penalty*L2_total)</span><br></pre></td></tr></table></figure></p>
<p>训练<br>剩下的代码就和代码LV1的相同了， 大家可以结合tensorboard来记录和分析所学习的权重矩阵和loss的下降等。使用方式请参考代码演示LV3，也可以和代码演示LV3中的代码结合着使用，根据自己的需要注意reshape和transpose即可。完整代码在我的<a href="https://github.com/YJango/tensorflow_basic_tutorial/" target="_blank" rel="noopener">github</a>上。</p>
<p>效果<br>loss：训练集的loss在0.022，验证集的loss在0.222，比feedforward要好很多<br>效果图：另外预测的轨迹也十分的平滑</p>
<p><img src="https://pic2.zhimg.com/80/v2-8eeb0cba9259ea5d9083bb41dd0651bd_720w.jpg" alt><br><img src="https://pic2.zhimg.com/80/v2-feb75672c765888a371f7decf8ac2f11_720w.jpg" alt><br>其他<br>速度：将所有gates的参数并在一起处理再分离可以节省很多时间，代价自然是更多的memory<br>rnn的dropout：每个gate其实也是一个再cell内部的具有物理意义的神经网络，那么同样也是可以利用dropout来防止gate在拟合物理意义时过拟合。<br>多层rnn：可以rnn层之后以相同的方式再来一层双向或单向rnn，比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 第一层双向rnn</span><br><span class="line">rnn_fcell &#x3D; GRUcell(inputs_T, num_units, num_units, orthogonal_initializer)</span><br><span class="line">rnn_bcell &#x3D; GRUcell(inputs_T, num_units, num_units, orthogonal_initializer)</span><br><span class="line">rnn0 &#x3D; RNN(rnn_fcell, rnn_bcell)</span><br><span class="line"># 第二层双向rnn</span><br><span class="line">rnn_fcell2 &#x3D; GRUcell(inputs_T, num_units, num_units, orthogonal_initializer)</span><br><span class="line">rnn_bcell2 &#x3D; GRUcell(inputs_T, num_units, num_units, orthogonal_initializer)</span><br><span class="line">rnn02 &#x3D; RNN(rnn_fcell2, rnn_bcell2)</span><br><span class="line"># 后续处理</span><br><span class="line">rnn1 &#x3D; tf.reshape(rnn02, [-1, num_units])</span><br></pre></td></tr></table></figure></p>
<h1 id="YJango的卷积神经网络——介绍"><a href="#YJango的卷积神经网络——介绍" class="headerlink" title="YJango的卷积神经网络——介绍"></a><span id="header13">YJango的卷积神经网络——介绍</span></h1><p>如果要提出一个新的神经网络结构，首先就需要引入像循环神经网络中“时间共享”这样的先验知识，降低学习所需要的训练数据需求量。 而卷积神经网络同样也引入了这样的先验知识：“空间共享”。下面就让我们以画面识别作为切入点，看看该先验知识是如何被引入到神经网络中的。</p>
<p>视觉感知<br>一、画面识别是什么任务？</p>
<p>学习知识的第一步就是明确任务，清楚该知识的输入输出。卷积神经网络最初是服务于画面识别的，所以我们先来看看画面识别的实质是什么。</p>
<p>先观看几组动物与人类视觉的差异对比图。</p>
<ol>
<li>苍蝇的视觉和人的视觉的差异<br><img src="https://pic1.zhimg.com/80/v2-55069445ed54ce163b76c611ba26b639_720w.jpg" alt><br><img src="https://pic3.zhimg.com/80/v2-4a0ea7ba42166b62bc4f42e8b150815d_720w.png" alt></li>
<li>蛇的视觉和人的视觉的差异<br><img src="https://pic2.zhimg.com/80/v2-a4d35c245931f264ed9a0716fdf20685_720w.jpg" alt><br><img src="https://pic3.zhimg.com/80/v2-3da84b5b80ba7a0d779284566f80be93_720w.png" alt><br>（更多对比图请参考<a href="http%3A//chuansong.me/n/2656056">链接</a>）</li>
</ol>
<p>通过上面的两组对比图可以知道，即便是相同的图片经过不同的视觉系统，也会得到不同的感知。</p>
<p>这里引出一条知识：生物所看到的景象并非世界的原貌，而是长期进化出来的适合自己生存环境的一种感知方式。 蛇的猎物一般是夜间行动，所以它就进化出了一种可以在夜间也能很好观察的感知系统，感热。</p>
<p>任何视觉系统都是将图像反光与脑中所看到的概念进行关联。<br><img src="https://pic4.zhimg.com/80/v2-2c82abd20c4e7c40f7f13f035b924b0b_720w.png" alt></p>
<p>所以画面识别实际上并非识别这个东西客观上是什么，而是寻找人类的视觉关联方式，并再次应用。 如果我们不是人类，而是蛇类，那么画面识别所寻找的?就和现在的不一样。</p>
<p>画面识别实际上是寻找（学习）人类的视觉关联方式?，并再次应用。<br>二、图片被识别成什么取决于哪些因素？</p>
<p>下面用两张图片来体会识别结果取决于哪些因素。</p>
<ol>
<li>老妇与少女<br><img src="https://pic1.zhimg.com/80/v2-c902a9e33b0322051a5f9165e9439247_720w.jpg" alt><br>请观察上面这张图片，你看到的是老妇还是少女？ 以不同的方式去观察这张图片会得出不同的答案。 图片可以观察成有大鼻子、大眼睛的老妇。也可以被观察成少女，但这时老妇的嘴会被识别成少女脖子上的项链，而老妇的眼睛则被识别为少女的耳朵。</li>
<li>海豚与男女<br><img src="https://pic3.zhimg.com/80/v2-7e5bc60a9e9bd0b597e4b650fecc439e_720w.jpg" alt></li>
</ol>
<p>上面这张图片如果是成人观察，多半看到的会是一对亲热的男女。倘若儿童看到这张图片，看到的则会是一群海豚（男女的轮廓是由海豚构造出的）。所以，识别结果受年龄，文化等因素的影响，换句话说：</p>
<blockquote>
<p>图片被识别成什么不仅仅取决于图片本身，还取决于图片是如何被观察的。</p>
</blockquote>
<p>图像表达<br>我们知道了“画面识别是从大量的(x,y)数据中寻找人类的视觉关联方式?，并再次应用。 其x-是输入，表示所看到的东西y-输出，表示该东西是什么。</p>
<p>在自然界中，x是物体的反光，那么在计算机中，图像又是如何被表达和存储的呢？<br><img src="https://pic2.zhimg.com/v2-d2859e5c486ed704492ab80079e99535_b.jpg" alt></p>
<p>图像在计算机中是一堆按顺序排列的数字，数值为0到255。0表示最暗，255表示最亮。 你可以把这堆数字用一个长长的向量来表示，也就是<a href="https%3A//www.tensorflow.org/get_started/mnist/beginners">tensorflow的mnist教程</a>中784维向量的表示方式。 然而这样会失去平面结构的信息，为保留该结构信息，通常选择矩阵的表示方式：28x28的矩阵。</p>
<p>上图是只有黑白颜色的灰度图，而更普遍的图片表达方式是RGB颜色模型，即红（Red）、绿（Green）、蓝（Blue）三原色的色光以不同的比例相加，以产生多种多样的色光。</p>
<p>这样，RGB颜色模型中，单个矩阵就扩展成了有序排列的三个矩阵，也可以用三维张量去理解，其中的每一个矩阵又叫这个图片的一个channel。</p>
<p>在电脑中，一张图片是数字构成的“长方体”。可用 宽width, 高height, 深depth 来描述，如图。<br><img src="https://pic2.zhimg.com/80/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_720w.png" alt></p>
<blockquote>
<p>画面识别的输入x是shape为(width, height, depth)的三维张量。</p>
</blockquote>
<p>接下来要考虑的就是该如何处理这样的“数字长方体”。</p>
<p>画面不变性<br>在决定如何处理“数字长方体”之前，需要清楚所建立的网络拥有什么样的特点。 我们知道一个物体不管在画面左侧还是右侧，都会被识别为同一物体，这一特点就是不变性（invariance），如下图所示。<br><img src="https://pic4.zhimg.com/80/v2-b9aed3dd68b9818561faa7d8ed24ea5a_720w.png" alt></p>
<p>我们希望所建立的网络可以尽可能的满足这些不变性特点。</p>
<p>为了理解卷积神经网络对这些不变性特点的贡献，我们将用不具备这些不变性特点的前馈神经网络来进行比较。</p>
<p>图片识别—前馈神经网络</p>
<p>方便起见，我们用depth只有1的灰度图来举例。 想要完成的任务是：在宽长为4x4的图片中识别是否有下图所示的“横折”。 图中，黄色圆点表示值为0的像素，深色圆点表示值为1的像素。 我们知道不管这个横折在图片中的什么位置，都会被认为是相同的横折。<br><img src="https://pic4.zhimg.com/80/v2-18c11c6f485e9f1bbc9a50eb3d248439_720w.png" alt><br>若训练前馈神经网络来完成该任务，那么表达图像的三维张量将会被摊平成一个向量，作为网络的输入，即(width, height, depth)为(4, 4, 1)的图片会被展成维度为16的向量作为网络的输入层。再经过几层不同节点个数的隐藏层，最终输出两个节点，分别表示“有横折的概率”和“没有横折的概率”，如下图所示。<br><img src="https://pic3.zhimg.com/80/v2-2b411af47b1cad7b727bb676c847ce59_720w.png" alt><br>下面我们用数字（16进制）对图片中的每一个像素点（pixel）进行编号。 当使用右侧那种物体位于中间的训练数据来训练网络时，网络就只会对编号为5,6,9,a的节点的权重进行调节。 若让该网络识别位于右下角的“横折”时，则无法识别。<br><img src="https://picb.zhimg.com/80/v2-ce9919e4930c1f29241afec0538b2605_720w.png" alt><br>解决办法是用大量物体位于不同位置的数据训练，同时增加网络的隐藏层个数从而扩大网络学习这些变体的能力。</p>
<p>然而这样做十分不效率，因为我们知道在左侧的“横折”也好，还是在右侧的“横折”也罢，大家都是“横折”。 为什么相同的东西在位置变了之后要重新学习？有没有什么方法可以将中间所学到的规律也运用在其他的位置？ 换句话说，也就是让不同位置用相同的权重。</p>
<p>图片识别—卷积神经网络<br>卷积神经网络就是让权重在不同位置共享的神经网络。</p>
<p>局部连接<br>在卷积神经网络中，我们先选择一个局部区域，用这个局部区域去扫描整张图片。 局部区域所圈起来的所有节点会被连接到下一层的一个节点上。</p>
<p>为了更好的和前馈神经网络做比较，我将这些以矩阵排列的节点展成了向量。 下图展示了被红色方框所圈中编号为0,1,4,5的节点是如何通过w1,w2,w3,w4连接到下一层的节点0上的。<br><img src="https://pic1.zhimg.com/80/v2-e877b9099b1139c1a34b0bf66bf92aa4_720w.png" alt><br>这个带有连接强弱的红色方框就叫做 filter 或 kernel 或 feature detector。 而filter的范围叫做filter size，这里所展示的是2x2的filter size。<br>$\left[ \begin{array}{cc} w_1&amp;w_2\\w_3&amp;w_4 \end{array} \right]$<br>第二层的节点0的数值就是局部区域的线性组合，即被圈中节点的数值乘以对应的权重后相加。 用x表示输入值，y表示输出值，用图中标注数字表示角标，则下面列出了两种计算编号为0的输出值$y_0$的表达式。</p>
<p>注：在局部区域的线性组合后，也会和前馈神经网络一样，加上一个偏移量$b_0$。<br><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D+y_0+%26%3D+x_0%2Aw_1+%2B+x_1%2Aw_2%2B+x_4%2Aw_3%2B+x_5%2Aw_4%2Bb_0%5C%5Cy_0+%26%3D+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+w_1%26w_2%26+w_3%26w_4+%5Cend%7Bmatrix%7D+%5Cright%5D+%5Ccdot+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_0%5C%5C+x_1%5C%5C+x_4%5C%5C+x_5%5C%5C+%5Cend%7Bmatrix%7D+%5Cright%5D%2Bb_0+%5Cend%7Bsplit%7D" alt></p>
<p>空间共享<br>当filter扫到其他位置计算输出节点[公式]时，[公式]，包括[公式]是共用的。</p>
<p>下面这张动态图展示了当filter扫过不同区域时，节点的链接方式。 动态图的最后一帧则显示了所有连接。 可以注意到，每个输出节点并非像前馈神经网络中那样与全部的输入节点连接，而是部分连接。 这也就是为什么大家也叫前馈神经网络（feedforward neural network）为fully-connected neural network。 图中显示的是一步一步的移动filter来扫描全图，一次移动多少叫做stride。<br>![](<a href="https://pic1.zhimg.com/v2-4fd0400ccebc8adb2dffe24aac163e70_b.jpg" target="_blank" rel="noopener">https://pic1.zhimg.com/v2-4fd0400ccebc8adb2dffe24aac163e70_b.jpg</a></p>
<blockquote>
<p>空间共享也就是卷积神经网络所引入的先验知识。</p>
</blockquote>
<p>输出表达<br>如先前在图像表达中提到的，图片不用向量去表示是为了保留图片平面结构的信息。 同样的，卷积后的输出若用上图的排列方式则丢失了平面结构信息。 所以我们依然用矩阵的方式排列它们，就得到了下图所展示的连接。<br><img src="https://pic4.zhimg.com/80/v2-e1691956fd1beb5d7a637924a1a73d91_720w.png" alt><br>这也就是你们在网上所看到的下面这张图。在看这张图的时候请结合上图的连接一起理解，即输入（绿色）的每九个节点连接到输出（粉红色）的一个节点上的。<br><img src="https://pic3.zhimg.com/v2-7fce29335f9b43bce1b373daa40cccba_b.jpg" alt><br>经过一个feature detector计算后得到的粉红色区域也叫做一个“Convolved Feature” 或 “Activation Map” 或 “Feature Map”。<br>Depth维的处理<br>现在我们已经知道了depth维度只有1的灰度图是如何处理的。 但前文提过，图片的普遍表达方式是下图这样有3个channels的RGB颜色模型。 当depth为复数的时候，每个feature detector是如何卷积的？<br><img src="https://pic2.zhimg.com/80/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_720w.png" alt></p>
<p>现象：2x2所表达的filter size中，一个2表示width维上的局部连接数，另一个2表示height维上的局部连接数，并却没有depth维上的局部连接数，是因为depth维上并非局部，而是全部连接的。</p>
<p>在2D卷积中，filter在张量的width维, height维上是局部连接，在depth维上是贯串全部channels的。</p>
<p>类比：想象在切蛋糕的时候，不管这个蛋糕有多少层，通常大家都会一刀切到底，但是在长和宽这两个维上是局部切割。</p>
<p>下面这张图展示了，在depth为复数时，filter是如何连接输入节点到输出节点的。 图中红、绿、蓝颜色的节点表示3个channels。 黄色节点表示一个feature detector卷积后得到的Feature Map。 其中被透明黑框圈中的12个节点会被连接到黄黑色的节点上。</p>
<p>在输入depth为1时：被filter size为2x2所圈中的4个输入节点连接到1个输出节点上。<br>在输入depth为3时：被filter size为2x2，但是贯串3个channels后，所圈中的12个输入节点连接到1个输出节点上。<br>在输入depth为n时：2x2xn个输入节点连接到1个输出节点上。<br><img src="https://pic2.zhimg.com/80/v2-23db15ec3f783bbb5cf811711e46dbba_720w.png" alt><br>(可从<a href="https%3A//www.vectary.com/u/yjango/cnn">vectary</a>在3D编辑下查看)</p>
<p>注意：三个channels的权重并不共享。 即当深度变为3后，权重也跟着扩增到了三组，如式子(3)所示，不同channels用的是自己的权重。 式子中增加的角标r,g,b分别表示red channel, green channel, blue channel的权重。<br><img src="https://www.zhihu.com/equation?tex=%5Cleft%5B+%5Cbegin%7Bmatrix%7D+w_%7Br1%7D%26w_%7Br2%7D%5C%5C+w_%7Br3%7D%26w_%7Br4%7D%5C%5C+%5Cend%7Bmatrix%7D+%5Cright%5D%2C+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+w_%7Bg1%7D%26w_%7Bg2%7D%5C%5C+w_%7Bg3%7D%26w_%7Bg4%7D%5C%5C+%5Cend%7Bmatrix%7D+%5Cright%5D%2C+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+w_%7Bb1%7D%26w_%7Bb2%7D%5C%5C+w_%7Bb3%7D%26w_%7Bb4%7D%5C%5C+%5Cend%7Bmatrix%7D+%5Cright%5D" alt></p>
<p>计算例子：用$x_{r0}$表示red channel的编号为0的输入节点，$x_{g5}$表示green channel编号为5个输入节点。$x_{b1}$表示blue channel。如式子(4)所表达，这时的一个输出节点实际上是12个输入节点的线性组合。<br><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D+y_0+%26%3D+x_%7Br0%7D%2Aw_%7Br1%7D+%2B+x_%7Br1%7D%2Aw_%7Br2%7D%2B+x_%7Br4%7D%2Aw_%7Br3%7D%2B+x_%7Br5%7D%2Aw_%7Br4%7D%2B+x_%7Bg0%7D%2Aw_%7Bg1%7D+%2B+x_%7Bg1%7D%2Aw_%7Bg2%7D%2B+x_%7Bg4%7D%2Aw_%7Bg3%7D%2B+x_%7Bg5%7D%2Aw_%7Bg4%7D%2B+x_%7Bb0%7D%2Aw_%7Bb1%7D+%2B+x_%7Bb1%7D%2Aw_%7Bb2%7D%2B+x_%7Bb4%7D%2Aw_%7Bb3%7D%2B+x_%7Bb5%7D%2Aw_%7Bb4%7D%2Bb_0%5C%5Cy_0+%26%3D+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+w_%7Br1%7D%26w_%7Br2%7D%26+w_%7Br3%7D%26w_%7Br4%7D+%5Cend%7Bmatrix%7D+%5Cright%5D+%5Ccdot+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_%7Br0%7D%5C%5C+x_%7Br1%7D%5C%5C+x_%7Br4%7D%5C%5C+x_%7Br5%7D%5C%5C+%5Cend%7Bmatrix%7D+%5Cright%5D+%2B%5Cleft%5B+%5Cbegin%7Bmatrix%7D+w_%7Bg1%7D%26w_%7Bg2%7D%26+w_%7Bg3%7D%26w_%7Bg4%7D+%5Cend%7Bmatrix%7D+%5Cright%5D+%5Ccdot+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_%7Bg0%7D%5C%5C+x_%7Bg1%7D%5C%5C+x_%7Bg4%7D%5C%5C+x_%7Bg5%7D%5C%5C+%5Cend%7Bmatrix%7D+%5Cright%5D%2B%5Cleft%5B+%5Cbegin%7Bmatrix%7D+w_%7Bb1%7D%26w_%7Bb2%7D%26+w_%7Bb3%7D%26w_%7Bb4%7D+%5Cend%7Bmatrix%7D+%5Cright%5D+%5Ccdot+%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_%7Bb0%7D%5C%5C+x_%7Bb1%7D%5C%5C+x_%7Bb4%7D%5C%5C+x_%7Bb5%7D%5C%5C+%5Cend%7Bmatrix%7D+%5Cright%5D%2Bb_0%5Cend%7Bsplit%7D" alt><br>当filter扫到其他位置计算输出节点[公式]时，那12个权重在不同位置是共用的，如下面的动态图所展示。 透明黑框圈中的12个节点会连接到被白色边框选中的黄色节点上。<br><img src="https://pic4.zhimg.com/v2-0bc83b72ef50099b70a10cc3ab528f62_b.jpg" alt></p>
<blockquote>
<p>每个filter会在width维, height维上，以局部连接和空间共享，并贯串整个depth维的方式得到一个Feature Map。</p>
</blockquote>
<p>Zero padding<br>细心的读者应该早就注意到了，4x4的图片被2x2的filter卷积后变成了3x3的图片，每次卷积后都会小一圈的话，经过若干层后岂不是变的越来越小？ Zero padding就可以在这时帮助控制Feature Map的输出尺寸，同时避免了边缘信息被一步步舍弃的问题。</p>
<p>例如：下面4x4的图片在边缘Zero padding一圈后，再用3x3的filter卷积后，得到的Feature Map尺寸依然是4x4不变。<br><img src="https://picb.zhimg.com/80/v2-c1010eb5dcf032ea95eab495a45f9b31_720w.png" alt><br>通常大家都想要在卷积时保持图片的原始尺寸。 选择3x3的filter和1的zero padding，或5x5的filter和2的zero padding可以保持图片的原始尺寸。 这也是为什么大家多选择3x3和5x5的filter的原因。 另一个原因是3x3的filter考虑到了像素与其距离为1以内的所有其他像素的关系，而5x5则是考虑像素与其距离为2以内的所有其他像素的关系。</p>
<p>尺寸：Feature Map的尺寸等于(input_size + 2 * padding_size − filter_size)/stride+1。</p>
<p>注意：上面的式子是计算width或height一维的。padding_size也表示的是单边补零的个数。例如(4+2-3)/1+1 = 4，保持原尺寸。</p>
<p>不用去背这个式子。其中(input_size + 2 * padding_size)是经过Zero padding扩充后真正要卷积的尺寸。 减去 filter_size后表示可以滑动的范围。 再除以可以一次滑动（stride）多少后得到滑动了多少次，也就意味着得到了多少个输出节点。 再加上第一个不需要滑动也存在的输出节点后就是最后的尺寸。</p>
<p>形状、概念抓取<br>知道了每个filter在做什么之后，我们再来思考这样的一个filter会抓取到什么样的信息。</p>
<p>我们知道不同的形状都可由细小的“零件”组合而成的。比如下图中，用2x2的范围所形成的16种形状可以组合成格式各样的“更大”形状。</p>
<p>卷积的每个filter可以探测特定的形状。又由于Feature Map保持了抓取后的空间结构。若将探测到细小图形的Feature Map作为新的输入再次卷积后，则可以由此探测到“更大”的形状概念。 比如下图的第一个“大”形状可由2,3,4,5基础形状拼成。第二个可由2,4,5,6组成。第三个可由6,1组成。<br><img src="https://pic4.zhimg.com/80/v2-f53f6ac43abd2555cfbbba6ea7fdc0e4_720w.png" alt><br>除了基础形状之外，颜色、对比度等概念对画面的识别结果也有影响。卷积层也会根据需要去探测特定的概念。</p>
<p>可以从下面这张图中感受到不同数值的filters所卷积过后的Feature Map可以探测边缘，棱角，模糊，突出等概念。<br><img src="https://pic4.zhimg.com/80/v2-644d108587a6ce7fa471ede5d2e11e98_720w.png" alt></p>
<p>如我们先前所提，图片被识别成什么不仅仅取决于图片本身，还取决于图片是如何被观察的。</p>
<p>而filter内的权重矩阵W是网络根据数据学习得到的，也就是说，我们让神经网络自己学习以什么样的方式去观察图片。</p>
<p>拿老妇与少女的那幅图片举例，当标签是少女时，卷积网络就会学习抓取可以成少女的形状、概念。 当标签是老妇时，卷积网络就会学习抓取可以成老妇的形状、概念。</p>
<p>下图展现了在人脸识别中经过层层的卷积后，所能够探测的形状、概念也变得越来越抽象和复杂。</p>
<p><img src="https://pic4.zhimg.com/80/v2-c78b8d059715bb5f42c93716a98d5a69_720w.jpg" alt></p>
<blockquote>
<p>卷积神经网络会尽可能寻找最能解释训练数据的抓取方式。</p>
</blockquote>
<p>多filters<br>每个filter可以抓取探测特定的形状的存在。 假如我们要探测下图的长方框形状时，可以用4个filters去探测4个基础“零件”。<br><img src="https://pic4.zhimg.com/80/v2-6df64fccc9a8e2f696626f85233acb3c_720w.png" alt><br><img src="https://pic3.zhimg.com/80/v2-65461a21a909eca2e190c54db59a2c8f_720w.png" alt></p>
<p>因此我们自然而然的会选择用多个不同的filters对同一个图片进行多次抓取。 如下图（动态图过大，如果显示不出，请看到该<a href="https://link.zhihu.com/?target=https%3A//ujwlkarn.files.wordpress.com/2016/08/giphy.gif">链接</a>观看），同一个图片，经过两个（红色、绿色）不同的filters扫描过后可得到不同特点的Feature Maps。 每增加一个filter，就意味着你想让网络多抓取一个特征。<br><img src="https://pic4.zhimg.com/v2-c7f1ea1d42820b4de30bd548c3986ecd_b.jpg" alt></p>
<p>这样卷积层的输出也不再是depth为1的一个平面，而是和输入一样是depth为复数的长方体。</p>
<p>如下图所示，当我们增加一个filter（紫色表示）后，就又可以得到一个Feature Map。 将不同filters所卷积得到的Feature Maps按顺序堆叠后，就得到了一个卷积层的最终输出。<br><img src="https://pic3.zhimg.com/80/v2-d11e1d2f2c41b6df713573f8155bc324_720w.png" alt></p>
<blockquote>
<p>卷积层的输入是长方体，输出也是长方体。</p>
</blockquote>
<p>这样卷积后输出的长方体可以作为新的输入送入另一个卷积层中处理。</p>
<p>加入非线性<br>和前馈神经网络一样，经过线性组合和偏移后，会加入非线性增强模型的拟合能力。</p>
<p>将卷积所得的Feature Map经过ReLU变换（elementwise）后所得到的output就如下图所展示。<br><img src="https://pic4.zhimg.com/80/v2-54a469b2873542e75abf2bc5d8fcaa1a_720w.png" alt></p>
<p>输出长方体<br>现在我们知道了一个卷积层的输出也是一个长方体。 那么这个输出长方体的(width, height, depth)由哪些因素决定和控制。</p>
<p>这里直接用CS231n的Summary：<br><img src="https://pic3.zhimg.com/80/v2-a9983c3cee935b68c73965bc1abe268c_720w.png" alt></p>
<p>计算例子：请体会<a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/">CS231n</a>的Convolution Demo部分的演示。</p>
<p>矩阵乘法执行卷积<br>如果按常规以扫描的方式一步步计算局部节点和filter的权重的点乘，则不能高效的利用GPU的并行能力。 所以更普遍的方法是用两个大矩阵的乘法来一次性囊括所有计算。</p>
<p>因为卷积层的每个输出节点都是由若干个输入节点的线性组合所计算。 因为输出的节点个数是$W_2\times H_2 \times D_2$，所以就有$W_2 \times H_2 \times D_2$个线性组合。</p>
<p>读过我写的线性代数教程的读者请回忆，矩阵乘矩阵的意义可以理解为批量的线性组合按顺序排列。 其中一个矩阵所表示的信息是多组权重，另一个矩阵所表示的信息是需要进行组合的向量。 大家习惯性的把组成成分放在矩阵乘法的右边，而把权重放在矩阵乘法的左边。 所以这个大型矩阵乘法可以用$W_{row}·X_{col}$表示，其中$W_{row}$和$X_{col}$都是矩阵。<br><img src="https://pic3.zhimg.com/80/v2-11a4d56793af815eb2b4585d64aec178_720w.png" alt></p>
<p>卷积的每个输出是由局部的输入节点和对应的filter权重展成向量后所计算的，如式子(2)。 那么$W_{row}$中的每一行则是每个filter的权重，有$F·F·D_1$个； 而$X_{col}$的每一列是所有需要进行组合的节点（上面的动态图中被黑色透明框圈中的节点），也有$F·F·D_1$个。 $X_{col}$的列的个数则表示每个filter要滑动多少次才可以把整个图片扫描完，有$W_2·H_2$次。 因为我们有多个filters，$W_{row}$的行的个数则是filter的个数K。<br>最后我们得到：<br>$W_{row} \in R^{K\times·F·D_1}$<br>$X_{col} \in R^{K·F·D_1 \times W_2·H_2}$<br>$W_{row}·X_{col} \in R^{K\times·H_2}$</p>
<p>当然矩阵乘法后需要将$W_{row} \times X_{col}$整理成形状为$W_2 \times H_2 \times D_2$的三维张量以供后续处理（如再送入另一个卷积层）。 $X_{col}$则也需要逐步的局部滑动图片，最后堆叠构成用于计算矩阵乘法的形式。</p>
<p>Max pooling<br>在卷积后还会有一个pooling的操作，尽管有其他的比如average pooling等，这里只提max pooling。</p>
<p>max pooling的操作如下图所示：整个图片被不重叠的分割成若干个同样大小的小块（pooling size）。每个小块内只取最大的数字，再舍弃其他节点后，保持原有的平面结构得出output。<br><img src="https://pic4.zhimg.com/80/v2-1a4b2a3795d8f073e921d766e70ce6ec_720w.jpg" alt></p>
<p>max pooling在不同的depth上是分开执行的，且不需要参数控制。 那么问题就max pooling有什么作用？部分信息被舍弃后难道没有影响吗？<br><img src="https://pic1.zhimg.com/80/v2-cd717414dcf32dac4df73c00f1e7c6c3_720w.jpg" alt></p>
<p>Max pooling的主要功能是downsamping，却不会损坏识别结果。 这意味着卷积后的Feature Map中有对于识别物体不必要的冗余信息。 那么我们就反过来思考，这些“冗余”信息是如何产生的。</p>
<p>直觉上，我们为了探测到某个特定形状的存在，用一个filter对整个图片进行逐步扫描。但只有出现了该特定形状的区域所卷积获得的输出才是真正有用的，用该filter卷积其他区域得出的数值就可能对该形状是否存在的判定影响较小。 比如下图中，我们还是考虑探测“横折”这个形状。 卷积后得到3x3的Feature Map中，真正有用的就是数字为3的那个节点，其余数值对于这个任务而言都是无关的。 所以用3x3的Max pooling后，并没有对“横折”的探测产生影响。 试想在这里例子中如果不使用Max pooling，而让网络自己去学习。 网络也会去学习与Max pooling近似效果的权重。因为是近似效果，增加了更多的parameters的代价，却还不如直接进行Max pooling。<br><img src="https://pic3.zhimg.com/80/v2-8e9d7ec0662e903e475bd93a64067554_720w.png" alt></p>
<p>Max pooling还有类似“选择句”的功能。假如有两个节点，其中第一个节点会在某些输入情况下最大，那么网络就只在这个节点上流通信息；而另一些输入又会让第二个节点的值最大，那么网络就转而走这个节点的分支。</p>
<p>但是Max pooling也有不好的地方。因为并非所有的抓取都像上图的例子。有些周边信息对某个概念是否存在的判定也有影响。 并且Max pooling是对所有的Feature Maps进行等价的操作。就好比用相同网孔的渔网打鱼，一定会有漏网之鱼。</p>
<p>全连接层<br>当抓取到足以用来识别图片的特征后，接下来的就是如何进行分类。 全连接层（也叫前馈层）就可以用来将最后的输出映射到<a href="https%3A//yjango.gitbooks.io/superorganism/content/ren_gong_shen_jing_wang_luo.html">线性可分的空间</a>。 通常卷积网络的最后会将末端得到的长方体平摊(flatten)成一个长长的向量，并送入全连接层配合输出层进行分类。</p>
<p>卷积神经网络大致就是covolutional layer, pooling layer, ReLu layer, fully-connected layer的组合，例如下图所示的结构。<br><img src="https://picb.zhimg.com/80/v2-cf87890eb8f2358f23a1ac78eb764257_720w.png" alt></p>
<p>这里也体现了深层神经网络或deep learning之所以称deep的一个原因：模型将特征抓取层和分类层合在了一起。 负责特征抓取的卷积层主要是用来学习“如何观察”。</p>
<p>下图简述了机器学习的发展，从最初的人工定义特征再放入分类器的方法，到让机器自己学习特征，再到如今尽量减少人为干涉的deep learning。<br><img src="https://pic3.zhimg.com/80/v2-60e7c1e89c5aed5b828cbb24fc1e5a80_720w.png" alt></p>
<p>结构发展<br>以上介绍了卷积神经网络的基本概念。 以下是几个比较有名的卷积神经网络结构，详细的请看<a href="http%3A//cs231n.github.io/convolutional-networks/">CS231n</a>。</p>
<p>LeNet：第一个成功的卷积神经网络应用<br>AlexNet：类似LeNet，但更深更大。使用了层叠的卷积层来抓取特征（通常是一个卷积层马上一个max pooling层）<br>ZF Net：增加了中间卷积层的尺寸，让第一层的stride和filter size更小。<br>GoogLeNet：减少parameters数量，最后一层用max pooling层代替了全连接层，更重要的是Inception-v4模块的使用。<br>VGGNet：只使用3x3 卷积层和2x2 pooling层从头到尾堆叠。<br>ResNet：引入了跨层连接和batch normalization。<br>DenseNet：将跨层连接从头进行到尾。</p>
<p>总结一下：这些结构的发展趋势有：</p>
<p>使用small filter size的卷积层和pooling<br>去掉parameters过多的全连接层<br>Inception（稍后会对其中的细节进行说明）<br>跳层连接</p>
<p>不变性的满足<br>接下来会谈谈我个人的，对于画面不变性是如何被卷积神经网络满足的想法。 同时结合不变性，对上面提到的结构发展的重要变动进行直觉上的解读。</p>
<p>需要明白的是为什么加入不变性可以提高网络表现。 并不是因为我们用了更炫酷的处理方式，而是加入了先验知识，无需从零开始用数据学习，节省了训练所需数据量。 思考表现提高的原因一定要从训练所需要的数据量切入。 提出满足新的不变性特点的神经网络是计算机视觉的一个主要研究方向。</p>
<p>平移不变性<br>可以说卷积神经网络最初引入局部连接和空间共享，就是为了满足平移不变性。</p>
<p><img src="https://pic1.zhimg.com/80/v2-1aac56212d5d143a006d569318e3ee8b_720w.png" alt></p>
<p>因为空间共享，在不同位置的同一形状就可以被等价识别，所以不需要对每个位置都进行学习</p>
<p><img src="https://pic4.zhimg.com/80/v2-18c11c6f485e9f1bbc9a50eb3d248439_720w.png" alt></p>
<p>旋转和视角不变性<br>个人觉得卷积神经网络克服这一不变性的主要手段还是靠大量的数据。 并没有明确加入“旋转和视角不变性”的先验特性。<br><img src="https://picb.zhimg.com/80/v2-0ce892f8b247f2b48a76cc57cbcba41d_720w.png" alt></p>
<p><a href="https%3A//arxiv.org/abs/1703.06211">Deformable Convolutional Networks</a>似乎是对此变性进行了进行增强。</p>
<p>尺寸不变性<br>与平移不变性不同，最初的卷积网络并没有明确照顾尺寸不变性这一特点。</p>
<p>我们知道filter的size是事先选择的，而不同的尺寸所寻找的形状（概念）范围不同。</p>
<p>从直观上思考，如果选择小范围，再一步步通过组合，仍然是可以得到大范围的形状。 如3x3尺寸的形状都是可以由2x2形状的图形组合而成。所以形状的尺寸不变性对卷积神经网络而言并不算问题。 这恐怕ZF Net让第一层的stride和filter size更小，VGGNet将所有filter size都设置成3x3仍可以得到优秀结果的一个原因。</p>
<p>但是，除了形状之外，很多概念的抓取通常需要考虑一个像素与周边更多像素之间的关系后得出。 也就是说5x5的filter也是有它的优点。 同时，小尺寸的堆叠需要很多个filters来共同完成，如果需要抓取的形状恰巧在5x5的范围，那么5x5会比3x3来的更有效率。 所以一次性使用多个不同filter size来抓取多个范围不同的概念是一种顺理成章的想法，而这个也就是Inception。 可以说Inception是为了尺寸不变性而引入的一个先验知识。</p>
<p>Inception<br>下图是Inception的结构，尽管也有不同的版本，但是其动机都是一样的：消除尺寸对于识别结果的影响，一次性使用多个不同filter size来抓取多个范围不同的概念，并让网络自己选择需要的特征。</p>
<p>你也一定注意到了蓝色的1x1卷积，撇开它，先看左边的这个结构。</p>
<p>输入（可以是被卷积完的长方体输出作为该层的输入）进来后，通常我们可以选择直接使用像素信息(1x1卷积)传递到下一层，可以选择3x3卷积，可以选择5x5卷积，还可以选择max pooling的方式downsample刚被卷积后的feature maps。 但在实际的网络设计中，究竟该如何选择需要大量的实验和经验的。 Inception就不用我们来选择，而是将4个选项给神经网络，让网络自己去选择最合适的解决方案。</p>
<p>接下来我们再看右边的这个结构，多了很多蓝色的1x1卷积。 这些1x1卷积的作用是为了让网络根据需要能够更灵活的控制数据的depth的。<br><img src="https://pic1.zhimg.com/80/v2-9692631d087622f1b34c80055f13fac5_720w.png" alt></p>
<p>1x1卷积核<br>如果卷积的输出输入都只是一个平面，那么1x1卷积核并没有什么意义，它是完全不考虑像素与周边其他像素关系。 但卷积的输出输入是长方体，所以1x1卷积实际上是对每个像素点，在不同的channels上进行线性组合（信息整合），且保留了图片的原有平面结构，调控depth，从而完成升维或降维的功能。</p>
<p>如下图所示，如果选择2个filters的1x1卷积层，那么数据就从原本的depth 3 降到了2。若用4个filters，则起到了升维的作用。</p>
<p>这就是为什么上面Inception的4个选择中都混合一个1x1卷积，如右侧所展示的那样。 其中，绿色的1x1卷积本身就1x1卷积，所以不需要再用另一个1x1卷积。 而max pooling用来去掉卷积得到的Feature Map中的冗余信息，所以出现在1x1卷积之前，紧随刚被卷积后的feature maps。（由于没做过实验，不清楚调换顺序会有什么影响。）<br><img src="https://pic1.zhimg.com/80/v2-59429b22ac90930c502736b33db0d8e0_720w.png" alt></p>
<p>跳层连接<br>前馈神经网络也好，卷积神经网络也好，都是一层一层逐步变换的，不允许跳层组合。 但现实中是否有跳层组合的现象？</p>
<p>比如说我们在判断一个人的时候，很多时候我们并不是观察它的全部，或者给你的图片本身就是残缺的。 这时我们会靠单个五官，外加这个人的着装，再加他的身形来综合判断这个人，如下图所示。 这样，即便图片本身是残缺的也可以很好的判断它是什么。 这和前馈神经网络的先验知识不同，它允许不同层级之间的因素进行信息交互、综合判断。</p>
<p>残差网络就是拥有这种特点的神经网络。大家喜欢用identity mappings去解释为什么残差网络更优秀。 这里我只是提供了一个以先验知识的角度去理解的方式。 需要注意的是每一层并不会像我这里所展示的那样，会形成明确的五官层。 只是有这样的组合趋势，实际无法保证神经网络到底学到了什么内容。<br><img src="https://pic2.zhimg.com/80/v2-40fb6ab7bf89ce43af1c52e673da65eb_720w.png" alt></p>
<p>用下图举一个更易思考的例子。 图形1,2,3,4,5,6是第一层卷积层抓取到的概念。 图形7,8,9是第二层卷积层抓取到的概念。 图形7,8,9是由1,2,3,4,5,6的基础上组合而成的。</p>
<p>但当我们想要探测的图形10并不是单纯的靠图形7,8,9组成，而是第一个卷积层的图形6和第二个卷积层的8,9组成的话，不允许跨层连接的卷积网络不得不用更多的filter来保持第一层已经抓取到的图形信息。并且每次传递到下一层都需要学习那个用于保留前一层图形概念的filter的权重。 当层数变深后，会越来越难以保持，还需要max pooling将冗余信息去掉。</p>
<p>一个合理的做法就是直接将上一层所抓取的概念也跳层传递给下下一层，不用让其每次都重新学习。 就好比在编程时构建了不同规模的functions。 每个function我们都是保留，而不是重新再写一遍。提高了重用性。</p>
<p>同时，因为ResNet使用了跳层连接的方式。也不需要max pooling对保留低层信息时所产生的冗余信息进行去除。<br><img src="https://pic2.zhimg.com/80/v2-87fc4b7449d751c59977c3a368ae6f7e_720w.png" alt></p>
<p>Inception中的第一个1x1的卷积通道也有类似的作用，但是1x1的卷积仍有权重需要学习。 并且Inception所使用的结合方式是concatenate的合并成一个更大的向量的方式，而ResNet的结合方式是sum。 两个结合方式各有优点。 concatenate当需要用不同的维度去组合成新观念的时候更有益。 而sum则更适用于并存的判断。比如既有油头发，又有胖身躯，同时穿着常年不洗的牛仔裤，三个不同层面的概念并存时，该人会被判定为程序员的情况。 又比如双向LSTM中正向和逆向序列抓取的结合常用相加的方式结合。在语音识别中，这表示既可以正向抓取某种特征，又可以反向抓取另一种特征。当两种特征同时存在时才会被识别成某个特定声音。</p>
<p>在下图的ResNet中，前一层的输入会跳过部分卷积层，将底层信息传递到高层。<br><img src="https://pic4.zhimg.com/80/v2-d3fd09f011583932b832ea64f78233af_720w.png" alt><br>在下图的DenseNet中，底层信息会被传递到所有的后续高层。<br><img src="https://pic1.zhimg.com/80/v2-0bebba2947e5e968a93e6def0ae5d00c_720w.png" alt></p>
<p>后续<br>随着时间推移，各个ResNet,GoogLeNet等框架也都在原有的基础上进行了发展和改进。 但基本都是上文描述的概念的组合使用加上其他的tricks。</p>
<p>如下图所展示的，加入跳层连接的Inception-ResNet。<br><img src="https://pic3.zhimg.com/80/v2-389496d1436895dfe43199a0f54c35ca_720w.jpg" alt></p>
<h1 id="YJango的Word-Embedding—介绍"><a href="#YJango的Word-Embedding—介绍" class="headerlink" title="YJango的Word Embedding—介绍"></a><span id="header14">YJango的Word Embedding—介绍</span></h1><p><img src="https://pic2.zhimg.com/v2-55dca535836121c65546bc11e2d457c1_1440w.jpg?source=172ae18b" alt><br>单词表达<br>先前在卷积神经网络的一节中，提到过图片是如何在计算机中被表达的。 同样的，单词也需要用计算机可以理解的方式表达后，才可以进行接下来的操作。</p>
<p>One hot representation<br>程序中编码单词的一个方法是one hot encoding。</p>
<p>实例：有1000个词汇量。排在第一个位置的代表英语中的冠词”a”，那么这个”a”是用[1,0,0,0,0,…]，只有第一个位置是1，其余位置都是0的1000维度的向量表示，如下图中的第一列所示。</p>
<p><img src="https://pic3.zhimg.com/80/v2-09e1bda72c4b903e25db203ab4aa6dc6_720w.jpg" alt></p>
<p>也就是说，</p>
<p>在one hot representation编码的每个单词都是一个维度，彼此independent。<br>Distributed representation<br>然而每个单词彼此无关这个特点明显不符合我们的现实情况。我们知道大量的单词都是有关。</p>
<p>语义：girl和woman虽然用在不同年龄上，但指的都是女性。</p>
<p>复数：word和words仅仅是复数和单数的差别。</p>
<p>时态：buy和bought表达的都是“买”，但发生的时间不同。</p>
<p>所以用one hot representation的编码方式，上面的特性都没有被考虑到。</p>
<p>我们更希望用诸如“语义”，“复数”，“时态”等维度去描述一个单词。每一个维度不再是0或1，而是连续的实数，表示不同的程度。</p>
<p>目的</p>
<p>但是说到底，为什么我们想要用Distributed representation的方式去表达一个单词呢? 这样做带来了什么好处？</p>
<p>数据量角度</p>
<p>这需要再次记住我们的目的：</p>
<p>机器学习：从大量的个样本 ${(x_i,y_i)_{i=1}^N}$ 中，寻找可以较好预测未见过 $x_{new}$ 所对应 $y_{new}$ 的函数 $f:x-&gt;y$ 。</p>
<p>实例：在我们日常生活的学习中，大量的 ${(x_i,y_i)_{i=1}^N}$ 就是历年真题， $x_i$ 是题目，而 $y_i$ 是对应的正确答案。高考时将会遇到的 $x_{new}$ 往往是我们没见过的题目，希望可以通过做题训练出来的解题方法 $f:x-&gt;y$ 来求解出正确的 $y_{new}$ 。</p>
<p>如果可以见到所有的情况，那么只需要记住所有的 $x_i$ 所对应的 $y_i$ 就可以完美预测。但正如高考无法见到所有类型的题一样，我们无法见到所有的情况。这意味着，</p>
<blockquote>
<p>机器学习需要从有限的例子中寻找到合理的 f 。</p>
</blockquote>
<p>高考有两个方向提高分数：</p>
<p>方向一：训练更多的数据：题海战术。<br>方向二：加入先验知识：尽可能排除不必要的可能性。<br>问题的关键在于训练所需要的数据量上。</p>
<p>同理，如果我们用One hot representation去学习，那么每一个单词我们都需要实例数据去训练，即便我们知道”Cat”和”Kitty”很多情况下可以被理解成一个意思。</p>
<p>为什么相同的东西却需要分别用不同的数据进行学习？</p>
<p>神经网络分析<br>假设我们的词汇只有4个，girl, woman, boy, man，下面就思考用两种不同的表达方式会有什么区别。</p>
<p>One hot representation<br>尽管我们知道他们彼此的关系，但是计算机并不知道。在神经网络的输入层中，每个单词都会被看作一个节点。 而我们知道训练神经网络就是要学习每个连接线的权重。如果只看第一层的权重，下面的情况需要确定4*3个连接线的关系，因为每个维度都彼此独立，girl的数据不会对其他单词的训练产生任何帮助，训练所需要的数据量，基本就固定在那里了。<br><img src="https://pic4.zhimg.com/80/v2-e153aa561b6d729f5023e077eb7f204c_720w.jpg" alt></p>
<p>Distributed representation<br>我们这里手动的寻找这四个单词之间的关系 f 。可以用两个节点去表示四个单词。每个节点取不同值时的意义如下表。 那么girl就可以被编码成向量[0,1]，man可以被编码成[1,1]（第一个维度是gender，第二个维度是age）。</p>
<p><img src="https://pic4.zhimg.com/80/v2-a6c5f337408f1e3ec31d67074a830bd6_720w.jpg" alt></p>
<p>那么这时再来看神经网络需要学习的连接线的权重就缩小到了2*3。同时，当送入girl为输入的训练数据时，因为它是由两个节点编码的。那么与girl共享相同连接的其他输入例子也可以被训练到（如可以帮助到与其共享female的woman，和child的boy的训练）。</p>
<p><img src="https://pic2.zhimg.com/80/v2-764d83497fecd09920e19cfd91fb1dd8_720w.jpg" alt></p>
<p>Word embedding也就是要达到第二个神经网络所表示的结果，降低训练所需要的数据量。</p>
<p>而上面的四个单词可以被拆成2个节点的是由我们人工提供的先验知识将原始的输入空间经过 f (上图中的黄色箭头)投射到了另一个空间（维度更小），所以才能够降低训练所需要的数据量。 但是我们没有办法一直人工提供，机器学习的宗旨就是让机器代替人力去发现pattern。</p>
<p>Word embedding就是要从数据中自动学习到输入空间到Distributed representation空间的 映射f 。</p>
<p>训练方法<br>问题来了，我们该如何自动寻找到类似上面的关系，将One hot representation转变成Distributed representation。 我们事先并不明确目标是什么，所以这是一个无监督学习任务。</p>
<p>无监督学习中常用思想是：当得到数据${(x_i,y_i)_{i=1}^N}$后，我们又不知道目标（输出）时，</p>
<p>方向一：从各个输入 {${xi}_{i=1}^N$}之间的关系找目标。 如聚类。<br>方向二：并接上以目标输出 $y_i$ 作为新输入的另一个任务 $g:y-&gt;z$ ，同时我们知道的对应$z_i$ 值。用数据 ${(x_i,y_i)_{i=1}^N}$训练得到 $k:x-&gt;z$ ,也就是 $z=g(f(x))$ ，中间的表达 $y=f(x)$ 则是我们真正想要的目标。如生成对抗网络。</p>
<p>Word embedding更偏向于方向二。 同样是学习一个 k:x-&gt;z ，但训练后并不使用 k ，而是只取前半部分的 f:x-&gt;y 。</p>
<p>到这里，我们希望所寻找的 k:x-&gt;z 既有标签 z ，又可以让 f(x) 所转换得到的 y 的表达具有Distributed representation中所演示的特点。</p>
<p>同时我们还知道，</p>
<blockquote>
<p>单词意思需要放在特定的上下文中去理解。</p>
</blockquote>
<p>那么具有相同上下文的单词，往往是有联系的。</p>
<p>实例：那这两个单词都狗的品种名，而上下文的内容已经暗指了该单词具有可爱，会舔人的特点。</p>
<ul>
<li>这个可爱的 泰迪 舔了我的脸。</li>
<li>这个可爱的 金巴 舔了我的脸。<br>而从上面这个例子中我们就可以找到一个 $k:x-&gt;z$ ：预测上下文。</li>
</ul>
<blockquote>
<p>用输入单词 x 作为中心单词去预测其他单词 z 出现在其周边的可能性。</p>
</blockquote>
<p>我们既知道对应的 z ，同时该任务 k 又可以让 f(x) 所转换得到的 y 的表达具有Distributed representation中所演示的特点。 因为我们让相似的单词（如泰迪和金巴）得到相同的输出（上下文），那么神经网络就会将泰迪的输入和金巴的输入经过神经网络 f(x) 得到的泰迪的输出和 金巴的输出几乎相同。</p>
<p>用输入单词作为中心单词去预测周边单词的方式叫做：<a href="http%3A//mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec The Skip-Gram Model</a>。</p>
<p>用输入单词作为周边单词去预测中心单词的方式叫做：<a href="https%3A//iksinc.wordpress.com/tag/continuous-bag-of-words-cbow/">Continuous Bag of Words (CBOW)</a>。</p>
<h1 id="YJango的Batch-Normalization—介绍"><a href="#YJango的Batch-Normalization—介绍" class="headerlink" title="YJango的Batch Normalization—介绍"></a><span id="header15">YJango的Batch Normalization—介绍</span></h1><p>做法<br>设，每个batch输入是 $x=[x_0,x_1,x_2,…,x_n]$ （其中每个 $x_i$ 都是一个样本， n 是batch size） 假如在第一层后加入Batch normalization layer后， $h_i$ 的计算就倍替换为下图所示的那样。<br><img src="https://pic4.zhimg.com/80/v2-cd6a1087598ee5a2ae24d1a86a6b2190_720w.jpg" alt></p>
<ul>
<li>矩阵 x 先经过 $W_{h_1}$ 的线性变换后得到 $s_1$<ul>
<li>注：因为减去batch的平均值 $u_B$ 后， b 的作用会被抵消掉，所以没必要加入 b （红色删除线）。</li>
</ul>
</li>
<li>将 $s_1$ 再减去batch的平均值 $u_B$ ，并除以batch的标准差 $\sqrt{\sigma_B+\epsilon}$ 得到 $s_2$ 。 $\epsilon$ 是为了避免除数为0的情况所使用的微小正数。<ul>
<li>$u_B=\frac{1}{m} \sum_{i=0}^{m}W_{h_i}x_i$</li>
<li>$\sigma^2=\frac{1}{m} \sum_{i=0}^{m}(W_{h_i}-u_B)^2$</li>
<li>注：但 $s_2$ 基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，引入两个新的parameters： $\gamma$ 和 $\beta$ 。 $\gamma$ 和 $\beta$ 是在训练时网络自己学习得到的。</li>
</ul>
</li>
<li>将 $s_2$ 乘以 $\gamma$ 调整数值大小，再加上 $\beta$ 增加偏移后得到 $s_3$ 。</li>
<li>为加入非线性能力， $s_3$ 也会跟随着ReLU等激活函数。</li>
<li>最终得到的 $h_1$ 会被送到下一层作为输入。</li>
</ul>
<p>需要注意的是，上述的计算方法用于在训练。因为测试时常会只预测一个新样本，也就是说batch size为1。若还用相同的方法计算 $u_B$ ， $u_B$ 就会是这个新样本自身， $s_1-u_B$ 就会成为0。</p>
<p>所以在测试时，所使用的 u 和 $\sigma^2$ 是整个训练集的均值 $u_p$ 和方差 $\sigma^2_p$ 。</p>
<p>而整个训练集的均值$u_p$和方差 $\sigma^2_p$ 的值通常也是在训练的同时用移动平均法来计算</p>
]]></content>
      <categories>
        <category>YJango</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>YJango</tag>
        <tag>tf1.x</tag>
      </tags>
  </entry>
</search>
