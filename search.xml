<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BPè¯†åˆ«æ•°å­—</title>
    <url>/2020/07/09/BP%E8%AF%86%E5%88%AB%E6%95%B0%E5%AD%97/</url>
    <content><![CDATA[<p>åŸç”ŸBPä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>



<p>BP(Back Propagation)ç¥ç»ç½‘ç»œ<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dsigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeruralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,layers)</span>:</span></span><br><span class="line">        self.V=np.random.random((layers[<span class="number">0</span>]+<span class="number">1</span>,layers[<span class="number">1</span>]+<span class="number">1</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">        self.W=np.random.random((layers[<span class="number">1</span>]+<span class="number">1</span>,layers[<span class="number">2</span>]))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,X,y,lr=<span class="number">0.11</span>,epochs=<span class="number">100000</span>)</span>:</span></span><br><span class="line">        temp = np.ones([X.shape[<span class="number">0</span>],X.shape[<span class="number">1</span>]+<span class="number">1</span>])</span><br><span class="line">        temp[:,<span class="number">0</span>:<span class="number">-1</span>]=X</span><br><span class="line">        X=temp</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(epochs+<span class="number">1</span>):</span><br><span class="line">            i = np.random.randint(X.shape[<span class="number">0</span>])</span><br><span class="line">            x=[X[i]]</span><br><span class="line">            x=np.atleast_2d(x)</span><br><span class="line">            L1=sigmoid(np.dot(x,self.V))</span><br><span class="line">            L2=sigmoid(np.dot(L1,self.W))</span><br><span class="line">            L2_delta=(y[i]-L2)*dsigmoid(L2)</span><br><span class="line">            L1_delta=L2_delta.dot((self.W.T))*dsigmoid(L1)</span><br><span class="line">            self.W+=lr*L1.T.dot(L2_delta)</span><br><span class="line">            self.V+=lr*x.T.dot(L1_delta)</span><br><span class="line">            <span class="keyword">if</span> n%<span class="number">1000</span>==<span class="number">0</span>:</span><br><span class="line">                predictions=[]</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(X_test.shape[<span class="number">0</span>]):</span><br><span class="line">                    o=self.predict(X_test[j])</span><br><span class="line">                    predictions.append(np.argmax(o))</span><br><span class="line">                accuracy=np.mean(np.equal(predictions,y_test))</span><br><span class="line">                print(<span class="string">'epoch:'</span>,n,<span class="string">'accuracy:'</span>,accuracy)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        temp=np.ones(x.shape[<span class="number">0</span>]+<span class="number">1</span>)</span><br><span class="line">        temp[<span class="number">0</span>:<span class="number">-1</span>]=x</span><br><span class="line">        x=temp</span><br><span class="line">        x=np.atleast_2d(x)</span><br><span class="line">        L1=sigmoid(np.dot(x,self.V))</span><br><span class="line">        L2=sigmoid(np.dot(L1,self.W))</span><br><span class="line">        <span class="keyword">return</span> L2</span><br><span class="line">    </span><br><span class="line">digits =load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">X -= X.min()</span><br><span class="line">X/=X.max()</span><br><span class="line"></span><br><span class="line">nm = NeruralNetwork([<span class="number">64</span>,<span class="number">100</span>,<span class="number">10</span>])</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y)</span><br><span class="line">labels_train = LabelBinarizer().fit_transform(y_train)</span><br><span class="line">labels_test = LabelBinarizer().fit_transform(y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'start'</span>)</span><br><span class="line">nm.train(X_train,labels_train,epochs=<span class="number">20000</span>)</span><br><span class="line">print(<span class="string">'end'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>start
epoch: 0 accuracy: 0.09777777777777778
epoch: 1000 accuracy: 0.5288888888888889
epoch: 2000 accuracy: 0.7333333333333333
epoch: 3000 accuracy: 0.8444444444444444
epoch: 4000 accuracy: 0.8666666666666667
epoch: 5000 accuracy: 0.92
epoch: 6000 accuracy: 0.9444444444444444
epoch: 7000 accuracy: 0.9555555555555556
epoch: 8000 accuracy: 0.9577777777777777
epoch: 9000 accuracy: 0.9488888888888889
epoch: 10000 accuracy: 0.9644444444444444
epoch: 11000 accuracy: 0.9666666666666667
epoch: 12000 accuracy: 0.96
epoch: 13000 accuracy: 0.9688888888888889
epoch: 14000 accuracy: 0.96
epoch: 15000 accuracy: 0.9644444444444444
epoch: 16000 accuracy: 0.9644444444444444
epoch: 17000 accuracy: 0.9666666666666667
epoch: 18000 accuracy: 0.9711111111111111
epoch: 19000 accuracy: 0.9688888888888889
epoch: 20000 accuracy: 0.9777777777777777
end</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç¥ç»ç½‘ç»œ</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>K-Means</title>
    <url>/2020/07/10/K-Means/</url>
    <content><![CDATA[<p>K-Meansä»£ç ç®€å•æ¼”ç¤º</p>
<p>æ•°æ®é›†ä¸‹è½½:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/kmeans.txt" target="_blank" rel="noopener">kmeans.txt</a></p>
<a id="more"></a>



<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kmeans7.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/kmeans.txt'</span>, delimiter=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclDistance</span><span class="params">(vector1,vector2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(sum((vector2-vector1)**<span class="number">2</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(data,k)</span>:</span></span><br><span class="line">    numSamples,dim = data.shape</span><br><span class="line">    centroids = np.zeros((k,dim))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        index = int(np.random.uniform(<span class="number">0</span>,numSamples))</span><br><span class="line">        centroids[i,:]=data[index,:]</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(data,k)</span>:</span></span><br><span class="line">    numSamples = data.shape[<span class="number">0</span>]</span><br><span class="line">    clusterData = np.array(np.zeros((numSamples,<span class="number">2</span>)))</span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    centroids = initCentroids(data,k)</span><br><span class="line">    <span class="keyword">while</span> clusterChanged:  </span><br><span class="line">        clusterChanged = <span class="literal">False</span>  </span><br><span class="line">        <span class="comment"># å¾ªç¯æ¯ä¸€ä¸ªæ ·æœ¬ </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  </span><br><span class="line">            <span class="comment"># æœ€å°è·ç¦»</span></span><br><span class="line">            minDist  = <span class="number">100000.0</span>  </span><br><span class="line">            <span class="comment"># å®šä¹‰æ ·æœ¬æ‰€å±çš„ç°‡</span></span><br><span class="line">            minIndex = <span class="number">0</span>  </span><br><span class="line">            <span class="comment"># å¾ªç¯è®¡ç®—æ¯ä¸€ä¸ªè´¨å¿ƒä¸è¯¥æ ·æœ¬çš„è·ç¦»</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </span><br><span class="line">                <span class="comment"># å¾ªç¯æ¯ä¸€ä¸ªè´¨å¿ƒå’Œæ ·æœ¬ï¼Œè®¡ç®—è·ç¦»</span></span><br><span class="line">                distance = euclDistance(centroids[j, :], data[i, :])  </span><br><span class="line">                <span class="comment"># å¦‚æœè®¡ç®—çš„è·ç¦»å°äºæœ€å°è·ç¦»ï¼Œåˆ™æ›´æ–°æœ€å°è·ç¦»</span></span><br><span class="line">                <span class="keyword">if</span> distance &lt; minDist:  </span><br><span class="line">                    minDist  = distance  </span><br><span class="line">                    <span class="comment"># æ›´æ–°æ ·æœ¬æ‰€å±çš„ç°‡</span></span><br><span class="line">                    minIndex = j  </span><br><span class="line">                    <span class="comment"># æ›´æ–°æœ€å°è·ç¦»</span></span><br><span class="line">                    clusterData[i, <span class="number">1</span>] = distance</span><br><span class="line">              </span><br><span class="line">            <span class="comment"># å¦‚æœæ ·æœ¬çš„æ‰€å±çš„ç°‡å‘ç”Ÿäº†å˜åŒ–</span></span><br><span class="line">            <span class="keyword">if</span> clusterData[i, <span class="number">0</span>] != minIndex:  </span><br><span class="line">                <span class="comment"># è´¨å¿ƒè¦é‡æ–°è®¡ç®—</span></span><br><span class="line">                clusterChanged = <span class="literal">True</span></span><br><span class="line">                <span class="comment"># æ›´æ–°æ ·æœ¬çš„ç°‡</span></span><br><span class="line">                clusterData[i, <span class="number">0</span>] = minIndex</span><br><span class="line">        <span class="comment"># æ›´æ–°è´¨å¿ƒ</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </span><br><span class="line">            <span class="comment"># è·å–ç¬¬jä¸ªç°‡æ‰€æœ‰çš„æ ·æœ¬æ‰€åœ¨çš„ç´¢å¼•</span></span><br><span class="line">            cluster_index = np.nonzero(clusterData[:, <span class="number">0</span>] == j)</span><br><span class="line">            <span class="comment"># ç¬¬jä¸ªç°‡æ‰€æœ‰çš„æ ·æœ¬ç‚¹</span></span><br><span class="line">            pointsInCluster = data[cluster_index]  </span><br><span class="line">            <span class="comment"># è®¡ç®—è´¨å¿ƒ</span></span><br><span class="line">            centroids[j, :] = np.mean(pointsInCluster, axis = <span class="number">0</span>) </span><br><span class="line"><span class="comment">#         showCluster(data, k, centroids, clusterData)</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterData  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ˜¾ç¤ºç»“æœ </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(data, k, centroids, clusterData)</span>:</span>  </span><br><span class="line">    numSamples, dim = data.shape  </span><br><span class="line">    <span class="keyword">if</span> dim != <span class="number">2</span>:  </span><br><span class="line">        print(<span class="string">"dimension of your data is not 2!"</span>)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># ç”¨ä¸åŒé¢œè‰²å½¢çŠ¶æ¥è¡¨ç¤ºå„ä¸ªç±»åˆ«</span></span><br><span class="line">    mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]  </span><br><span class="line">    <span class="keyword">if</span> k &gt; len(mark):  </span><br><span class="line">        print(<span class="string">"Your k is too large!"</span>)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># ç”»æ ·æœ¬ç‚¹  </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  </span><br><span class="line">        markIndex = int(clusterData[i, <span class="number">0</span>])  </span><br><span class="line">        plt.plot(data[i, <span class="number">0</span>], data[i, <span class="number">1</span>], mark[markIndex])  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># ç”¨ä¸åŒé¢œè‰²å½¢çŠ¶æ¥è¡¨ç¤ºå„ä¸ªç±»åˆ«</span></span><br><span class="line">    mark = [<span class="string">'*r'</span>, <span class="string">'*b'</span>, <span class="string">'*g'</span>, <span class="string">'*k'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]  </span><br><span class="line">    <span class="comment"># ç”»è´¨å¿ƒç‚¹ </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):  </span><br><span class="line">        plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize = <span class="number">20</span>)  </span><br><span class="line">  </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_lost=[]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">10</span>):</span><br><span class="line">    min_loss = <span class="number">10000</span></span><br><span class="line">    min_loss_centroids = np.array([])</span><br><span class="line">    min_loss_clusterData = np.array([])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        centroids,clusterData = kmeans(data,k)</span><br><span class="line">        loss = sum(clusterData[:,<span class="number">1</span>])/data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> loss&lt;min_loss:</span><br><span class="line">            min_loss = loss</span><br><span class="line">            min_loss_centroids = centroids</span><br><span class="line">            min_loss_clusterData = clusterData</span><br><span class="line">    list_lost.append(min_loss)</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\numpy\core\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\numpy\core\_methods.py:154: RuntimeWarning: invalid value encountered in true_divide
  ret, rcount, out=ret, casting=&apos;unsafe&apos;, subok=False)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_lost</span><br></pre></td></tr></table></figure>




<pre><code>[2.9811811738953176,
 1.9708559728104191,
 1.1675654672086735,
 1.0712368269135584,
 1.0019034362200374,
 0.9470283294527311,
 0.8835789709731454,
 0.8393052369848919]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(range(<span class="number">2</span>,<span class="number">10</span>),list_lost)</span><br><span class="line">plt.xlabel(<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/K-Means/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test = [<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k=<span class="number">6</span></span><br><span class="line">min_loss = <span class="number">10000</span></span><br><span class="line">min_loss_centroids = np.array([])</span><br><span class="line">min_loss_clusterData = np.array([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">    centroids,clusterData = kmeans(data,k)</span><br><span class="line">    loss = sum(clusterData[:,<span class="number">1</span>])/data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> loss&lt;min_loss:</span><br><span class="line">        min_loss = loss</span><br><span class="line">        min_loss_centroids = centroids</span><br><span class="line">        min_loss_clusterData = clusterData</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(datas)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array([np.argmin(((np.tile(data,(k,<span class="number">1</span>))-centroids)**<span class="number">2</span>).sum(axis=<span class="number">1</span>)) <span class="keyword">for</span> data <span class="keyword">in</span> datas])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è·å–æ•°æ®å€¼æ‰€åœ¨çš„èŒƒå›´</span></span><br><span class="line">x_min, x_max = data[:, <span class="number">0</span>].min() - <span class="number">1</span>, data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = data[:, <span class="number">1</span>].min() - <span class="number">1</span>, data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç”Ÿæˆç½‘æ ¼çŸ©é˜µ</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">z = predict(np.c_[xx.ravel(), yy.ravel()])<span class="comment"># ravelä¸flattenç±»ä¼¼ï¼Œå¤šç»´æ•°æ®è½¬ä¸€ç»´ã€‚flattenä¸ä¼šæ”¹å˜åŸå§‹æ•°æ®ï¼Œravelä¼šæ”¹å˜åŸå§‹æ•°æ®</span></span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># ç­‰é«˜çº¿å›¾</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># æ˜¾ç¤ºç»“æœ</span></span><br><span class="line">showCluster(data, k, centroids, clusterData)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/K-Means/output_9_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>èšç±»ç®—æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title>BPè§£å†³å¼‚æˆ–é—®é¢˜</title>
    <url>/2020/07/09/BP%E8%A7%A3%E5%86%B3%E5%BC%82%E6%88%96%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>BPè§£å†³å¼‚æˆ–ä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>


<p>BP(Back Propagation)ç¥ç»ç½‘ç»œ<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line">V = np.random.random((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">W = np.random.random((<span class="number">4</span>,<span class="number">1</span>))*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">print(V)</span><br><span class="line">print(W)</span><br><span class="line">lr = <span class="number">0.11</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dsigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> X,Y,W,V,lr</span><br><span class="line">    L1 = sigmoid(np.dot(X,V))</span><br><span class="line">    L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">    L2_delta = (Y.T-L2)*dsigmoid(L2)</span><br><span class="line">    L1_delta = L2_delta.dot(W.T)*dsigmoid(L1)</span><br><span class="line">    W_C=lr*L1.T.dot(L2_delta)</span><br><span class="line">    V_C=lr*X.T.dot(L1_delta)</span><br><span class="line">    W=W+W_C</span><br><span class="line">    V=V+V_C</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0.34261592  0.95360188  0.04334655 -0.11015956]
 [ 0.12224398 -0.95540579  0.276879   -0.46803064]
 [-0.27212006  0.92409323  0.67230364 -0.63426355]]
[[ 0.6872099 ]
 [-0.05683569]
 [ 0.53593031]
 [ 0.6657038 ]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    update()</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">500</span>==<span class="number">0</span>:</span><br><span class="line">        L1 = sigmoid(np.dot(X,V))</span><br><span class="line">        L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">        print(<span class="string">'Error:'</span>, np.mean(np.abs(Y.T-L2)))</span><br><span class="line"></span><br><span class="line">L1 = sigmoid(np.dot(X,V))</span><br><span class="line">L2 = sigmoid(np.dot(L1,W))</span><br><span class="line">print(L2)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">judge</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x&gt;=<span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> map(judge,L2):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>

<pre><code>Error: 0.5001898525337108
Error: 0.4934029320599436
Error: 0.476950391452333
Error: 0.43144945106972044
Error: 0.3591844985087166
Error: 0.2441743332168715
Error: 0.15809286523556107
Error: 0.11588525072232857
Error: 0.09306039789777351
Error: 0.0789263390463002
Error: 0.06928189244260918
Error: 0.06224125874868228
Error: 0.05684589474475504
Error: 0.05255899358953674
Error: 0.04905658260975497
Error: 0.04613132103488044
Error: 0.04364412894489313
Error: 0.04149806879631519
Error: 0.0396233922893207
Error: 0.03796855199862452
Error: 0.036494571680927006
Error: 0.03517139424513
Error: 0.033975439770446966
Error: 0.032887928957447604
Error: 0.031893705073012604
Error: 0.030980388990824857
Error: 0.030137761945244118
Error: 0.02935730716518244
Error: 0.028631864414259807
Error: 0.027955366108362894
Error: 0.027322633269370986
Error: 0.02672921597533191
Error: 0.026171267318935168
Error: 0.025645442893166143
Error: 0.025148819932565722
Error: 0.02467883173915275
Error: 0.024233214103394567
Error: 0.02380996121912868
Error: 0.02340728917277522
Error: 0.02302360552036342
[[0.01509335]
 [0.97846218]
 [0.97414206]
 [0.02814369]]
0
1
1
0</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç¥ç»ç½‘ç»œ</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>KNN</title>
    <url>/2020/07/10/KNN/</url>
    <content><![CDATA[<p>KNNä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line">x_data = np.array([[<span class="number">3</span>,<span class="number">104</span>],[<span class="number">2</span>,<span class="number">100</span>],[<span class="number">1</span>,<span class="number">81</span>],[<span class="number">101</span>,<span class="number">10</span>],[<span class="number">99</span>,<span class="number">5</span>],[<span class="number">81</span>,<span class="number">2</span>]])</span><br><span class="line">y_data = np.array([<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>])</span><br><span class="line">x_test = np.array([<span class="number">18</span>,<span class="number">90</span>])</span><br><span class="line"></span><br><span class="line">x_data_size = x_data.shape[<span class="number">0</span>]</span><br><span class="line">x_data_size</span><br></pre></td></tr></table></figure>




<pre><code>6</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.tile(x_test,(x_data_size,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[18, 90],
       [18, 90],
       [18, 90],
       [18, 90],
       [18, 90],
       [18, 90]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">diffMat = np.tile(x_test,(x_data_size,<span class="number">1</span>)) - x_data</span><br><span class="line">diffMat</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 15, -14],
       [ 16, -10],
       [ 17,   9],
       [-83,  80],
       [-81,  85],
       [-63,  88]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">sqDiffMat</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 225,  196],
       [ 256,  100],
       [ 289,   81],
       [6889, 6400],
       [6561, 7225],
       [3969, 7744]], dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">sqDistances</span><br></pre></td></tr></table></figure>




<pre><code>array([  421,   356,   370, 13289, 13786, 11713], dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distance = sqDistances**<span class="number">0.5</span></span><br><span class="line">distance</span><br></pre></td></tr></table></figure>




<pre><code>array([ 20.51828453,  18.86796226,  19.23538406, 115.27792503,
       117.41379817, 108.2266141 ])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">sortedDistances = distance.argsort()</span><br><span class="line">sortedDistances</span><br></pre></td></tr></table></figure>




<pre><code>array([1, 2, 0, 5, 3, 4], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classCount = &#123;&#125;</span><br><span class="line">k = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    votelabel = y_data[sortedDistances[i]]</span><br><span class="line">    classCount[votelabel] = classCount.get(votelabel,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">classCount</span><br></pre></td></tr></table></figure>




<pre><code>{&apos;A&apos;: 3, &apos;B&apos;: 2}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">classCount.items(),sortedClassCount</span><br></pre></td></tr></table></figure>




<pre><code>(dict_items([(&apos;A&apos;, 3), (&apos;B&apos;, 2)]), [(&apos;A&apos;, 3), (&apos;B&apos;, 2)])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knnclass = sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">knnclass</span><br></pre></td></tr></table></figure>




<pre><code>&apos;A&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report,confusion_matrix</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(x_test,x_data,y_data,k)</span>:</span></span><br><span class="line">    x_data_size = x_data.shape[<span class="number">0</span>]</span><br><span class="line">    np.tile(x_test,(x_data_size,<span class="number">1</span>))</span><br><span class="line">    diffMat = np.tile(x_test,(x_data_size,<span class="number">1</span>))-x_data</span><br><span class="line">    sqDiffMat=diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances=sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances=sqDistances**<span class="number">0.5</span></span><br><span class="line">    sortedDistances=distances.argsort()</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        votelabel = y_data[sortedDistances[i]]</span><br><span class="line">        classCount[votelabel]=classCount.get(votelabel,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">data_size=iris.data.shape[<span class="number">0</span>]</span><br><span class="line">index = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(data_size)]</span><br><span class="line">random.shuffle(index)</span><br><span class="line">iris.data = iris.data[index]</span><br><span class="line">iris.target = iris.target[index]</span><br><span class="line"></span><br><span class="line">test_size = <span class="number">40</span></span><br><span class="line">x_train = iris.data[test_size:]</span><br><span class="line">x_test = iris.data[:test_size]</span><br><span class="line">y_train = iris.target[test_size:]</span><br><span class="line">y_test = iris.target[:test_size]</span><br><span class="line"></span><br><span class="line">predictions = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x_test.shape[<span class="number">0</span>]):</span><br><span class="line">    predictions.append(knn(x_test[i],x_train,y_train,<span class="number">5</span>))</span><br><span class="line">    </span><br><span class="line">print(classification_report(y_test,predictions))</span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

           0       1.00      1.00      1.00        13
           1       1.00      1.00      1.00        11
           2       1.00      1.00      1.00        16

    accuracy                           1.00        40
   macro avg       1.00      1.00      1.00        40
weighted avg       1.00      1.00      1.00        40</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(confusion_matrix(y_test,predictions))</span><br></pre></td></tr></table></figure>

<pre><code>[[13  0  0]
 [ 0 11  0]
 [ 0  0 16]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>æœ€è¿‘é‚»ç®—æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2020/07/11/PCA/</url>
    <content><![CDATA[<p>PCAç®€å•ä»£ç å±•ç¤º:</p>
<a id="more"></a>

<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA7.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA9.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA10.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report,confusion_matrix</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">digits = load_digits()</span><br><span class="line">x_data = digits.data</span><br><span class="line">y_data = digits.target</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data.shape</span><br></pre></td></tr></table></figure>




<pre><code>(1797, 64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">100</span>,<span class="number">50</span>), max_iter=<span class="number">500</span>)</span><br><span class="line">mlp.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>MLPClassifier(activation=&apos;relu&apos;, alpha=0.0001, batch_size=&apos;auto&apos;, beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100, 50), learning_rate=&apos;constant&apos;,
              learning_rate_init=0.001, max_fun=15000, max_iter=500,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=None, shuffle=True, solver=&apos;adam&apos;,
              tol=0.0001, validation_fraction=0.1, verbose=False,
              warm_start=False)</code></pre><p>åæ–¹å·®ä»£ç å½¢å¼</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">å‡½æ•°åŸå‹ï¼šdef cov(m, y&#x3D;None, rowvar&#x3D;True, bias&#x3D;False, ddof&#x3D;None, fweights&#x3D;None,aweights&#x3D;None)</span><br><span class="line">m:ä¸€ç»´æˆ–åˆ™äºŒç»´çš„æ•°ç»„ï¼Œé»˜è®¤æƒ…å†µä¸‹æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå˜é‡ï¼ˆå±æ€§ï¼‰ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªè§‚æµ‹</span><br><span class="line">y:ä¸må…·æœ‰ä¸€æ ·çš„å½¢å¼çš„ä¸€ç»„æ•°æ®</span><br><span class="line">rowvar:é»˜è®¤ä¸ºTrue,æ­¤æ—¶æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå˜é‡ï¼ˆå±æ€§ï¼‰ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªè§‚æµ‹ï¼›ä¸ºFalseæ—¶ï¼Œåˆ™åä¹‹</span><br><span class="line">bias:é»˜è®¤ä¸ºFalse,æ­¤æ—¶æ ‡å‡†åŒ–æ—¶é™¤ä»¥n-1ï¼›åä¹‹ä¸ºnã€‚å…¶ä¸­nä¸ºè§‚æµ‹æ•°</span><br><span class="line">ddof:ç±»å‹æ˜¯intï¼Œå½“å…¶å€¼éNoneæ—¶ï¼Œbiaså‚æ•°ä½œç”¨å°†å¤±æ•ˆã€‚å½“ddof&#x3D;1æ—¶ï¼Œå°†ä¼šè¿”å›æ— åä¼°è®¡ï¼ˆé™¤ä»¥n-1ï¼‰ï¼Œå³ä½¿æŒ‡å®šäº†fweightså’Œaweightså‚æ•°ï¼›å½“ddof&#x3D;0æ—¶ï¼Œåˆ™è¿”å›ç®€å•å¹³å‡å€¼ã€‚</span><br><span class="line">frequency weights:ä¸€ç»´æ•°ç»„ï¼Œä»£è¡¨æ¯ä¸ªè§‚æµ‹è¦é‡å¤çš„æ¬¡æ•°ï¼ˆç›¸å½“äºç»™è§‚æµ‹èµ‹äºˆæƒé‡ï¼‰</span><br><span class="line">analytic weights:ä¸€ç»´æ•°ç»„ï¼Œä»£è¡¨è§‚æµ‹çŸ¢é‡æƒé‡ã€‚å¯¹äºè¢«è®¤ä¸ºâ€œé‡è¦â€çš„è§‚å¯Ÿ,è¿™äº›ç›¸å¯¹æƒé‡é€šå¸¸å¾ˆå¤§,è€Œå¯¹äºè¢«è®¤ä¸ºä¸å¤ªé‡è¦çš„è§‚å¯Ÿ,è¿™äº›ç›¸å¯¹æƒé‡è¾ƒå°ã€‚å¦‚æœddof &#x3D; 0,åˆ™å¯ä»¥ä½¿ç”¨æƒé‡æ•°ç»„å°†æ¦‚ç‡åˆ†é…ç»™è§‚æµ‹å‘é‡ã€‚</span><br></pre></td></tr></table></figure>
<p>ä»£ç ç¤ºä¾‹<br>åŸºæœ¬ä½¿ç”¨</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># è®¡ç®—åæ–¹å·®çš„æ—¶å€™ï¼Œä¸€è¡Œä»£è¡¨ä¸€ä¸ªç‰¹å¾</span><br><span class="line"># ä¸‹é¢è®¡ç®—cov(T, S, M)</span><br><span class="line">T &#x3D; np.array([9, 15, 25, 14, 10, 18, 0, 16, 5, 19, 16, 20])</span><br><span class="line">S &#x3D; np.array([39, 56, 93, 61, 50, 75, 32, 85, 42, 70, 66, 80])</span><br><span class="line">M &#x3D; np.asarray([38, 56, 90, 63, 56, 77, 30, 80, 41, 79, 64, 88])</span><br><span class="line">X &#x3D; np.vstack((T, S, M))</span><br><span class="line"># Xæ¯è¡Œä»£è¡¨ä¸€ä¸ªå±æ€§</span><br><span class="line">#  æ¯åˆ—ä»£è¡¨ä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ–è€…è¯´è§‚æµ‹</span><br><span class="line">print(np.cov(X))</span><br><span class="line"></span><br><span class="line"># [[ 47.71969697 122.9469697  129.59090909]</span><br><span class="line">#  [122.9469697  370.08333333 374.59090909]</span><br><span class="line">#  [129.59090909 374.59090909 399.        ]]</span><br></pre></td></tr></table></figure>
<p>é‡ç‚¹ï¼šåæ–¹å·®çŸ©é˜µè®¡ç®—çš„æ˜¯ä¸åŒç»´åº¦ä¹‹é—´çš„åæ–¹å·®ï¼Œè€Œä¸æ˜¯ä¸åŒæ ·æœ¬ä¹‹é—´ã€‚æ‹¿åˆ°ä¸€ä¸ªæ ·æœ¬çŸ©é˜µï¼Œé¦–å…ˆè¦æ˜ç¡®çš„å°±æ˜¯è¡Œä»£è¡¨ä»€ä¹ˆï¼Œåˆ—ä»£è¡¨ä»€ä¹ˆã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zeroMean</span><span class="params">(dataMat)</span>:</span></span><br><span class="line">    meanVal = np.mean(dataMat, axis=<span class="number">0</span>)</span><br><span class="line">    newData = dataMat - meanVal</span><br><span class="line">    <span class="keyword">return</span> newData, meanVal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(dataMat,top)</span>:</span></span><br><span class="line">    newData, meanVal = zeroMean(dataMat)</span><br><span class="line">    covMat = np.cov(newData,rowvar=<span class="number">0</span>) <span class="comment"># æ±‚æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µã€‚</span></span><br><span class="line">    eigVals,eigVects = np.linalg.eig(np.mat(covMat)) <span class="comment"># å¯¹åæ–¹å·®1m ğ‘‹ğ‘‹ğ‘‡çŸ©é˜µåšç‰¹å¾å€¼åˆ†è§£ã€‚</span></span><br><span class="line">    eigValIndice = np.argsort(eigVals)</span><br><span class="line">    n_eigValIndice = eigValIndice[<span class="number">-1</span>:-(top+<span class="number">1</span>):<span class="number">-1</span>] <span class="comment"># é€‰å‡ºæœ€å¤§çš„kä¸ªç‰¹å¾å€¼å¯¹åº”çš„kä¸ªç‰¹å¾å‘é‡ã€‚</span></span><br><span class="line">    n_eigVect = eigVects[:,n_eigValIndice]</span><br><span class="line">    lowDDataMat = newData*n_eigVect <span class="comment"># å°†åŸå§‹æ•°æ®æŠ•å½±åˆ°é€‰å–çš„ç‰¹å¾å‘é‡ä¸Šã€‚</span></span><br><span class="line">    reconMat = (lowDDataMat*n_eigVect.T)+meanVal</span><br><span class="line">    <span class="keyword">return</span> lowDDataMat,reconMat</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowDDataMat,reconMat = pca(x_data,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x,y,c=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/PCA/output_8_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = mlp.predict(x_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x,y,c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/PCA/output_10_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowDDataMat,reconMat = pca(x_data,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">x = np.array(lowDDataMat)[:,<span class="number">0</span>]</span><br><span class="line">y = np.array(lowDDataMat)[:,<span class="number">1</span>]</span><br><span class="line">z = np.array(lowDDataMat)[:,<span class="number">2</span>]</span><br><span class="line">ax = plt.figure().add_subplot(<span class="number">111</span>,projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(x,y,z,c=y_data,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/PCA/output_12_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>ä¸»æˆåˆ†åˆ†æ</tag>
      </tags>
  </entry>
  <entry>
    <title>hexoå¼€å‘åšå®¢</title>
    <url>/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">ä»0æ­å»º</a></li>
<li>2.<a href="#header2">é‡åˆ°çš„å‘</a></li>
<li>3.<a href="#header3">å›¾ç‰‡æ”¾ç½®é—®é¢˜</a></li>
<li>4.<a href="#header4">æ›´æ¢ä¸»é¢˜</a></li>
<li>5.<a href="#header5">æ”¯æŒLatex</a></li>
</ul>
<a id="more"></a>
<h1 id="è§†é¢‘æ‰‹æŠŠæ‰‹æ•™ä½ ä»0å¼€å§‹æ­å»ºè‡ªå·±çš„ä¸ªäººåšå®¢hexoå‚è€ƒ"><a href="#è§†é¢‘æ‰‹æŠŠæ‰‹æ•™ä½ ä»0å¼€å§‹æ­å»ºè‡ªå·±çš„ä¸ªäººåšå®¢hexoå‚è€ƒ" class="headerlink" title="è§†é¢‘æ‰‹æŠŠæ‰‹æ•™ä½ ä»0å¼€å§‹æ­å»ºè‡ªå·±çš„ä¸ªäººåšå®¢hexoå‚è€ƒ"></a><span id="header1">è§†é¢‘æ‰‹æŠŠæ‰‹æ•™ä½ ä»0å¼€å§‹æ­å»ºè‡ªå·±çš„ä¸ªäººåšå®¢hexo</span><a href="https://www.bilibili.com/video/BV1Yb411a7ty/?spm_id_from=333.788.videocard.4" target="_blank" rel="noopener">å‚è€ƒ</a></h1><ul>
<li>å®‰è£…node<br><a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">nodeå®˜ç½‘</a>å¯ä»¥ç›´æ¥ä¸‹è½½node,å¯ä»¥æ¢æº</li>
<li>npmæ¢æº(å¯é€‰)<br>é»˜è®¤çš„npmæº â€”â€” <a href="https://registry.npmjs.org" target="_blank" rel="noopener">https://registry.npmjs.org</a> æ¯”è¾ƒæ…¢,å¯ä»¥æ¢æˆæ·˜å®é•œåƒ â€”â€” <a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure>
æ¢å›æ¥åŒç†<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npmjs.org</span><br></pre></td></tr></table></figure></li>
<li>å…¨å±€å®‰è£…hexo-cli(å‰ææ˜¯å®‰è£…äº†node,å¯ä»¥ç”¨npmå‘½ä»¤)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm i -g hexo-cli</span><br></pre></td></tr></table></figure></li>
<li>åˆå§‹åŒ–<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo init (æ–‡ä»¶å¤¹å)</span><br></pre></td></tr></table></figure>
ä¸åŠ æ–‡ä»¶å¤¹åå°±æ˜¯åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹</li>
<li><p>æ–°å»ºä¸€ç¯‡æ–‡ç« </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo n æ–‡ç« å</span><br><span class="line">æˆ–è€…</span><br><span class="line">hexo new æ–‡ç« å</span><br></pre></td></tr></table></figure>
<p>æ–‡ç« å†…å®¹ä½¿ç”¨<a href="https://githubzhangshuai.github.io/2020/07/08/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/" target="_blank" rel="noopener">Markdown</a>è¯­æ³•è¿›è¡Œä¹¦å†™å³å¯</p>
</li>
<li><p>åœ¨çº¿é¢„è§ˆ</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server</span><br><span class="line">æˆ–è€…</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>æµè§ˆå™¨çª—å£è¾“å…¥localhost:4000å³å¯é¢„è§ˆ</p>
</li>
<li><p>éƒ¨ç½²</p>
<ul>
<li>å…ˆæ‰‹åŠ¨å®‰è£…hexo-deployer-git<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm i --save hexo-deployer-git</span><br></pre></td></tr></table></figure></li>
<li>ç„¶ågithubæ–°å»ºä»“åº“,ä»“åº“åä¸ºxxx.github.io,xxxä¸ºgithubè´¦å·å,å¦‚å›¾,æˆ‘çš„xxxä¸ºGitHubzhangshuaiæ•…ä»“åº“åä¸ºGitHubzhangshuai.github.io.git<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/hexo%E6%88%AA%E5%9B%BE.png" alt="ç¤ºä¾‹å›¾ç‰‡"></li>
<li>ä¿®æ”¹é…ç½®æ–‡ä»¶<br>æ‰¾åˆ°_config.ymlæ–‡ä»¶,ä¿®æ”¹deployéƒ¨åˆ†(100è¡Œå·¦å³)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;deployment.html</span><br><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo: https:&#x2F;&#x2F;github.com&#x2F;GitHubzhangshuai&#x2F;GitHubzhangshuai.github.io.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure></li>
<li>éƒ¨ç½²åˆ°githubä¸Š<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br><span class="line">æˆ–è€…</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></li>
<li>è®¿é—®<br>æµè§ˆå™¨åœ°å€æ è¾“å…¥xxx.github.io</li>
</ul>
</li>
</ul>
<h1 id="é‡åˆ°çš„å‘"><a href="#é‡åˆ°çš„å‘" class="headerlink" title="é‡åˆ°çš„å‘"></a><span id="header2">é‡åˆ°çš„å‘</span></h1><ul>
<li>npmå®‰è£…å¤±è´¥<br>æ£€æŸ¥ç½‘ç»œé—®é¢˜å¹¶ä¸”æ¢æ·˜å®æº<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li>
<li>éƒ¨ç½²æ—¶å‡ºç°é”™è¯¯,<a href="https://blog.csdn.net/HTL2018/article/details/106876940?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">å‚è€ƒ</a></li>
</ul>
<ul>
<li>1.é”™è¯¯:<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">errorï¼šspawn failed...</span><br><span class="line">æˆ–è€…:</span><br><span class="line">fatal: cannot lock ref &#39;HEAD&#39;: unable to resolve reference HEAD: Invalid argument error: src refspec</span><br><span class="line">æˆ–è€…:</span><br><span class="line">error: src refspec HEAD does not match any.</span><br></pre></td></tr></table></figure></li>
<li><ol>
<li>æ€»ç»“ä¸€ä¸‹<br>é—®é¢˜å¤§å¤šæ˜¯å› ä¸ºgitè¿›è¡Œpushæˆ–è€…hexo dçš„æ—¶å€™æ”¹å˜äº†ä¸€äº›.deploy_gitæ–‡ä»¶ä¸‹çš„å†…å®¹ã€‚</li>
</ol>
</li>
<li><ol>
<li>è§£å†³åŠæ³•<ul>
<li>3-1.åˆ é™¤.deploy_gitæ–‡ä»¶å¤¹;</li>
<li>3-2.å…¨å±€è®¾ç½®gitçš„core.autocrlfä¸ºfalse;<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">è¾“å…¥git config --global core.autocrlf false</span><br></pre></td></tr></table></figure></li>
<li>3-2.ç„¶åï¼Œä¾æ¬¡æ‰§è¡Œï¼š<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h1 id="hexoå›¾ç‰‡é—®é¢˜"><a href="#hexoå›¾ç‰‡é—®é¢˜" class="headerlink" title="hexoå›¾ç‰‡é—®é¢˜"></a><span id="header3">hexoå›¾ç‰‡é—®é¢˜</span></h1><ul>
<li>1.æ”¾åœ¨OSSä¸Š,æˆ‘ç”¨çš„é˜¿é‡Œäº‘çš„oss(è®°å¾—è®¾ç½®bucketçš„å…¬å…±è¯»æƒé™)<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/2.jpg" alt="xx"></li>
<li>2.ä½¿ç”¨base64æ ¼å¼</li>
<li>3.ä½¿ç”¨æ’ä»¶hexo-asset-image<br>å®‰è£…æ’ä»¶<br>æ³¨æ„:<br>ä½¿ç”¨ npm install <a href="https://github.com/CodeFalling/hexo-asset-image" target="_blank" rel="noopener">https://github.com/CodeFalling/hexo-asset-image</a> â€”save å®‰è£…0.0.5ç‰ˆæœ¬çš„hexo-asset-imageæ’ä»¶ã€‚ ä½¿ç”¨ npm install hexo-asset-image â€”save å®‰è£…çš„æ˜¯1.0.0ç‰ˆæœ¬çš„hexo-asset-imageæ’ä»¶ã€‚ ä¸¤è€…æœ€ç›´æ¥çš„åŒºåˆ«æ˜¯æ˜ å°„å…³ç³»ä¸åŒã€‚ <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install https:&#x2F;&#x2F;github.com&#x2F;CodeFalling&#x2F;hexo-asset-image --save (æœ‰æ•ˆ)</span><br><span class="line">npm install hexo-asset-image --save (æ— æ•ˆ)</span><br></pre></td></tr></table></figure>
åœ¨_config.ymlé…ç½®æ–‡ä»¶ä¸­(44è¡Œå·¦å³)ï¼Œä¿®æ”¹ä¸º <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>
è®°å¾—é‡å¯æœåŠ¡å™¨(VS codeçš„MDé¢„è§ˆæ’ä»¶ä¸‹æ— æ•ˆ)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="æ›´æ¢ä¸»é¢˜"><a href="#æ›´æ¢ä¸»é¢˜" class="headerlink" title="æ›´æ¢ä¸»é¢˜"></a><span id="#header4">æ›´æ¢ä¸»é¢˜</span></h1><p><a href="https://www.jianshu.com/p/33bc0a0a6e90" target="_blank" rel="noopener">å‚è€ƒ</a></p>
<ul>
<li>1.ä¸‹è½½ <a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">NexT</a> ä¸»é¢˜,å°†ä¸»é¢˜å…‹éš†åˆ° themes ç›®å½•ä¸‹ï¼Œä»¥ä¸‹æˆªå›¾å°±æ˜¯ clone ä¹‹åçš„ç»“æœã€‚<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &lt;åšå®¢å­˜æ”¾çš„ç›®å½•&gt;</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;iissnan&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure></li>
<li>2.æ‰“å¼€ _config.yml æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶ä¸ºç«™ç‚¹é…ç½®æ–‡ä»¶,å°†ä¸»é¢˜ä¿®æ”¹ä¸º next<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">theme: landscape</span><br><span class="line">æ”¹ä¸º</span><br><span class="line">theme: next</span><br></pre></td></tr></table></figure></li>
<li>3.é‡å¯æœåŠ¡å™¨å‘ç°æ‰€æœ‰çš„ç‚¹å‡»åè¾¹éƒ½å¸¦äº†%20,å¦‚å›¾<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/3.png" alt="x"><br><a href="https://blog.csdn.net/weixin_43790779/article/details/104854588" target="_blank" rel="noopener">è§£å†³</a>:<br>æ‰¾åˆ°nextçš„é…ç½®æ–‡ä»¶themes\next_config.yml,å°†é…ç½®æ–‡ä»¶é‡Œ ||ä¹‹å‰æ‰€æœ‰çš„ç©ºæ ¼åˆ æ‰,æ”¹ä¸ºä»¥ä¸‹å³å¯<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: &#x2F;||home</span><br><span class="line">  #about: &#x2F;about&#x2F;||user</span><br><span class="line">  #tags: &#x2F;tags&#x2F;||tags</span><br><span class="line">  #categories: &#x2F;categories&#x2F;||th</span><br><span class="line">  archives: &#x2F;archives&#x2F;||archive</span><br><span class="line">  #schedule: &#x2F;schedule&#x2F;||calendar</span><br><span class="line">  #sitemap: &#x2F;sitemap.xml||sitemap</span><br><span class="line">  #commonweal: &#x2F;404&#x2F;||heartbeat</span><br></pre></td></tr></table></figure></li>
<li>4.è®¾ç½®èœå•<br>èœå•é…ç½®åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼Œç¬¬ä¸€æ˜¯èœå•é¡¹ï¼ˆåç§°å’Œé“¾æ¥ï¼‰ï¼Œç¬¬äºŒæ˜¯èœå•é¡¹çš„æ˜¾ç¤ºæ–‡æœ¬ï¼Œç¬¬ä¸‰æ˜¯èœå•é¡¹å¯¹åº”çš„å›¾æ ‡<ul>
<li>4.1è®¾å®šèœå•é¡¹çš„åç§°å’Œé“¾æ¥<br>æ‰¾åˆ°nextçš„é…ç½®æ–‡ä»¶themes\next_config.yml<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">home: &#x2F;</span><br><span class="line">archives: &#x2F;archives</span><br><span class="line">about: &#x2F;about</span><br><span class="line">categories: &#x2F;categories</span><br><span class="line">tags: &#x2F;tags</span><br></pre></td></tr></table></figure></li>
<li>4.2è®¾å®šèœå•é¡¹çš„æ˜¾ç¤ºæ–‡æœ¬<br>åœ¨è®¾ç½® èœå•é¡¹çš„åç§°å’Œé“¾æ¥ä¸­çš„åç§°å¹¶ä¸ä¼šç›´æ¥æ˜¾ç¤ºåœ¨ç½‘é¡µä¸Šï¼Œè€Œæ˜¯ä¼šé€šè¿‡ NexT ä¸»é¢˜ç›®å½•ä¸‹çš„ languages/{language}.yml æ‰¾åˆ°å¯¹åº”çš„æ˜¾ç¤ºæ–‡æœ¬ã€‚<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/5.jpg" alt="x"></li>
<li>4.3 è®¾å®šèœå•é¡¹çš„å›¾æ ‡<br>å¯¹åº”çš„å­—æ®µæ˜¯ menu_iconsã€‚ æ­¤è®¾å®šæ ¼å¼æ˜¯ <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">item name: icon name</span><br></pre></td></tr></table></figure>
ï¼Œå…¶ä¸­ item name ä¸ä¸Šä¸€æ­¥æ‰€é…ç½®çš„èœå•åå­—å¯¹åº”ï¼Œicon name æ˜¯ Font Awesome å›¾æ ‡çš„ åå­—ã€‚è€Œ enable å¯ç”¨äºæ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå›¾æ ‡ï¼Œä½ å¯ä»¥è®¾ç½®æˆ false æ¥å»æ‰å›¾æ ‡ã€‚</li>
</ul>
</li>
<li>5.ç”Ÿæˆå­é¡µé¢<br>å‘½ä»¤è¡Œè¾“å…¥<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new page xxx</span><br></pre></td></tr></table></figure>
ç„¶ååœ¨sourceé‡Œä¼šç”Ÿæˆå’Œ_postsåŒçº§çš„æ–‡ä»¶å¤¹xxx,è¿›å…¥xxxç¼–è¾‘index.md,ä¿®æ”¹æˆä»¥ä¸‹<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2020-07-09 00:58:42</span><br><span class="line">type: &quot;xxx&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
å…¶ä¸­xxxå¯ä¸ºabout/categories/tags/schedule/sitemap/404</li>
<li><p>6.nexté€‰æ‹© Scheme<br>Scheme æ˜¯ NexT æä¾›çš„ä¸€ç§ç‰¹æ€§ï¼Œå€ŸåŠ©äº Schemeï¼ŒNexT ä¸ºä½ æä¾›å¤šç§ä¸åŒçš„å¤–è§‚ã€‚åŒæ—¶ï¼Œå‡ ä¹æ‰€æœ‰çš„é…ç½®éƒ½å¯ä»¥ åœ¨ Scheme ä¹‹é—´å…±ç”¨ã€‚ç›®å‰ NexT æ”¯æŒä¸‰ç§ Schemeï¼Œä»–ä»¬æ˜¯ï¼š<br>æ‰¾åˆ°nextçš„é…ç½®æ–‡ä»¶themes\next_config.yml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Muse - é»˜è®¤ Schemeï¼Œè¿™æ˜¯ NexT æœ€åˆçš„ç‰ˆæœ¬ï¼Œé»‘ç™½ä¸»è°ƒï¼Œå¤§é‡ç•™ç™½</span><br><span class="line">Mist - Muse çš„ç´§å‡‘ç‰ˆæœ¬ï¼Œæ•´æ´æœ‰åºçš„å•æ å¤–è§‚</span><br><span class="line">Pisces - åŒæ  Schemeï¼Œå°å®¶ç¢§ç‰ä¼¼çš„æ¸…æ–°</span><br></pre></td></tr></table></figure>
<p>scheme: Muse<br>æ•ˆæœå¦‚ä¸‹<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme1.png" alt="x"><br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme1-2.png" alt="x"><br>scheme: Mist<br>æ•ˆæœå¦‚ä¸‹<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme2.png" alt="x"><br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme2-1.png" alt="x"><br>scheme: Pisces<br>æ•ˆæœå¦‚ä¸‹<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme3.png" alt="x"><br>scheme: Gemini<br>æ•ˆæœå¦‚ä¸‹<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/theme4.png" alt="x"></p>
</li>
<li><p>7.æ·»åŠ ç™¾åº¦/è°·æ­Œ/æœ¬åœ° è‡ªå®šä¹‰ç«™ç‚¹å†…å®¹æœç´¢<br>å®‰è£… hexo-generator-searchdbï¼Œåœ¨ç«™ç‚¹çš„æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
<p>ç¼–è¾‘ ç«™ç‚¹é…ç½®æ–‡ä»¶_config.ymlï¼Œæ–°å¢ä»¥ä¸‹å†…å®¹åˆ°ä»»æ„ä½ç½®ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
<p>ç¼–è¾‘ ä¸»é¢˜é…ç½®æ–‡ä»¶themes\next_config.ymlï¼Œå¯ç”¨æœ¬åœ°æœç´¢åŠŸèƒ½ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Local search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>8.ä¾§è¾¹æ ç¤¾äº¤é“¾æ¥<br>ä¾§æ ç¤¾äº¤é“¾æ¥çš„ä¿®æ”¹åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼Œç¬¬ä¸€æ˜¯é“¾æ¥ï¼Œç¬¬äºŒæ˜¯é“¾æ¥å›¾æ ‡ã€‚ ä¸¤è€…é…ç½®å‡åœ¨ ä¸»é¢˜é…ç½®æ–‡ä»¶themes\next_config.yml ä¸­ã€‚</p>
</li>
</ul>
<p>é“¾æ¥æ”¾ç½®åœ¨ social å­—æ®µä¸‹ï¼Œä¸€è¡Œä¸€ä¸ªé“¾æ¥ã€‚å…¶é”®å€¼æ ¼å¼æ˜¯ æ˜¾ç¤ºæ–‡æœ¬: é“¾æ¥åœ°å€ã€‚</p>
<p>é…ç½®ç¤ºä¾‹<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Social links</span><br><span class="line">social:</span><br><span class="line">  GitHub: https:&#x2F;&#x2F;github.com&#x2F;your-user-name</span><br><span class="line">  Twitter: https:&#x2F;&#x2F;twitter.com&#x2F;your-user-name</span><br><span class="line">  å¾®åš: http:&#x2F;&#x2F;weibo.com&#x2F;your-user-name</span><br><span class="line">  è±†ç“£: http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;your-user-name</span><br><span class="line">  çŸ¥ä¹: http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;your-user-name</span><br><span class="line">  # ç­‰ç­‰</span><br></pre></td></tr></table></figure><br>è®¾å®šé“¾æ¥çš„å›¾æ ‡ï¼Œå¯¹åº”çš„å­—æ®µæ˜¯ social_iconsã€‚å…¶é”®å€¼æ ¼å¼æ˜¯ åŒ¹é…é”®: Font Awesome å›¾æ ‡åç§°ï¼Œ åŒ¹é…é”® ä¸ä¸Šä¸€æ­¥æ‰€é…ç½®çš„é“¾æ¥çš„ æ˜¾ç¤ºæ–‡æœ¬ ç›¸åŒï¼ˆå¤§å°å†™ä¸¥æ ¼åŒ¹é…ï¼‰ï¼Œå›¾æ ‡åç§° æ˜¯ Font Awesome å›¾æ ‡çš„åå­—ï¼ˆä¸å¿…å¸¦ fa- å‰ç¼€ï¼‰ã€‚ enable é€‰é¡¹ç”¨äºæ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå›¾æ ‡ï¼Œä½ å¯ä»¥è®¾ç½®æˆ false æ¥å»æ‰å›¾æ ‡ã€‚</p>
<p>é…ç½®ç¤ºä¾‹<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Social Icons</span><br><span class="line">social_icons:</span><br><span class="line">  enable: true</span><br><span class="line">  # Icon Mappings</span><br><span class="line">  GitHub: github</span><br><span class="line">  Twitter: twitter</span><br><span class="line">  å¾®åš: weibo</span><br></pre></td></tr></table></figure></p>
<ul>
<li>9.è®¾ç½®ä»£ç é«˜äº®ä¸»é¢˜<br>NexT ä½¿ç”¨ Tomorrow Theme ä½œä¸ºä»£ç é«˜äº®ï¼Œå…±æœ‰5æ¬¾ä¸»é¢˜ä¾›ä½ é€‰æ‹©ã€‚ NexT é»˜è®¤ä½¿ç”¨çš„æ˜¯ ç™½è‰²çš„ normal ä¸»é¢˜ï¼Œå¯é€‰çš„å€¼æœ‰ normalï¼Œnightï¼Œ night blueï¼Œ night brightï¼Œ night eightiesï¼š</li>
</ul>
<p>themes\next_config.ymlæ›´æ”¹ highlight_theme å­—æ®µï¼Œå°†å…¶å€¼è®¾å®šæˆä½ æ‰€å–œçˆ±çš„é«˜äº®ä¸»é¢˜ï¼Œä¾‹å¦‚ï¼š</p>
<p>é«˜äº®ä¸»é¢˜è®¾ç½®ç¤ºä¾‹<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Code Highlight theme</span><br><span class="line"># Available value: normal | night | night eighties | night blue | night bright</span><br><span class="line"># https:&#x2F;&#x2F;github.com&#x2F;chriskempson&#x2F;tomorrow-theme</span><br><span class="line">highlight_theme: normal</span><br></pre></td></tr></table></figure></p>
<ul>
<li>10.å¼€å¯æ‰“èµåŠŸèƒ½ ç”± habren è´¡çŒ®<br>è¶Šæ¥è¶Šå¤šçš„å¹³å°ï¼ˆå¾®ä¿¡å…¬ä¼—å¹³å°ï¼Œæ–°æµªå¾®åšï¼Œç®€ä¹¦ï¼Œç™¾åº¦æ‰“èµç­‰ï¼‰æ”¯æŒæ‰“èµåŠŸèƒ½ï¼Œä»˜è´¹é˜…è¯»æ—¶ä»£è¶Šæ¥è¶Šè¿‘ï¼Œç‰¹æ­¤å¢åŠ äº†æ‰“èµåŠŸèƒ½ï¼Œæ”¯æŒå¾®ä¿¡æ‰“èµå’Œæ”¯ä»˜å®æ‰“èµã€‚ åªéœ€è¦ ä¸»é¢˜é…ç½®æ–‡ä»¶themes\next_config.yml ä¸­å¡«å…¥ å¾®ä¿¡ å’Œ æ”¯ä»˜å® æ”¶æ¬¾äºŒç»´ç å›¾ç‰‡åœ°å€ å³å¯å¼€å¯è¯¥åŠŸèƒ½ã€‚</li>
</ul>
<p>æ‰“èµåŠŸèƒ½é…ç½®ç¤ºä¾‹<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reward_comment: åšæŒåŸåˆ›æŠ€æœ¯åˆ†äº«ï¼Œæ‚¨çš„æ”¯æŒå°†é¼“åŠ±æˆ‘ç»§ç»­åˆ›ä½œï¼</span><br><span class="line">wechatpay: &#x2F;path&#x2F;to&#x2F;wechat-reward-image</span><br><span class="line">alipay: &#x2F;path&#x2F;to&#x2F;alipay-reward-image</span><br></pre></td></tr></table></figure></p>
<ul>
<li>11.é¼ æ ‡ç‚¹å‡»ç‰¹æ•ˆ<br>å°çº¢å¿ƒæ•ˆæœ<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/click1.gif" alt><br>å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š<br>åœ¨/themes/next/source/js/srcä¸‹æ–°å»ºæ–‡ä»¶ clicklove.js ï¼Œæ¥ç€æŠŠä¸‹é¢çš„ä»£ç æ‹·è´ç²˜è´´åˆ° clicklove.js æ–‡ä»¶ä¸­<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &#39;&#39;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e&#x3D;0;e&lt;d.length;e++)d[e].alpha&lt;&#x3D;0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+&#x3D;.004,d[e].alpha-&#x3D;.013,d[e].el.style.cssText&#x3D;&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t&#x3D;&quot;function&quot;&#x3D;&#x3D;typeof e.onclick&amp;&amp;e.onclick;e.onclick&#x3D;function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a&#x3D;t.createElement(&quot;div&quot;);a.className&#x3D;&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a&#x3D;t.createElement(&quot;style&quot;);a.type&#x3D;&quot;text&#x2F;css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText&#x3D;e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d&#x3D;[];e.requestAnimationFrame&#x3D;function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3&#x2F;60)&#125;&#125;(),n()&#125;(window,document);</span><br></pre></td></tr></table></figure>
åœ¨\themes\next\layout_layout.swigæ–‡ä»¶æœ«å°¾æ·»åŠ ï¼š<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- é¡µé¢ç‚¹å‡»å°çº¢å¿ƒ --&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;src&#x2F;clicklove.js&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>å¦ä¸€ç§ç‚¹å‡»æ•ˆæœå¦‚å›¾æ‰€ç¤º<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/click2.png" alt><br>åœ¨themes/next/source/js/srcé‡Œé¢å»ºä¸€ä¸ªå«fireworks.jsçš„æ–‡ä»¶ï¼Œä»£ç å¦‚ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;;</span><br></pre></td></tr></table></figure><br>æ‰“å¼€themes/next/layout/_layout.swig,åœ¨&lt;/body&gt;ä¸Šé¢å†™ä¸‹å¦‚ä¸‹ä»£ç ï¼š<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% if theme.fireworks %&#125;</span><br><span class="line">   &lt;canvas class&#x3D;&quot;fireworks&quot; style&#x3D;&quot;position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;&quot; &gt;&lt;&#x2F;canvas&gt; </span><br><span class="line">   &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;&#x2F;cdn.bootcss.com&#x2F;animejs&#x2F;2.2.0&#x2F;anime.min.js&quot;&gt;&lt;&#x2F;script&gt; </span><br><span class="line">   &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;src&#x2F;fireworks.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><br>æ‰“å¼€ä¸»é¢˜é…ç½®æ–‡ä»¶themes\next_config.ymlï¼Œåœ¨é‡Œé¢æœ€åå†™ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Fireworks</span><br><span class="line">fireworks: true</span><br></pre></td></tr></table></figure></p>
<ul>
<li>12.åŠ¨æ€èƒŒæ™¯<br><img src="/2020/07/08/hexo%E5%BC%80%E5%8F%91%E5%8D%9A%E5%AE%A2/bg.gif" alt><br>ä¸Šé¢è¿™ç§åªæ˜¯å…¶ä¸­ä¸€ç§åŠ¨æ€èƒŒæ™¯ï¼Œæ–°ç‰ˆçš„Nextä¸»é¢˜é›†æˆäº†è¯¥åŠŸèƒ½ï¼Œåªéœ€è¦åœ¨ä¸»é¢˜é…ç½®themes\next_config.ymlä¸­è®¾ç½®å¦‚ä¸‹å³å¯ï¼Œä¸‹é¢æ¯ä¸ªæ¨¡å—åªè®¾ç½®å…¶ä¸­ä¸€ä¸ªä¸ºtrueï¼Œå…·ä½“æ•ˆæœå¦‚ä½•å¯è‡ªå·±å°è¯•ï¼š<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Canvas-nest</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-nest</span><br><span class="line">canvas_nest: # ç½‘ç»œèƒŒæ™¯</span><br><span class="line">  enable: true</span><br><span class="line">  onmobile: true # display on mobile or not</span><br><span class="line">  color: &#39;0,0,0&#39; # RGB values, use &#39;,&#39; to separate</span><br><span class="line">  opacity: 0.5 # the opacity of line: 0~1</span><br><span class="line">  zIndex: -1 # z-index property of the background</span><br><span class="line">  count: 150 # the number of lines</span><br><span class="line"></span><br><span class="line"># JavaScript 3D library.</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-three</span><br><span class="line"># three_waves</span><br><span class="line">three_waves: false</span><br><span class="line"># canvas_lines</span><br><span class="line">canvas_lines: false</span><br><span class="line"># canvas_sphere</span><br><span class="line">canvas_sphere: false</span><br><span class="line"></span><br><span class="line"># Canvas-ribbon</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-ribbon</span><br><span class="line"># size: The width of the ribbon.</span><br><span class="line"># alpha: The transparency of the ribbon.</span><br><span class="line"># zIndex: The display level of the ribbon.</span><br><span class="line">canvas_ribbon:</span><br><span class="line">  enable: false</span><br><span class="line">  size: 300</span><br><span class="line">  alpha: 0.6</span><br><span class="line">  zIndex: -1</span><br></pre></td></tr></table></figure>
å¦å¤–éœ€è¦åœ¨blogä¸­ä¸‹è½½ç›¸åº”èµ„æºåŒ…ï¼Œå…·ä½“è§ä¸Šé¢çš„é“¾æ¥ï¼Œä¸‹é¢æˆ‘ç»™å‡ºcanvas_nestçš„ä¸‹è½½æ–¹å¼ï¼š<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;theme-next-canvas-nest themes&#x2F;next&#x2F;source&#x2F;lib&#x2F;canvas-nest</span><br></pre></td></tr></table></figure></li>
<li><p>13.æ–‡ç« ç»“æŸæ ‡å¿—<br>åœ¨è·¯å¾„ \themes\next\layout_macro ä¸­æ–°å»º passage-end-tag.swig æ–‡ä»¶,å¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">        &lt;div style&#x3D;&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------æœ¬æ–‡ç»“æŸ&lt;i class&#x3D;&quot;fa fa-paw&quot;&gt;&lt;&#x2F;i&gt;æ„Ÿè°¢æ‚¨çš„é˜…è¯»-------------&lt;&#x2F;div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>
<p>æ¥ç€æ‰“å¼€\themes\next\layout_macro\post.swigæ–‡ä»¶ï¼Œåœ¨post-body ä¹‹å(END POST BODY)ï¼Œ post-footer ä¹‹å‰æ·»åŠ å¦‚ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &#39;passage-end-tag.swig&#39; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>
<p>ç„¶åæ‰“å¼€ä¸»é¢˜é…ç½®æ–‡ä»¶ï¼ˆ_config.yml),åœ¨æœ«å°¾æ·»åŠ ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># æ–‡ç« æœ«å°¾æ·»åŠ â€œæœ¬æ–‡ç»“æŸâ€æ ‡è®°</span><br><span class="line">passage_end_tag:</span><br><span class="line">  enabled: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>14.æ–‡å­—é˜´å½±æ•ˆæœ<br>æ‰“å¼€\themes\next\source\css_custom\custom.styl,å‘é‡Œé¢åŠ å…¥ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; ä¸»é¡µæ–‡ç« æ·»åŠ é˜´å½±æ•ˆæœ</span><br><span class="line"> .post &#123;</span><br><span class="line">   margin-top: 60px;</span><br><span class="line">   margin-bottom: 60px;</span><br><span class="line">   padding: 25px;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>15.åšå®¢åº•éƒ¨å¸ƒå±€<br>å¯¹åº”ä¸»é¢˜é…ç½®æ–‡ä»¶themes\next_config.ymlä¸­çš„</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  since: 2020 #å»ºç«™æ—¶é—´</span><br><span class="line"></span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  icon:</span><br><span class="line">    # Icon name in fontawesome, see: https:&#x2F;&#x2F;fontawesome.com&#x2F;v4.7.0&#x2F;icons&#x2F;</span><br><span class="line">    # &#96;heart&#96; is recommended with animation in red (#ff0000).</span><br><span class="line">    name: heart  #ä½œè€…å›¾æ ‡ï¼ˆé»˜è®¤æ˜¯authoräººåƒ)</span><br><span class="line">    # If you want to animate the icon, set it to true.</span><br><span class="line">    animated: true #å›¾æ ‡æ˜¯å¦é—ªåŠ¨</span><br><span class="line">    # Change the color of icon, using Hex Code.</span><br><span class="line">    color: &quot;#808080&quot; #å›¾æ ‡é¢œè‰²</span><br><span class="line"></span><br><span class="line">  # If not defined, &#96;author&#96; from Hexo main config will be used.</span><br><span class="line">  copyright: å¼ å¸… #åˆ«å¡«boolå‹ï¼Œæœ€åæ˜¾ç¤ºçš„ä¸œè¥¿æ˜¯copyright || authorï¼Œå³å·¦è¾¹æ²¡æœ‰è®¾ç½®çš„è¯å°±æ˜¾ç¤ºä½œè€…</span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  powered:</span><br><span class="line">    # Hexo link (Powered by Hexo).</span><br><span class="line">    enable: false #æ˜¯å¦æ˜¾ç¤º Powered by hexo</span><br><span class="line">    # Version info of Hexo after Hexo link (vX.X.X).</span><br><span class="line">    version: false #æ˜¯å¦æ˜¾ç¤ºHexoç‰ˆæœ¬</span><br><span class="line"></span><br><span class="line">  theme:</span><br><span class="line">    # Theme &amp; scheme info link (Theme - NexT.scheme).</span><br><span class="line">    enable: false #æ˜¯å¦æ˜¾ç¤ºä¸»é¢˜ä¿¡æ¯</span><br><span class="line">    # Version info of NexT after scheme info (vX.X.X).</span><br><span class="line">    version: false #æ˜¯å¦æ˜¾ç¤ºä¸»é¢˜ç‰ˆæœ¬</span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  # Beian icp information for Chinese users. In China, every legal website should have a beian icp in website footer.</span><br><span class="line">  # http:&#x2F;&#x2F;www.miitbeian.gov.cn</span><br><span class="line">  beian:</span><br><span class="line">    enable: false #æ˜¯å¦æ˜¾ç¤ºç½‘ç«™å¤‡æ¡ˆä¿¡æ¯</span><br><span class="line">    icp:</span><br><span class="line"></span><br><span class="line">  # -------------------------------------------------------------</span><br><span class="line">  # Any custom text can be defined here.</span><br><span class="line">  #custom_text: Hosted by &lt;a href&#x3D;&quot;https:&#x2F;&#x2F;pages.coding.me&quot; class&#x3D;&quot;theme-link&quot; rel&#x3D;&quot;noopener&quot; target&#x3D;&quot;_blank&quot;&gt;Coding Pages&lt;&#x2F;a&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>16.æ·»åŠ é¡µé¢å® ç‰©<br>é¦–å…ˆåœ¨åšå®¢ç›®å½•ä¸‹æ‰§è¡Œï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -save hexo-helper-live2d</span><br></pre></td></tr></table></figure>
<p>ç„¶ååœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸­åŠ å…¥ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  scriptFrom: local</span><br><span class="line">  pluginRootPath: live2dw&#x2F;</span><br><span class="line">  pluginJsPath: lib&#x2F;</span><br><span class="line">  pluginModelPath: assets&#x2F;</span><br><span class="line">  tagMode: false</span><br><span class="line">  model:</span><br><span class="line">    use: live2d-widget-model-wanko  #é€‰æ‹©å“ªç§æ¨¡å‹</span><br><span class="line">  display: #æ”¾ç½®ä½ç½®å’Œå¤§å°</span><br><span class="line">    position: right</span><br><span class="line">    width: 150</span><br><span class="line">    height: 300</span><br><span class="line">  mobile:</span><br><span class="line">    show: false #æ˜¯å¦åœ¨æ‰‹æœºç«¯æ˜¾ç¤º</span><br></pre></td></tr></table></figure>
<p>ä¸Šé¢æ¨¡å‹çš„é€‰æ‹©å¯åœ¨lived2dä¸­é€‰æ‹©ï¼Œå¹¶ä¸‹è½½ç›¸åº”çš„æ¨¡å‹ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install live2d-widget-model-wanko</span><br></pre></td></tr></table></figure>
</li>
<li><p>17.è®¾ç½®åšå®¢æ‘˜è¦æ˜¾ç¤º<br>å¯¹äºæ‘˜è¦æ˜¾ç¤ºï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦å¼€å¯æ‘˜è¦åŠŸèƒ½ï¼Œä¿®æ”¹ä¸»é¢˜é…ç½®æ–‡ä»¶themes\next_config.ymlï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Automatically scroll page to section which is under &lt;!-- more --&gt; mark.</span><br><span class="line">scroll_to_more: true #é€‰å–åšå®¢æ­£æ–‡&lt;!--more--&gt;å‰çš„å†…å®¹</span><br><span class="line"></span><br><span class="line"># Automatically saving scroll position on each post&#x2F;page in cookies.</span><br><span class="line">save_scroll: false</span><br><span class="line"></span><br><span class="line"># Automatically excerpt description in homepage as preamble text.</span><br><span class="line">excerpt_description: true #è‡ªåŠ¨æˆªå–æ‘˜è¦</span><br><span class="line"></span><br><span class="line"># Automatically Excerpt. Not recommend.</span><br><span class="line"># Use &lt;!-- more --&gt; in the post to control excerpt accurately.</span><br><span class="line">auto_excerpt: </span><br><span class="line">  enable: false #è‡ªåŠ¨æˆªå–ä¸€å®šç¨‹åº¦çš„æ‘˜è¦</span><br><span class="line">  length: 150</span><br><span class="line"></span><br><span class="line"># Read more button</span><br><span class="line"># If true, the read more button would be displayed in excerpt section.</span><br><span class="line">read_more_btn: true #æ˜¾ç¤ºé˜…è¯»å…¨æ–‡æŒ‰é’®</span><br></pre></td></tr></table></figure>
</li>
<li><p>18.è®¾ç½®RSSè®¢é˜…<br>åœ¨åšå®¢ä¸»ç›®å½•ä¸‹æ‰§è¡Œï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure>
<p>åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Extensions</span><br><span class="line">## Plugins: http:&#x2F;&#x2F;hexo.io&#x2F;plugins&#x2F;</span><br><span class="line">plugins: hexo-generate-feed</span><br></pre></td></tr></table></figure>
<p>ç„¶åè®¾ç½®ä¸»é¢˜é…ç½®æ–‡ä»¶ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set rss to false to disable feed link.</span><br><span class="line"># Leave rss as empty to use site&#39;s feed link.</span><br><span class="line"># Set rss to specific value if you have burned your feed already.</span><br><span class="line">rss: &#x2F;atom.xml</span><br></pre></td></tr></table></figure>
</li>
<li><p>19.ä¿®æ”¹æ–‡ç« é“¾æ¥æ ·å¼<br>ä¿®æ”¹æ–‡ä»¶ themes\next\source\css_common\components\post\post.stylï¼Œåœ¨æœ«å°¾æ·»åŠ å¦‚ä¸‹cssæ ·å¼ï¼Œï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; æ–‡ç« å†…é“¾æ¥æ–‡æœ¬æ ·å¼</span><br><span class="line">.post-body p a&#123;</span><br><span class="line">  color: #0593d3;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  border-bottom: 1px solid #0593d3;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #fc6423;</span><br><span class="line">    border-bottom: none;</span><br><span class="line">    border-bottom: 1px solid #fc6423;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="æ”¯æŒLatex"><a href="#æ”¯æŒLatex" class="headerlink" title="æ”¯æŒLatex"></a><span id="header5">æ”¯æŒLatex</span></h1><h2 id="ç¬¬ä¸€æ­¥ï¼š-å®‰è£…Kramed"><a href="#ç¬¬ä¸€æ­¥ï¼š-å®‰è£…Kramed" class="headerlink" title="ç¬¬ä¸€æ­¥ï¼š å®‰è£…Kramed"></a>ç¬¬ä¸€æ­¥ï¼š å®‰è£…Kramed</h2><p>æ›´æ¢Hexoçš„é»˜è®¤çš„hexo-renderer-markedæ¸²æŸ“å¼•æ“ï¼Œæ”¹ä¸ºhexo-renderer-kramedã€‚åœ¨ç»ˆç«¯è¾“å…¥å‘½ä»¤å¦‚ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p>
<h2 id="ç¬¬äºŒæ­¥ï¼šæ›´æ”¹æ–‡ä»¶é…ç½®"><a href="#ç¬¬äºŒæ­¥ï¼šæ›´æ”¹æ–‡ä»¶é…ç½®" class="headerlink" title="ç¬¬äºŒæ­¥ï¼šæ›´æ”¹æ–‡ä»¶é…ç½®"></a>ç¬¬äºŒæ­¥ï¼šæ›´æ”¹æ–‡ä»¶é…ç½®</h2><p>æ‰“å¼€/node_modules/hexo-renderer-kramed/lib/renderer.js,æ›´æ”¹ï¼š<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function (text) &#123;</span><br><span class="line">    &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + 1 + $$</span><br><span class="line">    return text.replace(&#x2F;&#96;$(.*?)$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>ä¸ºï¼Œç›´æ¥è¿”å›text<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function (text) &#123;</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="ç¬¬ä¸‰æ­¥-åœæ­¢ä½¿ç”¨-hexo-mathï¼Œå¹¶å®‰è£…mathjaxåŒ…"><a href="#ç¬¬ä¸‰æ­¥-åœæ­¢ä½¿ç”¨-hexo-mathï¼Œå¹¶å®‰è£…mathjaxåŒ…" class="headerlink" title="ç¬¬ä¸‰æ­¥: åœæ­¢ä½¿ç”¨ hexo-mathï¼Œå¹¶å®‰è£…mathjaxåŒ…"></a>ç¬¬ä¸‰æ­¥: åœæ­¢ä½¿ç”¨ hexo-mathï¼Œå¹¶å®‰è£…mathjaxåŒ…</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure>
<h2 id="ç¬¬å››æ­¥-æ›´æ–°-Mathjax-çš„-é…ç½®æ–‡ä»¶"><a href="#ç¬¬å››æ­¥-æ›´æ–°-Mathjax-çš„-é…ç½®æ–‡ä»¶" class="headerlink" title="ç¬¬å››æ­¥: æ›´æ–° Mathjax çš„ é…ç½®æ–‡ä»¶"></a>ç¬¬å››æ­¥: æ›´æ–° Mathjax çš„ é…ç½®æ–‡ä»¶</h2><p>æ‰“å¼€/node_modules/hexo-renderer-mathjax/mathjax.html ï¼Œæ³¨é‡Šæ‰ç¬¬äºŒä¸ª<code>&lt;script&gt;</code></p>
<h2 id="ç¬¬äº”æ­¥-æ›´æ”¹é»˜è®¤è½¬ä¹‰è§„åˆ™"><a href="#ç¬¬äº”æ­¥-æ›´æ”¹é»˜è®¤è½¬ä¹‰è§„åˆ™" class="headerlink" title="ç¬¬äº”æ­¥: æ›´æ”¹é»˜è®¤è½¬ä¹‰è§„åˆ™"></a>ç¬¬äº”æ­¥: æ›´æ”¹é»˜è®¤è½¬ä¹‰è§„åˆ™</h2><p>æ‰“å¼€/node_modules/kramed/lib/rules/inline.js<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">escape: &#x2F;^\([\&#96;*&#123;&#125;[]()#$+-.!_&gt;])&#x2F;,</span><br><span class="line">æ›´æ”¹ä¸º</span><br><span class="line">escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">    </span><br><span class="line">em: &#x2F;^b_((?:__|[sS])+?)_b|^*((?:**|[sS])+?)*(?!*)&#x2F;,</span><br><span class="line">æ›´æ”¹ä¸º</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure></p>
<h2 id="ç¬¬å…­æ­¥-å¼€å¯mathjax"><a href="#ç¬¬å…­æ­¥-å¼€å¯mathjax" class="headerlink" title="ç¬¬å…­æ­¥: å¼€å¯mathjax"></a>ç¬¬å…­æ­¥: å¼€å¯mathjax</h2><p>æ‰“å¼€ä½ æ‰€ä½¿ç”¨ä¸»é¢˜çš„_config.ymlæ–‡ä»¶<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure></p>
<h2 id="æœ€åçš„æœ€å"><a href="#æœ€åçš„æœ€å" class="headerlink" title="æœ€åçš„æœ€å"></a>æœ€åçš„æœ€å</h2><p>åœ¨æ¯ä¸ªæ–‡ç« çš„å¼€å¤´æ·»åŠ <br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkdownåŸºç¡€è¯­æ³•</title>
    <url>/2020/07/08/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h1 id="ç›®å½•"><a href="#ç›®å½•" class="headerlink" title="ç›®å½•"></a>ç›®å½•</h1><ul>
<li>1.<a href="#header1">æ ‡é¢˜</a></li>
<li>2.<a href="#header2">å­—ä½“</a></li>
<li>3.<a href="#header3">å›¾ç‰‡</a></li>
<li>4.<a href="#header4">è¶…é“¾æ¥</a></li>
<li>5.<a href="#header5">é”šç‚¹</a></li>
<li>6.<a href="#header6">å¼•ç”¨</a></li>
<li>7.<a href="#header7">ä»£ç </a></li>
<li>8.<a href="#header8">åˆ—è¡¨</a></li>
<li>9.<a href="#header9">åˆ†å‰²çº¿</a></li>
<li>10.<a href="#header10">è¡¨æ ¼</a></li>
<li>11.<a href="#header11">æ¢è¡Œ</a></li>
<li>12.<a href="#header12">æµç¨‹å›¾</a></li>
</ul>
<a id="more"></a>

<h1 id="æ ‡é¢˜"><a href="#æ ‡é¢˜" class="headerlink" title="æ ‡é¢˜"></a><span id="header1">æ ‡é¢˜</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># æ ‡é¢˜1</span><br><span class="line">## æ ‡é¢˜2</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<h1 id="æ ‡é¢˜1"><a href="#æ ‡é¢˜1" class="headerlink" title="æ ‡é¢˜1"></a>æ ‡é¢˜1</h1><h2 id="æ ‡é¢˜2"><a href="#æ ‡é¢˜2" class="headerlink" title="æ ‡é¢˜2"></a>æ ‡é¢˜2</h2><h1 id="å­—ä½“"><a href="#å­—ä½“" class="headerlink" title="å­—ä½“"></a><span id="header2">å­—ä½“</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**è¿™æ˜¯åŠ ç²—çš„æ–‡å­—**</span><br><span class="line">*è¿™æ˜¯å€¾æ–œçš„æ–‡å­—*&#96;</span><br><span class="line">***è¿™æ˜¯æ–œä½“åŠ ç²—çš„æ–‡å­—***</span><br><span class="line">~~è¿™æ˜¯åŠ åˆ é™¤çº¿çš„æ–‡å­—~~</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<p><strong>è¿™æ˜¯åŠ ç²—çš„æ–‡å­—</strong><br><em>è¿™æ˜¯å€¾æ–œçš„æ–‡å­—</em>`<br><strong><em>è¿™æ˜¯æ–œä½“åŠ ç²—çš„æ–‡å­—</em></strong><br><del>è¿™æ˜¯åŠ åˆ é™¤çº¿çš„æ–‡å­—</del></p>
<h1 id="å›¾ç‰‡"><a href="#å›¾ç‰‡" class="headerlink" title="å›¾ç‰‡"></a><span id="header3">å›¾ç‰‡</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![æˆ‘æ˜¯ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif)</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<p><img src="https://www.baidu.com/img/dong_54209c0ff3da32eecc31f340c08a18f6.gif" alt="æˆ‘æ˜¯ALT"></p>
<h1 id="è¶…é“¾æ¥"><a href="#è¶…é“¾æ¥" class="headerlink" title="è¶…é“¾æ¥"></a><span id="header4">è¶…é“¾æ¥</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[æˆ‘æ˜¯ç™¾åº¦](www.baidu.com)</span><br><span class="line">[![æˆ‘æ˜¯ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif)](![æˆ‘æ˜¯ALT](https:&#x2F;&#x2F;www.baidu.com&#x2F;img&#x2F;dong_54209c0ff3da32eecc31f340c08a18f6.gif))</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<p><a href="http://www.baidu.com" target="_blank" rel="noopener">ç‚¹æˆ‘æˆ–è€…å›¾ç‰‡å‡å¯è·³è½¬åˆ°ç™¾åº¦</a><br><a href="http://www.baidu.com" target="_blank" rel="noopener"><img src="https://www.baidu.com/img/dong_54209c0ff3da32eecc31f340c08a18f6.gif" alt="æˆ‘æ˜¯ALT"></a></p>
<h1 id="é”šç‚¹"><a href="#é”šç‚¹" class="headerlink" title="é”šç‚¹"></a><span id="header5">é”šç‚¹</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ç‚¹æˆ‘è·³è½¬](#mao)</span><br><span class="line">&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</span><br><span class="line">&lt;span id&#x3D;&quot;ma0&quot;&gt;è·³åˆ°äº†æˆ‘&lt;&#x2F;span&gt;</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<p><a href="#mao">ç‚¹æˆ‘è·³è½¬</a><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><span id="mao">è·³åˆ°äº†æˆ‘</span></p>
<h1 id="å¼•ç”¨"><a href="#å¼•ç”¨" class="headerlink" title="å¼•ç”¨"></a><span id="header6">å¼•ç”¨</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;æˆ‘æ˜¯å¼•ç”¨1</span><br><span class="line">&gt;&gt;æˆ‘æ˜¯å¼•ç”¨2</span><br><span class="line">&gt;&gt;&gt;æˆ‘æ˜¯å¼•ç”¨3</span><br><span class="line">&gt;&gt;&gt;&gt;æˆ‘æ˜¯å¼•ç”¨4</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<blockquote>
<p>æˆ‘æ˜¯å¼•ç”¨1</p>
<blockquote>
<p>æˆ‘æ˜¯å¼•ç”¨2</p>
<blockquote>
<p>æˆ‘æ˜¯å¼•ç”¨3</p>
<blockquote>
<p>æˆ‘æ˜¯å¼•ç”¨4</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h1 id="ä»£ç "><a href="#ä»£ç " class="headerlink" title="ä»£ç "></a><span id="header7">ä»£ç </span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#96;&#96;&#96;</span><br><span class="line">console.log(1)</span><br><span class="line">&#96;&#96;&#96;</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">console.log(1)</span><br></pre></td></tr></table></figure>

<h1 id="åˆ—è¡¨"><a href="#åˆ—è¡¨" class="headerlink" title="åˆ—è¡¨"></a><span id="header8">åˆ—è¡¨</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* æ— åºåˆ—è¡¨1</span><br><span class="line">    * æ— åºåˆ—è¡¨1-1</span><br><span class="line">    * æ— åºåˆ—è¡¨1-2</span><br><span class="line">    * æ— åºåˆ—è¡¨1-3</span><br><span class="line">* æ— åºåˆ—è¡¨2</span><br><span class="line">* æ— åºåˆ—è¡¨3</span><br><span class="line">* æ— åºåˆ—è¡¨4</span><br><span class="line">* æ— åºåˆ—è¡¨5</span><br><span class="line">+ æ— åºåˆ—è¡¨6</span><br><span class="line">    + æ— åºåˆ—è¡¨6-1</span><br><span class="line">        + æ— åºåˆ—è¡¨6-1-1</span><br><span class="line">            + æ— åºåˆ—è¡¨6-1-1-1</span><br><span class="line">                + æ— åºåˆ—è¡¨6-1-1-1-1</span><br><span class="line">+ æ— åºåˆ—è¡¨7</span><br><span class="line">- æ— åºåˆ—è¡¨8</span><br><span class="line">- æ— åºåˆ—è¡¨9</span><br><span class="line">    * æ— åºåˆ—è¡¨9-1</span><br><span class="line">    * æ— åºåˆ—è¡¨9-2</span><br><span class="line">    * æ— åºåˆ—è¡¨9-3</span><br><span class="line">    * æ— åºåˆ—è¡¨9-4</span><br><span class="line">- æ— åºåˆ—è¡¨10</span><br><span class="line">- 1.æœ‰åºåˆ—è¡¨</span><br><span class="line">    - 1.1æœ‰åºåˆ—è¡¨</span><br><span class="line">    - 1.2æœ‰åºåˆ—è¡¨</span><br><span class="line">    - 1.3æœ‰åºåˆ—è¡¨</span><br><span class="line">- 2.æœ‰åºåˆ—è¡¨</span><br><span class="line">- 3.æœ‰åºåˆ—è¡¨</span><br><span class="line">- 4.æœ‰åºåˆ—è¡¨</span><br><span class="line">- 5.æœ‰åºåˆ—è¡¨</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<ul>
<li>æ— åºåˆ—è¡¨1<ul>
<li>æ— åºåˆ—è¡¨1-1</li>
<li>æ— åºåˆ—è¡¨1-2</li>
<li>æ— åºåˆ—è¡¨1-3</li>
</ul>
</li>
<li>æ— åºåˆ—è¡¨2</li>
<li>æ— åºåˆ—è¡¨3</li>
<li>æ— åºåˆ—è¡¨4</li>
<li>æ— åºåˆ—è¡¨5</li>
</ul>
<ul>
<li>æ— åºåˆ—è¡¨6<ul>
<li>æ— åºåˆ—è¡¨6-1<ul>
<li>æ— åºåˆ—è¡¨6-1-1<ul>
<li>æ— åºåˆ—è¡¨6-1-1-1<ul>
<li>æ— åºåˆ—è¡¨6-1-1-1-1</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>æ— åºåˆ—è¡¨7</li>
</ul>
<ul>
<li>æ— åºåˆ—è¡¨8</li>
<li>æ— åºåˆ—è¡¨9<ul>
<li>æ— åºåˆ—è¡¨9-1</li>
<li>æ— åºåˆ—è¡¨9-2</li>
<li>æ— åºåˆ—è¡¨9-3</li>
<li>æ— åºåˆ—è¡¨9-4</li>
</ul>
</li>
<li>æ— åºåˆ—è¡¨10</li>
<li>1.æœ‰åºåˆ—è¡¨<ul>
<li>1.1æœ‰åºåˆ—è¡¨</li>
<li>1.2æœ‰åºåˆ—è¡¨</li>
<li>1.3æœ‰åºåˆ—è¡¨</li>
</ul>
</li>
<li>2.æœ‰åºåˆ—è¡¨</li>
<li>3.æœ‰åºåˆ—è¡¨</li>
<li>4.æœ‰åºåˆ—è¡¨</li>
<li>5.æœ‰åºåˆ—è¡¨</li>
</ul>
<h1 id="åˆ†å‰²çº¿"><a href="#åˆ†å‰²çº¿" class="headerlink" title="åˆ†å‰²çº¿"></a><span id="header9">åˆ†å‰²çº¿</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">****</span><br><span class="line">*****</span><br><span class="line">******</span><br><span class="line">----</span><br><span class="line">-----</span><br><span class="line">------</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<hr>
<hr>
<hr>
<hr>
<hr>
<hr>
<h1 id="è¡¨æ ¼"><a href="#è¡¨æ ¼" class="headerlink" title="è¡¨æ ¼"></a><span id="header10">è¡¨æ ¼</span></h1><ul>
<li>ç¬¬äºŒè¡Œåˆ†å‰²è¡¨å¤´å’Œå†…å®¹ã€‚</li>
<li>- æœ‰ä¸€ä¸ªå°±è¡Œï¼Œä¸ºäº†å¯¹é½ï¼Œå¤šåŠ äº†å‡ ä¸ª</li>
<li>æ–‡å­—é»˜è®¤å±…å·¦</li>
<li>-ä¸¤è¾¹åŠ ï¼šè¡¨ç¤ºæ–‡å­—å±…ä¸­</li>
<li>-å³è¾¹åŠ ï¼šè¡¨ç¤ºæ–‡å­—å±…å³</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">å§“å|æ€§åˆ«|å¹´é¾„|æˆ˜æ–—åŠ›</span><br><span class="line">:-:|:-|-:|-</span><br><span class="line">å¼ é£123|ç”·boy|100|2001</span><br><span class="line">åˆ˜å¤‡45|ç”·girl|10|101</span><br><span class="line">å…³ç¾½67890|ç”·å•Šå•Šå•Š|1000|5001</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<table>
<thead>
<tr>
<th align="center">å§“å</th>
<th align="left">æ€§åˆ«</th>
<th align="right">å¹´é¾„</th>
<th>æˆ˜æ–—åŠ›</th>
</tr>
</thead>
<tbody><tr>
<td align="center">å¼ é£123</td>
<td align="left">ç”·boy</td>
<td align="right">100</td>
<td>2001</td>
</tr>
<tr>
<td align="center">åˆ˜å¤‡45</td>
<td align="left">ç”·girl</td>
<td align="right">10</td>
<td>101</td>
</tr>
<tr>
<td align="center">å…³ç¾½67890</td>
<td align="left">ç”·å•Šå•Šå•Š</td>
<td align="right">1000</td>
<td>5001</td>
</tr>
</tbody></table>
<h1 id="æ¢è¡Œ"><a href="#æ¢è¡Œ" class="headerlink" title="æ¢è¡Œ"></a><span id="header11">æ¢è¡Œ</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">abcd</span><br><span class="line">efg</span><br><span class="line">&lt;br&gt;</span><br><span class="line">abcd</span><br><span class="line"></span><br><span class="line">efg</span><br><span class="line">&lt;br&gt;</span><br><span class="line">abcd&lt;br&gt;</span><br><span class="line">efg</span><br></pre></td></tr></table></figure>
<p>æ•ˆæœ</p>
<p>abcd<br>efg<br><br><br>abcd</p>
<p>efg<br><br><br>abcd<br><br>efg</p>
<h1 id="æµç¨‹å›¾-éƒ¨åˆ†ç½‘ç«™å¦‚githubä¸æ”¯æŒ-VS-codeçš„MDé¢„è§ˆæ’ä»¶ä¸æ”¯æŒ"><a href="#æµç¨‹å›¾-éƒ¨åˆ†ç½‘ç«™å¦‚githubä¸æ”¯æŒ-VS-codeçš„MDé¢„è§ˆæ’ä»¶ä¸æ”¯æŒ" class="headerlink" title="æµç¨‹å›¾(éƒ¨åˆ†ç½‘ç«™å¦‚githubä¸æ”¯æŒ,VS codeçš„MDé¢„è§ˆæ’ä»¶ä¸æ”¯æŒ)"></a><span id="header12">æµç¨‹å›¾(éƒ¨åˆ†ç½‘ç«™å¦‚githubä¸æ”¯æŒ,VS codeçš„MDé¢„è§ˆæ’ä»¶ä¸æ”¯æŒ)</span></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flow</span><br><span class="line">st&#x3D;&gt;start: Start</span><br><span class="line">op&#x3D;&gt;operation: Your Operation</span><br><span class="line">cond&#x3D;&gt;condition: Yes or No?</span><br><span class="line">e&#x3D;&gt;end</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>


<p>flow<br>st=&gt;start: Start<br>op=&gt;operation: Your Operation<br>cond=&gt;condition: Yes or No?<br>e=&gt;end<br>st-&gt;op-&gt;cond<br>cond(yes)-&gt;e<br>cond(no)-&gt;op</p>
]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>opencvä¸­çš„å›¾åƒå¤„ç†1</title>
    <url>/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">æ”¹å˜é¢œè‰²ç©ºé—´</a></li>
<li>2.<a href="#header2">å›¾åƒå‡ ä½•å˜æ¢</a></li>
<li>3.<a href="#header3">å›¾åƒé˜ˆå€¼</a></li>
<li>4.<a href="#header4">å›¾åƒå¹³æ»‘</a><a id="more"></a>

</li>
</ul>
<h1 id="æ”¹å˜é¢œè‰²ç©ºé—´"><a href="#æ”¹å˜é¢œè‰²ç©ºé—´" class="headerlink" title="æ”¹å˜é¢œè‰²ç©ºé—´"></a><span id="header1">æ”¹å˜é¢œè‰²ç©ºé—´</span></h1><h2 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><ul>
<li>åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•å°†å›¾åƒä»ä¸€ä¸ªè‰²å½©ç©ºé—´è½¬æ¢åˆ°å¦ä¸€ä¸ªï¼ŒåƒBGRâ†”ç°è‰²ï¼ŒBGRâ†”HSVç­‰</li>
<li>é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œä»¥æå–è§†é¢‘ä¸­çš„å½©è‰²å¯¹è±¡</li>
<li>ä½ å°†å­¦ä¹ ä»¥ä¸‹åŠŸèƒ½ï¼šcv.cvtColorï¼Œ<strong>cv.inRange</strong>ç­‰ã€‚</li>
</ul>
<h2 id="æ”¹å˜é¢œè‰²ç©ºé—´-1"><a href="#æ”¹å˜é¢œè‰²ç©ºé—´-1" class="headerlink" title="æ”¹å˜é¢œè‰²ç©ºé—´"></a>æ”¹å˜é¢œè‰²ç©ºé—´</h2><p>OpenCVä¸­æœ‰è¶…è¿‡150ç§é¢œè‰²ç©ºé—´è½¬æ¢æ–¹æ³•ã€‚ä½†æ˜¯æˆ‘ä»¬å°†ç ”ç©¶åªæœ‰ä¸¤ä¸ªæœ€å¹¿æ³›ä½¿ç”¨çš„,BGRâ†”ç°è‰²å’ŒBGRâ†”HSVã€‚</p>
<p>å¯¹äºé¢œè‰²è½¬æ¢ï¼Œæˆ‘ä»¬ä½¿ç”¨cvå‡½æ•°ã€‚cvtColor(input_image, flag)ï¼Œå…¶ä¸­flagå†³å®šè½¬æ¢çš„ç±»å‹ã€‚</p>
<p>å¯¹äºBGRâ†’ç°åº¦è½¬æ¢ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å¿—cv.COLOR_BGR2GRAYã€‚ç±»ä¼¼åœ°ï¼Œå¯¹äºBGRâ†’HSVï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å¿—cv.COLOR_BGR2HSVã€‚</p>
<p>è¦è·å–å…¶ä»–æ ‡è®°ï¼Œåªéœ€åœ¨Pythonç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤</p>
<p>æ³¨æ„ HSVçš„è‰²ç›¸èŒƒå›´ä¸º[0,179]ï¼Œé¥±å’Œåº¦èŒƒå›´ä¸º[0,255]ï¼Œå€¼èŒƒå›´ä¸º[0,255]ã€‚ä¸åŒçš„è½¯ä»¶ä½¿ç”¨ä¸åŒçš„è§„æ¨¡ã€‚</p>
<p>å› æ­¤ï¼Œå¦‚æœä½ è¦å°†OpenCVå€¼å’Œå®ƒä»¬æ¯”è¾ƒï¼Œä½ éœ€è¦å°†è¿™äº›èŒƒå›´æ ‡å‡†åŒ–ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">flags = [i <span class="keyword">for</span> i <span class="keyword">in</span> dir(cv) <span class="keyword">if</span> i.startswith(<span class="string">'COLOR_'</span>)]</span><br><span class="line">flags</span><br></pre></td></tr></table></figure>




<pre><code>[&apos;COLOR_BAYER_BG2BGR&apos;,
 &apos;COLOR_BAYER_BG2BGRA&apos;,
 &apos;COLOR_BAYER_BG2BGR_EA&apos;,
 &apos;COLOR_BAYER_BG2BGR_VNG&apos;,
 &apos;COLOR_BAYER_BG2GRAY&apos;,
 &apos;COLOR_BAYER_BG2RGB&apos;,
 &apos;COLOR_BAYER_BG2RGBA&apos;,
 &apos;COLOR_BAYER_BG2RGB_EA&apos;,
 &apos;COLOR_BAYER_BG2RGB_VNG&apos;,
 &apos;COLOR_BAYER_GB2BGR&apos;,
 &apos;COLOR_BAYER_GB2BGRA&apos;,
 &apos;COLOR_BAYER_GB2BGR_EA&apos;,
 &apos;COLOR_BAYER_GB2BGR_VNG&apos;,
 &apos;COLOR_BAYER_GB2GRAY&apos;,
 &apos;COLOR_BAYER_GB2RGB&apos;,
 &apos;COLOR_BAYER_GB2RGBA&apos;,
 &apos;COLOR_BAYER_GB2RGB_EA&apos;,
 &apos;COLOR_BAYER_GB2RGB_VNG&apos;,
 &apos;COLOR_BAYER_GR2BGR&apos;,
 &apos;COLOR_BAYER_GR2BGRA&apos;,
 &apos;COLOR_BAYER_GR2BGR_EA&apos;,
 &apos;COLOR_BAYER_GR2BGR_VNG&apos;,
 &apos;COLOR_BAYER_GR2GRAY&apos;,
 &apos;COLOR_BAYER_GR2RGB&apos;,
 &apos;COLOR_BAYER_GR2RGBA&apos;,
 &apos;COLOR_BAYER_GR2RGB_EA&apos;,
 &apos;COLOR_BAYER_GR2RGB_VNG&apos;,
 &apos;COLOR_BAYER_RG2BGR&apos;,
 &apos;COLOR_BAYER_RG2BGRA&apos;,
 &apos;COLOR_BAYER_RG2BGR_EA&apos;,
 &apos;COLOR_BAYER_RG2BGR_VNG&apos;,
 &apos;COLOR_BAYER_RG2GRAY&apos;,
 &apos;COLOR_BAYER_RG2RGB&apos;,
 &apos;COLOR_BAYER_RG2RGBA&apos;,
 &apos;COLOR_BAYER_RG2RGB_EA&apos;,
 &apos;COLOR_BAYER_RG2RGB_VNG&apos;,
 &apos;COLOR_BGR2BGR555&apos;,
 &apos;COLOR_BGR2BGR565&apos;,
 &apos;COLOR_BGR2BGRA&apos;,
 &apos;COLOR_BGR2GRAY&apos;,
 &apos;COLOR_BGR2HLS&apos;,
 &apos;COLOR_BGR2HLS_FULL&apos;,
 &apos;COLOR_BGR2HSV&apos;,
 &apos;COLOR_BGR2HSV_FULL&apos;,
 &apos;COLOR_BGR2LAB&apos;,
 &apos;COLOR_BGR2LUV&apos;,
 &apos;COLOR_BGR2Lab&apos;,
 &apos;COLOR_BGR2Luv&apos;,
 &apos;COLOR_BGR2RGB&apos;,
 &apos;COLOR_BGR2RGBA&apos;,
 &apos;COLOR_BGR2XYZ&apos;,
 &apos;COLOR_BGR2YCR_CB&apos;,
 &apos;COLOR_BGR2YCrCb&apos;,
 &apos;COLOR_BGR2YUV&apos;,
 &apos;COLOR_BGR2YUV_I420&apos;,
 &apos;COLOR_BGR2YUV_IYUV&apos;,
 &apos;COLOR_BGR2YUV_YV12&apos;,
 &apos;COLOR_BGR5552BGR&apos;,
 &apos;COLOR_BGR5552BGRA&apos;,
 &apos;COLOR_BGR5552GRAY&apos;,
 &apos;COLOR_BGR5552RGB&apos;,
 &apos;COLOR_BGR5552RGBA&apos;,
 &apos;COLOR_BGR5652BGR&apos;,
 &apos;COLOR_BGR5652BGRA&apos;,
 &apos;COLOR_BGR5652GRAY&apos;,
 &apos;COLOR_BGR5652RGB&apos;,
 &apos;COLOR_BGR5652RGBA&apos;,
 &apos;COLOR_BGRA2BGR&apos;,
 &apos;COLOR_BGRA2BGR555&apos;,
 &apos;COLOR_BGRA2BGR565&apos;,
 &apos;COLOR_BGRA2GRAY&apos;,
 &apos;COLOR_BGRA2RGB&apos;,
 &apos;COLOR_BGRA2RGBA&apos;,
 &apos;COLOR_BGRA2YUV_I420&apos;,
 &apos;COLOR_BGRA2YUV_IYUV&apos;,
 &apos;COLOR_BGRA2YUV_YV12&apos;,
 &apos;COLOR_BayerBG2BGR&apos;,
 &apos;COLOR_BayerBG2BGRA&apos;,
 &apos;COLOR_BayerBG2BGR_EA&apos;,
 &apos;COLOR_BayerBG2BGR_VNG&apos;,
 &apos;COLOR_BayerBG2GRAY&apos;,
 &apos;COLOR_BayerBG2RGB&apos;,
 &apos;COLOR_BayerBG2RGBA&apos;,
 &apos;COLOR_BayerBG2RGB_EA&apos;,
 &apos;COLOR_BayerBG2RGB_VNG&apos;,
 &apos;COLOR_BayerGB2BGR&apos;,
 &apos;COLOR_BayerGB2BGRA&apos;,
 &apos;COLOR_BayerGB2BGR_EA&apos;,
 &apos;COLOR_BayerGB2BGR_VNG&apos;,
 &apos;COLOR_BayerGB2GRAY&apos;,
 &apos;COLOR_BayerGB2RGB&apos;,
 &apos;COLOR_BayerGB2RGBA&apos;,
 &apos;COLOR_BayerGB2RGB_EA&apos;,
 &apos;COLOR_BayerGB2RGB_VNG&apos;,
 &apos;COLOR_BayerGR2BGR&apos;,
 &apos;COLOR_BayerGR2BGRA&apos;,
 &apos;COLOR_BayerGR2BGR_EA&apos;,
 &apos;COLOR_BayerGR2BGR_VNG&apos;,
 &apos;COLOR_BayerGR2GRAY&apos;,
 &apos;COLOR_BayerGR2RGB&apos;,
 &apos;COLOR_BayerGR2RGBA&apos;,
 &apos;COLOR_BayerGR2RGB_EA&apos;,
 &apos;COLOR_BayerGR2RGB_VNG&apos;,
 &apos;COLOR_BayerRG2BGR&apos;,
 &apos;COLOR_BayerRG2BGRA&apos;,
 &apos;COLOR_BayerRG2BGR_EA&apos;,
 &apos;COLOR_BayerRG2BGR_VNG&apos;,
 &apos;COLOR_BayerRG2GRAY&apos;,
 &apos;COLOR_BayerRG2RGB&apos;,
 &apos;COLOR_BayerRG2RGBA&apos;,
 &apos;COLOR_BayerRG2RGB_EA&apos;,
 &apos;COLOR_BayerRG2RGB_VNG&apos;,
 &apos;COLOR_COLORCVT_MAX&apos;,
 &apos;COLOR_GRAY2BGR&apos;,
 &apos;COLOR_GRAY2BGR555&apos;,
 &apos;COLOR_GRAY2BGR565&apos;,
 &apos;COLOR_GRAY2BGRA&apos;,
 &apos;COLOR_GRAY2RGB&apos;,
 &apos;COLOR_GRAY2RGBA&apos;,
 &apos;COLOR_HLS2BGR&apos;,
 &apos;COLOR_HLS2BGR_FULL&apos;,
 &apos;COLOR_HLS2RGB&apos;,
 &apos;COLOR_HLS2RGB_FULL&apos;,
 &apos;COLOR_HSV2BGR&apos;,
 &apos;COLOR_HSV2BGR_FULL&apos;,
 &apos;COLOR_HSV2RGB&apos;,
 &apos;COLOR_HSV2RGB_FULL&apos;,
 &apos;COLOR_LAB2BGR&apos;,
 &apos;COLOR_LAB2LBGR&apos;,
 &apos;COLOR_LAB2LRGB&apos;,
 &apos;COLOR_LAB2RGB&apos;,
 &apos;COLOR_LBGR2LAB&apos;,
 &apos;COLOR_LBGR2LUV&apos;,
 &apos;COLOR_LBGR2Lab&apos;,
 &apos;COLOR_LBGR2Luv&apos;,
 &apos;COLOR_LRGB2LAB&apos;,
 &apos;COLOR_LRGB2LUV&apos;,
 &apos;COLOR_LRGB2Lab&apos;,
 &apos;COLOR_LRGB2Luv&apos;,
 &apos;COLOR_LUV2BGR&apos;,
 &apos;COLOR_LUV2LBGR&apos;,
 &apos;COLOR_LUV2LRGB&apos;,
 &apos;COLOR_LUV2RGB&apos;,
 &apos;COLOR_Lab2BGR&apos;,
 &apos;COLOR_Lab2LBGR&apos;,
 &apos;COLOR_Lab2LRGB&apos;,
 &apos;COLOR_Lab2RGB&apos;,
 &apos;COLOR_Luv2BGR&apos;,
 &apos;COLOR_Luv2LBGR&apos;,
 &apos;COLOR_Luv2LRGB&apos;,
 &apos;COLOR_Luv2RGB&apos;,
 &apos;COLOR_M_RGBA2RGBA&apos;,
 &apos;COLOR_RGB2BGR&apos;,
 &apos;COLOR_RGB2BGR555&apos;,
 &apos;COLOR_RGB2BGR565&apos;,
 &apos;COLOR_RGB2BGRA&apos;,
 &apos;COLOR_RGB2GRAY&apos;,
 &apos;COLOR_RGB2HLS&apos;,
 &apos;COLOR_RGB2HLS_FULL&apos;,
 &apos;COLOR_RGB2HSV&apos;,
 &apos;COLOR_RGB2HSV_FULL&apos;,
 &apos;COLOR_RGB2LAB&apos;,
 &apos;COLOR_RGB2LUV&apos;,
 &apos;COLOR_RGB2Lab&apos;,
 &apos;COLOR_RGB2Luv&apos;,
 &apos;COLOR_RGB2RGBA&apos;,
 &apos;COLOR_RGB2XYZ&apos;,
 &apos;COLOR_RGB2YCR_CB&apos;,
 &apos;COLOR_RGB2YCrCb&apos;,
 &apos;COLOR_RGB2YUV&apos;,
 &apos;COLOR_RGB2YUV_I420&apos;,
 &apos;COLOR_RGB2YUV_IYUV&apos;,
 &apos;COLOR_RGB2YUV_YV12&apos;,
 &apos;COLOR_RGBA2BGR&apos;,
 &apos;COLOR_RGBA2BGR555&apos;,
 &apos;COLOR_RGBA2BGR565&apos;,
 &apos;COLOR_RGBA2BGRA&apos;,
 &apos;COLOR_RGBA2GRAY&apos;,
 &apos;COLOR_RGBA2M_RGBA&apos;,
 &apos;COLOR_RGBA2RGB&apos;,
 &apos;COLOR_RGBA2YUV_I420&apos;,
 &apos;COLOR_RGBA2YUV_IYUV&apos;,
 &apos;COLOR_RGBA2YUV_YV12&apos;,
 &apos;COLOR_RGBA2mRGBA&apos;,
 &apos;COLOR_XYZ2BGR&apos;,
 &apos;COLOR_XYZ2RGB&apos;,
 &apos;COLOR_YCR_CB2BGR&apos;,
 &apos;COLOR_YCR_CB2RGB&apos;,
 &apos;COLOR_YCrCb2BGR&apos;,
 &apos;COLOR_YCrCb2RGB&apos;,
 &apos;COLOR_YUV2BGR&apos;,
 &apos;COLOR_YUV2BGRA_I420&apos;,
 &apos;COLOR_YUV2BGRA_IYUV&apos;,
 &apos;COLOR_YUV2BGRA_NV12&apos;,
 &apos;COLOR_YUV2BGRA_NV21&apos;,
 &apos;COLOR_YUV2BGRA_UYNV&apos;,
 &apos;COLOR_YUV2BGRA_UYVY&apos;,
 &apos;COLOR_YUV2BGRA_Y422&apos;,
 &apos;COLOR_YUV2BGRA_YUNV&apos;,
 &apos;COLOR_YUV2BGRA_YUY2&apos;,
 &apos;COLOR_YUV2BGRA_YUYV&apos;,
 &apos;COLOR_YUV2BGRA_YV12&apos;,
 &apos;COLOR_YUV2BGRA_YVYU&apos;,
 &apos;COLOR_YUV2BGR_I420&apos;,
 &apos;COLOR_YUV2BGR_IYUV&apos;,
 &apos;COLOR_YUV2BGR_NV12&apos;,
 &apos;COLOR_YUV2BGR_NV21&apos;,
 &apos;COLOR_YUV2BGR_UYNV&apos;,
 &apos;COLOR_YUV2BGR_UYVY&apos;,
 &apos;COLOR_YUV2BGR_Y422&apos;,
 &apos;COLOR_YUV2BGR_YUNV&apos;,
 &apos;COLOR_YUV2BGR_YUY2&apos;,
 &apos;COLOR_YUV2BGR_YUYV&apos;,
 &apos;COLOR_YUV2BGR_YV12&apos;,
 &apos;COLOR_YUV2BGR_YVYU&apos;,
 &apos;COLOR_YUV2GRAY_420&apos;,
 &apos;COLOR_YUV2GRAY_I420&apos;,
 &apos;COLOR_YUV2GRAY_IYUV&apos;,
 &apos;COLOR_YUV2GRAY_NV12&apos;,
 &apos;COLOR_YUV2GRAY_NV21&apos;,
 &apos;COLOR_YUV2GRAY_UYNV&apos;,
 &apos;COLOR_YUV2GRAY_UYVY&apos;,
 &apos;COLOR_YUV2GRAY_Y422&apos;,
 &apos;COLOR_YUV2GRAY_YUNV&apos;,
 &apos;COLOR_YUV2GRAY_YUY2&apos;,
 &apos;COLOR_YUV2GRAY_YUYV&apos;,
 &apos;COLOR_YUV2GRAY_YV12&apos;,
 &apos;COLOR_YUV2GRAY_YVYU&apos;,
 &apos;COLOR_YUV2RGB&apos;,
 &apos;COLOR_YUV2RGBA_I420&apos;,
 &apos;COLOR_YUV2RGBA_IYUV&apos;,
 &apos;COLOR_YUV2RGBA_NV12&apos;,
 &apos;COLOR_YUV2RGBA_NV21&apos;,
 &apos;COLOR_YUV2RGBA_UYNV&apos;,
 &apos;COLOR_YUV2RGBA_UYVY&apos;,
 &apos;COLOR_YUV2RGBA_Y422&apos;,
 &apos;COLOR_YUV2RGBA_YUNV&apos;,
 &apos;COLOR_YUV2RGBA_YUY2&apos;,
 &apos;COLOR_YUV2RGBA_YUYV&apos;,
 &apos;COLOR_YUV2RGBA_YV12&apos;,
 &apos;COLOR_YUV2RGBA_YVYU&apos;,
 &apos;COLOR_YUV2RGB_I420&apos;,
 &apos;COLOR_YUV2RGB_IYUV&apos;,
 &apos;COLOR_YUV2RGB_NV12&apos;,
 &apos;COLOR_YUV2RGB_NV21&apos;,
 &apos;COLOR_YUV2RGB_UYNV&apos;,
 &apos;COLOR_YUV2RGB_UYVY&apos;,
 &apos;COLOR_YUV2RGB_Y422&apos;,
 &apos;COLOR_YUV2RGB_YUNV&apos;,
 &apos;COLOR_YUV2RGB_YUY2&apos;,
 &apos;COLOR_YUV2RGB_YUYV&apos;,
 &apos;COLOR_YUV2RGB_YV12&apos;,
 &apos;COLOR_YUV2RGB_YVYU&apos;,
 &apos;COLOR_YUV420P2BGR&apos;,
 &apos;COLOR_YUV420P2BGRA&apos;,
 &apos;COLOR_YUV420P2GRAY&apos;,
 &apos;COLOR_YUV420P2RGB&apos;,
 &apos;COLOR_YUV420P2RGBA&apos;,
 &apos;COLOR_YUV420SP2BGR&apos;,
 &apos;COLOR_YUV420SP2BGRA&apos;,
 &apos;COLOR_YUV420SP2GRAY&apos;,
 &apos;COLOR_YUV420SP2RGB&apos;,
 &apos;COLOR_YUV420SP2RGBA&apos;,
 &apos;COLOR_YUV420p2BGR&apos;,
 &apos;COLOR_YUV420p2BGRA&apos;,
 &apos;COLOR_YUV420p2GRAY&apos;,
 &apos;COLOR_YUV420p2RGB&apos;,
 &apos;COLOR_YUV420p2RGBA&apos;,
 &apos;COLOR_YUV420sp2BGR&apos;,
 &apos;COLOR_YUV420sp2BGRA&apos;,
 &apos;COLOR_YUV420sp2GRAY&apos;,
 &apos;COLOR_YUV420sp2RGB&apos;,
 &apos;COLOR_YUV420sp2RGBA&apos;,
 &apos;COLOR_mRGBA2RGBA&apos;]</code></pre><h2 id="å¦‚ä½•æ‰¾åˆ°è¦è¿½è¸ªçš„HSVå€¼ï¼Ÿ"><a href="#å¦‚ä½•æ‰¾åˆ°è¦è¿½è¸ªçš„HSVå€¼ï¼Ÿ" class="headerlink" title="å¦‚ä½•æ‰¾åˆ°è¦è¿½è¸ªçš„HSVå€¼ï¼Ÿ"></a>å¦‚ä½•æ‰¾åˆ°è¦è¿½è¸ªçš„HSVå€¼ï¼Ÿ</h2><p>è¿™æ˜¯åœ¨stackoverflow.comä¸Šå‘ç°çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚å®ƒéå¸¸ç®€å•ï¼Œä½ å¯ä»¥ä½¿ç”¨ç›¸åŒçš„å‡½æ•°<strong>cv.cvtColor()</strong>ã€‚</p>
<p>ä½ åªéœ€ä¼ é€’ä½ æƒ³è¦çš„BGRå€¼ï¼Œè€Œä¸æ˜¯ä¼ é€’å›¾åƒã€‚ä¾‹å¦‚ï¼Œè¦æŸ¥æ‰¾ç»¿è‰²çš„HSVå€¼ï¼Œè¯·åœ¨Pythonç»ˆç«¯ä¸­å°è¯•ä»¥ä¸‹å‘½ä»¤</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">green = np.uint8([[[<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>]]])</span><br><span class="line">hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)</span><br><span class="line">hsv_green</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 60, 255, 255]]], dtype=uint8)</code></pre><h1 id="å›¾åƒçš„å‡ ä½•å˜æ¢"><a href="#å›¾åƒçš„å‡ ä½•å˜æ¢" class="headerlink" title="å›¾åƒçš„å‡ ä½•å˜æ¢"></a><span id="header2">å›¾åƒçš„å‡ ä½•å˜æ¢</span></h1><h2 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>å­¦ä¹ å°†ä¸åŒçš„å‡ ä½•å˜æ¢åº”ç”¨åˆ°å›¾åƒä¸Šï¼Œå¦‚å¹³ç§»ã€æ—‹è½¬ã€ä»¿å°„å˜æ¢ç­‰ã€‚</p>
<p>ä½ ä¼šçœ‹åˆ°è¿™äº›å‡½æ•°: cv.getPerspectiveTransform</p>
<h2 id="å˜æ¢"><a href="#å˜æ¢" class="headerlink" title="å˜æ¢"></a>å˜æ¢</h2><p>OpenCVæä¾›äº†ä¸¤ä¸ªè½¬æ¢å‡½æ•°<strong>cv.warpAffine</strong>å’Œ<strong>cv.warpPerspective</strong>ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒä»¬è¿›è¡Œå„ç§è½¬æ¢ã€‚</p>
<p><strong>cv.warpAffine</strong>é‡‡ç”¨2x3è½¬æ¢çŸ©é˜µï¼Œè€Œ<strong>cv.warpPerspective</strong>é‡‡ç”¨3x3è½¬æ¢çŸ©é˜µä½œä¸ºè¾“å…¥ã€‚</p>
<h2 id="ç¼©æ”¾"><a href="#ç¼©æ”¾" class="headerlink" title="ç¼©æ”¾"></a>ç¼©æ”¾</h2><p>ç¼©æ”¾åªæ˜¯è°ƒæ•´å›¾åƒçš„å¤§å°ã€‚ä¸ºæ­¤ï¼ŒOpenCVå¸¦æœ‰ä¸€ä¸ªå‡½æ•°**cv.resize()ã€‚å›¾åƒçš„å¤§å°å¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼Œä¹Ÿå¯ä»¥æŒ‡å®šç¼©æ”¾æ¯”ä¾‹ã€‚</p>
<p>ä¹Ÿå¯ä½¿ç”¨ä¸åŒçš„æ’å€¼æ–¹æ³•ã€‚é¦–é€‰çš„æ’å€¼æ–¹æ³•æ˜¯<strong>cv.INTER_AREA</strong>ç”¨äºç¼©å°ï¼Œ<strong>cv.INTER_CUBICï¼ˆæ…¢ï¼‰å’Œ</strong>cv.INTER_LINEAR**ç”¨äºç¼©æ”¾ã€‚</p>
<p>é»˜è®¤æƒ…å†µä¸‹ï¼Œå‡ºäºæ‰€æœ‰è°ƒæ•´å¤§å°çš„ç›®çš„ï¼Œä½¿ç”¨çš„æ’å€¼æ–¹æ³•ä¸º<strong>cv.INTER_LINEAR</strong>ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•è°ƒæ•´è¾“å…¥å›¾åƒçš„å¤§å°</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">res = cv.resize(img,<span class="literal">None</span>,fx=<span class="number">2</span>, fy=<span class="number">2</span>, interpolation = cv.INTER_AREA)</span><br><span class="line">cv.imshow(<span class="string">'res1'</span>,res)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"><span class="comment">#æˆ–è€…</span></span><br><span class="line">height, width = img.shape[:<span class="number">2</span>]</span><br><span class="line">res = cv.resize(img,(<span class="number">2</span>*width, <span class="number">2</span>*height), interpolation = cv.INTER_CUBIC)</span><br><span class="line">cv.imshow(<span class="string">'res2'</span>,res)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="å¹³ç§»"><a href="#å¹³ç§»" class="headerlink" title="å¹³ç§»"></a>å¹³ç§»</h2><p>å¹³ç§»æ˜¯ç‰©ä½“ä½ç½®çš„ç§»åŠ¨ã€‚å¦‚æœæ‚¨çŸ¥é“åœ¨(x,y)æ–¹å‘ä¸Šçš„ä½ç§»ï¼Œåˆ™å°†å…¶è®¾ä¸º(tx,ty)ï¼Œä½ å¯ä»¥åˆ›å»ºè½¬æ¢çŸ©é˜µMï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<p>M = $$<br> \left[<br> \begin{matrix}<br>   1 &amp; 0 &amp; tx \<br>   0 &amp; 1 &amp; ty<br>  \end{matrix}<br>  \right] <br>$$<br>æ‚¨å¯ä»¥å°†å…¶æ”¾å…¥<strong>np.float32</strong>ç±»å‹çš„Numpyæ•°ç»„ä¸­ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™<strong>cv.warpAffine</strong>å‡½æ•°ã€‚</p>
<p>å‚è§ä¸‹é¢åç§»ä¸º(100, 50)çš„ç¤ºä¾‹ï¼š</p>
<p><strong>cv.warpAffine</strong>å‡½æ•°çš„ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯è¾“å‡ºå›¾åƒçš„å¤§å°ï¼Œå…¶å½¢å¼åº”ä¸º(widthï¼Œheight)ã€‚è®°ä½width =åˆ—æ•°ï¼Œheight =è¡Œæ•°ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">rows,cols = img.shape</span><br><span class="line">M = np.float32([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">100</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">50</span>]])</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="æ—‹è½¬"><a href="#æ—‹è½¬" class="headerlink" title="æ—‹è½¬"></a>æ—‹è½¬</h2><p>å›¾åƒæ—‹è½¬è§’åº¦ä¸ºÎ¸æ˜¯é€šè¿‡ä»¥ä¸‹å½¢å¼çš„å˜æ¢çŸ©é˜µå®ç°çš„ï¼š</p>
<p>M=$$<br> \left[<br> \begin{matrix}<br>   \cos \theta &amp; \sin \theta  \<br>   \sin \theta &amp; \cos \theta<br>  \end{matrix}<br>  \right] <br>$$<br>ä½†æ˜¯OpenCVæä¾›äº†å¯ç¼©æ”¾çš„æ—‹è½¬ä»¥åŠå¯è°ƒæ•´çš„æ—‹è½¬ä¸­å¿ƒï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨è‡ªå·±å–œæ¬¢çš„ä»»ä½•ä½ç½®æ—‹è½¬ã€‚ä¿®æ”¹åçš„å˜æ¢çŸ©é˜µä¸º</p>
<p>$$<br> \left[<br> \begin{matrix}<br>   \alpha &amp; \beta &amp; (1-\alpha)Â·centerÂ·x-\betaÂ·centerÂ·y \<br>   -\beta &amp; \alpha &amp; \betaÂ·centerÂ·x+(1-\alpha)Â·centerÂ·y<br>  \end{matrix}<br>  \right] <br>$$<br>å…¶ä¸­ï¼š</p>
<p>$$Î±=scaleâ‹…\cos \theta,Î²=scaleâ‹…\sin \theta $$</p>
<p>ä¸ºäº†æ‰¾åˆ°æ­¤è½¬æ¢çŸ©é˜µï¼ŒOpenCVæä¾›äº†ä¸€ä¸ªå‡½æ•°<strong>cv.getRotationMatrix2D</strong>ã€‚</p>
<p>è¯·æ£€æŸ¥ä»¥ä¸‹ç¤ºä¾‹ï¼Œè¯¥ç¤ºä¾‹å°†å›¾åƒç›¸å¯¹äºä¸­å¿ƒæ—‹è½¬90åº¦è€Œæ²¡æœ‰ä»»ä½•ç¼©æ”¾æ¯”ä¾‹ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">rows,cols = img.shape</span><br><span class="line"><span class="comment"># cols-1 å’Œ rows-1 æ˜¯åæ ‡é™åˆ¶</span></span><br><span class="line">M = cv.getRotationMatrix2D(((cols<span class="number">-1</span>)/<span class="number">2.0</span>,(rows<span class="number">-1</span>)/<span class="number">2.0</span>),<span class="number">90</span>,<span class="number">2</span>)</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="ä»¿å°„å˜æ¢"><a href="#ä»¿å°„å˜æ¢" class="headerlink" title="ä»¿å°„å˜æ¢"></a>ä»¿å°„å˜æ¢</h2><p>åœ¨ä»¿å°„å˜æ¢ä¸­ï¼ŒåŸå§‹å›¾åƒä¸­çš„æ‰€æœ‰å¹³è¡Œçº¿åœ¨è¾“å‡ºå›¾åƒä¸­ä»å°†å¹³è¡Œã€‚</p>
<p>ä¸ºäº†æ‰¾åˆ°å˜æ¢çŸ©é˜µï¼Œæˆ‘ä»¬éœ€è¦è¾“å…¥å›¾åƒä¸­çš„ä¸‰ä¸ªç‚¹åŠå…¶åœ¨è¾“å‡ºå›¾åƒä¸­çš„å¯¹åº”ä½ç½®ã€‚</p>
<p>ç„¶å<strong>cv.getAffineTransform</strong>å°†åˆ›å»ºä¸€ä¸ª2x3çŸ©é˜µï¼Œè¯¥çŸ©é˜µå°†ä¼ é€’ç»™<strong>cv.warpAffine</strong>ã€‚</p>
<p>æŸ¥çœ‹ä»¥ä¸‹ç¤ºä¾‹ï¼Œå¹¶æŸ¥çœ‹æˆ‘é€‰æ‹©çš„ç‚¹ï¼ˆä»¥ç»¿è‰²æ ‡è®°ï¼‰ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'drawing.png'</span>)</span><br><span class="line">rows,cols,ch = img.shape</span><br><span class="line">pts1 = np.float32([[<span class="number">50</span>,<span class="number">50</span>],[<span class="number">200</span>,<span class="number">50</span>],[<span class="number">50</span>,<span class="number">200</span>]])</span><br><span class="line">pts2 = np.float32([[<span class="number">10</span>,<span class="number">100</span>],[<span class="number">200</span>,<span class="number">50</span>],[<span class="number">100</span>,<span class="number">250</span>]])</span><br><span class="line">M = cv.getAffineTransform(pts1,pts2)</span><br><span class="line">dst = cv.warpAffine(img,M,(cols,rows))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Input'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Output'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(&lt;matplotlib.axes._subplots.AxesSubplot at 0x1bb7651dcc0&gt;,
 &lt;matplotlib.image.AxesImage at 0x1bb76bc5c18&gt;,
 Text(0.5, 1.0, &apos;Output&apos;))</code></pre><p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_9_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>)</span><br><span class="line">rows,cols,ch = img.shape</span><br><span class="line">pts1 = np.float32([[<span class="number">56</span>,<span class="number">65</span>],[<span class="number">368</span>,<span class="number">52</span>],[<span class="number">28</span>,<span class="number">387</span>],[<span class="number">389</span>,<span class="number">390</span>]])</span><br><span class="line">pts2 = np.float32([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">300</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">300</span>],[<span class="number">300</span>,<span class="number">300</span>]])</span><br><span class="line">M = cv.getPerspectiveTransform(pts1,pts2)</span><br><span class="line">dst = cv.warpPerspective(img,M,(<span class="number">300</span>,<span class="number">300</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Input'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Output'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_10_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="å›¾åƒé˜ˆå€¼"><a href="#å›¾åƒé˜ˆå€¼" class="headerlink" title="å›¾åƒé˜ˆå€¼"></a><span id="header3">å›¾åƒé˜ˆå€¼</span></h1><h2 id="ç›®æ ‡-2"><a href="#ç›®æ ‡-2" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ ç®€å•é˜ˆå€¼ï¼Œè‡ªé€‚åº”é˜ˆå€¼å’ŒOtsué˜ˆå€¼ã€‚</p>
<p>ä½ å°†å­¦ä¹ å‡½æ•°<strong>cv.threshold</strong>å’Œ<strong>cv.adaptiveThreshold</strong>ã€‚</p>
<h2 id="ç®€å•é˜ˆå€¼"><a href="#ç®€å•é˜ˆå€¼" class="headerlink" title="ç®€å•é˜ˆå€¼"></a>ç®€å•é˜ˆå€¼</h2><p>åœ¨è¿™é‡Œï¼Œé—®é¢˜ç›´æˆªäº†å½“ã€‚å¯¹äºæ¯ä¸ªåƒç´ ï¼Œåº”ç”¨ç›¸åŒçš„é˜ˆå€¼ã€‚</p>
<p>å¦‚æœåƒç´ å€¼å°äºé˜ˆå€¼ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º0ï¼Œå¦åˆ™å°†å…¶è®¾ç½®ä¸ºæœ€å¤§å€¼ã€‚å‡½æ•°<strong>cv.threshold</strong>ç”¨äºåº”ç”¨é˜ˆå€¼ã€‚</p>
<ul>
<li>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æºå›¾åƒï¼Œå®ƒ<strong>åº”è¯¥æ˜¯ç°åº¦å›¾åƒ</strong>ã€‚</li>
<li>ç¬¬äºŒä¸ªå‚æ•°æ˜¯é˜ˆå€¼ï¼Œç”¨äºå¯¹åƒç´ å€¼è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯åˆ†é…ç»™è¶…è¿‡é˜ˆå€¼çš„åƒç´ å€¼çš„æœ€å¤§å€¼ã€‚</li>
<li>ç¬¬å››ä¸ªå‚æ•°OpenCVæä¾›äº†ä¸åŒç±»å‹çš„é˜ˆå€¼,é€šè¿‡ä½¿ç”¨<strong>cv.THRESH_BINARY</strong>ç±»å‹ã€‚æ‰€æœ‰ç®€å•çš„é˜ˆå€¼ç±»å‹ä¸ºï¼š<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.THRESH_BINARY</span><br><span class="line">cv.THRESH_BINARY_INV</span><br><span class="line">cv.THRESH_TRUNC</span><br><span class="line">cv.THRESH_TOZERO</span><br><span class="line">cv.THRESH_TOZERO_INV</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>è¯¥æ–¹æ³•è¿”å›ä¸¤ä¸ªè¾“å‡ºã€‚ç¬¬ä¸€ä¸ªæ˜¯ä½¿ç”¨çš„é˜ˆå€¼ï¼Œç¬¬äºŒä¸ªè¾“å‡ºæ˜¯<strong>é˜ˆå€¼åçš„å›¾åƒ</strong>ã€‚</p>
<p>æ­¤ä»£ç æ¯”è¾ƒäº†ä¸åŒçš„ç®€å•é˜ˆå€¼ç±»å‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'gradient.png'</span>,<span class="number">0</span>)</span><br><span class="line">ret,thresh1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line">ret,thresh2 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">ret,thresh3 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TRUNC)</span><br><span class="line">ret,thresh4 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO)</span><br><span class="line">ret,thresh5 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO_INV)</span><br><span class="line">titles = [<span class="string">'Original Image'</span>,<span class="string">'BINARY'</span>,<span class="string">'BINARY_INV'</span>,<span class="string">'TRUNC'</span>,<span class="string">'TOZERO'</span>,<span class="string">'TOZERO_INV'</span>]</span><br><span class="line">images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_2_1.png" alt="png"></p>
<h2 id="è‡ªé€‚åº”é˜ˆå€¼"><a href="#è‡ªé€‚åº”é˜ˆå€¼" class="headerlink" title="è‡ªé€‚åº”é˜ˆå€¼"></a>è‡ªé€‚åº”é˜ˆå€¼</h2><p>åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå…¨å±€å€¼ä½œä¸ºé˜ˆå€¼ã€‚ä½†è¿™å¯èƒ½å¹¶éåœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½å¾ˆå¥½ï¼Œä¾‹å¦‚ï¼Œå¦‚æœå›¾åƒåœ¨ä¸åŒåŒºåŸŸå…·æœ‰ä¸åŒçš„å…‰ç…§æ¡ä»¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡ªé€‚åº”é˜ˆå€¼é˜ˆå€¼åŒ–å¯ä»¥æä¾›å¸®åŠ©ã€‚åœ¨æ­¤ï¼Œç®—æ³•åŸºäºåƒç´ å‘¨å›´çš„å°åŒºåŸŸç¡®å®šåƒç´ çš„é˜ˆå€¼ã€‚å› æ­¤ï¼Œå¯¹äºåŒä¸€å›¾åƒçš„ä¸åŒåŒºåŸŸï¼Œæˆ‘ä»¬è·å¾—äº†ä¸åŒçš„é˜ˆå€¼ï¼Œè¿™ä¸ºå…‰ç…§åº¦å˜åŒ–çš„å›¾åƒæä¾›äº†æ›´å¥½çš„ç»“æœã€‚</p>
<p>é™¤ä¸Šè¿°å‚æ•°å¤–ï¼Œæ–¹æ³•<strong>cv.adaptiveThreshold</strong>è¿˜åŒ…å«ä¸‰ä¸ªè¾“å…¥å‚æ•°ï¼š</p>
<p>è¯¥<strong>adaptiveMethod</strong>å†³å®šé˜ˆå€¼æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼š</p>
<p>cv.ADAPTIVE_THRESH_MEAN_C::é˜ˆå€¼æ˜¯é‚»è¿‘åŒºåŸŸçš„å¹³å‡å€¼å‡å»å¸¸æ•°<strong>C</strong>ã€‚ </p>
<p>cv.ADAPTIVE_THRESH_GAUSSIAN_C:é˜ˆå€¼æ˜¯é‚»åŸŸå€¼çš„é«˜æ–¯åŠ æƒæ€»å’Œå‡å»å¸¸æ•°<strong>C</strong>ã€‚</p>
<p>è¯¥<strong>BLOCKSIZE</strong>ç¡®å®šé™„è¿‘åŒºåŸŸçš„å¤§å°ï¼Œ<strong>C</strong>æ˜¯ä»é‚»åŸŸåƒç´ çš„å¹³å‡æˆ–åŠ æƒæ€»å’Œä¸­å‡å»çš„ä¸€ä¸ªå¸¸æ•°ã€‚</p>
<p>ä¸‹é¢çš„ä»£ç æ¯”è¾ƒäº†å…‰ç…§å˜åŒ–çš„å›¾åƒçš„å…¨å±€é˜ˆå€¼å’Œè‡ªé€‚åº”é˜ˆå€¼ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line">ret,th1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line">th2 = cv.adaptiveThreshold(img,<span class="number">255</span>,cv.ADAPTIVE_THRESH_MEAN_C,\</span><br><span class="line">            cv.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line">th3 = cv.adaptiveThreshold(img,<span class="number">255</span>,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\</span><br><span class="line">            cv.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line">titles = [<span class="string">'Original Image'</span>, <span class="string">'Global Thresholding (v = 127)'</span>,</span><br><span class="line">            <span class="string">'Adaptive Mean Thresholding'</span>, <span class="string">'Adaptive Gaussian Thresholding'</span>]</span><br><span class="line">images = [img, th1, th2, th3]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">2</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_4_0.png" alt="png"></p>
<h2 id="Otsuçš„äºŒå€¼åŒ–"><a href="#Otsuçš„äºŒå€¼åŒ–" class="headerlink" title="Otsuçš„äºŒå€¼åŒ–"></a>Otsuçš„äºŒå€¼åŒ–</h2><p>åœ¨å…¨å±€é˜ˆå€¼åŒ–ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»»æ„é€‰æ‹©çš„å€¼ä½œä¸ºé˜ˆå€¼ã€‚ç›¸åï¼ŒOtsuçš„æ–¹æ³•é¿å…äº†å¿…é¡»é€‰æ‹©ä¸€ä¸ªå€¼å¹¶è‡ªåŠ¨ç¡®å®šå®ƒçš„æƒ…å†µã€‚</p>
<p>è€ƒè™‘ä»…å…·æœ‰ä¸¤ä¸ªä¸åŒå›¾åƒå€¼çš„å›¾åƒï¼ˆåŒå³°å›¾åƒï¼‰ï¼Œå…¶ä¸­ç›´æ–¹å›¾å°†ä»…åŒ…å«ä¸¤ä¸ªå³°ã€‚ä¸€ä¸ªå¥½çš„é˜ˆå€¼åº”è¯¥åœ¨è¿™ä¸¤ä¸ªå€¼çš„ä¸­é—´ã€‚ç±»ä¼¼åœ°ï¼ŒOtsuçš„æ–¹æ³•ä»å›¾åƒç›´æ–¹å›¾ä¸­ç¡®å®šæœ€ä½³å…¨å±€é˜ˆå€¼ã€‚</p>
<p>ä¸ºæ­¤ï¼Œä½¿ç”¨äº†<strong>cv.threshold</strong>ä½œä¸ºé™„åŠ æ ‡å¿—ä¼ é€’ã€‚é˜ˆå€¼å¯ä»¥ä»»æ„é€‰æ‹©ã€‚ç„¶åï¼Œç®—æ³•æ‰¾åˆ°æœ€ä½³é˜ˆå€¼ï¼Œè¯¥é˜ˆå€¼ä½œä¸ºç¬¬ä¸€è¾“å‡ºè¿”å›ã€‚</p>
<p>æŸ¥çœ‹ä»¥ä¸‹ç¤ºä¾‹ã€‚è¾“å…¥å›¾åƒä¸ºå™ªç‚¹å›¾åƒã€‚</p>
<p>åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œé‡‡ç”¨å€¼ä¸º127çš„å…¨å±€é˜ˆå€¼ã€‚</p>
<p>åœ¨ç¬¬äºŒç§æƒ…å†µä¸‹ï¼Œç›´æ¥é‡‡ç”¨Otsué˜ˆå€¼æ³•ã€‚</p>
<p>åœ¨ç¬¬ä¸‰ç§æƒ…å†µä¸‹ï¼Œé¦–å…ˆä½¿ç”¨5x5é«˜æ–¯æ ¸å¯¹å›¾åƒè¿›è¡Œæ»¤æ³¢ä»¥å»é™¤å™ªå£°ï¼Œç„¶ååº”ç”¨Otsué˜ˆå€¼å¤„ç†ã€‚äº†è§£å™ªå£°æ»¤æ³¢å¦‚ä½•æ”¹å–„ç»“æœã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># å…¨å±€é˜ˆå€¼</span></span><br><span class="line">ret1,th1 = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY)</span><br><span class="line"><span class="comment"># Otsué˜ˆå€¼</span></span><br><span class="line">ret2,th2 = cv.threshold(img,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line"><span class="comment"># é«˜æ–¯æ»¤æ³¢åå†é‡‡ç”¨Otsué˜ˆå€¼</span></span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line">ret3,th3 = cv.threshold(blur,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line"><span class="comment"># ç»˜åˆ¶æ‰€æœ‰å›¾åƒåŠå…¶ç›´æ–¹å›¾</span></span><br><span class="line">images = [img, <span class="number">0</span>, th1,</span><br><span class="line">          img, <span class="number">0</span>, th2,</span><br><span class="line">          blur, <span class="number">0</span>, th3]</span><br><span class="line">titles = [<span class="string">'Original Noisy Image'</span>,<span class="string">'Histogram'</span>,<span class="string">'Global Thresholding (v=127)'</span>,</span><br><span class="line">          <span class="string">'Original Noisy Image'</span>,<span class="string">'Histogram'</span>,<span class="string">"Otsu's Thresholding"</span>,</span><br><span class="line">          <span class="string">'Gaussian filtered Image'</span>,<span class="string">'Histogram'</span>,<span class="string">"Otsu's Thresholding"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">1</span>),plt.imshow(images[i*<span class="number">3</span>],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">2</span>),plt.hist(images[i*<span class="number">3</span>].ravel(),<span class="number">256</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>+<span class="number">1</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i*<span class="number">3</span>+<span class="number">3</span>),plt.imshow(images[i*<span class="number">3</span>+<span class="number">2</span>],<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i*<span class="number">3</span>+<span class="number">2</span>]), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_6_0.png" alt="png"></p>
<h2 id="Otsuçš„äºŒå€¼åŒ–å¦‚ä½•å®ç°ï¼Ÿ"><a href="#Otsuçš„äºŒå€¼åŒ–å¦‚ä½•å®ç°ï¼Ÿ" class="headerlink" title="Otsuçš„äºŒå€¼åŒ–å¦‚ä½•å®ç°ï¼Ÿ"></a>Otsuçš„äºŒå€¼åŒ–å¦‚ä½•å®ç°ï¼Ÿ</h2><p>æœ¬èŠ‚æ¼”ç¤ºäº†OtsuäºŒå€¼åŒ–çš„Pythonå®ç°ï¼Œä»¥å±•ç¤ºå…¶å®é™…å·¥ä½œæ–¹å¼ã€‚</p>
<p>ç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†åŒå³°å›¾åƒï¼Œå› æ­¤Otsuçš„ç®—æ³•å°è¯•æ‰¾åˆ°ä¸€ä¸ªé˜ˆå€¼(t)ï¼Œè¯¥é˜ˆå€¼å°†ç”±å…³ç³»å¼ç»™å‡ºçš„<strong>åŠ æƒç±»å†…æ–¹å·®</strong>æœ€å°åŒ–ï¼š</p>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Otsu.png" alt></p>
<p>å®é™…ä¸Šï¼Œå®ƒæ‰¾åˆ°ä½äºä¸¤ä¸ªå³°å€¼ä¹‹é—´çš„tå€¼ï¼Œä»¥ä½¿ä¸¤ä¸ªç±»åˆ«çš„å·®å¼‚æœ€å°</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'sudoku.png'</span>,<span class="number">0</span>)</span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line"><span class="comment"># å¯»æ‰¾å½’ä¸€åŒ–ç›´æ–¹å›¾å’Œå¯¹åº”çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°</span></span><br><span class="line">hist = cv.calcHist([blur],[<span class="number">0</span>],<span class="literal">None</span>,[<span class="number">256</span>],[<span class="number">0</span>,<span class="number">256</span>])</span><br><span class="line">hist_norm = hist.ravel()/hist.max()</span><br><span class="line">Q = hist_norm.cumsum()</span><br><span class="line">bins = np.arange(<span class="number">256</span>)</span><br><span class="line">fn_min = np.inf</span><br><span class="line">thresh = <span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">256</span>):</span><br><span class="line">    p1,p2 = np.hsplit(hist_norm,[i]) <span class="comment"># æ¦‚ç‡</span></span><br><span class="line">    q1,q2 = Q[i],Q[<span class="number">255</span>]-Q[i] <span class="comment"># å¯¹ç±»æ±‚å’Œ</span></span><br><span class="line">    b1,b2 = np.hsplit(bins,[i]) <span class="comment"># æƒé‡</span></span><br><span class="line">    <span class="comment"># å¯»æ‰¾å‡å€¼å’Œæ–¹å·®</span></span><br><span class="line">    m1,m2 = np.sum(p1*b1)/q1, np.sum(p2*b2)/q2</span><br><span class="line">    v1,v2 = np.sum(((b1-m1)**<span class="number">2</span>)*p1)/q1,np.sum(((b2-m2)**<span class="number">2</span>)*p2)/q2</span><br><span class="line">    <span class="comment"># è®¡ç®—æœ€å°åŒ–å‡½æ•°</span></span><br><span class="line">    fn = v1*q1 + v2*q2</span><br><span class="line">    <span class="keyword">if</span> fn &lt; fn_min:</span><br><span class="line">        fn_min = fn</span><br><span class="line">        thresh = i</span><br><span class="line"><span class="comment"># ä½¿ç”¨OpenCVå‡½æ•°æ‰¾åˆ°otsuçš„é˜ˆå€¼</span></span><br><span class="line">ret, otsu = cv.threshold(blur,<span class="number">0</span>,<span class="number">255</span>,cv.THRESH_BINARY+cv.THRESH_OTSU)</span><br><span class="line">print( <span class="string">"&#123;&#125; &#123;&#125;"</span>.format(thresh,ret) )</span><br></pre></td></tr></table></figure>

<pre><code>101 100.0


c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars
  from ipykernel import kernelapp as app
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in double_scalars
  from ipykernel import kernelapp as app
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply
  app.launch_new_instance()</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="å›¾åƒå¹³æ»‘"><a href="#å›¾åƒå¹³æ»‘" class="headerlink" title="å›¾åƒå¹³æ»‘"></a><span id="header4">å›¾åƒå¹³æ»‘</span></h1><h2 id="ç›®æ ‡-3"><a href="#ç›®æ ‡-3" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>å­¦ä¼šï¼š - ä½¿ç”¨å„ç§ä½é€šæ»¤é•œæ¨¡ç³Šå›¾åƒ - å°†å®šåˆ¶çš„æ»¤é•œåº”ç”¨äºå›¾åƒï¼ˆ2Då·ç§¯ï¼‰</p>
<h2 id="2Då·ç§¯ï¼ˆå›¾åƒè¿‡æ»¤ï¼‰"><a href="#2Då·ç§¯ï¼ˆå›¾åƒè¿‡æ»¤ï¼‰" class="headerlink" title="2Då·ç§¯ï¼ˆå›¾åƒè¿‡æ»¤ï¼‰"></a>2Då·ç§¯ï¼ˆå›¾åƒè¿‡æ»¤ï¼‰</h2><p>ä¸ä¸€ç»´ä¿¡å·ä¸€æ ·ï¼Œè¿˜å¯ä»¥ä½¿ç”¨å„ç§ä½é€šæ»¤æ³¢å™¨ï¼ˆLPFï¼‰ï¼Œé«˜é€šæ»¤æ³¢å™¨ï¼ˆHPFï¼‰ç­‰å¯¹å›¾åƒè¿›è¡Œæ»¤æ³¢ã€‚LPFæœ‰åŠ©äºæ¶ˆé™¤å™ªå£°ï¼Œä½¿å›¾åƒæ¨¡ç³Šç­‰ã€‚HPFæ»¤æ³¢å™¨æœ‰åŠ©äºåœ¨å›¾åƒä¸­æ‰¾åˆ°è¾¹ç¼˜ã€‚</p>
<p>OpenCVæä¾›äº†ä¸€ä¸ªå‡½æ•°<strong>cv.filter2D</strong>æ¥å°†å†…æ ¸ä¸å›¾åƒè¿›è¡Œå·ç§¯ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å°†å°è¯•å¯¹å›¾åƒè¿›è¡Œå¹³å‡æ»¤æ³¢ã€‚5x5å¹³å‡æ»¤æ³¢å™¨å†…æ ¸å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<p>K=$$\frac{1}{25}<br> \left[<br> \begin{matrix}<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1<br>  \end{matrix}<br>  \right] <br>$$<br>æ“ä½œå¦‚ä¸‹:ä¿æŒè¿™ä¸ªå†…æ ¸åœ¨ä¸€ä¸ªåƒç´ ä¸Šï¼Œå°†æ‰€æœ‰ä½äºè¿™ä¸ªå†…æ ¸çš„25ä¸ªåƒç´ ç›¸åŠ ï¼Œå–å…¶å¹³å‡å€¼ï¼Œç„¶åç”¨æ–°çš„å¹³å‡å€¼æ›¿æ¢ä¸­å¿ƒåƒç´ ã€‚å®ƒå°†å¯¹å›¾åƒä¸­çš„æ‰€æœ‰åƒç´ ç»§ç»­æ­¤æ“ä½œã€‚è¯•è¯•è¿™ä¸ªä»£ç ï¼Œå¹¶æ£€æŸ¥ç»“æœ:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.float32)/<span class="number">25</span></span><br><span class="line">dst = cv.filter2D(img,<span class="number">-1</span>,kernel)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Averaging'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_2_0.png" alt="png"></p>
<h2 id="å›¾åƒæ¨¡ç³Šï¼ˆå›¾åƒå¹³æ»‘ï¼‰"><a href="#å›¾åƒæ¨¡ç³Šï¼ˆå›¾åƒå¹³æ»‘ï¼‰" class="headerlink" title="å›¾åƒæ¨¡ç³Šï¼ˆå›¾åƒå¹³æ»‘ï¼‰"></a>å›¾åƒæ¨¡ç³Šï¼ˆå›¾åƒå¹³æ»‘ï¼‰</h2><p>é€šè¿‡å°†å›¾åƒä¸ä½é€šæ»¤æ³¢å™¨å†…æ ¸è¿›è¡Œå·ç§¯æ¥å®ç°å›¾åƒæ¨¡ç³Šã€‚è¿™å¯¹äºæ¶ˆé™¤å™ªéŸ³å¾ˆæœ‰ç”¨ã€‚å®ƒå®é™…ä¸Šä»å›¾åƒä¸­æ¶ˆé™¤äº†é«˜é¢‘éƒ¨åˆ†ï¼ˆä¾‹å¦‚å™ªå£°ï¼Œè¾¹ç¼˜ï¼‰ã€‚</p>
<p>å› æ­¤ï¼Œåœ¨æ­¤æ“ä½œä¸­è¾¹ç¼˜æœ‰äº›æ¨¡ç³Šã€‚ï¼ˆæœ‰ä¸€äº›æ¨¡ç³ŠæŠ€æœ¯ä¹Ÿå¯ä»¥ä¸æ¨¡ç³Šè¾¹ç¼˜ï¼‰ã€‚OpenCVä¸»è¦æä¾›å››ç§ç±»å‹çš„æ¨¡ç³ŠæŠ€æœ¯ã€‚</p>
<h3 id="1-å¹³å‡"><a href="#1-å¹³å‡" class="headerlink" title="1.å¹³å‡"></a>1.å¹³å‡</h3><p>è¿™æ˜¯é€šè¿‡å°†å›¾åƒä¸å½’ä¸€åŒ–æ¡†æ»¤é•œè¿›è¡Œå·ç§¯æ¥å®Œæˆçš„ã€‚å®ƒä»…è·å–å†…æ ¸åŒºåŸŸä¸‹æ‰€æœ‰åƒç´ çš„å¹³å‡å€¼ï¼Œå¹¶æ›¿æ¢ä¸­å¿ƒå…ƒç´ ã€‚è¿™æ˜¯é€šè¿‡åŠŸèƒ½<strong>cv.blur()æˆ–</strong>cv.boxFilter()å®Œæˆçš„ã€‚æ£€æŸ¥æ–‡æ¡£ä»¥è·å–æœ‰å…³å†…æ ¸çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚æˆ‘ä»¬åº”è¯¥æŒ‡å®šå†…æ ¸çš„å®½åº¦å’Œé«˜åº¦ã€‚3x3å½’ä¸€åŒ–æ¡†å¼è¿‡æ»¤å™¨å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<p>K=$$\frac{1}{9}<br> \left[<br> \begin{matrix}<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br>    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1<br>  \end{matrix}<br>  \right] <br>$$<br>æ³¨æ„ å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨æ ‡å‡†åŒ–çš„æ¡†å¼è¿‡æ»¤å™¨ï¼Œè¯·ä½¿ç”¨<strong>cv.boxFilter()</strong>ã€‚å°†å‚æ•°normalize = Falseä¼ é€’ç»™å‡½æ•°ã€‚</p>
<p>æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºï¼Œå…¶å†…æ ¸å¤§å°ä¸º5x5ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.blur(img,(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_5_0.png" alt="png"></p>
<h3 id="2-é«˜æ–¯æ¨¡ç³Š"><a href="#2-é«˜æ–¯æ¨¡ç³Š" class="headerlink" title="2.é«˜æ–¯æ¨¡ç³Š"></a>2.é«˜æ–¯æ¨¡ç³Š</h3><p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»£æ›¿ç›’å¼æ»¤æ³¢å™¨ï¼Œä½¿ç”¨äº†é«˜æ–¯æ ¸ã€‚</p>
<p>è¿™æ˜¯é€šè¿‡åŠŸèƒ½<strong>cv.GaussianBlur()</strong> å®Œæˆçš„ã€‚æˆ‘ä»¬åº”æŒ‡å®šå†…æ ¸çš„å®½åº¦å’Œé«˜åº¦ï¼Œè¯¥å®½åº¦å’Œé«˜åº¦åº”ä¸ºæ­£æ•°å’Œå¥‡æ•°ã€‚</p>
<p>æˆ‘ä»¬è¿˜åº”æŒ‡å®šXå’ŒYæ–¹å‘çš„æ ‡å‡†åå·®ï¼Œåˆ†åˆ«ä¸ºsigmaXå’ŒsigmaYã€‚å¦‚æœä»…æŒ‡å®šsigmaXï¼Œåˆ™å°†sigmaYä¸sigmaXç›¸åŒã€‚</p>
<p>å¦‚æœä¸¤ä¸ªéƒ½ä¸ºé›¶ï¼Œåˆ™æ ¹æ®å†…æ ¸å¤§å°è¿›è¡Œè®¡ç®—ã€‚é«˜æ–¯æ¨¡ç³Šå¯¹äºä»å›¾åƒä¸­å»é™¤é«˜æ–¯å™ªå£°éå¸¸æœ‰æ•ˆã€‚</p>
<p>å¦‚æœéœ€è¦ï¼Œå¯ä»¥ä½¿ç”¨å‡½æ•°<strong>cv.getGaussianKernel()</strong> åˆ›å»ºé«˜æ–¯å†…æ ¸ã€‚</p>
<p>å¯ä»¥ä¿®æ”¹ä»¥ä¸Šä»£ç ä»¥å®ç°é«˜æ–¯æ¨¡ç³Šï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">0</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_7_0.png" alt="png"></p>
<h3 id="3-ä¸­ä½æ¨¡ç³Š"><a href="#3-ä¸­ä½æ¨¡ç³Š" class="headerlink" title="3.ä¸­ä½æ¨¡ç³Š"></a>3.ä¸­ä½æ¨¡ç³Š</h3><p>åœ¨è¿™é‡Œï¼Œå‡½æ•°<strong>cv.medianBlur()</strong> æå–å†…æ ¸åŒºåŸŸä¸‹æ‰€æœ‰åƒç´ çš„ä¸­å€¼ï¼Œå¹¶å°†ä¸­å¿ƒå…ƒç´ æ›¿æ¢ä¸ºè¯¥ä¸­å€¼ã€‚</p>
<p>è¿™å¯¹äºæ¶ˆé™¤å›¾åƒä¸­çš„æ¤’ç›å™ªå£°éå¸¸æœ‰æ•ˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨ä¸Šè¿°è¿‡æ»¤å™¨ä¸­ï¼Œä¸­å¿ƒå…ƒç´ æ˜¯æ–°è®¡ç®—çš„å€¼ï¼Œè¯¥å€¼å¯ä»¥æ˜¯å›¾åƒä¸­çš„åƒç´ å€¼æˆ–æ–°å€¼ã€‚</p>
<p>ä½†æ˜¯åœ¨ä¸­å€¼æ¨¡ç³Šä¸­ï¼Œä¸­å¿ƒå…ƒç´ æ€»æ˜¯è¢«å›¾åƒä¸­çš„æŸäº›åƒç´ å€¼ä»£æ›¿ã€‚æœ‰æ•ˆé™ä½å™ªéŸ³ã€‚å…¶å†…æ ¸å¤§å°åº”ä¸ºæ­£å¥‡æ•°æ•´æ•°ã€‚</p>
<p>åœ¨æ­¤æ¼”ç¤ºä¸­ï¼Œæˆ‘å‘åŸå§‹å›¾åƒæ·»åŠ äº†50ï¼…çš„å™ªå£°å¹¶åº”ç”¨äº†ä¸­å€¼æ¨¡ç³Šã€‚æ£€æŸ¥ç»“æœï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">median = cv.medianBlur(img,<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(median),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_9_0.png" alt="png"></p>
<h3 id="4-åŒè¾¹æ»¤æ³¢"><a href="#4-åŒè¾¹æ»¤æ³¢" class="headerlink" title="4.åŒè¾¹æ»¤æ³¢"></a>4.åŒè¾¹æ»¤æ³¢</h3><p>cv.bilateralFilter() åœ¨å»é™¤å™ªå£°çš„åŒæ—¶ä¿æŒè¾¹ç¼˜æ¸…æ™°é”åˆ©éå¸¸æœ‰æ•ˆã€‚</p>
<p>ä½†æ˜¯ï¼Œä¸å…¶ä»–è¿‡æ»¤å™¨ç›¸æ¯”ï¼Œè¯¥æ“ä½œé€Ÿåº¦è¾ƒæ…¢ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œé«˜æ–¯æ»¤æ³¢å™¨é‡‡ç”¨åƒç´ å‘¨å›´çš„é‚»åŸŸå¹¶æ‰¾åˆ°å…¶é«˜æ–¯åŠ æƒå¹³å‡å€¼ã€‚</p>
<p>é«˜æ–¯æ»¤æ³¢å™¨ä»…æ˜¯ç©ºé—´çš„å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ»¤æ³¢æ—¶ä¼šè€ƒè™‘é™„è¿‘çš„åƒç´ ã€‚å®ƒä¸è€ƒè™‘åƒç´ æ˜¯å¦å…·æœ‰å‡ ä¹ç›¸åŒçš„å¼ºåº¦ã€‚å®ƒä¸è€ƒè™‘åƒç´ æ˜¯å¦æ˜¯è¾¹ç¼˜åƒç´ ã€‚å› æ­¤å®ƒä¹Ÿæ¨¡ç³Šäº†è¾¹ç¼˜ï¼Œè¿™æ˜¯æˆ‘ä»¬ä¸æƒ³åšçš„ã€‚</p>
<p>åŒè¾¹æ»¤æ³¢å™¨åœ¨ç©ºé—´ä¸­ä¹Ÿé‡‡ç”¨é«˜æ–¯æ»¤æ³¢å™¨ï¼Œä½†æ˜¯åˆæœ‰ä¸€ä¸ªé«˜æ–¯æ»¤æ³¢å™¨ï¼Œå®ƒæ˜¯åƒç´ å·®çš„å‡½æ•°ã€‚</p>
<p>ç©ºé—´çš„é«˜æ–¯å‡½æ•°ç¡®ä¿ä»…è€ƒè™‘é™„è¿‘åƒç´ çš„æ¨¡ç³Šï¼Œè€Œå¼ºåº¦å·®çš„é«˜æ–¯å‡½æ•°ç¡®ä¿ä»…è€ƒè™‘å¼ºåº¦ä¸ä¸­å¿ƒåƒç´ ç›¸ä¼¼çš„é‚£äº›åƒç´ çš„æ¨¡ç³Šã€‚ç”±äºè¾¹ç¼˜çš„åƒç´ å¼ºåº¦å˜åŒ–è¾ƒå¤§ï¼Œå› æ­¤å¯ä»¥ä¿ç•™è¾¹ç¼˜ã€‚</p>
<p>ä»¥ä¸‹ç¤ºä¾‹æ˜¾ç¤ºäº†ä½¿ç”¨åŒè¾¹è¿‡æ»¤å™¨</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">blur = cv.bilateralFilter(img,<span class="number">9</span>,<span class="number">75</span>,<span class="number">75</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img),plt.title(<span class="string">'Original'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(blur),plt.title(<span class="string">'Blurred'</span>)</span><br><span class="line">plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%861/output_11_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>å›¾åƒ</tag>
      </tags>
  </entry>
  <entry>
    <title>opencvä¸­çš„å›¾åƒå¤„ç†2</title>
    <url>/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/</url>
    <content><![CDATA[<ul>
<li>5.<a href="#header1">å½¢æ€è½¬æ¢</a></li>
<li>6.<a href="#header2">å›¾åƒæ¢¯åº¦</a></li>
<li>7.<a href="#header3">Cannyè¾¹ç¼˜æ£€æµ‹</a></li>
<li>8.<a href="#header4">å›¾åƒé‡‘å­—å¡”</a><a id="more"></a>


</li>
</ul>
<h1 id="å½¢æ€å­¦è½¬æ¢"><a href="#å½¢æ€å­¦è½¬æ¢" class="headerlink" title="å½¢æ€å­¦è½¬æ¢"></a><span id="header1">å½¢æ€å­¦è½¬æ¢</span></h1><h2 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨è¿™ä¸€ç« å½“ä¸­ï¼Œ æˆ‘ä»¬å°†å­¦ä¹ ä¸åŒçš„å½¢æ€å­¦æ“ä½œï¼Œä¾‹å¦‚ä¾µèš€ï¼Œè†¨èƒ€ï¼Œå¼€è¿ç®—ï¼Œé—­è¿ç®—ç­‰ã€‚<br>æˆ‘ä»¬å°†çœ‹åˆ°ä¸åŒçš„åŠŸèƒ½ï¼Œ</p>
<p>ä¾‹å¦‚ï¼šcv.erode(),cv.dilate(), cv.morphologyEx()ç­‰ã€‚</p>
<h2 id="ç†è®º"><a href="#ç†è®º" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>å½¢æ€å˜æ¢æ˜¯ä¸€äº›åŸºäºå›¾åƒå½¢çŠ¶çš„ç®€å•æ“ä½œã€‚é€šå¸¸åœ¨äºŒè¿›åˆ¶å›¾åƒä¸Šæ‰§è¡Œã€‚</p>
<p>å®ƒéœ€è¦ä¸¤ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªæ˜¯æˆ‘ä»¬çš„åŸå§‹å›¾åƒï¼Œç¬¬äºŒä¸ªæ˜¯å†³å®š<strong>æ“ä½œæ€§è´¨çš„ç»“æ„å…ƒç´ </strong>æˆ–<strong>å†…æ ¸</strong>ã€‚</p>
<p>ä¸¤ç§åŸºæœ¬çš„å½¢æ€å­¦ç®—å­æ˜¯ä¾µèš€å’Œè†¨èƒ€ã€‚</p>
<p>ç„¶åï¼Œå®ƒçš„å˜ä½“å½¢å¼ï¼ˆå¦‚â€œæ‰“å¼€â€ï¼Œâ€œå…³é—­â€ï¼Œâ€œæ¸å˜â€ç­‰ï¼‰ä¹Ÿå¼€å§‹èµ·ä½œç”¨ã€‚åœ¨ä¸‹å›¾çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬å°†ä¸€ä¸€çœ‹åˆ°å®ƒä»¬ï¼š</p>
<h2 id="1-ä¾µèš€"><a href="#1-ä¾µèš€" class="headerlink" title="1. ä¾µèš€"></a>1. ä¾µèš€</h2><p>ä¾µèš€çš„åŸºæœ¬æ€æƒ³å°±åƒåœŸå£¤ä¾µèš€ä¸€æ ·ï¼Œå®ƒä¾µèš€å‰æ™¯ç‰©ä½“çš„è¾¹ç•Œ(å°½é‡ä½¿å‰æ™¯ä¿æŒç™½è‰²)ã€‚</p>
<p>å®ƒæ˜¯åšä»€ä¹ˆçš„å‘¢?å†…æ ¸æ»‘åŠ¨é€šè¿‡å›¾åƒ(åœ¨2Då·ç§¯ä¸­)ã€‚</p>
<p>åŸå§‹å›¾åƒä¸­çš„ä¸€ä¸ªåƒç´ (æ— è®ºæ˜¯1è¿˜æ˜¯0)åªæœ‰å½“å†…æ ¸ä¸‹çš„æ‰€æœ‰åƒç´ éƒ½æ˜¯1æ—¶æ‰è¢«è®¤ä¸ºæ˜¯1ï¼Œå¦åˆ™å®ƒå°±ä¼šè¢«ä¾µèš€(å˜æˆ0)ã€‚</p>
<p>ç»“æœæ˜¯ï¼Œæ ¹æ®å†…æ ¸çš„å¤§å°ï¼Œè¾¹ç•Œé™„è¿‘çš„æ‰€æœ‰åƒç´ éƒ½ä¼šè¢«ä¸¢å¼ƒã€‚</p>
<p>å› æ­¤ï¼Œå‰æ™¯ç‰©ä½“çš„åšåº¦æˆ–å¤§å°å‡å°ï¼Œæˆ–åªæ˜¯å›¾åƒä¸­çš„ç™½è‰²åŒºåŸŸå‡å°ã€‚</p>
<p>å®ƒæœ‰åŠ©äºå»é™¤å°çš„ç™½è‰²å™ªå£°(æ­£å¦‚æˆ‘ä»¬åœ¨é¢œè‰²ç©ºé—´ç« èŠ‚ä¸­çœ‹åˆ°çš„)ï¼Œåˆ†ç¦»ä¸¤ä¸ªè¿æ¥çš„å¯¹è±¡ç­‰ã€‚</p>
<p>åœ¨è¿™é‡Œï¼Œä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘å°†ä½¿ç”¨ä¸€ä¸ª5x5å†…æ ¸ï¼Œå®ƒåŒ…å«äº†æ‰€æœ‰çš„1ã€‚</p>
<h2 id="2-æ‰©å¼ "><a href="#2-æ‰©å¼ " class="headerlink" title="2. æ‰©å¼ "></a>2. æ‰©å¼ </h2><p>å®ƒä¸ä¾µèš€æ­£å¥½ç›¸åã€‚å¦‚æœå†…æ ¸ä¸‹çš„è‡³å°‘ä¸€ä¸ªåƒç´ ä¸ºâ€œ 1â€ï¼Œåˆ™åƒç´ å…ƒç´ ä¸ºâ€œ 1â€ã€‚</p>
<p>å› æ­¤ï¼Œå®ƒä¼šå¢åŠ å›¾åƒä¸­çš„ç™½è‰²åŒºåŸŸæˆ–å¢åŠ å‰æ™¯å¯¹è±¡çš„å¤§å°ã€‚</p>
<p>é€šå¸¸ï¼Œåœ¨æ¶ˆé™¤å™ªéŸ³çš„æƒ…å†µä¸‹ï¼Œè…èš€åä¼šè†¨èƒ€ã€‚å› ä¸ºè…èš€ä¼šæ¶ˆé™¤ç™½å™ªå£°ï¼Œä½†ä¹Ÿä¼šç¼©å°ç‰©ä½“ã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†æ‰©å±•ã€‚ç”±äºå™ªéŸ³æ¶ˆå¤±äº†ï¼Œå®ƒä»¬ä¸ä¼šå›æ¥ï¼Œä½†æ˜¯æˆ‘ä»¬çš„ç›®æ ‡åŒºåŸŸå¢åŠ äº†ã€‚åœ¨è¿æ¥å¯¹è±¡çš„æŸåéƒ¨åˆ†æ—¶ä¹Ÿå¾ˆæœ‰ç”¨ã€‚</p>
<h2 id="3-å¼€è¿ç®—"><a href="#3-å¼€è¿ç®—" class="headerlink" title="3. å¼€è¿ç®—"></a>3. å¼€è¿ç®—</h2><p>å¼€æ”¾åªæ˜¯<strong>ä¾µèš€ç„¶åæ‰©å¼ </strong>çš„å¦ä¸€ä¸ªåç§°ã€‚</p>
<p>å¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œå®ƒå¯¹äºæ¶ˆé™¤å™ªéŸ³å¾ˆæœ‰ç”¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨å‡½æ•°<strong>cv.morphologyEx</strong>()</p>
<h2 id="4-é—­è¿ç®—"><a href="#4-é—­è¿ç®—" class="headerlink" title="4. é—­è¿ç®—"></a>4. é—­è¿ç®—</h2><p>é—­è¿ç®—ä¸å¼€è¿ç®—ç›¸åï¼Œå…ˆæ‰©å¼ ç„¶åå†ä¾µèš€ã€‚</p>
<p>åœ¨å…³é—­å‰æ™¯å¯¹è±¡å†…éƒ¨çš„å°å­”æˆ–å¯¹è±¡ä¸Šçš„å°é»‘ç‚¹æ—¶å¾ˆæœ‰ç”¨ã€‚</p>
<h2 id="5-å½¢æ€å­¦æ¢¯åº¦"><a href="#5-å½¢æ€å­¦æ¢¯åº¦" class="headerlink" title="5. å½¢æ€å­¦æ¢¯åº¦"></a>5. å½¢æ€å­¦æ¢¯åº¦</h2><p>è¿™æ˜¯å›¾åƒæ‰©å¼ å’Œä¾µèš€ä¹‹é—´çš„åŒºåˆ«ã€‚</p>
<p>ç»“æœå°†çœ‹èµ·æ¥åƒå¯¹è±¡çš„è½®å»“ã€‚</p>
<h2 id="6-é¡¶å¸½"><a href="#6-é¡¶å¸½" class="headerlink" title="6. é¡¶å¸½"></a>6. é¡¶å¸½</h2><p>å®ƒæ˜¯è¾“å…¥å›¾åƒå’Œå›¾åƒå¼€è¿ç®—ä¹‹å·®ã€‚ä¸‹é¢çš„ç¤ºä¾‹é’ˆå¯¹9x9å†…æ ¸å®Œæˆã€‚</p>
<h2 id="7-é»‘å¸½"><a href="#7-é»‘å¸½" class="headerlink" title="7. é»‘å¸½"></a>7. é»‘å¸½</h2><p>è¿™æ˜¯è¾“å…¥å›¾åƒå’Œå›¾åƒé—­è¿ç®—ä¹‹å·®ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl </span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>] <span class="comment"># æŒ‡å®šé»˜è®¤å­—ä½“ </span></span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span> <span class="comment"># è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜</span></span><br><span class="line">img = cv.imread(<span class="string">'j.png'</span>,<span class="number">0</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)</span><br><span class="line">erosion = cv.erode(img,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">dilation = cv.dilate(img,kernel,iterations = <span class="number">1</span>) </span><br><span class="line">opening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel) </span><br><span class="line">closing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel) </span><br><span class="line">gradient = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel) </span><br><span class="line">tophat = cv.morphologyEx(img, cv.MORPH_TOPHAT, kernel) </span><br><span class="line">blackhat = cv.morphologyEx(img, cv.MORPH_BLACKHAT, kernel) </span><br><span class="line">imgs = [img,erosion,dilation,opening,closing,gradient,tophat,blackhat]</span><br><span class="line">titles = [<span class="string">'åŸå›¾'</span>,<span class="string">'ä¾µèš€'</span>,<span class="string">'è†¨èƒ€'</span>,<span class="string">'å¼€è¿ç®—'</span>,<span class="string">'é—­è¿ç®—'</span>,<span class="string">'å½¢æ€å­¦æ¢¯åº¦'</span>,<span class="string">'é¡¶å¸½'</span>,<span class="string">'é»‘å¸½'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(imgs)):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">4</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(imgs[i])</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_9_0.png" alt="png"></p>
<h2 id="ç»“æ„å…ƒç´ "><a href="#ç»“æ„å…ƒç´ " class="headerlink" title="ç»“æ„å…ƒç´ "></a>ç»“æ„å…ƒç´ </h2><p>åœ¨Numpyçš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­æ‰‹åŠ¨åˆ›å»ºäº†ä¸€ä¸ªç»“æ„å…ƒç´ ã€‚</p>
<p>å®ƒæ˜¯çŸ©å½¢ã€‚ä½†æ˜¯åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦æ¤­åœ†å½¢/åœ†å½¢çš„å†…æ ¸ã€‚</p>
<p>å› æ­¤ï¼Œä¸ºæ­¤ï¼ŒOpenCVå…·æœ‰ä¸€ä¸ªå‡½æ•°<strong>cv.getStructuringElement</strong>()ã€‚</p>
<p>æ‚¨åªéœ€ä¼ é€’å†…æ ¸çš„å½¢çŠ¶å’Œå¤§å°ï¼Œå³å¯è·å¾—æ‰€éœ€çš„å†…æ ¸</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># çŸ©å½¢</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_RECT,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1]], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¤­åœ†å†…æ ¸</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_ELLIPSE,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0, 1, 0, 0],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1],
       [0, 0, 1, 0, 0]], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åå­—å†…æ ¸</span></span><br><span class="line">cv.getStructuringElement(cv.MORPH_CROSS,(<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0],
       [1, 1, 1, 1, 1],
       [0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0]], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="å›¾åƒæ¢¯åº¦"><a href="#å›¾åƒæ¢¯åº¦" class="headerlink" title="å›¾åƒæ¢¯åº¦"></a><span id="header2">å›¾åƒæ¢¯åº¦</span></h1><h2 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š - æŸ¥æ‰¾å›¾åƒæ¢¯åº¦ï¼Œè¾¹ç¼˜ç­‰ - </p>
<p>æˆ‘ä»¬å°†çœ‹åˆ°ä»¥ä¸‹å‡½æ•°ï¼šcv.Sobel()ï¼Œcv.Scharr()ï¼Œcv.Laplacian()ç­‰</p>
<h2 id="ç†è®º-1"><a href="#ç†è®º-1" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>OpenCVæä¾›ä¸‰ç§ç±»å‹çš„æ¢¯åº¦æ»¤æ³¢å™¨æˆ–é«˜é€šæ»¤æ³¢å™¨ï¼Œå³Sobelï¼ŒScharrå’ŒLaplacianã€‚æˆ‘ä»¬å°†çœ‹åˆ°ä»–ä»¬æ¯ä¸€ç§ã€‚</p>
<h2 id="1-Sobel-å’Œ-Scharr-ç®—å­"><a href="#1-Sobel-å’Œ-Scharr-ç®—å­" class="headerlink" title="1. Sobel å’Œ Scharr ç®—å­"></a>1. Sobel å’Œ Scharr ç®—å­</h2><p>Sobelç®—å­æ˜¯é«˜æ–¯å¹³æ»‘åŠ å¾®åˆ†è¿ç®—çš„è”åˆè¿ç®—ï¼Œå› æ­¤å®ƒæ›´æŠ—å™ªå£°ã€‚é€†å¯ä»¥æŒ‡å®šè¦é‡‡ç”¨çš„å¯¼æ•°æ–¹å‘ï¼Œå‚ç›´æˆ–æ°´å¹³ï¼ˆåˆ†åˆ«é€šè¿‡å‚æ•°yorderå’Œxorderï¼‰ã€‚é€†è¿˜å¯ä»¥é€šè¿‡å‚æ•°ksizeæŒ‡å®šå†…æ ¸çš„å¤§å°ã€‚å¦‚æœksize = -1ï¼Œåˆ™ä½¿ç”¨3x3 Scharræ»¤æ³¢å™¨ï¼Œæ¯”3x3 Sobelæ»¤æ³¢å™¨å…·æœ‰æ›´å¥½çš„ç»“æœã€‚è¯·å‚é˜…æ–‡æ¡£ä»¥äº†è§£æ‰€ä½¿ç”¨çš„å†…æ ¸ã€‚</p>
<h2 id="2-Laplacian-ç®—å­"><a href="#2-Laplacian-ç®—å­" class="headerlink" title="2. Laplacian ç®—å­"></a>2. Laplacian ç®—å­</h2><p>å®ƒè®¡ç®—äº†ç”±å…³ç³»Î”src=$\frac{\delta^2 src}{\delta x^2}+\frac{\delta^2 src}{\delta y^2}$ç»™å‡ºçš„å›¾åƒçš„æ‹‰æ™®æ‹‰æ–¯å›¾,å®ƒæ˜¯æ¯ä¸€é˜¶å¯¼æ•°é€šè¿‡Sobelç®—å­è®¡ç®—ã€‚å¦‚æœksize = 1,ç„¶åä½¿ç”¨ä»¥ä¸‹å†…æ ¸ç”¨äºè¿‡æ»¤:</p>
<p>kernel=$$<br> \left[<br> \begin{matrix}<br>   0 &amp; 1 &amp; 0 \<br>   1 &amp; -4 &amp; 1 \<br>   0 &amp; 1 &amp; 0<br>  \end{matrix}<br>  \right] <br>$$<br>ä»£ç <br>ä¸‹é¢çš„ä»£ç æ˜¾ç¤ºäº†å•ä¸ªå›¾è¡¨ä¸­çš„æ‰€æœ‰ç®—å­ã€‚æ‰€æœ‰å†…æ ¸éƒ½æ˜¯5x5å¤§å°ã€‚è¾“å‡ºå›¾åƒçš„æ·±åº¦é€šè¿‡-1å¾—åˆ°ç»“æœçš„np.uint8å‹ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'dave.png'</span>,<span class="number">0</span>)</span><br><span class="line">laplacian = cv.Laplacian(img,cv.CV_64F)</span><br><span class="line">sobelx = cv.Sobel(img,cv.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize=<span class="number">5</span>)</span><br><span class="line">sobely = cv.Sobel(img,cv.CV_64F,<span class="number">0</span>,<span class="number">1</span>,ksize=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),plt.imshow(img,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Original'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>),plt.imshow(laplacian,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Laplacian'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>),plt.imshow(sobelx,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Sobel X'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),plt.imshow(sobely,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Sobel Y'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="Cannyè¾¹ç¼˜æ£€æµ‹"><a href="#Cannyè¾¹ç¼˜æ£€æµ‹" class="headerlink" title="Cannyè¾¹ç¼˜æ£€æµ‹"></a><span id="header3">Cannyè¾¹ç¼˜æ£€æµ‹</span></h1><h2 id="ç›®æ ‡-2"><a href="#ç›®æ ‡-2" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  - Cannyè¾¹ç¼˜æ£€æµ‹çš„æ¦‚å¿µ - OpenCVå‡½æ•°: cv.Canny()</p>
<h2 id="ç†è®º-2"><a href="#ç†è®º-2" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>Canny Edge Detectionæ˜¯ä¸€ç§æµè¡Œçš„è¾¹ç¼˜æ£€æµ‹ç®—æ³•ã€‚å®ƒç”±John F. Cannyå‘æ˜</p>
<p>è¿™æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µç®—æ³•ï¼Œæˆ‘ä»¬å°†ç»å†æ¯ä¸ªé˜¶æ®µã€‚</p>
<h2 id="é™å™ª"><a href="#é™å™ª" class="headerlink" title="é™å™ª"></a>é™å™ª</h2><p>ç”±äºè¾¹ç¼˜æ£€æµ‹å®¹æ˜“å—åˆ°å›¾åƒä¸­å™ªå£°çš„å½±å“ï¼Œå› æ­¤ç¬¬ä¸€æ­¥æ˜¯ä½¿ç”¨5x5é«˜æ–¯æ»¤æ³¢å™¨æ¶ˆé™¤å›¾åƒä¸­çš„å™ªå£°ã€‚æˆ‘ä»¬å·²ç»åœ¨å‰é¢çš„ç« èŠ‚ä¸­çœ‹åˆ°äº†è¿™ä¸€ç‚¹ã€‚</p>
<p>æŸ¥æ‰¾å›¾åƒçš„å¼ºåº¦æ¢¯åº¦<br>ç„¶åä½¿ç”¨Sobelæ ¸åœ¨æ°´å¹³å’Œå‚ç›´æ–¹å‘ä¸Šå¯¹å¹³æ»‘çš„å›¾åƒè¿›è¡Œæ»¤æ³¢ï¼Œä»¥åœ¨æ°´å¹³æ–¹å‘(Gx)å’Œå‚ç›´æ–¹å‘(Gy)ä¸Šè·å¾—ä¸€é˜¶å¯¼æ•°ã€‚ä»è¿™ä¸¤å¼ å›¾ç‰‡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°æ¯ä¸ªåƒç´ çš„è¾¹ç¼˜æ¸å˜å’Œæ–¹å‘ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<p>$$<br>Edge_Gradient ; (G) = \sqrt{G_x^2 + G_y^2} \ Angle ; (\theta) = \tan^{-1} \bigg(\frac{G_y}{G_x}\bigg)<br>$$<br>æ¸å˜æ–¹å‘å§‹ç»ˆå‚ç›´äºè¾¹ç¼˜ã€‚å°†å…¶èˆå…¥ä¸ºä»£è¡¨å‚ç›´ï¼Œæ°´å¹³å’Œä¸¤ä¸ªå¯¹è§’çº¿æ–¹å‘çš„å››ä¸ªè§’åº¦ä¹‹ä¸€ã€‚</p>
<p>éæå¤§å€¼æŠ‘åˆ¶ åœ¨è·å¾—æ¢¯åº¦å¤§å°å’Œæ–¹å‘åï¼Œå°†å¯¹å›¾åƒè¿›è¡Œå…¨é¢æ‰«æï¼Œä»¥å»é™¤å¯èƒ½ä¸æ„æˆè¾¹ç¼˜çš„æ‰€æœ‰ä¸éœ€è¦çš„åƒç´ ã€‚</p>
<p>ä¸ºæ­¤ï¼Œåœ¨æ¯ä¸ªåƒç´ å¤„ï¼Œæ£€æŸ¥åƒç´ æ˜¯å¦æ˜¯å…¶åœ¨æ¢¯åº¦æ–¹å‘ä¸Šé™„è¿‘çš„å±€éƒ¨æœ€å¤§å€¼ã€‚æŸ¥çœ‹ä¸‹é¢çš„å›¾ç‰‡ï¼š<br><img src="http://qiniu.aihubs.net/nms.jpg" alt></p>
<p>ç‚¹Aåœ¨è¾¹ç¼˜ï¼ˆå‚ç›´æ–¹å‘ï¼‰ä¸Šã€‚æ¸å˜æ–¹å‘å‚ç›´äºè¾¹ç¼˜ã€‚ç‚¹Bå’ŒCåœ¨æ¢¯åº¦æ–¹å‘ä¸Šã€‚å› æ­¤ï¼Œå°†Aç‚¹ä¸Bç‚¹å’ŒCç‚¹è¿›è¡Œæ£€æŸ¥ï¼Œçœ‹æ˜¯å¦å½¢æˆå±€éƒ¨æœ€å¤§å€¼ã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œåˆ™è€ƒè™‘å°†å…¶ç”¨äºä¸‹ä¸€é˜¶æ®µï¼Œå¦åˆ™å°†å…¶æŠ‘åˆ¶ï¼ˆç½®ä¸ºé›¶ï¼‰ã€‚ ç®€è€Œè¨€ä¹‹ï¼Œä½ å¾—åˆ°çš„ç»“æœæ˜¯å¸¦æœ‰â€œç»†è¾¹â€çš„äºŒè¿›åˆ¶å›¾åƒã€‚</p>
<h2 id="ç£æ»é˜ˆå€¼"><a href="#ç£æ»é˜ˆå€¼" class="headerlink" title="ç£æ»é˜ˆå€¼"></a>ç£æ»é˜ˆå€¼</h2><p>è¯¥é˜¶æ®µç¡®å®šå“ªäº›è¾¹ç¼˜å…¨éƒ¨æ˜¯çœŸæ­£çš„è¾¹ç¼˜ï¼Œå“ªäº›ä¸æ˜¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªé˜ˆå€¼minValå’ŒmaxValã€‚å¼ºåº¦æ¢¯åº¦å¤§äºmaxValçš„ä»»ä½•è¾¹ç¼˜å¿…å®šæ˜¯è¾¹ç¼˜ï¼Œè€Œå°äºminValçš„é‚£äº›è¾¹ç¼˜å¿…å®šæ˜¯éè¾¹ç¼˜ï¼Œå› æ­¤å°†å…¶ä¸¢å¼ƒã€‚ä»‹äºè¿™ä¸¤ä¸ªé˜ˆå€¼ä¹‹é—´çš„å¯¹è±¡æ ¹æ®å…¶è¿é€šæ€§è¢«åˆ†ç±»ä¸ºè¾¹ç¼˜æˆ–éè¾¹ç¼˜ã€‚å¦‚æœå°†å®ƒä»¬è¿æ¥åˆ°â€œè¾¹ç¼˜â€åƒç´ ï¼Œåˆ™å°†å®ƒä»¬è§†ä¸ºè¾¹ç¼˜çš„ä¸€éƒ¨åˆ†ã€‚å¦åˆ™ï¼Œå®ƒä»¬ä¹Ÿå°†è¢«ä¸¢å¼ƒã€‚è§ä¸‹å›¾ï¼š</p>
<p><img src="http://qiniu.aihubs.net/hysteresis.jpg" alt><br>è¾¹ç¼˜Aåœ¨maxValä¹‹ä¸Šï¼Œå› æ­¤è¢«è§†ä¸ºâ€œç¡®å®šè¾¹ç¼˜â€ã€‚å°½ç®¡è¾¹Cä½äºmaxValï¼Œä½†å®ƒè¿æ¥åˆ°è¾¹Aï¼Œå› æ­¤ä¹Ÿè¢«è§†ä¸ºæœ‰æ•ˆè¾¹ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å®Œæ•´çš„æ›²çº¿ã€‚ä½†æ˜¯è¾¹ç¼˜Bå°½ç®¡åœ¨minValä¹‹ä¸Šå¹¶ä¸”ä¸è¾¹ç¼˜Cå¤„äºåŒä¸€åŒºåŸŸï¼Œä½†æ˜¯å®ƒæ²¡æœ‰è¿æ¥åˆ°ä»»ä½•â€œç¡®ä¿è¾¹ç¼˜â€ï¼Œå› æ­¤è¢«ä¸¢å¼ƒã€‚å› æ­¤ï¼Œéå¸¸é‡è¦çš„ä¸€ç‚¹æ˜¯æˆ‘ä»¬å¿…é¡»ç›¸åº”åœ°é€‰æ‹©minValå’ŒmaxValä»¥è·å¾—æ­£ç¡®çš„ç»“æœã€‚</p>
<p>åœ¨è¾¹ç¼˜ä¸ºé•¿çº¿çš„å‡è®¾ä¸‹ï¼Œè¯¥é˜¶æ®µè¿˜æ¶ˆé™¤äº†å°åƒç´ å™ªå£°ã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„æ˜¯å›¾åƒä¸­çš„å¼ºè¾¹ç¼˜ã€‚</p>
<h2 id="OpenCVä¸­çš„Canny-Edgeæ£€æµ‹"><a href="#OpenCVä¸­çš„Canny-Edgeæ£€æµ‹" class="headerlink" title="OpenCVä¸­çš„Canny Edgeæ£€æµ‹"></a>OpenCVä¸­çš„Canny Edgeæ£€æµ‹</h2><p>OpenCVå°†ä»¥ä¸Šæ‰€æœ‰å†…å®¹æ”¾åœ¨å•ä¸ªå‡½æ•°<strong>cv.Canny</strong>()ä¸­ã€‚æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨å®ƒã€‚</p>
<p>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æˆ‘ä»¬çš„è¾“å…¥å›¾åƒã€‚</p>
<p>ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªå‚æ•°åˆ†åˆ«æ˜¯æˆ‘ä»¬çš„minValå’ŒmaxValã€‚</p>
<p>ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯perture_sizeã€‚å®ƒæ˜¯ç”¨äºæŸ¥æ‰¾å›¾åƒæ¸å˜çš„Sobelå†…æ ¸çš„å¤§å°ã€‚é»˜è®¤æƒ…å†µä¸‹ä¸º3ã€‚</p>
<p>æœ€åä¸€ä¸ªå‚æ•°æ˜¯L2gradientï¼Œå®ƒæŒ‡å®šç”¨äºæŸ¥æ‰¾æ¢¯åº¦å¹…åº¦çš„æ–¹ç¨‹å¼ã€‚</p>
<p>å¦‚æœä¸ºTrueï¼Œåˆ™ä½¿ç”¨ä¸Šé¢æåˆ°çš„æ›´ç²¾ç¡®çš„å…¬å¼ï¼Œå¦åˆ™ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼šEdge_Gradient(G)=|Gx|+|Gy|ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¸ºFalseã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">edges = cv.Canny(img,<span class="number">100</span>,<span class="number">200</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Original Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(edges,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Edge Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%862/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="å›¾åƒé‡‘å­—å¡”"><a href="#å›¾åƒé‡‘å­—å¡”" class="headerlink" title="å›¾åƒé‡‘å­—å¡”"></a><span id="header4">å›¾åƒé‡‘å­—å¡”</span></h1><h1 id="ç›®æ ‡-3"><a href="#ç›®æ ‡-3" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h1><p>åœ¨æœ¬ç« ä¸­ï¼Œ - æˆ‘ä»¬å°†å­¦ä¹ å›¾åƒé‡‘å­—å¡” - æˆ‘ä»¬å°†ä½¿ç”¨å›¾åƒé‡‘å­—å¡”åˆ›å»ºä¸€ä¸ªæ–°çš„æ°´æœâ€œOrappleâ€ - </p>
<p>æˆ‘ä»¬å°†çœ‹åˆ°ä»¥ä¸‹åŠŸèƒ½ï¼šcv.pyrUp()ï¼Œcv.pyrDown()</p>
<h2 id="ç†è®º-3"><a href="#ç†è®º-3" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>é€šå¸¸ï¼Œæˆ‘ä»¬è¿‡å»ä½¿ç”¨çš„æ˜¯æ’å®šå¤§å°çš„å›¾åƒã€‚ä½†æ˜¯åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡çš„ï¼ˆç›¸åŒï¼‰å›¾åƒã€‚</p>
<p>ä¾‹å¦‚ï¼Œå½“åœ¨å›¾åƒä¸­æœç´¢æŸäº›ä¸œè¥¿ï¼ˆä¾‹å¦‚äººè„¸ï¼‰æ—¶ï¼Œæˆ‘ä»¬ä¸ç¡®å®šå¯¹è±¡å°†ä»¥å¤šå¤§çš„å°ºå¯¸æ˜¾ç¤ºåœ¨å›¾åƒä¸­ã€‚</p>
<p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†éœ€è¦åˆ›å»ºä¸€ç»„å…·æœ‰ä¸åŒåˆ†è¾¨ç‡çš„ç›¸åŒå›¾åƒï¼Œå¹¶åœ¨æ‰€æœ‰å›¾åƒä¸­æœç´¢å¯¹è±¡ã€‚</p>
<p>è¿™äº›å…·æœ‰ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒé›†ç§°ä¸ºâ€œå›¾åƒé‡‘å­—å¡”â€ï¼ˆå› ä¸ºå½“å®ƒä»¬å †å åœ¨åº•éƒ¨æ—¶ï¼Œæœ€é«˜åˆ†è¾¨ç‡çš„å›¾åƒä½äºåº•éƒ¨ï¼Œæœ€ä½åˆ†è¾¨ç‡çš„å›¾åƒä½äºé¡¶éƒ¨æ—¶ï¼Œçœ‹èµ·æ¥åƒé‡‘å­—å¡”ï¼‰ã€‚</p>
<p>æœ‰ä¸¤ç§å›¾åƒé‡‘å­—å¡”ã€‚1ï¼‰é«˜æ–¯é‡‘å­—å¡”<strong>å’Œ2ï¼‰</strong>æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”</p>
<p>é«˜æ–¯é‡‘å­—å¡”ä¸­çš„è¾ƒé«˜çº§åˆ«ï¼ˆä½åˆ†è¾¨ç‡ï¼‰æ˜¯é€šè¿‡åˆ é™¤è¾ƒä½çº§åˆ«ï¼ˆè¾ƒé«˜åˆ†è¾¨ç‡ï¼‰å›¾åƒä¸­çš„è¿ç»­è¡Œå’Œåˆ—è€Œå½¢æˆçš„ã€‚</p>
<p>ç„¶åï¼Œè¾ƒé«˜çº§åˆ«çš„æ¯ä¸ªåƒç´ ç”±åŸºç¡€çº§åˆ«çš„5ä¸ªåƒç´ çš„è´¡çŒ®ä¸é«˜æ–¯æƒé‡å½¢æˆã€‚</p>
<p>é€šè¿‡è¿™æ ·åšï¼ŒMÃ—Nå›¾åƒå˜æˆM/2Ã—N/2å›¾åƒã€‚å› æ­¤é¢ç§¯å‡å°‘åˆ°åŸå§‹é¢ç§¯çš„å››åˆ†ä¹‹ä¸€ã€‚</p>
<p>å®ƒç§°ä¸ºOctaveã€‚å½“æˆ‘ä»¬åœ¨é‡‘å­—å¡”ä¸­è¶Šé ä¸Šæ—¶ï¼ˆå³åˆ†è¾¨ç‡ä¸‹é™ï¼‰ï¼Œè¿™ç§æ¨¡å¼å°±ä¼šç»§ç»­ã€‚</p>
<p>åŒæ ·ï¼Œåœ¨æ‰©å±•æ—¶ï¼Œæ¯ä¸ªçº§åˆ«çš„é¢ç§¯å˜ä¸º4å€ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥ä½¿ç”¨<strong>cv.pyrDown</strong>()å’Œ<strong>cv.pyrUp</strong>()å‡½æ•°æ‰¾åˆ°é«˜æ–¯é‡‘å­—å¡”</p>
<p>ä»¥ä¸‹æ˜¯å›¾åƒé‡‘å­—å¡”ä¸­çš„4ä¸ªçº§åˆ«ã€‚<br><img src="http://qiniu.aihubs.net/messipyr.jpg" alt><br>ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨<strong>cv.pyrUp</strong>()å‡½æ•°æŸ¥çœ‹å›¾åƒé‡‘å­—å¡”ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">higher_reso2 &#x3D; cv.pyrUp(lower_reso)</span><br></pre></td></tr></table></figure>
<p>è®°ä½ï¼Œhigher_reso2ä¸ç­‰äºhigher_resoï¼Œå› ä¸ºä¸€æ—¦é™ä½äº†åˆ†è¾¨ç‡ï¼Œå°±ä¼šä¸¢å¤±ä¿¡æ¯ã€‚ä¸‹é¢çš„å›¾åƒæ˜¯3å±‚çš„é‡‘å­—å¡”ä»æœ€å°çš„å›¾åƒåœ¨å‰é¢çš„æƒ…å†µä¸‹åˆ›å»ºã€‚ä¸åŸå›¾å¯¹æ¯”:<br><img src="http://qiniu.aihubs.net/messiup.jpg" alt></p>
<p>æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”ç”±é«˜æ–¯é‡‘å­—å¡”å½¢æˆã€‚æ²¡æœ‰ä¸“ç”¨åŠŸèƒ½ã€‚</p>
<p>æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”å›¾åƒä»…åƒè¾¹ç¼˜å›¾åƒã€‚å®ƒçš„å¤§å¤šæ•°å…ƒç´ ä¸ºé›¶ã€‚å®ƒä»¬ç”¨äºå›¾åƒå‹ç¼©ã€‚</p>
<p>æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”çš„å±‚ç”±é«˜æ–¯é‡‘å­—å¡”çš„å±‚ä¸é«˜æ–¯é‡‘å­—å¡”çš„é«˜å±‚çš„æ‰©å±•ç‰ˆæœ¬ä¹‹é—´çš„å·®å½¢æˆã€‚</p>
<p>æ‹‰æ™®æ‹‰æ–¯ç­‰çº§çš„ä¸‰ä¸ªç­‰çº§å¦‚ä¸‹æ‰€ç¤ºï¼ˆè°ƒæ•´å¯¹æ¯”åº¦ä»¥å¢å¼ºå†…å®¹ï¼‰ï¼š<br><img src="http://qiniu.aihubs.net/lap.jpg" alt></p>
<h2 id="ä½¿ç”¨é‡‘å­—å¡”è¿›è¡Œå›¾åƒèåˆ"><a href="#ä½¿ç”¨é‡‘å­—å¡”è¿›è¡Œå›¾åƒèåˆ" class="headerlink" title="ä½¿ç”¨é‡‘å­—å¡”è¿›è¡Œå›¾åƒèåˆ"></a>ä½¿ç”¨é‡‘å­—å¡”è¿›è¡Œå›¾åƒèåˆ</h2><p>é‡‘å­—å¡”çš„ä¸€ç§åº”ç”¨æ˜¯å›¾åƒèåˆã€‚</p>
<p>ä¾‹å¦‚ï¼Œåœ¨å›¾åƒæ‹¼æ¥ä¸­ï¼Œæ‚¨éœ€è¦å°†ä¸¤ä¸ªå›¾åƒå †å åœ¨ä¸€èµ·ï¼Œä½†æ˜¯ç”±äºå›¾åƒä¹‹é—´çš„ä¸è¿ç»­æ€§ï¼Œå¯èƒ½çœ‹èµ·æ¥ä¸å¤ªå¥½ã€‚</p>
<p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨é‡‘å­—å¡”æ··åˆå›¾åƒå¯ä»¥æ— ç¼æ··åˆï¼Œè€Œä¸ä¼šåœ¨å›¾åƒä¸­ä¿ç•™å¤§é‡æ•°æ®ã€‚</p>
<p>ä¸€ä¸ªç»å…¸çš„ä¾‹å­æ˜¯å°†ä¸¤ç§æ°´æœï¼Œæ©™å’Œè‹¹æœæ··åˆåœ¨ä¸€èµ·</p>
<p>åªéœ€å®Œæˆä»¥ä¸‹æ­¥éª¤å³å¯ï¼š</p>
<ul>
<li>åŠ è½½è‹¹æœå’Œæ©™å­çš„ä¸¤ä¸ªå›¾åƒ</li>
<li>æŸ¥æ‰¾è‹¹æœå’Œæ©™å­çš„é«˜æ–¯é‡‘å­—å¡”ï¼ˆåœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œ çº§åˆ«æ•°ä¸º6ï¼‰</li>
<li>åœ¨é«˜æ–¯é‡‘å­—å¡”ä¸­ï¼Œæ‰¾åˆ°å…¶æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”</li>
<li>ç„¶ååœ¨æ¯ä¸ªæ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”çº§åˆ«ä¸­åŠ å…¥è‹¹æœçš„å·¦åŠéƒ¨åˆ†å’Œæ©™å­çš„å³åŠéƒ¨åˆ†</li>
<li>æœ€åä»æ­¤è”åˆå›¾åƒé‡‘å­—å¡”ä¸­é‡å»ºåŸå§‹å›¾åƒã€‚<br><img src="http://qiniu.aihubs.net/orapple.jpg" alt></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np,sys</span><br><span class="line">A = cv.imread(<span class="string">'apple.png'</span>)</span><br><span class="line">B = cv.imread(<span class="string">'orange.png'</span>)</span><br><span class="line"><span class="comment"># ç”ŸæˆAçš„é«˜æ–¯é‡‘å­—å¡”</span></span><br><span class="line">G = A.copy()</span><br><span class="line">gpA = [G]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    G = cv.pyrDown(G)</span><br><span class="line">    gpA.append(G)</span><br><span class="line"><span class="comment"># ç”ŸæˆBçš„é«˜æ–¯é‡‘å­—å¡”</span></span><br><span class="line">G = B.copy()</span><br><span class="line">gpB = [G]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    G = cv.pyrDown(G)</span><br><span class="line">    gpB.append(G)</span><br><span class="line"><span class="comment"># ç”ŸæˆAçš„æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”</span></span><br><span class="line">lpA = [gpA[<span class="number">5</span>]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    GE = cv.pyrUp(gpA[i])</span><br><span class="line">    L = cv.subtract(gpA[i<span class="number">-1</span>],GE)</span><br><span class="line">    lpA.append(L)</span><br><span class="line"><span class="comment"># ç”ŸæˆBçš„æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”</span></span><br><span class="line">lpB = [gpB[<span class="number">5</span>]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    GE = cv.pyrUp(gpB[i])</span><br><span class="line">    L = cv.subtract(gpB[i<span class="number">-1</span>],GE)</span><br><span class="line">    lpB.append(L)</span><br><span class="line"><span class="comment"># ç°åœ¨åœ¨æ¯ä¸ªçº§åˆ«ä¸­æ·»åŠ å·¦å³ä¸¤åŠå›¾åƒ </span></span><br><span class="line">LS = []</span><br><span class="line"><span class="keyword">for</span> la,lb <span class="keyword">in</span> zip(lpA,lpB):</span><br><span class="line">    rows,cols,dpt = la.shape</span><br><span class="line">    ls = np.hstack((la[:,<span class="number">0</span>:cols/<span class="number">2</span>], lb[:,cols/<span class="number">2</span>:]))</span><br><span class="line">    LS.append(ls)</span><br><span class="line"><span class="comment"># ç°åœ¨é‡å»º</span></span><br><span class="line">ls_ = LS[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    ls_ = cv.pyrUp(ls_)</span><br><span class="line">    ls_ = cv.add(ls_, LS[i])</span><br><span class="line"><span class="comment"># å›¾åƒä¸ç›´æ¥è¿æ¥çš„æ¯ä¸€åŠ</span></span><br><span class="line">real = np.hstack((A[:,:cols/<span class="number">2</span>],B[:,cols/<span class="number">2</span>:]))</span><br><span class="line">cv.imwrite(<span class="string">'Pyramid_blending2.jpg'</span>,ls_)</span><br><span class="line">cv.imwrite(<span class="string">'Direct_blending.jpg'</span>,real)</span><br><span class="line"><span class="comment">##</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>å›¾åƒ</tag>
      </tags>
  </entry>
  <entry>
    <title>opencvä¸­çš„å›¾åƒå¤„ç†3-è½®å»“</title>
    <url>/2020/07/13/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%863-%E8%BD%AE%E5%BB%93/</url>
    <content><![CDATA[<ul>
<li>9.1.<a href="#header1">è½®å»“å…¥é—¨</a></li>
<li>9.2.<a href="#header2">è½®å»“ç‰¹å¾</a></li>
<li>9.3.<a href="#header3">è½®å»“å±æ€§</a></li>
<li>9.4.<a href="#header4">è½®å»“æ›´å¤šå±æ€§</a></li>
<li>9.5.<a href="#header5">è½®å»“åˆ†å±‚</a><a id="more"></a>

</li>
</ul>
<h1 id="è½®å»“ï¼šå…¥é—¨"><a href="#è½®å»“ï¼šå…¥é—¨" class="headerlink" title="è½®å»“ï¼šå…¥é—¨"></a><span id="header1">è½®å»“ï¼šå…¥é—¨</span></h1><p>##ç›®æ ‡<br>äº†è§£è½®å»“æ˜¯ä»€ä¹ˆã€‚</p>
<p>å­¦ä¹ æŸ¥æ‰¾è½®å»“ï¼Œç»˜åˆ¶è½®å»“ç­‰ã€‚</p>
<p>ä½ å°†çœ‹åˆ°ä»¥ä¸‹åŠŸèƒ½ï¼šcv.findContours()ï¼Œcv.drawContours()</p>
<h2 id="ä»€ä¹ˆæ˜¯è½®å»“"><a href="#ä»€ä¹ˆæ˜¯è½®å»“" class="headerlink" title="ä»€ä¹ˆæ˜¯è½®å»“?"></a>ä»€ä¹ˆæ˜¯è½®å»“?</h2><p>è½®å»“å¯ä»¥ç®€å•åœ°è§£é‡Šä¸ºè¿æ¥å…·æœ‰ç›¸åŒé¢œè‰²æˆ–å¼ºåº¦çš„æ‰€æœ‰è¿ç»­ç‚¹ï¼ˆæ²¿è¾¹ç•Œï¼‰çš„æ›²çº¿ã€‚</p>
<p>è½®å»“æ˜¯ç”¨äºå½¢çŠ¶åˆ†æä»¥åŠå¯¹è±¡æ£€æµ‹å’Œè¯†åˆ«çš„æœ‰ç”¨å·¥å…·ã€‚</p>
<p>ä¸ºäº†è·å¾—æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè¯·ä½¿ç”¨äºŒè¿›åˆ¶å›¾åƒã€‚å› æ­¤ï¼Œåœ¨æ‰¾åˆ°è½®å»“ä¹‹å‰ï¼Œè¯·åº”ç”¨é˜ˆå€¼æˆ–cannyè¾¹ç¼˜æ£€æµ‹ã€‚</p>
<p>ä»OpenCV 3.2å¼€å§‹ï¼ŒfindContours()ä¸å†ä¿®æ”¹æºå›¾åƒã€‚</p>
<p>åœ¨OpenCVä¸­ï¼Œæ‰¾åˆ°è½®å»“å°±åƒä»é»‘è‰²èƒŒæ™¯ä¸­æ‰¾åˆ°ç™½è‰²ç‰©ä½“ã€‚å› æ­¤è¯·è®°ä½ï¼Œè¦æ‰¾åˆ°çš„å¯¹è±¡åº”è¯¥æ˜¯ç™½è‰²ï¼ŒèƒŒæ™¯åº”è¯¥æ˜¯é»‘è‰²ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv.threshold(imgray, <span class="number">127</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">unknown,contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)</span><br></pre></td></tr></table></figure>

<p>findcontour()å‡½æ•°ä¸­æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œ</p>
<p>ç¬¬ä¸€ä¸ªæ˜¯æºå›¾åƒï¼Œ</p>
<p>ç¬¬äºŒä¸ªæ˜¯è½®å»“æ£€ç´¢æ¨¡å¼ï¼Œ</p>
<p>ç¬¬ä¸‰ä¸ªæ˜¯è½®å»“é€¼è¿‘æ–¹æ³•ã€‚</p>
<p>è¾“å‡ºæœªçŸ¥é‡,ç­‰é«˜çº¿å’Œå±‚æ¬¡ç»“æ„ã€‚</p>
<p>è½®å»“æ˜¯å›¾åƒä¸­æ‰€æœ‰è½®å»“çš„Pythonåˆ—è¡¨ã€‚</p>
<p>æ¯ä¸ªå•ç‹¬çš„è½®å»“æ˜¯ä¸€ä¸ª(x,y)åæ ‡çš„Numpyæ•°ç»„çš„è¾¹ç•Œç‚¹çš„å¯¹è±¡ã€‚</p>
<p>æ³¨æ„ ç¨åæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºç¬¬äºŒå’Œç¬¬ä¸‰ä¸ªå‚æ•°ä»¥åŠæœ‰å…³å±‚æ¬¡ç»“æ„ã€‚</p>
<p>åœ¨æ­¤ä¹‹å‰ï¼Œä»£ç ç¤ºä¾‹ä¸­èµ‹äºˆå®ƒä»¬çš„å€¼å°†é€‚ç”¨äºæ‰€æœ‰å›¾åƒã€‚</p>
<h2 id="å¦‚ä½•ç»˜åˆ¶è½®å»“"><a href="#å¦‚ä½•ç»˜åˆ¶è½®å»“" class="headerlink" title="å¦‚ä½•ç»˜åˆ¶è½®å»“?"></a>å¦‚ä½•ç»˜åˆ¶è½®å»“?</h2><p>è¦ç»˜åˆ¶è½®å»“ï¼Œè¯·ä½¿ç”¨<strong>cv.drawContours</strong>å‡½æ•°ã€‚åªè¦æœ‰è¾¹ç•Œç‚¹ï¼Œå®ƒä¹Ÿå¯ä»¥ç”¨æ¥ç»˜åˆ¶ä»»ä½•å½¢çŠ¶ã€‚</p>
<p>å®ƒçš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æºå›¾åƒï¼Œ</p>
<p>ç¬¬äºŒä¸ªå‚æ•°æ˜¯åº”è¯¥ä½œä¸ºPythonåˆ—è¡¨ä¼ é€’çš„è½®å»“ï¼Œ</p>
<p>ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯è½®å»“çš„ç´¢å¼•ï¼ˆåœ¨ç»˜åˆ¶å•ä¸ªè½®å»“æ—¶æœ‰ç”¨ã€‚è¦ç»˜åˆ¶æ‰€æœ‰è½®å»“ï¼Œè¯·ä¼ é€’-1ï¼‰ï¼Œå…¶ä½™å‚æ•°æ˜¯é¢œè‰²ï¼Œåšåº¦ç­‰ç­‰</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åœ¨å›¾åƒä¸­ç»˜åˆ¶æ‰€æœ‰è½®å»“ï¼š</span></span><br><span class="line">cv.drawContours(img, contours, <span class="number">-1</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å•ä¸ªè½®å»“ï¼Œå¦‚ç¬¬å››ä¸ªè½®å»“ï¼š</span></span><br><span class="line">cnt = contours[<span class="number">4</span>]</span><br><span class="line">cv.drawContours(img, [cnt], <span class="number">0</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ä½†æ˜¯åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä»¥ä¸‹æ–¹æ³•ä¼šå¾ˆæœ‰ç”¨ï¼š</span></span><br><span class="line">cnt = contours[<span class="number">4</span>]</span><br><span class="line">cv.drawContours(img, [cnt], <span class="number">0</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<p>è½®å»“è¿‘ä¼¼æ–¹æ³•<br>è¿™æ˜¯<strong>cv.findContours</strong>å‡½æ•°ä¸­çš„ç¬¬ä¸‰ä¸ªå‚æ•°ã€‚å®ƒå®é™…ä¸Šè¡¨ç¤ºä»€ä¹ˆï¼Ÿ</p>
<p>ä¸Šé¢æˆ‘ä»¬å‘Šè¯‰æˆ‘ä»¬è½®å»“æ˜¯å¼ºåº¦ç›¸åŒçš„å½¢çŠ¶çš„è¾¹ç•Œã€‚</p>
<p>å®ƒå­˜å‚¨å½¢çŠ¶è¾¹ç•Œçš„(x,y)åæ ‡ã€‚ä½†æ˜¯å®ƒå­˜å‚¨æ‰€æœ‰åæ ‡å—ï¼Ÿè¿™æ˜¯é€šè¿‡è¿™ç§è½®å»“è¿‘ä¼¼æ–¹æ³•æŒ‡å®šçš„ã€‚</p>
<p>å¦‚æœä¼ é€’<strong>cv.CHAIN_APPROX_NONE</strong>ï¼Œåˆ™å°†å­˜å‚¨æ‰€æœ‰è¾¹ç•Œç‚¹ã€‚</p>
<p>ä½†æ˜¯å®é™…ä¸Šæˆ‘ä»¬éœ€è¦æ‰€æœ‰è¿™äº›è¦ç‚¹å—ï¼Ÿ</p>
<p>ä¾‹å¦‚ï¼Œæ‚¨æ‰¾åˆ°äº†ä¸€æ¡ç›´çº¿çš„è½®å»“ã€‚æ‚¨æ˜¯å¦éœ€è¦çº¿ä¸Šçš„æ‰€æœ‰ç‚¹æ¥ä»£è¡¨è¯¥çº¿ï¼Ÿ</p>
<p>ä¸ï¼Œæˆ‘ä»¬åªéœ€è¦è¯¥çº¿çš„ä¸¤ä¸ªç«¯ç‚¹å³å¯ã€‚</p>
<p>è¿™å°±æ˜¯<strong>cv.CHAIN_APPROX_SIMPLE</strong>æ‰€åšçš„ã€‚å®ƒåˆ é™¤æ‰€æœ‰å†—ä½™ç‚¹å¹¶å‹ç¼©è½®å»“ï¼Œä»è€ŒèŠ‚çœå†…å­˜ã€‚</p>
<p>ä¸‹é¢çš„çŸ©å½¢å›¾åƒæ¼”ç¤ºäº†æ­¤æŠ€æœ¯ã€‚</p>
<p>åªéœ€åœ¨è½®å»“æ•°ç»„ä¸­çš„æ‰€æœ‰åæ ‡ä¸Šç»˜åˆ¶ä¸€ä¸ªåœ†ï¼ˆä»¥è“è‰²ç»˜åˆ¶ï¼‰ã€‚</p>
<p>ç¬¬ä¸€å¹…å›¾åƒæ˜¾ç¤ºäº†æˆ‘ç”¨<strong>cv.CHAIN_APPROX_NONE</strong>è·å¾—çš„ç§¯åˆ†ï¼ˆ734ä¸ªç‚¹ï¼‰ï¼Œ</p>
<p>ç¬¬äºŒå¹…å›¾åƒæ˜¾ç¤ºäº†æˆ‘ç”¨<strong>cv.CHAIN_APPROX_SIMPLE</strong>è·å¾—çš„æ•ˆæœï¼ˆåªæœ‰4ä¸ªç‚¹ï¼‰ã€‚</p>
<p>çœ‹ï¼Œå®ƒå¯ä»¥èŠ‚çœå¤šå°‘å†…å­˜ï¼ï¼ï¼</p>
<p><img src="http://qiniu.aihubs.net/none.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="è½®å»“ç‰¹å¾"><a href="#è½®å»“ç‰¹å¾" class="headerlink" title="è½®å»“ç‰¹å¾"></a><span id="header2">è½®å»“ç‰¹å¾</span></h1><h2 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  - å¦‚ä½•æ‰¾åˆ°è½®å»“çš„ä¸åŒç‰¹å¾ï¼Œ</p>
<p>ä¾‹å¦‚é¢ç§¯ï¼Œå‘¨é•¿ï¼Œè´¨å¿ƒï¼Œè¾¹ç•Œæ¡†ç­‰ã€‚ - æ‚¨å°†çœ‹åˆ°å¤§é‡ä¸è½®å»“æœ‰å…³çš„åŠŸèƒ½ã€‚</p>
<h2 id="1-ç‰¹å¾çŸ©"><a href="#1-ç‰¹å¾çŸ©" class="headerlink" title="1. ç‰¹å¾çŸ©"></a>1. ç‰¹å¾çŸ©</h2><p>ç‰¹å¾çŸ©å¯ä»¥å¸®åŠ©æ‚¨è®¡ç®—ä¸€äº›ç‰¹å¾ï¼Œä¾‹å¦‚ç‰©ä½“çš„è´¨å¿ƒï¼Œç‰©ä½“çš„é¢ç§¯ç­‰ã€‚</p>
<p>è¯·æŸ¥çœ‹ç‰¹å¾çŸ©ä¸Šçš„ç»´åŸºç™¾ç§‘é¡µé¢ã€‚</p>
<p>å‡½æ•°<strong>cv.moments</strong>()æä¾›äº†æ‰€æœ‰è®¡ç®—å‡ºçš„çŸ©å€¼çš„å­—å…¸ã€‚è§ä¸‹æ–‡ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">ret,thresh = cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line">unknown,contours,hierarchy = cv.findContours(thresh, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line">M = cv.moments(cnt)</span><br><span class="line">print( M )</span><br></pre></td></tr></table></figure>

<pre><code>{&apos;m00&apos;: 2.0, &apos;m10&apos;: 536.0, &apos;m01&apos;: 1272.0, &apos;m20&apos;: 143648.3333333333, &apos;m11&apos;: 340896.0, &apos;m02&apos;: 808992.3333333333, &apos;m30&apos;: 38497932.0, &apos;m21&apos;: 91360340.0, &apos;m12&apos;: 216809945.33333334, &apos;m03&apos;: 514519548.0, &apos;mu20&apos;: 0.3333333333139308, &apos;mu11&apos;: 0.0, &apos;mu02&apos;: 0.3333333332557231, &apos;mu30&apos;: 1.4901161193847656e-08, &apos;mu21&apos;: 1.234002411365509e-08, &apos;mu12&apos;: 3.073364496231079e-08, &apos;mu03&apos;: 1.1920928955078125e-07, &apos;nu20&apos;: 0.0833333333284827, &apos;nu11&apos;: 0.0, &apos;nu02&apos;: 0.08333333331393078, &apos;nu30&apos;: 2.634178031930877e-09, &apos;nu21&apos;: 2.1814286826927578e-09, &apos;nu12&apos;: 5.432992190857434e-09, &apos;nu03&apos;: 2.1073424255447017e-08}</code></pre><p>ä»è¿™ä¸€åˆ»èµ·ï¼Œæ‚¨å¯ä»¥æå–æœ‰ç”¨çš„æ•°æ®ï¼Œ<br>ä¾‹å¦‚é¢ç§¯ï¼Œè´¨å¿ƒç­‰ã€‚è´¨å¿ƒç”±å…³ç³»ç»™å‡ºï¼Œ$C_x\frac{M_{10}}{M_{00}}$ å’Œ $C_y\frac{M_{01}}{M_{00}}$ã€‚å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cx = int(M[<span class="string">'m10'</span>]/M[<span class="string">'m00'</span>])</span><br><span class="line">cy = int(M[<span class="string">'m01'</span>]/M[<span class="string">'m00'</span>])</span><br></pre></td></tr></table></figure>

<h2 id="2-è½®å»“é¢ç§¯"><a href="#2-è½®å»“é¢ç§¯" class="headerlink" title="2. è½®å»“é¢ç§¯"></a>2. è½®å»“é¢ç§¯</h2><p>è½®å»“åŒºåŸŸç”±å‡½æ•°<strong>cv.contourArea</strong>()æˆ–ä»çŸ©M[â€˜m00â€™]ä¸­ç»™å‡ºã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">area = cv.contourArea(cnt) </span><br><span class="line">print(area)</span><br><span class="line">print(M[<span class="string">'m00'</span>])</span><br></pre></td></tr></table></figure>

<pre><code>2.0
2.0</code></pre><h2 id="3-è½®å»“å‘¨é•¿"><a href="#3-è½®å»“å‘¨é•¿" class="headerlink" title="3. è½®å»“å‘¨é•¿"></a>3. è½®å»“å‘¨é•¿</h2><p>ä¹Ÿç§°ä¸ºå¼§é•¿ã€‚å¯ä»¥ä½¿ç”¨<strong>cv.arcLength</strong>()å‡½æ•°æ‰¾åˆ°å®ƒã€‚ç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šå½¢çŠ¶æ˜¯é—­åˆè½®å»“(True)è¿˜æ˜¯æ›²çº¿ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">perimeter = cv.arcLength(cnt,<span class="literal">True</span>)</span><br><span class="line">perimeter</span><br></pre></td></tr></table></figure>




<pre><code>5.656854152679443</code></pre><h2 id="4-è½®å»“è¿‘ä¼¼"><a href="#4-è½®å»“è¿‘ä¼¼" class="headerlink" title="4. è½®å»“è¿‘ä¼¼"></a>4. è½®å»“è¿‘ä¼¼</h2><p>æ ¹æ®æˆ‘ä»¬æŒ‡å®šçš„ç²¾åº¦ï¼Œå®ƒå¯ä»¥å°†è½®å»“å½¢çŠ¶è¿‘ä¼¼ä¸ºé¡¶ç‚¹æ•°é‡è¾ƒå°‘çš„å…¶ä»–å½¢çŠ¶ã€‚</p>
<p>å®ƒæ˜¯Douglas-Peuckerç®—æ³•çš„å®ç°ã€‚æ£€æŸ¥ç»´åŸºç™¾ç§‘é¡µé¢ä¸Šçš„ç®—æ³•å’Œæ¼”ç¤ºã€‚</p>
<p>ä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ï¼Œå‡è®¾æ‚¨è¯•å›¾åœ¨å›¾åƒä¸­æ‰¾åˆ°ä¸€ä¸ªæ­£æ–¹å½¢ï¼Œä½†æ˜¯ç”±äºå›¾åƒä¸­çš„æŸäº›é—®é¢˜ï¼Œæ‚¨æ²¡æœ‰å¾—åˆ°ä¸€ä¸ªå®Œç¾çš„æ­£æ–¹å½¢ï¼Œè€Œæ˜¯ä¸€ä¸ªâ€œåå½¢çŠ¶â€ï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ã€‚</p>
<p>ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½æ¥è¿‘ä¼¼å½¢çŠ¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¬¬äºŒä¸ªå‚æ•°ç§°ä¸ºepsilonï¼Œå®ƒæ˜¯ä»è½®å»“åˆ°è¿‘ä¼¼è½®å»“çš„æœ€å¤§è·ç¦»ã€‚</p>
<p>å®ƒæ˜¯ä¸€ä¸ªç²¾åº¦å‚æ•°ã€‚éœ€è¦æ­£ç¡®é€‰æ‹©epsilonæ‰èƒ½è·å¾—æ­£ç¡®çš„è¾“å‡ºã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">0.1</span>*cv.arcLength(cnt,<span class="literal">True</span>) </span><br><span class="line">approx = cv.approxPolyDP(cnt,epsilon,<span class="literal">True</span>)</span><br><span class="line">print(epsilon)</span><br><span class="line">print(approx)</span><br></pre></td></tr></table></figure>

<pre><code>0.5656854152679444
[[[267 636]]

 [[268 635]]

 [[269 636]]

 [[268 637]]]</code></pre><p>ä¸‹é¢ï¼Œåœ¨ç¬¬äºŒå¼ å›¾ç‰‡ä¸­ï¼Œç»¿çº¿æ˜¾ç¤ºäº†Îµ=å¼§é•¿çš„10ï¼…æ—¶çš„è¿‘ä¼¼æ›²çº¿ã€‚ç¬¬ä¸‰å¹…å›¾æ˜¾ç¤ºäº†Îµ=å¼§é•¿åº¦çš„1ï¼…æ—¶çš„æƒ…å†µã€‚ç¬¬ä¸‰ä¸ªå‚æ•°æŒ‡å®šæ›²çº¿æ˜¯å¦é—­åˆã€‚<br><img src="http://qiniu.aihubs.net/approx.jpg" alt></p>
<h2 id="5-è½®å»“å‡¸åŒ…"><a href="#5-è½®å»“å‡¸åŒ…" class="headerlink" title="5. è½®å»“å‡¸åŒ…"></a>5. è½®å»“å‡¸åŒ…</h2><p>å‡¸åŒ…å¤–è§‚çœ‹èµ·æ¥ä¸è½®å»“é€¼è¿‘ç›¸ä¼¼ï¼Œä½†ä¸ç›¸ä¼¼ï¼ˆåœ¨æŸäº›æƒ…å†µä¸‹ä¸¤è€…å¯èƒ½æä¾›ç›¸åŒçš„ç»“æœï¼‰ã€‚</p>
<p>åœ¨è¿™é‡Œï¼Œcv.convexHull()å‡½æ•°æ£€æŸ¥æ›²çº¿æ˜¯å¦å­˜åœ¨å‡¸å‡¹ç¼ºé™·å¹¶å¯¹å…¶è¿›è¡Œæ ¡æ­£ã€‚</p>
<p>ä¸€èˆ¬è€Œè¨€ï¼Œå‡¸æ›²çº¿æ˜¯å§‹ç»ˆå‡¸å‡ºæˆ–è‡³å°‘å¹³å¦çš„æ›²çº¿ã€‚å¦‚æœåœ¨å†…éƒ¨å‡¸å‡ºï¼Œåˆ™ç§°ä¸ºå‡¸åº¦ç¼ºé™·ã€‚</p>
<p>ä¾‹å¦‚ï¼Œæ£€æŸ¥ä¸‹é¢çš„æ‰‹çš„å›¾åƒã€‚çº¢çº¿æ˜¾ç¤ºæ‰‹çš„å‡¸åŒ…ã€‚åŒå‘ç®­å¤´æ ‡è®°æ˜¾ç¤ºå‡¸åº¦ç¼ºé™·ï¼Œè¿™æ˜¯å‡¸åŒ…ä¸è½®å»“çº¿ä¹‹é—´çš„å±€éƒ¨æœ€å¤§åå·®ã€‚</p>
<p><img src="http://qiniu.aihubs.net/convexitydefects.jpg" alt></p>
<p>å…³äºå®ƒçš„è¯­æ³•ï¼Œæœ‰ä¸€äº›éœ€è¦è®¨è®ºï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hull &#x3D; cv.convexHull(points[, hull[, clockwise[, returnPoints]]</span><br></pre></td></tr></table></figure>
<p>å‚æ•°è¯¦ç»†ä¿¡æ¯ï¼š </p>
<ul>
<li>ç‚¹**æ˜¯æˆ‘ä»¬ä¼ é€’åˆ°çš„è½®å»“ã€‚ </li>
<li><strong>å‡¸åŒ…</strong>æ˜¯è¾“å‡ºï¼Œé€šå¸¸æˆ‘ä»¬å¿½ç•¥å®ƒã€‚ </li>
<li>**é¡ºæ—¶é’ˆæ–¹å‘ï¼šæ–¹å‘æ ‡è®°ã€‚å¦‚æœä¸ºTrueï¼Œåˆ™è¾“å‡ºå‡¸åŒ…ä¸ºé¡ºæ—¶é’ˆæ–¹å‘ã€‚å¦åˆ™ï¼Œå…¶æ–¹å‘ä¸ºé€†æ—¶é’ˆæ–¹å‘ã€‚ </li>
<li>returnPointsï¼šé»˜è®¤æƒ…å†µä¸‹ä¸ºTrueã€‚ç„¶åè¿”å›å‡¸åŒ…çš„åæ ‡ã€‚å¦‚æœä¸ºFalseï¼Œåˆ™è¿”å›ä¸å‡¸åŒ…ç‚¹ç›¸å¯¹åº”çš„è½®å»“ç‚¹çš„ç´¢å¼•ã€‚</li>
</ul>
<p>å› æ­¤ï¼Œè¦è·å¾—å¦‚ä¸Šå›¾æ‰€ç¤ºçš„å‡¸åŒ…ï¼Œä»¥ä¸‹å†…å®¹å°±è¶³å¤Ÿäº†ï¼š<br>``<br>hull = cv.convexHull(cnt) </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ä½†æ˜¯ï¼Œå¦‚æœè¦æŸ¥æ‰¾å‡¸åº¦ç¼ºé™·ï¼Œåˆ™éœ€è¦ä¼ é€’returnPoints &#x3D; Falseã€‚</span><br><span class="line"></span><br><span class="line">ä¸ºäº†ç†è§£å®ƒï¼Œæˆ‘ä»¬å°†æ‹æ‘„ä¸Šé¢çš„çŸ©å½¢å›¾åƒã€‚</span><br><span class="line"></span><br><span class="line">é¦–å…ˆï¼Œæˆ‘å‘ç°å®ƒçš„è½®å»“ä¸ºcntã€‚ç°åœ¨ï¼Œæˆ‘å‘ç°å®ƒçš„å¸¦æœ‰returnPoints &#x3D; Trueçš„å‡¸åŒ…ï¼Œ</span><br><span class="line"></span><br><span class="line">å¾—åˆ°ä»¥ä¸‹å€¼ï¼š[[[234 202]]ï¼Œ[[51 202]]ï¼Œ[[51 79]]ï¼Œ[[234 79]]]ï¼Œå®ƒä»¬æ˜¯å››ä¸ªè§’ çŸ©å½¢çš„ç‚¹ã€‚</span><br><span class="line"></span><br><span class="line">ç°åœ¨ï¼Œå¦‚æœå¯¹returnPoints &#x3D; Falseæ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œ</span><br><span class="line"></span><br><span class="line">åˆ™ä¼šå¾—åˆ°ä»¥ä¸‹ç»“æœï¼š[[129]ï¼Œ[67]ï¼Œ[0]ï¼Œ[142]]ã€‚è¿™äº›æ˜¯è½®å»“ä¸­ç›¸åº”ç‚¹çš„ç´¢å¼•ã€‚</span><br><span class="line"></span><br><span class="line">ä¾‹å¦‚ï¼Œæ£€æŸ¥ç¬¬ä¸€ä¸ªå€¼ï¼šcnt [129] &#x3D; [[234ï¼Œ202]]ä¸ç¬¬ä¸€ä¸ªç»“æœç›¸åŒï¼ˆå¯¹äºå…¶ä»–ç»“æœä¾æ­¤ç±»æ¨ï¼‰ã€‚</span><br><span class="line"></span><br><span class="line">å½“æˆ‘ä»¬è®¨è®ºå‡¸åº¦ç¼º</span><br><span class="line"></span><br><span class="line">## 6. æ£€æŸ¥å‡¸åº¦</span><br><span class="line">cv.isContourConvex()å…·æœ‰æ£€æŸ¥æ›²çº¿æ˜¯å¦å‡¸å‡ºçš„åŠŸèƒ½ã€‚å®ƒåªæ˜¯è¿”å›Trueè¿˜æ˜¯Falseã€‚æ²¡ä»€ä¹ˆå¤§ä¸äº†çš„ã€‚</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">k &#x3D; cv.isContourConvex(cnt) </span><br><span class="line">k</span><br></pre></td></tr></table></figure>




<pre><code>True</code></pre><h2 id="7-è¾¹ç•ŒçŸ©å½¢"><a href="#7-è¾¹ç•ŒçŸ©å½¢" class="headerlink" title="7. è¾¹ç•ŒçŸ©å½¢"></a>7. è¾¹ç•ŒçŸ©å½¢</h2><p>æœ‰ä¸¤ç§ç±»å‹çš„è¾¹ç•ŒçŸ©å½¢ã€‚</p>
<h3 id="7-a-ç›´è§’çŸ©å½¢"><a href="#7-a-ç›´è§’çŸ©å½¢" class="headerlink" title="7.a.ç›´è§’çŸ©å½¢"></a>7.a.ç›´è§’çŸ©å½¢</h3><p>å®ƒæ˜¯ä¸€ä¸ªçŸ©å½¢ï¼Œä¸è€ƒè™‘ç‰©ä½“çš„æ—‹è½¬ã€‚æ‰€ä»¥è¾¹ç•ŒçŸ©å½¢çš„é¢ç§¯ä¸æ˜¯æœ€å°çš„ã€‚</p>
<p>å®ƒæ˜¯ç”±å‡½æ•°<strong>cv.boundingRect</strong>()æ‰¾åˆ°çš„ã€‚</p>
<p>ä»¤(xï¼Œy)ä¸ºçŸ©å½¢çš„å·¦ä¸Šè§’åæ ‡ï¼Œè€Œ(wï¼Œh)ä¸ºçŸ©å½¢çš„å®½åº¦å’Œé«˜åº¦ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x,y,w,h &#x3D; cv.boundingRect(cnt)</span><br><span class="line">cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)</span><br></pre></td></tr></table></figure>
<h3 id="7-b-æ—‹è½¬çŸ©å½¢"><a href="#7-b-æ—‹è½¬çŸ©å½¢" class="headerlink" title="7.b. æ—‹è½¬çŸ©å½¢"></a>7.b. æ—‹è½¬çŸ©å½¢</h3><p>è¿™é‡Œï¼Œè¾¹ç•ŒçŸ©å½¢æ˜¯ç”¨æœ€å°é¢ç§¯ç»˜åˆ¶çš„ï¼Œæ‰€ä»¥å®ƒä¹Ÿè€ƒè™‘äº†æ—‹è½¬ã€‚</p>
<p>ä½¿ç”¨çš„å‡½æ•°æ˜¯<strong>cv.minAreaRect</strong>()ã€‚</p>
<p>å®ƒè¿”å›ä¸€ä¸ªBox2Dç»“æ„ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹ç»†èŠ‚ -(ä¸­å¿ƒ(x,y)ï¼Œ(å®½åº¦ï¼Œé«˜åº¦)ï¼Œæ—‹è½¬è§’åº¦)ã€‚</p>
<p>ä½†è¦ç”»å‡ºè¿™ä¸ªçŸ©å½¢ï¼Œæˆ‘ä»¬éœ€è¦çŸ©å½¢çš„å››ä¸ªè§’ã€‚</p>
<p>å®ƒç”±å‡½æ•°<strong>cv.boxPoints</strong>()è·å¾—</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rect &#x3D; cv.minAreaRect(cnt)</span><br><span class="line">box &#x3D; cv.boxPoints(rect)</span><br><span class="line">box &#x3D; np.int0(box)</span><br><span class="line">cv.drawContours(img,[box],0,(0,0,255),2)</span><br></pre></td></tr></table></figure>
<p>ä¸¤ä¸ªçŸ©å½¢éƒ½æ˜¾ç¤ºåœ¨ä¸€å¼ å•ç‹¬çš„å›¾åƒä¸­ã€‚ç»¿è‰²çŸ©å½¢æ˜¾ç¤ºæ­£å¸¸çš„è¾¹ç•ŒçŸ©å½¢ã€‚çº¢è‰²çŸ©å½¢æ˜¯æ—‹è½¬åçš„çŸ©å½¢ã€‚</p>
<p><img src="http://qiniu.aihubs.net/boundingrect.png" alt></p>
<h2 id="8-æœ€å°é—­åˆåœˆ"><a href="#8-æœ€å°é—­åˆåœˆ" class="headerlink" title="8. æœ€å°é—­åˆåœˆ"></a>8. æœ€å°é—­åˆåœˆ</h2><p>æ¥ä¸‹æ¥ï¼Œä½¿ç”¨å‡½æ•°<em>*cv.minEnclosingCircle(</em>()æŸ¥æ‰¾å¯¹è±¡çš„åœ†å‘¨ã€‚å®ƒæ˜¯ä¸€ä¸ªä»¥æœ€å°é¢ç§¯å®Œå…¨è¦†ç›–ç‰©ä½“çš„åœ†ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(x,y),radius &#x3D; cv.minEnclosingCircle(cnt)</span><br><span class="line">center &#x3D; (int(x),int(y))</span><br><span class="line">radius &#x3D; int(radius)</span><br><span class="line">cv.circle(img,center,radius,(0,255,0),2)</span><br><span class="line">&#96;&#96;&#96;                            </span><br><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;circumcircle.png)</span><br><span class="line"></span><br><span class="line">## 9. æ‹Ÿåˆä¸€ä¸ªæ¤­åœ†</span><br><span class="line">ä¸‹ä¸€ä¸ªæ˜¯æŠŠä¸€ä¸ªæ¤­åœ†æ‹Ÿåˆåˆ°ä¸€ä¸ªç‰©ä½“ä¸Šã€‚å®ƒè¿”å›å†…æ¥æ¤­åœ†çš„æ—‹è½¬çŸ©å½¢ã€‚</span><br></pre></td></tr></table></figure>
<p>ellipse = cv.fitEllipse(cnt)<br>cv.ellipse(img,ellipse,(0,255,0),2)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;fitellipse.png)</span><br><span class="line"></span><br><span class="line">## 10. æ‹Ÿåˆç›´çº¿</span><br><span class="line">åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸€æ¡ç›´çº¿æ‹Ÿåˆåˆ°ä¸€ç»„ç‚¹ã€‚ä¸‹å›¾åŒ…å«ä¸€ç»„ç™½ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥è¿‘ä¼¼ä¸€æ¡ç›´çº¿ã€‚</span><br></pre></td></tr></table></figure>
<p>rows,cols = img.shape[:2]<br>[vx,vy,x,y] = cv.fitLine(cnt, cv.DIST_L2,0,0.01,0.01)<br>lefty = int((-x<em>vy/vx) + y)<br>righty = int(((cols-x)</em>vy/vx)+y)<br>cv.line(img,(cols-1,righty),(0,lefty),(0,255,0),2)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># &lt;span id&#x3D;&quot;header3&quot;&gt;è½®å»“å±æ€§&lt;&#x2F;span&gt;</span><br><span class="line">åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å­¦ä¹ æå–ä¸€äº›å¸¸ç”¨çš„ç‰©ä½“å±æ€§ï¼Œ</span><br><span class="line"></span><br><span class="line">å¦‚åšå®åº¦ï¼Œç­‰æ•ˆç›´å¾„ï¼Œæ©æ¨¡å›¾åƒï¼Œå¹³å‡å¼ºåº¦ç­‰ã€‚æ›´å¤šçš„åŠŸèƒ½å¯ä»¥åœ¨Matlab regionpropsæ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚</span><br><span class="line"></span><br><span class="line">(æ³¨:è´¨å¿ƒã€é¢ç§¯ã€å‘¨é•¿ç­‰ä¹Ÿå±äºè¿™ä¸€ç±»ï¼Œä½†æˆ‘ä»¬åœ¨ä¸Šä¸€ç« å·²ç»è§è¿‡)</span><br><span class="line"></span><br><span class="line">## 1. é•¿å®½æ¯”</span><br><span class="line">å®ƒæ˜¯å¯¹è±¡è¾¹ç•ŒçŸ©å½¢çš„å®½åº¦ä¸é«˜åº¦çš„æ¯”å€¼ã€‚</span><br><span class="line"></span><br><span class="line">Aspect Ratio&#x3D;$\frac&#123;Width&#125;&#123;Height&#125;$</span><br></pre></td></tr></table></figure>
<p>x,y,w,h = cv.boundingRect(cnt)<br>aspect_ratio = float(w)/h</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 2. èŒƒå›´</span><br><span class="line">èŒƒå›´æ˜¯è½®å»“åŒºåŸŸä¸è¾¹ç•ŒçŸ©å½¢åŒºåŸŸçš„æ¯”å€¼ã€‚</span><br><span class="line"></span><br><span class="line">Extent&#x3D;$\frac&#123;Object Area&#125;&#123;Bounding Rectangle Area&#125;$</span><br></pre></td></tr></table></figure>
<p>area = cv.contourArea(cnt)<br>x,y,w,h = cv.boundingRect(cnt)<br>rect_area = w*h<br>extent = float(area)/rect_area</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 3. åšå®åº¦</span><br><span class="line">åšå®åº¦æ˜¯ç­‰é«˜çº¿é¢ç§¯ä¸å…¶å‡¸åŒ…é¢ç§¯ä¹‹æ¯”ã€‚</span><br><span class="line"></span><br><span class="line">Solidity&#x3D;$\frac&#123;Contour Area&#125;&#123;ConvexHull Area&#125;$</span><br></pre></td></tr></table></figure>
<p>area = cv.contourArea(cnt)<br>hull = cv.convexHull(cnt)<br>hull_area = cv.contourArea(hull)<br>solidity = float(area)/hull_area</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 4. ç­‰æ•ˆç›´å¾„</span><br><span class="line">ç­‰æ•ˆç›´å¾„æ˜¯é¢ç§¯ä¸è½®å»“é¢ç§¯ç›¸åŒçš„åœ†çš„ç›´å¾„ã€‚</span><br><span class="line"></span><br><span class="line">EquivalentDiameter&#x3D;$\sqrt&#123;\frac&#123;4Ã—ContourArea&#125;&#123;\Pi&#125;&#125;$</span><br></pre></td></tr></table></figure>
<p>area = cv.contourArea(cnt)<br>equi_diameter = np.sqrt(4*area/np.pi)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 5. å–å‘</span><br><span class="line">å–å‘æ˜¯ç‰©ä½“æŒ‡å‘çš„è§’åº¦ã€‚ä»¥ä¸‹æ–¹æ³•è¿˜ç»™å‡ºäº†ä¸»è½´å’Œå‰¯è½´çš„é•¿åº¦ã€‚</span><br></pre></td></tr></table></figure>
<p>(x,y),(MA,ma),angle = cv.fitEllipse(cnt)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 6. æ©ç å’Œåƒç´ ç‚¹</span><br><span class="line">åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æ„æˆè¯¥å¯¹è±¡çš„æ‰€æœ‰ç‚¹ã€‚å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆï¼š</span><br></pre></td></tr></table></figure>
<p>mask = np.zeros(imgray.shape,np.uint8)<br>cv.drawContours(mask,[cnt],0,255,-1)<br>pixelpoints = np.transpose(np.nonzero(mask))<br>#pixelpoints = cv.findNonZero(mask)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">è¿™é‡Œæä¾›äº†ä¸¤ä¸ªæ–¹æ³•ï¼Œä¸€ä¸ªä½¿ç”¨Numpyå‡½æ•°ï¼Œå¦ä¸€ä¸ªä½¿ç”¨OpenCVå‡½æ•°(æœ€åçš„æ³¨é‡Šè¡Œ)ã€‚ç»“æœä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯ç•¥æœ‰ä¸åŒã€‚</span><br><span class="line"></span><br><span class="line">Numpyç»™å‡ºçš„åæ ‡æ˜¯(è¡Œã€åˆ—)æ ¼å¼ï¼Œ</span><br><span class="line"></span><br><span class="line">è€ŒOpenCVç»™å‡ºçš„åæ ‡æ˜¯(x,y)æ ¼å¼ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šç­”æ¡ˆæ˜¯å¯ä»¥äº’æ¢çš„ã€‚æ³¨æ„ï¼Œrow &#x3D; x, column &#x3D; yã€‚</span><br><span class="line"></span><br><span class="line">## 7. æœ€å¤§å€¼ï¼Œæœ€å°å€¼å’Œå®ƒä»¬çš„ä½ç½®</span><br><span class="line">æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ©ç å›¾åƒæ‰¾åˆ°è¿™äº›å‚æ•°ã€‚</span><br></pre></td></tr></table></figure>
<p>min_val, max_val, min_loc, max_loc = cv.minMaxLoc(imgray,mask = mask)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 8. å¹³å‡é¢œè‰²æˆ–å¹³å‡å¼ºåº¦</span><br><span class="line">åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°å¯¹è±¡çš„å¹³å‡é¢œè‰²ã€‚æˆ–è€…å¯ä»¥æ˜¯ç°åº¦æ¨¡å¼ä¸‹ç‰©ä½“çš„å¹³å‡å¼ºåº¦ã€‚æˆ‘ä»¬å†æ¬¡ä½¿ç”¨ç›¸åŒçš„æ©ç è¿›è¡Œæ­¤æ“ä½œã€‚</span><br></pre></td></tr></table></figure>
<p>mean_val = cv.mean(im,mask = mask)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 9. æç«¯ç‚¹</span><br><span class="line">æç‚¹æ˜¯æŒ‡å¯¹è±¡çš„æœ€é¡¶éƒ¨ï¼Œæœ€åº•éƒ¨ï¼Œæœ€å³ä¾§å’Œæœ€å·¦ä¾§çš„ç‚¹ã€‚</span><br></pre></td></tr></table></figure>
<p>leftmost = tuple(cnt[cnt[:,:,0].argmin()][0])<br>rightmost = tuple(cnt[cnt[:,:,0].argmax()][0])<br>topmost = tuple(cnt[cnt[:,:,1].argmin()][0])<br>bottommost = tuple(cnt[cnt[:,:,1].argmax()][0])</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘å°†å…¶åº”ç”¨äºå°åº¦åœ°å›¾ï¼Œåˆ™ä¼šå¾—åˆ°ä»¥ä¸‹ç»“æœï¼š</span><br><span class="line">![](http:&#x2F;&#x2F;qiniu.aihubs.net&#x2F;extremepoints.jpg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br></pre></td></tr></table></figure>


<h1 id="è½®å»“ï¼šæ›´å¤šå±æ€§"><a href="#è½®å»“ï¼šæ›´å¤šå±æ€§" class="headerlink" title="è½®å»“ï¼šæ›´å¤šå±æ€§"></a><span id="header4">è½®å»“ï¼šæ›´å¤šå±æ€§</span></h1><h2 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  - å‡¸æ€§ç¼ºé™·ä»¥åŠå¦‚ä½•æ‰¾åˆ°å®ƒä»¬ - æŸ¥æ‰¾ç‚¹åˆ°å¤šè¾¹å½¢çš„æœ€çŸ­è·ç¦» - åŒ¹é…ä¸åŒçš„å½¢çŠ¶</p>
<h2 id="ç†è®ºå’Œä»£ç "><a href="#ç†è®ºå’Œä»£ç " class="headerlink" title="ç†è®ºå’Œä»£ç "></a>ç†è®ºå’Œä»£ç </h2><h3 id="1-å‡¸æ€§ç¼ºé™·"><a href="#1-å‡¸æ€§ç¼ºé™·" class="headerlink" title="1. å‡¸æ€§ç¼ºé™·"></a>1. å‡¸æ€§ç¼ºé™·</h3><p>æˆ‘ä»¬çœ‹åˆ°äº†å…³äºè½®å»“çš„ç¬¬äºŒç« çš„å‡¸åŒ…ã€‚ä»è¿™ä¸ªå‡¸åŒ…ä¸Šçš„ä»»ä½•åå·®éƒ½å¯ä»¥è¢«è®¤ä¸ºæ˜¯å‡¸æ€§ç¼ºé™·ã€‚ OpenCVæœ‰ä¸€ä¸ªå‡½æ•°æ¥æ‰¾åˆ°è¿™ä¸ª,cv.convexityDefects()ã€‚ä¸€ä¸ªåŸºæœ¬çš„å‡½æ•°è°ƒç”¨å¦‚ä¸‹:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hull &#x3D; cv.convexHull(cnt,returnPoints &#x3D; False)</span><br><span class="line">defects &#x3D; cv.convexityDefects(cnt,hull)</span><br></pre></td></tr></table></figure>
<p>æ³¨æ„ è®°ä½,æˆ‘ä»¬å¿…é¡»åœ¨å‘ç°å‡¸åŒ…æ—¶,ä¼ é€’returnPoints= False,ä»¥æ‰¾åˆ°å‡¸æ€§ç¼ºé™·ã€‚</p>
<p>å®ƒè¿”å›ä¸€ä¸ªæ•°ç»„ï¼Œå…¶ä¸­æ¯è¡ŒåŒ…å«è¿™äº›å€¼â€”[èµ·ç‚¹ã€ç»ˆç‚¹ã€æœ€è¿œç‚¹ã€åˆ°æœ€è¿œç‚¹çš„è¿‘ä¼¼è·ç¦»]ã€‚æˆ‘ä»¬å¯ä»¥ç”¨å›¾åƒæŠŠå®ƒå½¢è±¡åŒ–ã€‚æˆ‘ä»¬ç”»ä¸€æ¡è¿æ¥èµ·ç‚¹å’Œç»ˆç‚¹çš„çº¿ï¼Œç„¶ååœ¨æœ€è¿œå¤„ç”»ä¸€ä¸ªåœ†ã€‚è®°ä½ï¼Œè¿”å›çš„å‰ä¸‰ä¸ªå€¼æ˜¯cntçš„ç´¢å¼•ã€‚æ‰€ä»¥æˆ‘ä»¬å¿…é¡»ä»cntä¸­è·å–è¿™äº›å€¼ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(&#39;star.jpg&#39;)</span><br><span class="line">img_gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,thresh &#x3D; cv.threshold(img_gray, 127, 255,0)</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh,2,1)</span><br><span class="line">cnt &#x3D; contours[0]</span><br><span class="line">hull &#x3D; cv.convexHull(cnt,returnPoints &#x3D; False)</span><br><span class="line">defects &#x3D; cv.convexityDefects(cnt,hull)</span><br><span class="line">for i in range(defects.shape[0]):</span><br><span class="line">    s,e,f,d &#x3D; defects[i,0]</span><br><span class="line">    start &#x3D; tuple(cnt[s][0])</span><br><span class="line">    end &#x3D; tuple(cnt[e][0])</span><br><span class="line">    far &#x3D; tuple(cnt[f][0])</span><br><span class="line">    cv.line(img,start,end,[0,255,0],2)</span><br><span class="line">    cv.circle(img,far,5,[0,0,255],-1)</span><br><span class="line">cv.imshow(&#39;img&#39;,img)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>æŸ¥çœ‹ç»“æœï¼š<br><img src="http://qiniu.aihubs.net/defects.jpg" alt></p>
<h3 id="2-ç‚¹å¤šè¾¹å½¢æµ‹è¯•"><a href="#2-ç‚¹å¤šè¾¹å½¢æµ‹è¯•" class="headerlink" title="2. ç‚¹å¤šè¾¹å½¢æµ‹è¯•"></a>2. ç‚¹å¤šè¾¹å½¢æµ‹è¯•</h3><p>è¿™ä¸ªå‡½æ•°æ‰¾å‡ºå›¾åƒä¸­ä¸€ç‚¹åˆ°è½®å»“çº¿çš„æœ€çŸ­è·ç¦»ã€‚å®ƒè¿”å›çš„è·ç¦»ï¼Œç‚¹åœ¨è½®å»“çº¿å¤–æ—¶ä¸ºè´Ÿï¼Œç‚¹åœ¨è½®å»“çº¿å†…æ—¶ä¸ºæ­£ï¼Œç‚¹åœ¨è½®å»“çº¿ä¸Šæ—¶ä¸ºé›¶ã€‚</p>
<p>ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç‚¹(50,50)å¦‚ä¸‹:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dist &#x3D; cv.pointPolygonTest(cnt,(50,50),True)</span><br></pre></td></tr></table></figure>
<p>åœ¨å‡½æ•°ä¸­ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯measureDistã€‚å¦‚æœå®ƒæ˜¯çœŸçš„ï¼Œå®ƒä¼šæ‰¾åˆ°æœ‰ç¬¦å·çš„è·ç¦»ã€‚å¦‚æœä¸ºå‡ï¼Œåˆ™æŸ¥æ‰¾è¯¥ç‚¹æ˜¯åœ¨è½®å»“çº¿å†…éƒ¨è¿˜æ˜¯å¤–éƒ¨(åˆ†åˆ«è¿”å›+1ã€-1å’Œ0)ã€‚</p>
<p>æ³¨æ„ å¦‚æœæ‚¨ä¸æƒ³æ‰¾åˆ°è·ç¦»ï¼Œè¯·ç¡®ä¿ç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºFalseï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹ã€‚å› æ­¤ï¼Œå°†å…¶è®¾ç½®ä¸ºFalseå¯ä½¿é€Ÿåº¦æé«˜2-3å€ã€‚</p>
<h3 id="3-å½¢çŠ¶åŒ¹é…"><a href="#3-å½¢çŠ¶åŒ¹é…" class="headerlink" title="3. å½¢çŠ¶åŒ¹é…"></a>3. å½¢çŠ¶åŒ¹é…</h3><p>OpenCVé™„å¸¦ä¸€ä¸ªå‡½æ•°<strong>cv.matchShapes</strong>()ï¼Œè¯¥å‡½æ•°ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¯”è¾ƒä¸¤ä¸ªå½¢çŠ¶æˆ–ä¸¤ä¸ªè½®å»“ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ˜¾ç¤ºç›¸ä¼¼æ€§çš„åº¦é‡ã€‚ç»“æœè¶Šä½ï¼ŒåŒ¹é…è¶Šå¥½ã€‚å®ƒæ˜¯æ ¹æ®çŸ©å€¼è®¡ç®—å‡ºæ¥çš„ã€‚ä¸åŒçš„æµ‹é‡æ–¹æ³•åœ¨æ–‡æ¡£ä¸­æœ‰è§£é‡Šã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img1 &#x3D; cv.imread(&#39;star.jpg&#39;,0)</span><br><span class="line">img2 &#x3D; cv.imread(&#39;star2.jpg&#39;,0)</span><br><span class="line">ret, thresh &#x3D; cv.threshold(img1, 127, 255,0)</span><br><span class="line">ret, thresh2 &#x3D; cv.threshold(img2, 127, 255,0)</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh,2,1)</span><br><span class="line">cnt1 &#x3D; contours[0]</span><br><span class="line">contours,hierarchy &#x3D; cv.findContours(thresh2,2,1)</span><br><span class="line">cnt2 &#x3D; contours[0]</span><br><span class="line">ret &#x3D; cv.matchShapes(cnt1,cnt2,1,0.0)</span><br><span class="line">print( ret )</span><br></pre></td></tr></table></figure>
<p>æˆ‘å°è¯•è¿‡åŒ¹é…ä¸‹é¢ç»™å‡ºçš„ä¸åŒå½¢çŠ¶çš„å½¢çŠ¶ï¼š<br><img src="http://qiniu.aihubs.net/matchshapes.jpg" alt></p>
<p>æˆ‘å¾—åˆ°ä»¥ä¸‹ç»“æœ: - åŒ¹é…çš„å›¾åƒAä¸æœ¬èº«= 0.0 - åŒ¹é…å›¾åƒAä¸å›¾åƒB = 0.001946 - åŒ¹é…å›¾åƒAä¸å›¾åƒC = 0.326911</p>
<p>çœ‹,å³ä½¿æ˜¯å›¾åƒæ—‹è½¬ä¹Ÿä¸ä¼šå¯¹è¿™ä¸ªæ¯”è¾ƒäº§ç”Ÿå¾ˆå¤§çš„å½±å“ã€‚</p>
<p>å‚è€ƒ HuçŸ©æ˜¯å¹³ç§»ã€æ—‹è½¬å’Œæ¯”ä¾‹ä¸å˜çš„ä¸ƒä¸ªçŸ©ã€‚ç¬¬ä¸ƒä¸ªæ˜¯æ— åæ–œé‡ã€‚è¿™äº›å€¼å¯ä»¥ä½¿ç”¨<strong>cpu.HuMoments</strong>()å‡½æ•°æ‰¾åˆ°ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="è½®å»“åˆ†å±‚"><a href="#è½®å»“åˆ†å±‚" class="headerlink" title="è½®å»“åˆ†å±‚"></a><span id="header5">è½®å»“åˆ†å±‚</span></h1><h2 id="ç›®æ ‡-2"><a href="#ç›®æ ‡-2" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>è¿™æ¬¡æˆ‘ä»¬å­¦ä¹ è½®å»“çš„å±‚æ¬¡ï¼Œå³è½®å»“ä¸­çš„çˆ¶å­å…³ç³»ã€‚</p>
<h2 id="ç†è®º"><a href="#ç†è®º" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>åœ¨å‰å‡ ç¯‡å…³äºè½®å»“çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å·²ç»è®¨è®ºäº†ä¸OpenCVæä¾›çš„è½®å»“ç›¸å…³çš„å‡ ä¸ªå‡½æ•°ã€‚</p>
<p>ä½†æ˜¯å½“æˆ‘ä»¬ä½¿ç”¨<strong>cv.findcontour</strong>()å‡½æ•°åœ¨å›¾åƒä¸­æ‰¾åˆ°è½®å»“æ—¶ï¼Œæˆ‘ä»¬å·²ç»ä¼ é€’äº†ä¸€ä¸ªå‚æ•°ï¼Œè½®å»“æ£€ç´¢æ¨¡å¼ã€‚</p>
<p>æˆ‘ä»¬é€šå¸¸é€šè¿‡äº†<strong>cv.RETR_LIST</strong>æˆ–<strong>cv.RETR_TREE</strong>ï¼Œæ•ˆæœå¾ˆå¥½ã€‚ä½†è¿™åˆ°åº•æ„å‘³ç€ä»€ä¹ˆå‘¢?</p>
<p>å¦å¤–ï¼Œåœ¨è¾“å‡ºä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸‰ä¸ªæ•°ç»„ï¼Œç¬¬ä¸€ä¸ªæ˜¯å›¾åƒï¼Œç¬¬äºŒä¸ªæ˜¯è½®å»“ï¼Œè¿˜æœ‰ä¸€ä¸ªæˆ‘ä»¬å‘½åä¸º<strong>hierarchy</strong>çš„è¾“å‡º(è¯·æ£€æŸ¥å‰é¢æ–‡ç« ä¸­çš„ä»£ç )ã€‚</p>
<p>ä½†æˆ‘ä»¬ä»æœªåœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨è¿‡è¿™ç§å±‚æ¬¡ç»“æ„ã€‚é‚£ä¹ˆè¿™ä¸ªå±‚çº§æ˜¯ä»€ä¹ˆ?å®ƒæ˜¯ç”¨æ¥åšä»€ä¹ˆçš„?å®ƒä¸å‰é¢æåˆ°çš„å‡½æ•°å‚æ•°æœ‰ä»€ä¹ˆå…³ç³»?</p>
<p>è¿™å°±æ˜¯æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­è¦è®¨è®ºçš„å†…å®¹ã€‚</p>
<h2 id="å±‚æ¬¡ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ"><a href="#å±‚æ¬¡ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ" class="headerlink" title="å±‚æ¬¡ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ"></a>å±‚æ¬¡ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ</h2><p>é€šå¸¸æˆ‘ä»¬ä½¿ç”¨<strong>cv.findcontour</strong>()å‡½æ•°æ¥æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡ï¼Œå¯¹å§ï¼Ÿ</p>
<p>æœ‰æ—¶å¯¹è±¡åœ¨ä¸åŒçš„ä½ç½®ã€‚ä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒæŸäº›å½¢çŠ¶åœ¨å…¶ä»–å½¢çŠ¶ä¸­ã€‚</p>
<p>å°±åƒåµŒå¥—çš„å›¾å½¢ä¸€æ ·ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æŠŠå¤–éƒ¨çš„ç§°ä¸º<strong>çˆ¶ç±»</strong>ï¼ŒæŠŠå†…éƒ¨çš„ç§°ä¸º<strong>å­ç±»</strong>ã€‚</p>
<p>è¿™æ ·ï¼Œå›¾åƒä¸­çš„è½®å»“å°±æœ‰äº†ä¸€å®šçš„ç›¸äº’å…³ç³»ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥æŒ‡å®šä¸€ä¸ªè½®å»“æ˜¯å¦‚ä½•ç›¸äº’è¿æ¥çš„ï¼Œæ¯”å¦‚ï¼Œå®ƒæ˜¯å¦ä¸€ä¸ªè½®å»“çš„å­è½®å»“ï¼Œè¿˜æ˜¯çˆ¶è½®å»“ç­‰ç­‰ã€‚è¿™ç§å…³ç³»çš„è¡¨ç¤ºç§°ä¸º<strong>å±‚æ¬¡ç»“æ„</strong>ã€‚</p>
<p><img src="http://qiniu.aihubs.net/hierarchy.png" alt><br>åœ¨è¿™å¼ å›¾ä¸­ï¼Œæœ‰ä¸€äº›å½¢çŠ¶æˆ‘å·²ç»ä»<strong>0-5</strong>å¼€å§‹ç¼–å·ã€‚<em>2</em>å’Œ<em>2a</em>è¡¨ç¤ºæœ€å¤–å±‚ç›’å­çš„å¤–éƒ¨å’Œå†…éƒ¨è½®å»“ã€‚</p>
<p>è¿™é‡Œï¼Œç­‰é«˜çº¿0,1,2åœ¨<strong>å¤–éƒ¨æˆ–æœ€å¤–é¢</strong>ã€‚æˆ‘ä»¬å¯ä»¥è¯´ï¼Œå®ƒä»¬åœ¨<strong>å±‚çº§-0</strong>ä¸­ï¼Œæˆ–è€…ç®€å•åœ°è¯´ï¼Œå®ƒä»¬åœ¨<strong>åŒä¸€ä¸ªå±‚çº§</strong>ä¸­ã€‚</p>
<p>å…¶æ¬¡æ˜¯<strong>contour-2a</strong>ã€‚å®ƒå¯ä»¥è¢«è®¤ä¸ºæ˜¯<strong>contour-2çš„å­çº§</strong>(æˆ–è€…åè¿‡æ¥ï¼Œcontour-2æ˜¯contour-2açš„çˆ¶çº§)ã€‚</p>
<p>å‡è®¾å®ƒåœ¨<strong>å±‚çº§-1</strong>ä¸­ã€‚ç±»ä¼¼åœ°ï¼Œcontour-3æ˜¯contour-2çš„å­çº§ï¼Œå®ƒä½äºä¸‹ä¸€ä¸ªå±‚æ¬¡ç»“æ„ä¸­ã€‚</p>
<p>æœ€åï¼Œè½®å»“4,5æ˜¯contour-3açš„å­çº§ï¼Œä»–ä»¬åœ¨æœ€åä¸€ä¸ªå±‚çº§ã€‚</p>
<p>ä»å¯¹æ–¹æ¡†çš„ç¼–å·æ¥çœ‹ï¼Œæˆ‘è®¤ä¸ºcontour-4æ˜¯contour-3açš„ç¬¬ä¸€ä¸ªå­çº§(å®ƒä¹Ÿå¯ä»¥æ˜¯contour-5)ã€‚</p>
<p>æˆ‘æåˆ°è¿™äº›æ˜¯ä¸ºäº†ç†è§£ä¸€äº›æœ¯è¯­ï¼Œæ¯”å¦‚<strong>ç›¸åŒå±‚çº§</strong>ï¼Œå¤–éƒ¨è½®å»“ï¼Œå­è½®å»“ï¼Œçˆ¶è½®å»“ï¼Œ<strong>ç¬¬ä¸€ä¸ªå­è½®å»“</strong>ç­‰ç­‰ã€‚ç°åœ¨è®©æˆ‘ä»¬è¿›å…¥OpenCVã€‚</p>
<h2 id="OpenCVä¸­çš„åˆ†çº§è¡¨ç¤º"><a href="#OpenCVä¸­çš„åˆ†çº§è¡¨ç¤º" class="headerlink" title="OpenCVä¸­çš„åˆ†çº§è¡¨ç¤º"></a>OpenCVä¸­çš„åˆ†çº§è¡¨ç¤º</h2><p>æ‰€ä»¥æ¯ä¸ªè½®å»“éƒ½æœ‰å®ƒè‡ªå·±çš„ä¿¡æ¯å…³äºå®ƒæ˜¯ä»€ä¹ˆå±‚æ¬¡ï¼Œè°æ˜¯å®ƒçš„å­©å­ï¼Œè°æ˜¯å®ƒçš„çˆ¶æ¯ç­‰ç­‰ã€‚</p>
<p>OpenCVå°†å®ƒè¡¨ç¤ºä¸ºä¸€ä¸ªåŒ…å«å››ä¸ªå€¼çš„æ•°ç»„:[Next, Previous, First_Child, Parent]</p>
<p>â€œNextè¡¨ç¤ºåŒä¸€å±‚æ¬¡çš„ä¸‹ä¸€ä¸ªè½®å»“ã€‚â€</p>
<p>ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„å›¾ç‰‡ä¸­å–contour-0ã€‚è°æ˜¯ä¸‹ä¸€ä¸ªåŒçº§åˆ«çš„ç­‰é«˜çº¿?è¿™æ˜¯contour-1ã€‚</p>
<p>ç®€å•åœ°ä»¤Next = 1ã€‚ç±»ä¼¼åœ°ï¼ŒContour-1ä¹Ÿæ˜¯contour-2ã€‚æ‰€ä»¥Next = 2ã€‚ contour-2å‘¢?åŒä¸€æ°´å¹³çº¿ä¸Šæ²¡æœ‰ä¸‹ä¸€æ¡ç­‰é«˜çº¿ã€‚</p>
<p>ç®€å•åœ°ï¼Œè®©Next = -1ã€‚contour-4å‘¢?å®ƒä¸contour-5å¤„äºåŒä¸€çº§åˆ«ã€‚å®ƒçš„ä¸‹ä¸€æ¡ç­‰é«˜çº¿æ˜¯contour-5ï¼Œæ‰€ä»¥next = 5ã€‚</p>
<p>â€œPreviousè¡¨ç¤ºåŒä¸€å±‚æ¬¡ä¸Šçš„å…ˆå‰è½®å»“ã€‚â€</p>
<p>å’Œä¸Šé¢ä¸€æ ·ã€‚contour-1ä¹‹å‰çš„ç­‰å€¼çº¿ä¸ºåŒçº§åˆ«çš„contour-0ã€‚</p>
<p>ç±»ä¼¼åœ°ï¼Œcontour-2ä¹Ÿæ˜¯contour-1ã€‚å¯¹äºcontour-0ï¼Œæ²¡æœ‰å‰é¡¹ï¼Œæ‰€ä»¥è®¾ä¸º-1ã€‚</p>
<p>â€œFirst_Childè¡¨ç¤ºå®ƒçš„ç¬¬ä¸€ä¸ªå­è½®å»“ã€‚â€</p>
<p>æ²¡æœ‰å¿…è¦ä½œä»»ä½•è§£é‡Šã€‚å¯¹äºcontour-2, childæ˜¯contour-2aã€‚ä»è€Œå¾—åˆ°contour-2aå¯¹åº”çš„æŒ‡æ ‡å€¼ã€‚</p>
<p>contour-3aå‘¢?å®ƒæœ‰ä¸¤ä¸ªå­©å­ã€‚ä½†æˆ‘ä»¬åªå…³æ³¨ç¬¬ä¸€ä¸ªå­©å­ã€‚å®ƒæ˜¯contour-4ã€‚é‚£ä¹ˆFirst_Child = 4 å¯¹contour-3aè€Œè¨€ã€‚</p>
<p>â€œParentè¡¨ç¤ºå…¶çˆ¶è½®å»“çš„ç´¢å¼•ã€‚â€</p>
<p>å®ƒä¸<strong>First_Child</strong>ç›¸åã€‚å¯¹äºè½®å»“çº¿-4å’Œè½®å»“çº¿-5ï¼Œçˆ¶è½®å»“çº¿éƒ½æ˜¯è½®å»“çº¿-3aã€‚å¯¹äºè½®å»“3aï¼Œå®ƒæ˜¯è½®å»“-3ï¼Œä»¥æ­¤ç±»æ¨ã€‚</p>
<p>æ³¨æ„ å¦‚æœæ²¡æœ‰å­å…ƒç´ æˆ–çˆ¶å…ƒç´ ï¼Œåˆ™è¯¥å­—æ®µè¢«è§†ä¸º-1</p>
<p>ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†OpenCVä¸­ä½¿ç”¨çš„å±‚æ¬¡æ ·å¼ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ©ä¸Šé¢ç»™å‡ºçš„ç›¸åŒå›¾åƒæ¥æ£€æŸ¥OpenCVä¸­çš„è½®å»“æ£€ç´¢æ¨¡å¼ã€‚</p>
<p>ä¸€äº›æ ‡å¿—å¦‚ cv.RETR_LIST, cv.RETR_TREE,cv.RETR_CCOMP, <strong>cv.RETR_EXTERNAL</strong>ç­‰ç­‰çš„å«ä¹‰ã€‚</p>
<h2 id="è½®å»“æ£€ç´¢æ¨¡å¼"><a href="#è½®å»“æ£€ç´¢æ¨¡å¼" class="headerlink" title="è½®å»“æ£€ç´¢æ¨¡å¼"></a>è½®å»“æ£€ç´¢æ¨¡å¼</h2><h3 id="1-RETR-LIST"><a href="#1-RETR-LIST" class="headerlink" title="1. RETR_LIST"></a>1. RETR_LIST</h3><p>è¿™æ˜¯å››ä¸ªæ ‡å¿—ä¸­æœ€ç®€å•çš„ä¸€ä¸ª(ä»è§£é‡Šçš„è§’åº¦æ¥çœ‹)ã€‚å®ƒåªæ˜¯æ£€ç´¢æ‰€æœ‰çš„è½®å»“ï¼Œä½†ä¸åˆ›å»ºä»»ä½•äº²å­å…³ç³»ã€‚</p>
<p>åœ¨è¿™ä¸ªè§„åˆ™ä¸‹ï¼Œçˆ¶è½®å»“å’Œå­è½®å»“æ˜¯å¹³ç­‰çš„ï¼Œä»–ä»¬åªæ˜¯è½®å»“ã€‚ä»–ä»¬éƒ½å±äºåŒä¸€å±‚çº§ã€‚</p>
<p>è¿™é‡Œï¼Œç¬¬3å’Œç¬¬4é¡¹æ€»æ˜¯-1ã€‚ä½†æ˜¯å¾ˆæ˜æ˜¾ï¼Œä¸‹ä¸€é¡¹å’Œä¸Šä¸€é¡¹éƒ½æœ‰å¯¹åº”çš„å€¼ã€‚ä½ è‡ªå·±æ£€æŸ¥ä¸€ä¸‹å°±å¯ä»¥äº†ã€‚</p>
<p>ä¸‹é¢æ˜¯æˆ‘å¾—åˆ°çš„ç»“æœï¼Œæ¯ä¸€è¡Œæ˜¯å¯¹åº”è½®å»“çš„å±‚æ¬¡ç»†èŠ‚ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€è¡Œå¯¹åº”äºè½®å»“0ã€‚ä¸‹ä¸€æ¡è½®å»“æ˜¯è½®å»“1ã€‚æ‰€ä»¥Next = 1ã€‚</p>
<p>æ²¡æœ‰å…ˆå‰çš„è½®å»“ï¼Œæ‰€ä»¥Previous=-1ã€‚å‰©ä¸‹çš„ä¸¤ä¸ªï¼Œå¦‚å‰æ‰€è¿°ï¼Œæ˜¯-1ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 1, -1, -1, -1],</span><br><span class="line">        [ 2,  0, -1, -1],</span><br><span class="line">        [ 3,  1, -1, -1],</span><br><span class="line">        [ 4,  2, -1, -1],</span><br><span class="line">        [ 5,  3, -1, -1],</span><br><span class="line">        [ 6,  4, -1, -1],</span><br><span class="line">        [ 7,  5, -1, -1],</span><br><span class="line">        [-1,  6, -1, -1]]])</span><br></pre></td></tr></table></figure>
<p>å¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨ä»»ä½•å±‚æ¬¡ç»“æ„ç‰¹æ€§ï¼Œé‚£ä¹ˆè¿™æ˜¯åœ¨æ‚¨çš„ä»£ç ä¸­ä½¿ç”¨çš„æœ€ä½³é€‰æ‹©ã€‚</p>
<h3 id="2-RETR-EXTERNAL"><a href="#2-RETR-EXTERNAL" class="headerlink" title="2. RETR_EXTERNAL"></a>2. RETR_EXTERNAL</h3><p>å¦‚æœä½¿ç”¨æ­¤æ ‡å¿—ï¼Œå®ƒåªè¿”å›æç«¯å¤–éƒ¨æ ‡å¿—ã€‚æ‰€æœ‰å­©å­çš„è½®å»“éƒ½è¢«ç•™ä¸‹äº†ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥è¯´ï¼Œæ ¹æ®è¿™é¡¹è§„åˆ™ï¼Œæ¯ä¸ªå®¶åº­åªæœ‰é•¿å­å¾—åˆ°å…³æ³¨ã€‚å®ƒä¸å…³å¿ƒå®¶åº­çš„å…¶ä»–æˆå‘˜:)ã€‚</p>
<p>æ‰€ä»¥åœ¨æˆ‘ä»¬çš„å›¾åƒä¸­ï¼Œæœ‰å¤šå°‘ä¸ªæç«¯çš„å¤–è½®å»“?åœ¨ç­‰çº§0çº§?æœ‰3ä¸ªï¼Œå³ç­‰å€¼çº¿æ˜¯0 1 2ï¼Œå¯¹å§?</p>
<p>ç°åœ¨è¯•ç€ç”¨è¿™ä¸ªæ ‡å¿—æ‰¾å‡ºç­‰é«˜çº¿ã€‚è¿™é‡Œï¼Œç»™æ¯ä¸ªå…ƒç´ çš„å€¼ä¸ä¸Šé¢ç›¸åŒã€‚å¹¶ä¸ä¸Šè¿°ç»“æœè¿›è¡Œäº†æ¯”è¾ƒã€‚ä»¥ä¸‹æ˜¯æˆ‘å¾—åˆ°çš„:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 1, -1, -1, -1],</span><br><span class="line">        [ 2,  0, -1, -1],</span><br><span class="line">        [-1,  1, -1, -1]]])</span><br></pre></td></tr></table></figure>
<p>å¦‚æœåªæƒ³æå–å¤–éƒ¨è½®å»“ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ ‡å¿—ã€‚å®ƒåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æœ‰ç”¨ã€‚</p>
<h3 id="3-RETR-CCOMP"><a href="#3-RETR-CCOMP" class="headerlink" title="3. RETR_CCOMP"></a>3. RETR_CCOMP</h3><p>æ­¤æ ‡å¿—æ£€ç´¢æ‰€æœ‰è½®å»“å¹¶å°†å…¶æ’åˆ—ä¸º2çº§å±‚æ¬¡ç»“æ„ã€‚ç‰©ä½“çš„å¤–éƒ¨è½®å»“(å³ç‰©ä½“çš„è¾¹ç•Œ)æ”¾åœ¨å±‚æ¬¡ç»“æ„-1ä¸­ã€‚</p>
<p>å¯¹è±¡å†…éƒ¨å­”æ´çš„è½®å»“(å¦‚æœæœ‰)æ”¾åœ¨å±‚æ¬¡ç»“æ„-2ä¸­ã€‚å¦‚æœå…¶ä¸­æœ‰ä»»ä½•å¯¹è±¡ï¼Œåˆ™å…¶è½®å»“ä»…åœ¨å±‚æ¬¡ç»“æ„1ä¸­é‡æ–°æ”¾ç½®ã€‚ä»¥åŠå®ƒåœ¨å±‚çº§2ä¸­çš„æ¼æ´ç­‰ç­‰ã€‚</p>
<p>åªéœ€è€ƒè™‘åœ¨é»‘è‰²èƒŒæ™¯ä¸Šçš„â€œç™½è‰²çš„é›¶â€å›¾åƒã€‚é›¶çš„å¤–åœ†å±äºç¬¬ä¸€çº§ï¼Œé›¶çš„å†…åœ†å±äºç¬¬äºŒçº§ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„å›¾åƒæ¥è§£é‡Šå®ƒã€‚è¿™é‡Œæˆ‘ç”¨çº¢è‰²æ ‡æ³¨äº†ç­‰é«˜çº¿çš„é¡ºåºå’Œå®ƒä»¬æ‰€å±çš„å±‚æ¬¡ï¼Œç”¨ç»¿è‰²æ ‡æ³¨(1æˆ–2)ï¼Œé¡ºåºä¸OpenCVæ£€æµ‹ç­‰é«˜çº¿çš„é¡ºåºç›¸åŒã€‚<br><img src="http://qiniu.aihubs.net/ccomp_hierarchy.png" alt><br>è€ƒè™‘ç¬¬ä¸€ä¸ªè½®å»“ï¼Œå³contour-0ã€‚è¿™æ˜¯hierarchy-1ã€‚å®ƒæœ‰ä¸¤ä¸ªå­”ï¼Œåˆ†åˆ«æ˜¯ç­‰é«˜çº¿1å’Œ2ï¼Œå±äºç¬¬äºŒçº§ã€‚</p>
<p>å› æ­¤ï¼Œå¯¹äºè½®å»“-0ï¼Œåœ¨åŒä¸€å±‚æ¬¡çš„ä¸‹ä¸€ä¸ªè½®å»“æ˜¯è½®å»“-3ã€‚previousä¹Ÿæ²¡æœ‰ã€‚åœ¨hierarchy-2ä¸­ï¼Œå®ƒçš„ç¬¬ä¸€ä¸ªå­ç»“ç‚¹æ˜¯contour-1ã€‚</p>
<p>å®ƒæ²¡æœ‰çˆ¶ç±»ï¼Œå› ä¸ºå®ƒåœ¨hierarchy-1ä¸­ã€‚æ‰€ä»¥å®ƒçš„å±‚æ¬¡æ•°ç»„æ˜¯[3ï¼Œ-1,1ï¼Œ-1]</p>
<p>ç°åœ¨contour-1ã€‚å®ƒåœ¨å±‚çº§-2ä¸­ã€‚ç›¸åŒå±‚æ¬¡ç»“æ„ä¸­çš„ä¸‹ä¸€ä¸ª(åœ¨contour-1çš„çˆ¶æ¯å…³ç³»ä¸‹)æ˜¯contour-2ã€‚</p>
<p>æ²¡æœ‰previousã€‚æ²¡æœ‰childï¼Œä½†æ˜¯parentæ˜¯contour-0ã€‚æ‰€ä»¥æ•°ç»„æ˜¯[2ï¼Œ-1ï¼Œ-1,0]</p>
<p>ç±»ä¼¼çš„contour-2:å®ƒåœ¨hierarchy-2ä¸­ã€‚åœ¨contour-0ä¸‹ï¼ŒåŒä¸€å±‚æ¬¡ç»“æ„ä¸­æ²¡æœ‰ä¸‹ä¸€ä¸ªè½®å»“ã€‚</p>
<p>æ‰€ä»¥æ²¡æœ‰Nextã€‚previousæ˜¯contour-1ã€‚æ²¡æœ‰childï¼Œparentæ˜¯contour0ã€‚æ‰€ä»¥æ•°ç»„æ˜¯[-1,1ï¼Œ-1,0]</p>
<p>contour-3:å±‚æ¬¡-1çš„ä¸‹ä¸€ä¸ªæ˜¯è½®å»“-5ã€‚ä»¥å‰æ˜¯contour-0ã€‚childæ˜¯contour4ï¼Œæ²¡æœ‰parentã€‚æ‰€ä»¥æ•°ç»„æ˜¯[5,0,4ï¼Œ-1]</p>
<p>contour-4:å®ƒåœ¨contour-3ä¸‹çš„å±‚æ¬¡ç»“æ„2ä¸­ï¼Œå®ƒæ²¡æœ‰å…„å¼Ÿå§å¦¹ã€‚æ²¡æœ‰nextï¼Œæ²¡æœ‰previousï¼Œæ²¡æœ‰childï¼Œparentæ˜¯contour-3ã€‚</p>
<p>æ‰€ä»¥æ•°ç»„æ˜¯[-1ï¼Œ-1ï¼Œ-1,3]</p>
<p>å‰©ä¸‹çš„ä½ å¯ä»¥è¡¥å……ã€‚è¿™æ˜¯æˆ‘å¾—åˆ°çš„æœ€ç»ˆç­”æ¡ˆ:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 3, -1,  1, -1],</span><br><span class="line">        [ 2, -1, -1,  0],</span><br><span class="line">        [-1,  1, -1,  0],</span><br><span class="line">        [ 5,  0,  4, -1],</span><br><span class="line">        [-1, -1, -1,  3],</span><br><span class="line">        [ 7,  3,  6, -1],</span><br><span class="line">        [-1, -1, -1,  5],</span><br><span class="line">        [ 8,  5, -1, -1],</span><br><span class="line">        [-1,  7, -1, -1]]])</span><br></pre></td></tr></table></figure>
<h3 id="4-RETR-TREE"><a href="#4-RETR-TREE" class="headerlink" title="4. RETR_TREE"></a>4. RETR_TREE</h3><p>è¿™æ˜¯æœ€åä¸€ä¸ªå®¶ä¼™ï¼Œå®Œç¾å…ˆç”Ÿã€‚å®ƒæ£€ç´¢æ‰€æœ‰çš„è½®å»“å¹¶åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„å®¶æ—å±‚æ¬¡ç»“æ„åˆ—è¡¨ã€‚å®ƒç”šè‡³å‘Šè¯‰ï¼Œè°æ˜¯çˆ·çˆ·ï¼Œçˆ¶äº²ï¼Œå„¿å­ï¼Œå­™å­ï¼Œç”šè‡³æ›´å¤šâ€¦:)ã€‚</p>
<p>ä¾‹å¦‚ï¼Œæˆ‘æ‹¿ä¸Šé¢çš„å›¾ç‰‡ï¼Œé‡å†™äº†cvçš„ä»£ç ã€‚RETR_TREEï¼Œæ ¹æ®OpenCVç»™å‡ºçš„ç»“æœé‡æ–°æ’åºç­‰é«˜çº¿å¹¶è¿›è¡Œåˆ†æã€‚</p>
<p>åŒæ ·ï¼Œçº¢è‰²çš„å­—æ¯è¡¨ç¤ºè½®å»“æ•°ï¼Œç»¿è‰²çš„å­—æ¯è¡¨ç¤ºå±‚æ¬¡é¡ºåºã€‚<br><img src="http://qiniu.aihubs.net/tree_hierarchy.png" alt><br>å–contour-0:å®ƒåœ¨hierarchy-0ä¸­ã€‚åŒä¸€å±‚æ¬¡ç»“æ„çš„nextè½®å»“æ˜¯è½®å»“-7ã€‚æ²¡æœ‰previousçš„è½®å»“ã€‚childæ˜¯contour-1ï¼Œæ²¡æœ‰parentã€‚æ‰€ä»¥æ•°ç»„æ˜¯[7ï¼Œ-1,1ï¼Œ-1]</p>
<p>ä»¥contour-2ä¸ºä¾‹:å®ƒåœ¨hierarchy-1ä¸­ã€‚æ²¡æœ‰è½®å»“åœ¨åŒä¸€æ°´å¹³ã€‚æ²¡æœ‰previousã€‚childæ˜¯contour-3ã€‚çˆ¶æ¯æ˜¯contour-1ã€‚æ‰€ä»¥æ•°ç»„æ˜¯[-1ï¼Œ-1,3,1]</p>
<p>å‰©ä¸‹çš„ï¼Œä½ è‡ªå·±è¯•è¯•ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´ç­”æ¡ˆ:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; hierarchy</span><br><span class="line">array([[[ 7, -1,  1, -1],</span><br><span class="line">        [-1, -1,  2,  0],</span><br><span class="line">        [-1, -1,  3,  1],</span><br><span class="line">        [-1, -1,  4,  2],</span><br><span class="line">        [-1, -1,  5,  3],</span><br><span class="line">        [ 6, -1, -1,  4],</span><br><span class="line">        [-1,  5, -1,  4],</span><br><span class="line">        [ 8,  0, -1, -1],</span><br><span class="line">        [-1,  7, -1, -1]]])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>å›¾åƒ</tag>
      </tags>
  </entry>
  <entry>
    <title>notebookä½¿ç”¨</title>
    <url>/2020/07/08/notebook%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">ä¸»é¢˜ä¿®æ”¹</a></li>
<li>2.<a href="#header2">å¯†ç ä¿®æ”¹</a></li>
<li>3.<a href="#header3">æ·»åŠ å†…æ ¸</a></li>
</ul>
<a id="more"></a>

<h1 id="ä¸»é¢˜ä¿®æ”¹å‚è€ƒ"><a href="#ä¸»é¢˜ä¿®æ”¹å‚è€ƒ" class="headerlink" title="ä¸»é¢˜ä¿®æ”¹å‚è€ƒ"></a><span id="header1">ä¸»é¢˜ä¿®æ”¹<a href="https://blog.csdn.net/wh8514/article/details/81532286/" target="_blank" rel="noopener">å‚è€ƒ</a></span></h1><p>å®‰è£…Jupyterä¸»é¢˜ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install  jupyterthemes</span><br></pre></td></tr></table></figure>
<p>æ›´æ–°Jupyterä¸»é¢˜ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install --upgrade jupyterthemes</span><br></pre></td></tr></table></figure>
<p>è£…å’Œæ›´æ–°æˆåŠŸä»¥åï¼Œå¯ä»¥æŸ¥çœ‹å¯ç”¨ä¸»é¢˜ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jt -l</span><br></pre></td></tr></table></figure>
<p>å¦‚å›¾<img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/2.jpg" alt="x"><br>å‚æ•°å«ä¹‰å¦‚ä¸‹<br><img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/3.png" alt="x"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jt -t monokai -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T -N</span><br><span class="line">-f(å­—ä½“)  -fs(å­—ä½“å¤§å°) -cellw(å å±æ¯”æˆ–å®½åº¦)  -ofs(è¾“å‡ºæ®µçš„å­—å·)  -T(æ˜¾ç¤ºå·¥å…·æ )  -N(æ˜¾ç¤ºè‡ªå·±ä¸»æœºå)</span><br></pre></td></tr></table></figure>
<p>è§£å†³è¾“å‡ºæ˜¾ç¤ºä¸å…¨çš„é—®é¢˜<br><br>åœ¨C:\Users\XXX.jupyter\custom\custom.cssé‡Œ,æ‰¾åˆ°div.output_area</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">div.output_area&#123;</span><br><span class="line">    display:-webkit-box;</span><br><span class="line">&#125;</span><br><span class="line">æ”¹ä¸º</span><br><span class="line">div.output_area&#123;</span><br><span class="line">    display:-webkit-box;</span><br><span class="line">    padding:13px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="å¯†ç ä¿®æ”¹"><a href="#å¯†ç ä¿®æ”¹" class="headerlink" title="å¯†ç ä¿®æ”¹"></a><span id="header2">å¯†ç ä¿®æ”¹</span></h1><p>ç»ˆç«¯è¿è¡Œï¼Œè¾“å…¥æ–°çš„å¯†ç å³å¯<a href="https://blog.csdn.net/qq_36950604/article/details/103848631" target="_blank" rel="noopener">å‚è€ƒ</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/08/notebook%E4%BD%BF%E7%94%A8/1.jpg" alt="x"></p>
<h1 id="æ·»åŠ å†…æ ¸"><a href="#æ·»åŠ å†…æ ¸" class="headerlink" title="æ·»åŠ å†…æ ¸"></a><span id="header3">æ·»åŠ å†…æ ¸</span></h1><ul>
<li>é¦–å…ˆæ·»åŠ è™šæ‹Ÿç¯å¢ƒ,æ¯”å¦‚æˆ‘è¦æ·»åŠ pyç‰ˆæœ¬ä¸º2.7,ç¯å¢ƒåä¸ºpy2çš„ç¯å¢ƒ<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n py2 python&#x3D;2.7</span><br></pre></td></tr></table></figure></li>
<li>ç„¶åè¿›å…¥å½“å‰è™šæ‹Ÿç¯å¢ƒä¸‹<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">activate py2</span><br></pre></td></tr></table></figure></li>
<li>åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…ipykernel<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install ipykernel</span><br></pre></td></tr></table></figure></li>
<li>æ·»åŠ å†…æ ¸åˆ°jupyteré‡Œ:python -m ipykernel install â€“user â€“name ç¯å¢ƒåç§° â€“display-name â€œåœ¨jupyterä¸­æ˜¾ç¤ºçš„ç¯å¢ƒåç§°â€ï¼Œæ³¨æ„ä¸è¦å¿˜è®°äº†åŒå¼•å·<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m ipykernel install --user --name py2 --display-name &quot;py2&quot;</span><br></pre></td></tr></table></figure>
æˆåŠŸ<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/py2-1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/py2.png" alt></li>
</ul>
]]></content>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>opencvå›¾åƒæ ¸å¿ƒæ“ä½œ</title>
    <url>/2020/07/12/opencv%E5%9B%BE%E5%83%8F%E6%A0%B8%E5%BF%83%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">å›¾ç‰‡çš„åŸºæœ¬æ“ä½œ</a></li>
<li>2.<a href="#header2">å›¾ç‰‡çš„ç®—æ³•æ“ä½œ</a></li>
<li>3.<a href="#header3">æ€§èƒ½è¡¡é‡å’Œæå‡æŠ€æœ¯</a><a id="more"></a>

</li>
</ul>
<h1 id="å›¾åƒçš„åŸºæœ¬æ“ä½œ"><a href="#å›¾åƒçš„åŸºæœ¬æ“ä½œ" class="headerlink" title="å›¾åƒçš„åŸºæœ¬æ“ä½œ"></a><span id="header1">å›¾åƒçš„åŸºæœ¬æ“ä½œ</span></h1><h2 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>å­¦ä¼šï¼š - è®¿é—®åƒç´ å€¼å¹¶ä¿®æ”¹å®ƒä»¬ - è®¿é—®å›¾åƒå±æ€§ - è®¾ç½®æ„Ÿå…´è¶£åŒºåŸŸ(ROI) - åˆ†å‰²å’Œåˆå¹¶å›¾åƒ</p>
<p>æœ¬èŠ‚ä¸­çš„å‡ ä¹æ‰€æœ‰æ“ä½œéƒ½ä¸»è¦ä¸Numpyç›¸å…³ï¼Œè€Œä¸æ˜¯ä¸OpenCVç›¸å…³ã€‚è¦ä½¿ç”¨OpenCVç¼–å†™æ›´å¥½çš„ä¼˜åŒ–ä»£ç ï¼Œéœ€è¦Numpyçš„ä¸°å¯ŒçŸ¥è¯†ã€‚</p>
<h2 id="è®¿é—®å’Œä¿®æ”¹åƒç´ å€¼"><a href="#è®¿é—®å’Œä¿®æ”¹åƒç´ å€¼" class="headerlink" title="è®¿é—®å’Œä¿®æ”¹åƒç´ å€¼"></a>è®¿é—®å’Œä¿®æ”¹åƒç´ å€¼</h2><p>å¯¹äº BGR å›¾åƒï¼Œå®ƒè¿”å›ä¸€ä¸ªç”±è“è‰²ã€ç»¿è‰²å’Œçº¢è‰²å€¼ç»„æˆçš„æ•°ç»„ã€‚å¯¹äºç°åº¦å›¾åƒï¼Œåªè¿”å›ç›¸åº”çš„ç°åº¦ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>) <span class="comment"># è½½å…¥å½©è‰²å›¾åƒ</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">px = img[<span class="number">210</span>,<span class="number">490</span>] <span class="comment"># è®¿é—®210,490ç‚¹å¤„çš„å…¨éƒ¨å…ƒç´ </span></span><br><span class="line">px</span><br></pre></td></tr></table></figure>




<pre><code>array([237, 189, 147], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blue = img[<span class="number">210</span>,<span class="number">490</span>,<span class="number">0</span>] <span class="comment"># ä»…è®¿é—®è“è‰²å…ƒç´ </span></span><br><span class="line">blue</span><br></pre></td></tr></table></figure>




<pre><code>237</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img[<span class="number">100</span>,<span class="number">100</span>] = [<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>]<span class="comment"># ä¿®æ”¹åƒç´ å€¼</span></span><br><span class="line">img[<span class="number">100</span>,<span class="number">100</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([255, 255, 255], dtype=uint8)</code></pre><h3 id="è­¦å‘Š"><a href="#è­¦å‘Š" class="headerlink" title="è­¦å‘Š"></a>è­¦å‘Š</h3><p>Numpyæ˜¯ç”¨äºå¿«é€Ÿæ•°ç»„è®¡ç®—çš„ä¼˜åŒ–åº“ã€‚å› æ­¤ï¼Œç®€å•åœ°è®¿é—®æ¯ä¸ªåƒç´ å€¼å¹¶å¯¹å…¶è¿›è¡Œä¿®æ”¹å°†éå¸¸ç¼“æ…¢ï¼Œå› æ­¤ä¸å»ºè®®ä½¿ç”¨ã€‚</p>
<h3 id="æ³¨æ„"><a href="#æ³¨æ„" class="headerlink" title="æ³¨æ„"></a>æ³¨æ„</h3><p>ä¸Šé¢çš„æ–¹æ³•é€šå¸¸ç”¨äºé€‰æ‹©æ•°ç»„çš„åŒºåŸŸï¼Œä¾‹å¦‚å‰5è¡Œå’Œå3åˆ—ã€‚</p>
<p>å¯¹äºå•ä¸ªåƒç´ è®¿é—®ï¼ŒNumpyæ•°ç»„æ–¹æ³•array.item()å’Œarray.itemset())è¢«è®¤ä¸ºæ›´å¥½ï¼Œä½†æ˜¯å®ƒä»¬å§‹ç»ˆè¿”å›æ ‡é‡ã€‚</p>
<p>å¦‚æœè¦è®¿é—®æ‰€æœ‰Bï¼ŒGï¼ŒRå€¼ï¼Œåˆ™éœ€è¦åˆ†åˆ«è°ƒç”¨æ‰€æœ‰çš„array.item()ã€‚</p>
<p>ä¸‹é¢æ˜¯æ›´å¥½çš„åƒç´ è®¿é—®å’Œç¼–è¾‘æ–¹æ³•</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">red = img.item(<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">red</span><br></pre></td></tr></table></figure>




<pre><code>255</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.itemset((<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>),<span class="number">222</span>)</span><br><span class="line">red = img.item(<span class="number">100</span>,<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">red</span><br></pre></td></tr></table></figure>




<pre><code>222</code></pre><h2 id="è®¿é—®å›¾åƒå±æ€§"><a href="#è®¿é—®å›¾åƒå±æ€§" class="headerlink" title="è®¿é—®å›¾åƒå±æ€§"></a>è®¿é—®å›¾åƒå±æ€§</h2><p>å›¾åƒå±æ€§åŒ…æ‹¬è¡Œæ•°ï¼Œåˆ—æ•°å’Œé€šé“æ•°ï¼Œå›¾åƒæ•°æ®ç±»å‹ï¼Œåƒç´ æ•°ç­‰ã€‚</p>
<p>å›¾åƒçš„å½¢çŠ¶å¯é€šè¿‡img.shapeè®¿é—®ã€‚å®ƒè¿”å›è¡Œï¼Œåˆ—å’Œé€šé“æ•°çš„å…ƒç»„ï¼ˆå¦‚æœå›¾åƒæ˜¯å½©è‰²çš„ï¼‰ï¼š</p>
<p>æ³¨æ„ å¦‚æœå›¾åƒæ˜¯ç°åº¦çš„ï¼Œåˆ™è¿”å›çš„å…ƒç»„ä»…åŒ…å«è¡Œæ•°å’Œåˆ—æ•°ï¼Œå› æ­¤è¿™æ˜¯æ£€æŸ¥åŠ è½½çš„å›¾åƒæ˜¯ç°åº¦è¿˜æ˜¯å½©è‰²çš„å¥½æ–¹æ³•ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.shape</span><br></pre></td></tr></table></figure>




<pre><code>(640, 640, 3)</code></pre><p>åƒç´ æ€»æ•°å¯é€šè¿‡è®¿é—®img.sizeï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.size</span><br></pre></td></tr></table></figure>




<pre><code>1228800</code></pre><p>å›¾åƒæ•°æ®ç±»å‹é€šè¿‡img.dtypeè·å¾—ï¼š</p>
<p>æ³¨æ„ img.dtypeåœ¨è°ƒè¯•æ—¶éå¸¸é‡è¦ï¼Œå› ä¸ºOpenCV-Pythonä»£ç ä¸­çš„å¤§é‡é”™è¯¯æ˜¯ç”±æ— æ•ˆçš„æ•°æ®ç±»å‹å¼•èµ·çš„ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img.dtype</span><br></pre></td></tr></table></figure>




<pre><code>dtype(&apos;uint8&apos;)</code></pre><h2 id="å›¾åƒæ„Ÿå…´è¶£åŒºåŸŸROI"><a href="#å›¾åƒæ„Ÿå…´è¶£åŒºåŸŸROI" class="headerlink" title="å›¾åƒæ„Ÿå…´è¶£åŒºåŸŸROI"></a>å›¾åƒæ„Ÿå…´è¶£åŒºåŸŸROI</h2><p>æœ‰æ—¶å€™ï¼Œä½ ä¸å¾—ä¸å¤„ç†ä¸€äº›ç‰¹å®šåŒºåŸŸçš„å›¾åƒã€‚</p>
<p>å¯¹äºå›¾åƒä¸­çš„çœ¼ç›æ£€æµ‹ï¼Œé¦–å…ˆå¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œäººè„¸æ£€æµ‹ã€‚</p>
<p>åœ¨è·å–äººè„¸å›¾åƒæ—¶ï¼Œæˆ‘ä»¬åªé€‰æ‹©äººè„¸åŒºåŸŸï¼Œæœç´¢å…¶ä¸­çš„çœ¼ç›ï¼Œè€Œä¸æ˜¯æœç´¢æ•´ä¸ªå›¾åƒã€‚</p>
<p>å®ƒæé«˜äº†å‡†ç¡®æ€§(å› ä¸ºçœ¼ç›æ€»æ˜¯åœ¨é¢éƒ¨ä¸Š:D )å’Œæ€§èƒ½(å› ä¸ºæˆ‘ä»¬æœç´¢çš„åŒºåŸŸå¾ˆå°)ã€‚</p>
<p>ä½¿ç”¨Numpyç´¢å¼•å†æ¬¡è·å¾—ROIã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ball = img[<span class="number">180</span>:<span class="number">240</span>, <span class="number">230</span>:<span class="number">290</span>]</span><br><span class="line">img[<span class="number">273</span>:<span class="number">333</span>, <span class="number">100</span>:<span class="number">160</span>] = ball </span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="æ‹†åˆ†å’Œåˆå¹¶å›¾åƒé€šé“"><a href="#æ‹†åˆ†å’Œåˆå¹¶å›¾åƒé€šé“" class="headerlink" title="æ‹†åˆ†å’Œåˆå¹¶å›¾åƒé€šé“"></a>æ‹†åˆ†å’Œåˆå¹¶å›¾åƒé€šé“</h2><p>æœ‰æ—¶ä½ éœ€è¦åˆ†åˆ«å¤„ç†å›¾åƒçš„Bï¼ŒGï¼ŒRé€šé“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦å°†BGRå›¾åƒæ‹†åˆ†ä¸ºå•ä¸ªé€šé“ã€‚</p>
<p>åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½éœ€è¦å°†è¿™äº›å•ç‹¬çš„é¢‘é“åŠ å…¥BGRå›¾ç‰‡ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç®€å•åœ°åšåˆ°è¿™ä¸€ç‚¹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b,g,r = cv.split(img)</span><br><span class="line">img = cv.merge((b,g,r))</span><br></pre></td></tr></table></figure>

<p>å‡è®¾ä½ è¦å°†æ‰€æœ‰çº¢è‰²åƒç´ éƒ½è®¾ç½®ä¸ºé›¶ï¼Œåˆ™æ— éœ€å…ˆæ‹†åˆ†é€šé“ã€‚numpyç´¢å¼•æ›´å¿«</p>
<p>è­¦å‘Š</p>
<p>cv.split()æ˜¯ä¸€é¡¹è€—æ—¶çš„æ“ä½œï¼ˆå°±æ—¶é—´è€Œè¨€ï¼‰ã€‚å› æ­¤ï¼Œä»…åœ¨å¿…è¦æ—¶æ‰è¿™æ ·åšã€‚å¦åˆ™è¯·è¿›è¡ŒNumpyç´¢å¼•ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img [:, :, <span class="number">2</span>] = <span class="number">0</span></span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="ä¸ºå›¾åƒè®¾ç½®è¾¹æ¡†ï¼ˆå¡«å……ï¼‰"><a href="#ä¸ºå›¾åƒè®¾ç½®è¾¹æ¡†ï¼ˆå¡«å……ï¼‰" class="headerlink" title="ä¸ºå›¾åƒè®¾ç½®è¾¹æ¡†ï¼ˆå¡«å……ï¼‰"></a>ä¸ºå›¾åƒè®¾ç½®è¾¹æ¡†ï¼ˆå¡«å……ï¼‰</h2><p>å¦‚æœè¦åœ¨å›¾åƒå‘¨å›´åˆ›å»ºè¾¹æ¡†ï¼ˆå¦‚ç›¸æ¡†ï¼‰ï¼Œåˆ™å¯ä»¥ä½¿ç”¨cv.copyMakeBorder()ã€‚ä½†æ˜¯å®ƒåœ¨å·ç§¯è¿ç®—ï¼Œé›¶å¡«å……ç­‰æ–¹é¢æœ‰æ›´å¤šåº”ç”¨ã€‚æ­¤å‡½æ•°é‡‡ç”¨ä»¥ä¸‹å‚æ•°ï¼š</p>
<p>src - è¾“å…¥å›¾åƒ</p>
<p>topï¼Œbottomï¼Œleftï¼Œright è¾¹ç•Œå®½åº¦ï¼ˆä»¥ç›¸åº”æ–¹å‘ä¸Šçš„åƒç´ æ•°ä¸ºå•ä½ï¼‰</p>
<p>borderType - å®šä¹‰è¦æ·»åŠ å“ªç§è¾¹æ¡†çš„æ ‡å¿—ã€‚å®ƒå¯ä»¥æ˜¯ä»¥ä¸‹ç±»å‹ï¼š</p>
<ul>
<li>cv.BORDER_CONSTANT - æ·»åŠ æ’å®šçš„å½©è‰²è¾¹æ¡†ã€‚è¯¥å€¼åº”ä½œä¸ºä¸‹ä¸€ä¸ªå‚æ•°ç»™å‡ºã€‚</li>
<li>cv.BORDER_REFLECT - è¾¹æ¡†å°†æ˜¯è¾¹æ¡†å…ƒç´ çš„é•œåƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š fedcba | abcdefgh | hgfedcb</li>
<li>cv.BORDER_REFLECT_101æˆ– cv.BORDER_DEFAULTä¸ä¸Šè¿°ç›¸åŒï¼Œä½†ç•¥æœ‰å˜åŒ–ï¼Œä¾‹å¦‚ï¼š gfedcb | abcdefgh | gfedcba</li>
<li>cv.BORDER_REPLICATEæœ€åä¸€ä¸ªå…ƒç´ è¢«å¤åˆ¶ï¼Œåƒè¿™æ ·ï¼š aaaaaa | abcdefgh | hhhhhhh</li>
<li>cv.BORDER_WRAPéš¾ä»¥è§£é‡Šï¼Œå®ƒçœ‹èµ·æ¥åƒè¿™æ ·ï¼š cdefgh | abcdefgh | abcdefg</li>
</ul>
<p>value -è¾¹æ¡†çš„é¢œè‰²ï¼Œå¦‚æœè¾¹æ¡†ç±»å‹ä¸º<strong>cv.BORDER_CONSTANT</strong></p>
<p>ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ä»£ç ï¼Œæ¼”ç¤ºäº†æ‰€æœ‰è¿™äº›è¾¹æ¡†ç±»å‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£ï¼š<br>(å›¾åƒä¸matplotlibä¸€èµ·æ˜¾ç¤ºã€‚å› æ­¤çº¢è‰²å’Œè“è‰²é€šé“å°†äº’æ¢)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">BLUE = [<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">img1 = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line">replicate = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REPLICATE)</span><br><span class="line">reflect = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REFLECT)</span><br><span class="line">reflect101 = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_REFLECT_101)</span><br><span class="line">wrap = cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_WRAP)</span><br><span class="line">constant= cv.copyMakeBorder(img1,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>,cv.BORDER_CONSTANT,value=BLUE)</span><br><span class="line">plt.subplot(<span class="number">231</span>),plt.imshow(img1,<span class="string">'gray'</span>),plt.title(<span class="string">'ORIGINAL'</span>)</span><br><span class="line">plt.subplot(<span class="number">232</span>),plt.imshow(replicate,<span class="string">'gray'</span>),plt.title(<span class="string">'REPLICATE'</span>)</span><br><span class="line">plt.subplot(<span class="number">233</span>),plt.imshow(reflect,<span class="string">'gray'</span>),plt.title(<span class="string">'REFLECT'</span>)</span><br><span class="line">plt.subplot(<span class="number">234</span>),plt.imshow(reflect101,<span class="string">'gray'</span>),plt.title(<span class="string">'REFLECT_101'</span>)</span><br><span class="line">plt.subplot(<span class="number">235</span>),plt.imshow(wrap,<span class="string">'gray'</span>),plt.title(<span class="string">'WRAP'</span>)</span><br><span class="line">plt.subplot(<span class="number">236</span>),plt.imshow(constant,<span class="string">'gray'</span>),plt.title(<span class="string">'CONSTANT'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv%E5%9B%BE%E5%83%8F%E6%A0%B8%E5%BF%83%E6%93%8D%E4%BD%9C/output_21_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="å›¾åƒä¸Šçš„ç®—æœ¯è¿ç®—"><a href="#å›¾åƒä¸Šçš„ç®—æœ¯è¿ç®—" class="headerlink" title="å›¾åƒä¸Šçš„ç®—æœ¯è¿ç®—"></a><span id="header2">å›¾åƒä¸Šçš„ç®—æœ¯è¿ç®—</span></h1><h2 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>å­¦ä¹ å›¾åƒçš„å‡ ç§ç®—æœ¯è¿ç®—ï¼Œä¾‹å¦‚åŠ æ³•ï¼Œå‡æ³•ï¼ŒæŒ‰ä½è¿ç®—ç­‰ã€‚</p>
<p>æ‚¨å°†å­¦ä¹ ä»¥ä¸‹åŠŸèƒ½ï¼šcv.addï¼Œ<strong>cv.addWeighted</strong>ç­‰ã€‚</p>
<h2 id="å›¾åƒåŠ æ³•"><a href="#å›¾åƒåŠ æ³•" class="headerlink" title="å›¾åƒåŠ æ³•"></a>å›¾åƒåŠ æ³•</h2><p>æ‚¨å¯ä»¥é€šè¿‡OpenCVå‡½æ•°cv.add()æˆ–ä»…é€šè¿‡numpyæ“ä½œres = img1 + img2æ·»åŠ ä¸¤ä¸ªå›¾åƒã€‚</p>
<p>ä¸¤ä¸ªå›¾åƒåº”å…·æœ‰ç›¸åŒçš„æ·±åº¦å’Œç±»å‹ï¼Œæˆ–è€…ç¬¬äºŒä¸ªå›¾åƒå¯ä»¥åªæ˜¯ä¸€ä¸ªæ ‡é‡å€¼ã€‚</p>
<p>æ³¨æ„ OpenCVåŠ æ³•å’ŒNumpyåŠ æ³•ä¹‹é—´æœ‰åŒºåˆ«ã€‚OpenCVåŠ æ³•æ˜¯é¥±å’Œè¿ç®—ï¼Œè€ŒNumpyåŠ æ³•æ˜¯æ¨¡è¿ç®—ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.uint8([<span class="number">250</span>])</span><br><span class="line">y = np.uint8([<span class="number">10</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x+y  <span class="comment"># 250+10 = 260 % 256 = 4</span></span><br></pre></td></tr></table></figure>




<pre><code>array([4], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.add(x,y) <span class="comment"># 250+10 = 260 =&gt; 255</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[255]], dtype=uint8)</code></pre><h2 id="å›¾åƒèåˆ"><a href="#å›¾åƒèåˆ" class="headerlink" title="å›¾åƒèåˆ"></a>å›¾åƒèåˆ</h2><p>è¿™ä¹Ÿæ˜¯å›¾åƒåŠ æ³•ï¼Œä½†æ˜¯å¯¹å›¾åƒèµ‹äºˆä¸åŒçš„æƒé‡ï¼Œä»¥ä½¿å…¶å…·æœ‰èåˆæˆ–é€æ˜çš„æ„Ÿè§‰ã€‚æ ¹æ®ä»¥ä¸‹ç­‰å¼æ·»åŠ å›¾åƒï¼š</p>
<p>G(x)=(1âˆ’Î±)f0(x)+Î±f1(x)<br>é€šè¿‡ä» Î± ä» 0â†’1 æ›´æ”¹ï¼Œæ‚¨å¯ä»¥åœ¨ä¸€ä¸ªå›¾åƒåˆ°å¦ä¸€ä¸ªå›¾åƒä¹‹é—´æ‰§è¡Œå¾ˆé…·çš„è¿‡æ¸¡ã€‚</p>
<p>å°†ä¸¤å¹…å›¾åƒåˆåœ¨ä¸€èµ·ã€‚ç¬¬ä¸€å¹…å›¾åƒçš„æƒé‡ä¸º0.7ï¼Œç¬¬äºŒå¹…å›¾åƒçš„æƒé‡ä¸º0.3ã€‚</p>
<p>cv.addWeighted()åœ¨å›¾åƒä¸Šåº”ç”¨ä»¥ä¸‹å…¬å¼ã€‚</p>
<p>dst=Î±â‹…img1+Î²â‹…img2+Î³<br>åœ¨è¿™é‡Œï¼ŒÎ³ è¢«è§†ä¸ºé›¶ã€‚</p>
<p>å…ˆä¿å­˜ä¸€ä¸ªå’Œavatar1å¤§å°ä¸€æ ·ä¸Šä¸‹ç›¸åçš„å›¾åƒavatar2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">newImg = img.copy() <span class="comment"># æ·±æ‹·è´</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(img.shape[<span class="number">0</span>]):</span><br><span class="line">    newImg[img.shape[<span class="number">0</span>]<span class="number">-1</span>-i] = img[i]</span><br><span class="line">cv.imshow(<span class="string">'x'</span>,newImg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imwrite(<span class="string">'avatar2.jpg'</span>,newImg)</span><br></pre></td></tr></table></figure>




<pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img1 = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">'avatar2.jpg'</span>)</span><br><span class="line">dst = cv.addWeighted(img1,<span class="number">0.7</span>,img2,<span class="number">0.3</span>,<span class="number">0</span>)</span><br><span class="line">cv.imshow(<span class="string">'dst'</span>,dst)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="æŒ‰ä½è¿ç®—"><a href="#æŒ‰ä½è¿ç®—" class="headerlink" title="æŒ‰ä½è¿ç®—"></a>æŒ‰ä½è¿ç®—</h2><p>è¿™åŒ…æ‹¬æŒ‰ä½ ANDã€ ORã€NOT å’Œ XOR æ“ä½œã€‚å®ƒä»¬åœ¨æå–å›¾åƒçš„ä»»ä½•éƒ¨åˆ†(æˆ‘ä»¬å°†åœ¨åé¢çš„ç« èŠ‚ä¸­çœ‹åˆ°)ã€å®šä¹‰å’Œå¤„ç†éçŸ©å½¢ ROI ç­‰æ–¹é¢éå¸¸æœ‰ç”¨ã€‚ </p>
<p>ä¸‹é¢æˆ‘ä»¬å°†çœ‹åˆ°ä¸€ä¸ªä¾‹å­ï¼Œå¦‚ä½•æ”¹å˜ä¸€ä¸ªå›¾åƒçš„ç‰¹å®šåŒºåŸŸã€‚ </p>
<p>æˆ‘æƒ³æŠŠ OpenCV çš„æ ‡å¿—æ”¾åœ¨ä¸€ä¸ªå›¾åƒä¸Šé¢ã€‚å¦‚æœæˆ‘æ·»åŠ ä¸¤ä¸ªå›¾åƒï¼Œå®ƒä¼šæ”¹å˜é¢œè‰²ã€‚å¦‚æœæˆ‘æ··åˆå®ƒï¼Œæˆ‘å¾—åˆ°ä¸€ä¸ªé€æ˜çš„æ•ˆæœã€‚</p>
<p>ä½†æˆ‘å¸Œæœ›å®ƒæ˜¯ä¸é€æ˜çš„ã€‚å¦‚æœæ˜¯ä¸€ä¸ªçŸ©å½¢åŒºåŸŸï¼Œ</p>
<p>æˆ‘å¯ä»¥ä½¿ç”¨ ROIï¼Œå°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­æ‰€åšçš„é‚£æ ·ã€‚</p>
<p>ä½†æ˜¯ OpenCV çš„ logo ä¸æ˜¯é•¿æ–¹å½¢çš„ã€‚æ‰€ä»¥ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹çš„æŒ‰ä½æ“ä½œæ¥å®ç°:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åŠ è½½ä¸¤å¼ å›¾ç‰‡</span></span><br><span class="line">img1 = cv.imread(<span class="string">'avatar1.jpg'</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">'opencv-logo.png'</span>)</span><br><span class="line"><span class="comment"># æˆ‘æƒ³æŠŠlogoæ”¾åœ¨å·¦ä¸Šè§’ï¼Œæ‰€ä»¥æˆ‘åˆ›å»ºäº†ROI</span></span><br><span class="line">rows,cols,channels = img2.shape</span><br><span class="line">roi = img1[<span class="number">0</span>:rows, <span class="number">0</span>:cols ]</span><br><span class="line"><span class="comment"># ç°åœ¨åˆ›å»ºlogoçš„æ©ç ï¼Œå¹¶åŒæ—¶åˆ›å»ºå…¶ç›¸åæ©ç </span></span><br><span class="line">img2gray = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, mask = cv.threshold(img2gray, <span class="number">10</span>, <span class="number">255</span>, cv.THRESH_BINARY)</span><br><span class="line">mask_inv = cv.bitwise_not(mask)</span><br><span class="line">cv.imshow(<span class="string">'img2gray'</span>,img2gray)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'mask'</span>,mask)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'mask_inv'</span>,mask_inv)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"><span class="comment"># ç°åœ¨å°†ROIä¸­logoçš„åŒºåŸŸæ¶‚é»‘</span></span><br><span class="line">img1_bg = cv.bitwise_and(roi,roi,mask = mask_inv)</span><br><span class="line"><span class="comment"># ä»…ä»logoå›¾åƒä¸­æå–logoåŒºåŸŸ</span></span><br><span class="line">img2_fg = cv.bitwise_and(img2,img2,mask = mask)</span><br><span class="line"><span class="comment"># å°†logoæ”¾å…¥ROIå¹¶ä¿®æ”¹ä¸»å›¾åƒ</span></span><br><span class="line">cv.imshow(<span class="string">'img1_bg'</span>,img1_bg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">cv.imshow(<span class="string">'img2_bg'</span>,img2_fg)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">dst = cv.add(img1_bg,img2_fg)</span><br><span class="line">img1[<span class="number">0</span>:rows, <span class="number">0</span>:cols ] = dst</span><br><span class="line">cv.imshow(<span class="string">'res'</span>,img1)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<h1 id="æ€§èƒ½è¡¡é‡å’Œæå‡æŠ€æœ¯"><a href="#æ€§èƒ½è¡¡é‡å’Œæå‡æŠ€æœ¯" class="headerlink" title="æ€§èƒ½è¡¡é‡å’Œæå‡æŠ€æœ¯"></a><span id="header3">æ€§èƒ½è¡¡é‡å’Œæå‡æŠ€æœ¯</span></h1><h2 id="ç›®æ ‡-2"><a href="#ç›®æ ‡-2" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨å›¾åƒå¤„ç†ä¸­ï¼Œç”±äºæ¯ç§’è¦å¤„ç†å¤§é‡æ“ä½œï¼Œå› æ­¤å¿…é¡»ä½¿ä»£ç ä¸ä»…æä¾›æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸”è¿˜å¿…é¡»ä»¥æœ€å¿«çš„æ–¹å¼æä¾›ã€‚</p>
<p>å°†å­¦ä¹ è¡¡é‡ä»£ç çš„æ€§èƒ½,ä¸€äº›æé«˜ä»£ç æ€§èƒ½çš„æŠ€å·§ã€‚</p>
<p>ä½ å°†çœ‹åˆ°ä»¥ä¸‹åŠŸèƒ½ï¼šcv.getTickCountï¼Œ<strong>cv.getTickFrequency</strong>ç­‰ã€‚</p>
<p>é™¤äº†OpenCVï¼ŒPythonè¿˜æä¾›äº†ä¸€ä¸ªæ¨¡å—<strong>time</strong>ï¼Œè¿™æœ‰åŠ©äºè¡¡é‡æ‰§è¡Œæ—¶é—´ã€‚</p>
<p>å¦ä¸€ä¸ªæ¨¡å—<strong>profile</strong>æœ‰åŠ©äºè·å–æœ‰å…³ä»£ç çš„è¯¦ç»†æŠ¥å‘Šï¼Œä¾‹å¦‚ä»£ç ä¸­æ¯ä¸ªå‡½æ•°èŠ±è´¹äº†å¤šå°‘æ—¶é—´ï¼Œè°ƒç”¨äº†å‡½æ•°çš„æ¬¡æ•°ç­‰ã€‚</p>
<p>ä½†æ˜¯ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯IPythonï¼Œåˆ™æ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½é›†æˆåœ¨ç”¨æˆ·å‹å¥½çš„ç•Œé¢ä¸­æ–¹å¼ã€‚</p>
<h2 id="ä½¿ç”¨OpenCVè¡¡é‡æ€§èƒ½"><a href="#ä½¿ç”¨OpenCVè¡¡é‡æ€§èƒ½" class="headerlink" title="ä½¿ç”¨OpenCVè¡¡é‡æ€§èƒ½"></a>ä½¿ç”¨OpenCVè¡¡é‡æ€§èƒ½</h2><p><strong>cv.getTickCount</strong>å‡½æ•°è¿”å›ä»å‚è€ƒäº‹ä»¶ï¼ˆå¦‚æ‰“å¼€æœºå™¨çš„é‚£ä¸€åˆ»ï¼‰åˆ°è°ƒç”¨æ­¤å‡½æ•°é‚£ä¸€åˆ»ä¹‹é—´çš„æ—¶é’Ÿå‘¨æœŸæ•°ã€‚å› æ­¤ï¼Œå¦‚æœåœ¨å‡½æ•°æ‰§è¡Œä¹‹å‰å’Œä¹‹åè°ƒç”¨å®ƒï¼Œåˆ™ä¼šè·å¾—ç”¨äºæ‰§è¡Œå‡½æ•°çš„æ—¶é’Ÿå‘¨æœŸæ•°ã€‚</p>
<p><strong>cv.getTickFrequency</strong>å‡½æ•°è¿”å›æ—¶é’Ÿå‘¨æœŸçš„é¢‘ç‡æˆ–æ¯ç§’çš„æ—¶é’Ÿå‘¨æœŸæ•°ã€‚å› æ­¤ï¼Œè¦æ‰¾åˆ°æ‰§è¡Œæ—¶é—´ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰ï¼Œä½ å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img1 = cv.imread(<span class="string">'avatar2gray.jpg.jpg'</span>)</span><br><span class="line">e1 = cv.getTickCount()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">49</span>,<span class="number">2</span>):</span><br><span class="line">    img1 = cv.medianBlur(img1,i)</span><br><span class="line">e2 = cv.getTickCount()</span><br><span class="line">t = (e2 - e1)/cv.getTickFrequency()</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>0.0002413</code></pre><h2 id="OpenCVä¸­çš„é»˜è®¤ä¼˜åŒ–"><a href="#OpenCVä¸­çš„é»˜è®¤ä¼˜åŒ–" class="headerlink" title="OpenCVä¸­çš„é»˜è®¤ä¼˜åŒ–"></a>OpenCVä¸­çš„é»˜è®¤ä¼˜åŒ–</h2><p>è®¸å¤š OpenCV å‡½æ•°éƒ½æ˜¯ä½¿ç”¨ SSE2ã€ AVX ç­‰è¿›è¡Œä¼˜åŒ–çš„ã€‚ å®ƒè¿˜åŒ…å«æœªä¼˜åŒ–çš„ä»£ç ã€‚</p>
<p>å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒè¿™äº›ç‰¹æ€§ï¼Œæˆ‘ä»¬å°±åº”è¯¥åˆ©ç”¨å®ƒä»¬(å‡ ä¹æ‰€æœ‰ç°ä»£çš„å¤„ç†å™¨éƒ½æ”¯æŒå®ƒä»¬)ã€‚</p>
<p>åœ¨ç¼–è¯‘æ—¶é»˜è®¤å¯ç”¨å®ƒã€‚å› æ­¤ï¼Œå¦‚æœå¯ç”¨äº† OpenCVï¼Œå®ƒå°†è¿è¡Œä¼˜åŒ–çš„ä»£ç ï¼Œå¦åˆ™å®ƒå°†è¿è¡Œæœªä¼˜åŒ–çš„ä»£ç ã€‚</p>
<p>ä½ å¯ä»¥ä½¿ç”¨ cvUseoptimized æ£€æŸ¥æ˜¯å¦å¯ç”¨ / ç¦ç”¨å’Œ cvSetuseoptimized ä»¥å¯ç”¨ / ç¦ç”¨å®ƒã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.useOptimized() <span class="comment"># æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†ä¼˜åŒ–</span></span><br></pre></td></tr></table></figure>




<pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit res = cv.medianBlur(img1,<span class="number">49</span>)</span><br></pre></td></tr></table></figure>

<pre><code>748 ns Â± 45.3 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.setUseOptimized(<span class="literal">False</span>)</span><br><span class="line">print(cv.useOptimized())</span><br></pre></td></tr></table></figure>

<pre><code>False</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit res = cv.medianBlur(img1,<span class="number">49</span>)</span><br></pre></td></tr></table></figure>

<pre><code>752 ns Â± 34.2 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.setUseOptimized(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="åœ¨IPythonä¸­è¡¡é‡æ€§èƒ½"><a href="#åœ¨IPythonä¸­è¡¡é‡æ€§èƒ½" class="headerlink" title="åœ¨IPythonä¸­è¡¡é‡æ€§èƒ½"></a>åœ¨IPythonä¸­è¡¡é‡æ€§èƒ½</h2><p>æœ‰æ—¶ä½ å¯èƒ½éœ€è¦æ¯”è¾ƒä¸¤ä¸ªç±»ä¼¼æ“ä½œçš„æ€§èƒ½ã€‚</p>
<p>IPythonä¸ºä½ æä¾›äº†ä¸€ä¸ªç¥å¥‡çš„å‘½ä»¤è®¡æ—¶å™¨æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚å®ƒ</p>
<p>ä¼šå¤šæ¬¡è¿è¡Œä»£ç ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœã€‚åŒæ ·ï¼Œå®ƒä»¬é€‚ç”¨äºæµ‹é‡å•è¡Œä»£ç ã€‚</p>
<p>ä¾‹å¦‚ï¼Œä½ çŸ¥é“ä»¥ä¸‹å“ªä¸ªåŠ æ³•è¿ç®—æ›´å¥½ï¼Œ</p>
<p>x = 5 y = x**2, </p>
<p>x = 5  y = x*x, </p>
<p>x = np.uint8([5]) y = x*xæˆ–y = np.square(x)?æˆ‘ä»¬å°†åœ¨IPython shellä¸­ä½¿ç”¨timeit</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="number">5</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=x**<span class="number">2</span></span><br></pre></td></tr></table></figure>

<pre><code>515 ns Â± 17 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=x*x</span><br></pre></td></tr></table></figure>

<pre><code>121 ns Â± 5.63 ns per loop (mean Â± std. dev. of 7 runs, 10000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">z = np.uint8([<span class="number">5</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=z*z</span><br></pre></td></tr></table></figure>

<pre><code>1.04 Âµs Â± 61.6 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit y=np.square(z)</span><br></pre></td></tr></table></figure>

<pre><code>1.04 Âµs Â± 57.5 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)</code></pre><p>å¯ä»¥çœ‹åˆ°x = 5; y = x * xæœ€å¿«ï¼Œæ¯”Numpyå¿«20å€å·¦å³ã€‚å¦‚æœä½ è¿˜è€ƒè™‘é˜µåˆ—çš„åˆ›å»ºï¼Œå®ƒå¯èƒ½ä¼šå¿«100å€ã€‚</p>
<p>æ³¨æ„ Pythonæ ‡é‡æ“ä½œæ¯”Numpyæ ‡é‡æ“ä½œå¿«ã€‚</p>
<p>å› æ­¤ï¼Œå¯¹äºåŒ…å«ä¸€ä¸¤ä¸ªå…ƒç´ çš„è¿ç®—ï¼ŒPythonæ ‡é‡æ¯”Numpyæ•°ç»„å¥½ã€‚</p>
<p>å½“æ•°ç»„å¤§å°ç¨å¤§æ—¶ï¼ŒNumpyä¼šå ä¼˜åŠ¿ã€‚</p>
<p>æˆ‘ä»¬å°†æ¯”è¾ƒ<strong>cv.countNonZero</strong>å’Œ<strong>np.count_nonzero</strong>å¯¹äºåŒä¸€å¼ å›¾ç‰‡çš„æ€§èƒ½ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit z = np.count_nonzero(img1)</span><br></pre></td></tr></table></figure>

<pre><code>2.58 Âµs Â± 321 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit z = cv.countNonZero(img1)</span><br></pre></td></tr></table></figure>

<pre><code>722 ns Â± 25.6 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)</code></pre><p>OpenCV å‡½æ•°æ¯” Numpy å‡½æ•°å¿«è¿‘25å€ã€‚</p>
<p>æ³¨æ„ </p>
<p>é€šå¸¸ï¼ŒOpenCVå‡½æ•°æ¯”Numpyå‡½æ•°è¦å¿«ã€‚å› æ­¤ï¼Œå¯¹äºç›¸åŒçš„æ“ä½œï¼Œé¦–é€‰OpenCVåŠŸèƒ½ã€‚</p>
<p>ä½†æ˜¯ï¼Œå¯èƒ½ä¼šæœ‰ä¾‹å¤–ï¼Œå°¤å…¶æ˜¯å½“Numpyå¤„ç†è§†å›¾è€Œä¸æ˜¯å‰¯æœ¬æ—¶ã€‚</p>
<h2 id="æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯"><a href="#æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯" class="headerlink" title="æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯"></a>æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯</h2><p>æœ‰å‡ ç§æŠ€æœ¯å’Œç¼–ç æ–¹æ³•å¯ä»¥å……åˆ†åˆ©ç”¨ Python å’Œ Numpy çš„æœ€å¤§æ€§èƒ½ã€‚</p>
<p>è¿™é‡Œè¦æ³¨æ„çš„ä¸»è¦äº‹æƒ…æ˜¯ï¼Œé¦–å…ˆå°è¯•ä»¥ä¸€ç§ç®€å•çš„æ–¹å¼å®ç°ç®—æ³•ã€‚</p>
<p>ä¸€æ—¦å®ƒè¿è¡Œèµ·æ¥ï¼Œåˆ†æå®ƒï¼Œæ‰¾åˆ°ç“¶é¢ˆå¹¶ä¼˜åŒ–å®ƒä»¬ã€‚</p>
<ul>
<li>å°½é‡é¿å…åœ¨Pythonä¸­ä½¿ç”¨å¾ªç¯ï¼Œå°¤å…¶æ˜¯åŒ/ä¸‰é‡å¾ªç¯ç­‰ã€‚å®ƒä»¬æœ¬æ¥å°±å¾ˆæ…¢ã€‚</li>
<li>ç”±äºNumpyå’ŒOpenCVå·²é’ˆå¯¹å‘é‡è¿ç®—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå› æ­¤å°†ç®—æ³•/ä»£ç å‘é‡åŒ–åˆ°æœ€å¤§ç¨‹åº¦ã€‚</li>
<li>åˆ©ç”¨ç¼“å­˜ä¸€è‡´æ€§ã€‚</li>
<li>é™¤ééœ€è¦ï¼Œå¦åˆ™åˆ‡å‹¿åˆ›å»ºæ•°ç»„çš„å‰¯æœ¬ã€‚å°è¯•æ”¹ç”¨è§†å›¾ã€‚æ•°ç»„å¤åˆ¶æ˜¯ä¸€é¡¹æ˜‚è´µçš„æ“ä½œã€‚</li>
<li>å³ä½¿æ‰§è¡Œäº†æ‰€æœ‰è¿™äº›æ“ä½œåï¼Œå¦‚æœä½ çš„ä»£ç ä»ç„¶å¾ˆæ…¢ï¼Œæˆ–è€…ä¸å¯é¿å…åœ°éœ€è¦ä½¿ç”¨å¤§å¾ªç¯ï¼Œè¯·ä½¿ç”¨Cythonç­‰å…¶ä»–åº“æ¥ä½¿å…¶æ›´å¿«ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>å›¾åƒ</tag>
      </tags>
  </entry>
  <entry>
    <title>å•å±‚æ„ŸçŸ¥å™¨</title>
    <url>/2020/07/09/%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8/</url>
    <content><![CDATA[<p>å•å±‚æ„ŸçŸ¥å™¨ä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">-1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">-1</span>]])</span><br><span class="line">W = (np.random.random([<span class="number">3</span>,<span class="number">1</span>])<span class="number">-0.5</span>)*<span class="number">2</span></span><br><span class="line">print(W)</span><br><span class="line">lr = <span class="number">0.11</span></span><br><span class="line">O = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> X,Y,W,lr</span><br><span class="line">    O = np.sign(np.dot(X,W))</span><br><span class="line">    W_C = lr*(X.T.dot(Y-O))/int(X.shape[<span class="number">0</span>])</span><br><span class="line">    W = W + W_C</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0.23015384]
 [-0.72847367]
 [ 0.52092108]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    update()</span><br><span class="line">    O = np.sign(np.dot(X,W))</span><br><span class="line">    <span class="keyword">if</span>(O==Y).all():</span><br><span class="line">        print(<span class="string">'Finished'</span>)</span><br><span class="line">        print(<span class="string">'epoch:'</span>,i)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">x1 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">y1 = [<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">x2 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">y2 = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">k = -W[<span class="number">1</span>]/W[<span class="number">2</span>]</span><br><span class="line">d = -W[<span class="number">0</span>]/W[<span class="number">2</span>]</span><br><span class="line">print(<span class="string">'k='</span>,k)</span><br><span class="line">print(<span class="string">'d='</span>,d)</span><br><span class="line">xdata = (<span class="number">-2</span>,<span class="number">3</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(xdata,xdata*k+d,<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(x1,y1,c=<span class="string">'b'</span>)</span><br><span class="line">plt.scatter(x2,y2,c=<span class="string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>k= [0.16650386]
d= [-0.80515292]</code></pre><p><img src="/2020/07/09/%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8/output_2_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç¥ç»ç½‘ç»œ</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>opencv-GUIç‰¹æ€§</title>
    <url>/2020/07/12/opencv-GUI%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<ul>
<li>1.<a href="#header1">å›¾åƒå…¥é—¨</a></li>
<li>2.<a href="#header2">è§†é¢‘å…¥é—¨</a></li>
<li>3.<a href="#header3">ç»˜å›¾</a></li>
<li>4.<a href="#header4">é¼ æ ‡ä½œä¸ºç”»ç¬”</a></li>
<li>5.<a href="#header5">è½¨è¿¹æ ä½œä¸ºè°ƒè‰²æ¿</a></li>
</ul>
<h1 id="å›¾åƒå…¥é—¨"><a href="#å›¾åƒå…¥é—¨" class="headerlink" title="å›¾åƒå…¥é—¨"></a><span id="header1">å›¾åƒå…¥é—¨</span></h1><h2 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><ul>
<li>åœ¨è¿™é‡Œï¼Œä½ å°†å­¦ä¹ å¦‚ä½•è¯»å–å›¾åƒï¼Œå¦‚ä½•æ˜¾ç¤ºå›¾åƒä»¥åŠå¦‚ä½•å°†å…¶ä¿å­˜å›å»</li>
<li>ä½ å°†å­¦ä¹ ä»¥ä¸‹åŠŸèƒ½ï¼šcv.imread()ï¼Œcv.imshow()ï¼Œcv.imwrite()</li>
<li>(å¯é€‰)ä½ å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨Matplotlibæ˜¾ç¤ºå›¾åƒ<a id="more"></a>


</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h2 id="è¯»å–å›¾åƒ"><a href="#è¯»å–å›¾åƒ" class="headerlink" title="è¯»å–å›¾åƒ"></a>è¯»å–å›¾åƒ</h2><ul>
<li>ä½¿ç”¨<strong>cv.imread</strong>()å‡½æ•°è¯»å–å›¾åƒã€‚</li>
<li>å›¾åƒåº”è¯¥åœ¨å·¥ä½œç›®å½•æˆ–å›¾åƒçš„å®Œæ•´è·¯å¾„åº”ç»™å‡ºã€‚<br>ç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ªæ ‡å¿—ï¼Œå®ƒæŒ‡å®šäº†è¯»å–å›¾åƒçš„æ–¹å¼ã€‚<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.IMREAD_COLORï¼š åŠ è½½å½©è‰²å›¾åƒã€‚ä»»ä½•å›¾åƒçš„é€æ˜åº¦éƒ½ä¼šè¢«å¿½è§†ã€‚å®ƒæ˜¯é»˜è®¤æ ‡å¿—ã€‚</span><br><span class="line">cv.IMREAD_GRAYSCALEï¼šä»¥ç°åº¦æ¨¡å¼åŠ è½½å›¾åƒ</span><br><span class="line">cv.IMREAD_UNCHANGEDï¼šåŠ è½½å›¾åƒï¼ŒåŒ…æ‹¬alphaé€šé“</span><br></pre></td></tr></table></figure>
æ³¨æ„ é™¤äº†è¿™ä¸‰ä¸ªæ ‡å¿—ï¼Œä½ å¯ä»¥åˆ†åˆ«ç®€å•åœ°ä¼ é€’æ•´æ•°1ã€0æˆ–-1ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img=cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img</span><br></pre></td></tr></table></figure>




<pre><code>array([[181, 247, 251, ..., 203, 177, 170],
       [217, 255, 250, ..., 211, 178, 167],
       [247, 255, 245, ..., 218, 180, 164],
       ...,
       [ 13,  13,  12, ...,   0,  16,   0],
       [ 11,  10,  10, ...,   0,   0,   0],
       [ 17,  17,  16, ...,   0,   4,   0]], dtype=uint8)</code></pre><h2 id="æ˜¾ç¤ºå›¾åƒ"><a href="#æ˜¾ç¤ºå›¾åƒ" class="headerlink" title="æ˜¾ç¤ºå›¾åƒ"></a>æ˜¾ç¤ºå›¾åƒ</h2><ul>
<li>ä½¿ç”¨å‡½æ•°<strong>cv.imshow()</strong>åœ¨çª—å£ä¸­æ˜¾ç¤ºå›¾åƒã€‚çª—å£è‡ªåŠ¨é€‚åˆå›¾åƒå°ºå¯¸ã€‚</li>
<li>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯çª—å£åç§°ï¼Œå®ƒæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚</li>
<li>ç¬¬äºŒä¸ªå‚æ•°æ˜¯æˆ‘ä»¬çš„å¯¹è±¡ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦åˆ›å»ºä»»æ„å¤šä¸ªçª—å£ï¼Œä½†å¯ä»¥ä½¿ç”¨ä¸åŒçš„çª—å£åç§°ã€‚</li>
</ul>
<p>cv.waitKey()æ˜¯ä¸€ä¸ªé”®ç›˜ç»‘å®šå‡½æ•°ã€‚å…¶å‚æ•°æ˜¯ä»¥æ¯«ç§’ä¸ºå•ä½çš„æ—¶é—´ã€‚</p>
<p>è¯¥å‡½æ•°ç­‰å¾…ä»»ä½•é”®ç›˜äº‹ä»¶æŒ‡å®šçš„æ¯«ç§’ã€‚å¦‚æœæ‚¨åœ¨è¿™æ®µæ—¶é—´å†…æŒ‰ä¸‹ä»»ä½•é”®ï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œã€‚</p>
<p>å¦‚æœ<strong>0</strong>è¢«ä¼ é€’ï¼Œå®ƒå°†æ— é™æœŸåœ°ç­‰å¾…ä¸€æ¬¡æ•²å‡»é”®ã€‚</p>
<p>å®ƒä¹Ÿå¯ä»¥è®¾ç½®ä¸ºæ£€æµ‹ç‰¹å®šçš„æŒ‰é”®ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæŒ‰ä¸‹é”® a ç­‰</p>
<p>cv.destroyAllWindows()åªä¼šç ´åæˆ‘ä»¬åˆ›å»ºçš„æ‰€æœ‰çª—å£ã€‚</p>
<p>å¦‚æœè¦é”€æ¯ä»»ä½•ç‰¹å®šçš„çª—å£ï¼Œè¯·ä½¿ç”¨å‡½æ•° cv.destroyWindow()åœ¨å…¶ä¸­ä¼ é€’ç¡®åˆ‡çš„çª—å£åç§°ä½œä¸ºå‚æ•°ã€‚</p>
<p>åœ¨ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªç©ºçª—å£ï¼Œç„¶åå†å°†å›¾åƒåŠ è½½åˆ°è¯¥çª—å£ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥æŒ‡å®šçª—å£æ˜¯å¦å¯è°ƒæ•´å¤§å°ã€‚</p>
<p>è¿™æ˜¯é€šè¿‡åŠŸèƒ½<strong>cv.namedWindow</strong>()å®Œæˆçš„ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥æ ‡å¿—ä¸º<strong>cv.WINDOW_AUTOSIZE</strong>ã€‚</p>
<p>ä½†æ˜¯ï¼Œå¦‚æœå°†æ ‡å¿—æŒ‡å®šä¸º<strong>cv.WINDOW_NORMAL</strong>ï¼Œåˆ™å¯ä»¥è°ƒæ•´çª—å£å¤§å°ã€‚å½“å›¾åƒå°ºå¯¸è¿‡å¤§ä»¥åŠå‘çª—å£æ·»åŠ è·Ÿè¸ªæ æ—¶ï¼Œè¿™å°†å¾ˆæœ‰å¸®åŠ©ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv.namedWindow(<span class="string">'image'</span>,cv.WINDOW_NORMAL)</span><br><span class="line">cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h2 id="å†™å…¥å›¾åƒ"><a href="#å†™å…¥å›¾åƒ" class="headerlink" title="å†™å…¥å›¾åƒ"></a>å†™å…¥å›¾åƒ</h2><p>ä½¿ç”¨å‡½æ•°<strong>cv.imwrite</strong>()ä¿å­˜å›¾åƒã€‚</p>
<p>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ–‡ä»¶åï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯è¦ä¿å­˜çš„å›¾åƒã€‚</p>
<p>cv.imwrite(â€˜messigray.pngâ€™ï¼Œimg)è¿™ä¼šå°†å›¾åƒä»¥PNGæ ¼å¼ä¿å­˜åœ¨å·¥ä½œç›®å½•ä¸­ã€‚</p>
<p>åœ¨ä¸‹é¢çš„ç¨‹åºä¸­ï¼Œä»¥ç°åº¦åŠ è½½å›¾åƒï¼Œæ˜¾ç¤ºå›¾åƒï¼ŒæŒ‰sä¿å­˜å›¾åƒå¹¶é€€å‡ºï¼Œæˆ–è€…æŒ‰ESCé”®ç›´æ¥é€€å‡ºè€Œä¸ä¿å­˜ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">k =cv.waitKey(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> k == <span class="number">27</span>: <span class="comment"># æŒ‰ä¸‹escæ—¶</span></span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"><span class="keyword">elif</span> k == ord(<span class="string">'s'</span>): <span class="comment"># æŒ‰ä¸‹sæ—¶</span></span><br><span class="line">    cv.imwrite(<span class="string">'copy1.jpg'</span>,img)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img = cv.imread(<span class="string">'1.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">plt.imshow(img, interpolation = <span class="string">'bicubic'</span>)</span><br><span class="line"><span class="comment"># plt.imshow(img)</span></span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/12/opencv-GUI%E7%89%B9%E6%80%A7/output_13_0.png" alt="png"></p>
<h1 id="è§†é¢‘å…¥é—¨"><a href="#è§†é¢‘å…¥é—¨" class="headerlink" title="è§†é¢‘å…¥é—¨"></a><span id="header2">è§†é¢‘å…¥é—¨</span></h1><h2 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><ul>
<li>å­¦ä¹ è¯»å–è§†é¢‘ï¼Œæ˜¾ç¤ºè§†é¢‘å’Œä¿å­˜è§†é¢‘ã€‚</li>
<li>å­¦ä¹ ä»ç›¸æœºæ•æ‰å¹¶æ˜¾ç¤ºå®ƒã€‚</li>
<li>ä½ å°†å­¦ä¹ ä»¥ä¸‹åŠŸèƒ½ï¼šcv.VideoCapture()ï¼Œcv.VideoWriter()<!--more-->

</li>
</ul>
<p>è¦æ•è·è§†é¢‘ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ª VideoCapture å¯¹è±¡ã€‚</p>
<p>å®ƒçš„å‚æ•°å¯ä»¥æ˜¯è®¾å¤‡ç´¢å¼•æˆ–è§†é¢‘æ–‡ä»¶çš„åç§°ã€‚è®¾å¤‡ç´¢å¼•å°±æ˜¯æŒ‡å®šå“ªä¸ªæ‘„åƒå¤´çš„æ•°å­—ã€‚</p>
<p>æ­£å¸¸æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ‘„åƒå¤´ä¼šè¢«è¿æ¥(å°±åƒæˆ‘çš„æƒ…å†µä¸€æ ·)ã€‚æ‰€ä»¥æˆ‘ç®€å•åœ°ä¼ 0(æˆ–-1)ã€‚ä½ å¯ä»¥é€šè¿‡ä¼ é€’1æ¥é€‰æ‹©ç¬¬äºŒä¸ªç›¸æœºï¼Œä»¥æ­¤ç±»æ¨ã€‚</p>
<p>åœ¨æ­¤ä¹‹åï¼Œä½ å¯ä»¥é€å¸§æ•è·ã€‚ä½†æ˜¯åœ¨æœ€åï¼Œä¸è¦å¿˜è®°é‡Šæ”¾ä¿˜è™ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> cap.isOpened():</span><br><span class="line">    print(<span class="string">"Cannot open camera"</span>)</span><br><span class="line">    exit()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># é€å¸§æ•è·</span></span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="comment"># å¦‚æœæ­£ç¡®è¯»å–å¸§ï¼Œretä¸ºTrue</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># æˆ‘ä»¬åœ¨æ¡†æ¶ä¸Šçš„æ“ä½œåˆ°è¿™é‡Œ</span></span><br><span class="line">    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># æ˜¾ç¤ºç»“æœå¸§e</span></span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, gray)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment"># å®Œæˆæ‰€æœ‰æ“ä½œåï¼Œé‡Šæ”¾æ•è·å™¨</span></span><br><span class="line">cap.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<pre><code>Cannot open camera
Can&apos;t receive frame (stream end?). Exiting ...</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="string">'video1.flv'</span>)</span><br><span class="line"><span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="comment"># å¦‚æœæ­£ç¡®è¯»å–å¸§ï¼Œretä¸ºTrue</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, gray)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cap.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(<span class="string">'video1.flv'</span>)</span><br><span class="line"><span class="comment"># å®šä¹‰ç¼–è§£ç å™¨å¹¶åˆ›å»ºVideoWriterå¯¹è±¡</span></span><br><span class="line">fourcc = cv.VideoWriter_fourcc(*<span class="string">'DIVX'</span>)</span><br><span class="line">out = cv.VideoWriter(<span class="string">'output.flv'</span>, fourcc, <span class="number">20.0</span>, (<span class="number">640</span>,  <span class="number">480</span>))</span><br><span class="line"><span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        print(<span class="string">"Can't receive frame (stream end?). Exiting ..."</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    frame = cv.flip(frame, <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># å†™ç¿»è½¬çš„æ¡†æ¶</span></span><br><span class="line">    out.write(frame)</span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>, frame)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment"># å®Œæˆå·¥ä½œåé‡Šæ”¾æ‰€æœ‰å†…å®¹</span></span><br><span class="line">cap.release()</span><br><span class="line">out.release()</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<pre><code>Can&apos;t receive frame (stream end?). Exiting ...</code></pre><h1 id="ç»˜å›¾"><a href="#ç»˜å›¾" class="headerlink" title="ç»˜å›¾"></a><span id="header3">ç»˜å›¾</span></h1><h2 id="ç›®æ ‡-2"><a href="#ç›®æ ‡-2" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><ul>
<li>å­¦ä¹ ä½¿ç”¨OpenCVç»˜åˆ¶ä¸åŒçš„å‡ ä½•å½¢çŠ¶</li>
<li>æ‚¨å°†å­¦ä¹ ä»¥ä¸‹åŠŸèƒ½ï¼šcv.line()ï¼Œcv.circle()ï¼Œcv.rectangle()ï¼Œcv.ellipse()ï¼Œcv.putText()ç­‰ã€‚</li>
<li>åœ¨ä¸Šè¿°æ‰€æœ‰åŠŸèƒ½ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°ä¸€äº›å¸¸è§çš„å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">imgï¼šæ‚¨è¦ç»˜åˆ¶å½¢çŠ¶çš„å›¾åƒ</span><br><span class="line">colorï¼šå½¢çŠ¶çš„é¢œè‰²ã€‚å¯¹äºBGRï¼Œå°†å…¶ä½œä¸ºå…ƒç»„ä¼ é€’ï¼Œä¾‹å¦‚ï¼š(255,0,0)å¯¹äºè“è‰²ã€‚å¯¹äºç°åº¦ï¼Œåªéœ€ä¼ é€’æ ‡é‡å€¼å³å¯ã€‚</span><br><span class="line">åšåº¦ï¼šçº¿æˆ–åœ†ç­‰çš„ç²—ç»†ã€‚å¦‚æœå¯¹é—­åˆå›¾å½¢ï¼ˆå¦‚åœ†ï¼‰ä¼ é€’-1 ï¼Œå®ƒå°†å¡«å……å½¢çŠ¶ã€‚é»˜è®¤åšåº¦&#x3D; 1</span><br><span class="line">lineTypeï¼šçº¿çš„ç±»å‹ï¼Œæ˜¯å¦ä¸º8è¿æ¥çº¿ï¼ŒæŠ—é”¯é½¿çº¿ç­‰ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸º8è¿æ¥çº¿ã€‚**cv.LINE_AA**ç»™å‡ºäº†æŠ—é”¯é½¿çš„çº¿æ¡ï¼Œçœ‹èµ·æ¥éå¸¸é€‚åˆæ›²çº¿ã€‚</span><br></pre></td></tr></table></figure>


</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¦ç»˜åˆ¶ä¸€æ¡çº¿ï¼Œæ‚¨éœ€è¦ä¼ é€’çº¿çš„å¼€å§‹å’Œç»“æŸåæ ‡ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªé»‘è‰²å›¾åƒï¼Œå¹¶ä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’åœ¨å…¶ä¸Šç»˜åˆ¶ä¸€æ¡è“çº¿ã€‚</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># åˆ›å»ºé»‘è‰²çš„å›¾åƒ</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line"><span class="comment"># ç»˜åˆ¶ä¸€æ¡åšåº¦ä¸º5çš„è“è‰²å¯¹è§’çº¿</span></span><br><span class="line">cv.line(img,(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">511</span>,<span class="number">511</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">5</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¦ç»˜åˆ¶çŸ©å½¢ï¼Œæ‚¨éœ€è¦çŸ©å½¢çš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’ã€‚è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†åœ¨å›¾åƒçš„å³ä¸Šè§’ç»˜åˆ¶ä¸€ä¸ªç»¿è‰²çŸ©å½¢ã€‚</span></span><br><span class="line"><span class="comment"># åˆ›å»ºé»‘è‰²çš„å›¾åƒ</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.rectangle(img,(<span class="number">384</span>,<span class="number">0</span>),(<span class="number">510</span>,<span class="number">128</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¦ç»˜åˆ¶ä¸€ä¸ªåœ†ï¼Œéœ€è¦å…¶ä¸­å¿ƒåæ ‡å’ŒåŠå¾„ã€‚æˆ‘ä»¬å°†åœ¨ä¸Šé¢ç»˜åˆ¶çš„çŸ©å½¢å†…ç»˜åˆ¶ä¸€ä¸ªåœ†ã€‚</span></span><br><span class="line"><span class="comment"># åˆ›å»ºé»‘è‰²çš„å›¾åƒ</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.circle(img,(<span class="number">447</span>,<span class="number">63</span>), <span class="number">63</span>, (<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="number">-1</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¦ç»˜åˆ¶æ¤­åœ†ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’å‡ ä¸ªå‚æ•°ã€‚ä¸€ä¸ªå‚æ•°æ˜¯ä¸­å¿ƒä½ç½®ï¼ˆxï¼Œyï¼‰ã€‚</span></span><br><span class="line"><span class="comment"># ä¸‹ä¸€ä¸ªå‚æ•°æ˜¯è½´é•¿åº¦ï¼ˆé•¿è½´é•¿åº¦ï¼ŒçŸ­è½´é•¿åº¦ï¼‰ã€‚</span></span><br><span class="line"><span class="comment"># angleæ˜¯æ¤­åœ†æ²¿é€†æ—¶é’ˆæ–¹å‘æ—‹è½¬çš„è§’åº¦ã€‚</span></span><br><span class="line"><span class="comment"># startAngleå’ŒendAngleè¡¨ç¤ºä»ä¸»è½´æ²¿é¡ºæ—¶é’ˆæ–¹å‘æµ‹é‡çš„æ¤­åœ†å¼§çš„å¼€å§‹å’Œç»“æŸã€‚å³ç»™å‡º0å’Œ360ç»™å‡ºå®Œæ•´çš„æ¤­åœ†</span></span><br><span class="line"><span class="comment"># ä¸‹é¢çš„ç¤ºä¾‹åœ¨å›¾åƒçš„ä¸­å¿ƒç»˜åˆ¶ä¸€ä¸ªæ¤­åœ†å½¢ã€‚</span></span><br><span class="line"><span class="comment"># åˆ›å»ºé»‘è‰²çš„å›¾åƒ</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.ellipse(img,(<span class="number">256</span>,<span class="number">256</span>),(<span class="number">100</span>,<span class="number">50</span>),<span class="number">0</span>,<span class="number">0</span>,<span class="number">180</span>,<span class="number">255</span>,<span class="number">-1</span>)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¦ç»˜åˆ¶å¤šè¾¹å½¢ï¼Œé¦–å…ˆéœ€è¦é¡¶ç‚¹çš„åæ ‡ã€‚å°†è¿™äº›ç‚¹ç»„æˆå½¢çŠ¶ä¸ºROWSx1x2çš„æ•°ç»„ï¼Œå…¶ä¸­ROWSæ˜¯é¡¶ç‚¹æ•°ï¼Œå¹¶ä¸”å…¶ç±»å‹åº”ä¸ºint32ã€‚</span></span><br><span class="line"><span class="comment"># åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†ä¸€ä¸ªå¸¦æœ‰å››ä¸ªé¡¶ç‚¹çš„é»„è‰²å°å¤šè¾¹å½¢ã€‚</span></span><br><span class="line"><span class="comment"># åˆ›å»ºé»‘è‰²çš„å›¾åƒ</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">pts = np.array([[<span class="number">10</span>,<span class="number">5</span>],[<span class="number">20</span>,<span class="number">30</span>],[<span class="number">270</span>,<span class="number">20</span>],[<span class="number">150</span>,<span class="number">5</span>]], np.int32)</span><br><span class="line">pts = pts.reshape((<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">cv.polylines(img,[pts],<span class="literal">True</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å¦‚æœç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºFalseï¼Œæ‚¨å°†è·å¾—ä¸€æ¡è¿æ¥æ‰€æœ‰ç‚¹çš„æŠ˜çº¿ï¼Œè€Œä¸æ˜¯é—­åˆå½¢çŠ¶ã€‚ </span></span><br><span class="line"><span class="comment"># cv.polylines()å¯ç”¨äºç»˜åˆ¶å¤šæ¡çº¿ã€‚åªéœ€åˆ›å»ºè¦ç»˜åˆ¶çš„æ‰€æœ‰çº¿æ¡çš„åˆ—è¡¨ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™å‡½æ•°å³å¯ã€‚</span></span><br><span class="line"><span class="comment"># æ‰€æœ‰çº¿æ¡å°†å•ç‹¬ç»˜åˆ¶ã€‚ä¸ä¸ºæ¯æ¡çº¿è°ƒç”¨**cv.line**ç›¸æ¯”ï¼Œç»˜åˆ¶ä¸€ç»„çº¿æ˜¯ä¸€ç§æ›´å¥½ï¼Œæ›´å¿«çš„æ–¹æ³•ã€‚</span></span><br><span class="line"><span class="comment"># åˆ›å»ºé»‘è‰²çš„å›¾åƒ</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">pts = np.array([[<span class="number">10</span>,<span class="number">5</span>],[<span class="number">20</span>,<span class="number">30</span>],[<span class="number">270</span>,<span class="number">20</span>],[<span class="number">150</span>,<span class="number">5</span>]], np.int32)</span><br><span class="line">pts = pts.reshape((<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">cv.polylines(img,[pts],<span class="literal">False</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¦å°†æ–‡æœ¬æ”¾å…¥å›¾åƒä¸­ï¼Œéœ€è¦æŒ‡å®šä»¥ä¸‹å†…å®¹ã€‚ </span></span><br><span class="line"><span class="comment"># - æ‚¨è¦å†™å…¥çš„æ–‡å­—æ•°æ® </span></span><br><span class="line"><span class="comment"># - æ‚¨è¦æ”¾ç½®å®ƒçš„ä½ç½®åæ ‡ï¼ˆå³æ•°æ®å¼€å§‹çš„å·¦ä¸‹è§’ï¼‰ã€‚ </span></span><br><span class="line"><span class="comment"># - å­—ä½“ç±»å‹ï¼ˆæ£€æŸ¥**cv.putText**æ–‡æ¡£ä»¥è·å–å—æ”¯æŒçš„å­—ä½“ï¼‰</span></span><br><span class="line"><span class="comment"># - å­—ä½“æ¯”ä¾‹ï¼ˆæŒ‡å®šå­—ä½“å¤§å°ï¼‰ </span></span><br><span class="line"><span class="comment"># - å¸¸è§„çš„å†…å®¹ï¼Œä¾‹å¦‚é¢œè‰²ï¼Œåšåº¦ï¼Œçº¿æ¡ç±»å‹ç­‰ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„å¤–è§‚ï¼Œå»ºè®®ä½¿ç”¨lineType = cv.LINE_AAã€‚</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">font = cv.FONT_HERSHEY_SIMPLEX</span><br><span class="line">cv.putText(img,<span class="string">'OpenCV'</span>,(<span class="number">10</span>,<span class="number">500</span>), font, <span class="number">4</span>,(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>),<span class="number">2</span>,cv.LINE_AA)</span><br><span class="line">cv.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<h1 id="é¼ æ ‡ä½œä¸ºç”»ç¬”"><a href="#é¼ æ ‡ä½œä¸ºç”»ç¬”" class="headerlink" title="é¼ æ ‡ä½œä¸ºç”»ç¬”"></a><span id="header4">é¼ æ ‡ä½œä¸ºç”»ç¬”</span></h1><h2 id="ç›®æ ‡-3"><a href="#ç›®æ ‡-3" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><ul>
<li>äº†è§£å¦‚ä½•åœ¨OpenCVä¸­å¤„ç†é¼ æ ‡äº‹ä»¶</li>
<li>æ‚¨å°†å­¦ä¹ ä»¥ä¸‹åŠŸèƒ½ï¼šcv.setMouseCallback()</li>
</ul>
<p>ç®€å•æ¼”ç¤º<br>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åºï¼Œæ— è®ºæˆ‘ä»¬åœ¨å“ªé‡ŒåŒå‡»å®ƒï¼Œéƒ½å¯ä»¥åœ¨å›¾åƒä¸Šç»˜åˆ¶ä¸€ä¸ªåœ†ã€‚</p>
<p>é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªé¼ æ ‡å›è°ƒå‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨å‘ç”Ÿé¼ æ ‡äº‹ä»¶æ—¶æ‰§è¡Œã€‚</p>
<p>é¼ æ ‡äº‹ä»¶å¯ä»¥æ˜¯ä¸é¼ æ ‡ç›¸å…³çš„ä»»ä½•äº‹ç‰©ï¼Œä¾‹å¦‚å·¦é”®æŒ‰ä¸‹ï¼Œå·¦é”®æŒ‰ä¸‹ï¼Œå·¦é”®åŒå‡»ç­‰ã€‚</p>
<p>å®ƒä¸ºæˆ‘ä»¬æä¾›äº†æ¯ä¸ªé¼ æ ‡äº‹ä»¶çš„åæ ‡(xï¼Œy)ã€‚é€šè¿‡æ­¤æ´»åŠ¨å’Œåœ°ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åšä»»ä½•æˆ‘ä»¬å–œæ¬¢çš„äº‹æƒ…ã€‚</p>
<p>è¦åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å¯ç”¨äº‹ä»¶ï¼Œè¯·åœ¨Pythonç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹ä»£ç ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">events = [i <span class="keyword">for</span> i <span class="keyword">in</span> dir(cv) <span class="keyword">if</span> <span class="string">'EVENT'</span> <span class="keyword">in</span> i]</span><br><span class="line">print( events )</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;EVENT_FLAG_ALTKEY&apos;, &apos;EVENT_FLAG_CTRLKEY&apos;, &apos;EVENT_FLAG_LBUTTON&apos;, &apos;EVENT_FLAG_MBUTTON&apos;, &apos;EVENT_FLAG_RBUTTON&apos;, &apos;EVENT_FLAG_SHIFTKEY&apos;, &apos;EVENT_LBUTTONDBLCLK&apos;, &apos;EVENT_LBUTTONDOWN&apos;, &apos;EVENT_LBUTTONUP&apos;, &apos;EVENT_MBUTTONDBLCLK&apos;, &apos;EVENT_MBUTTONDOWN&apos;, &apos;EVENT_MBUTTONUP&apos;, &apos;EVENT_MOUSEHWHEEL&apos;, &apos;EVENT_MOUSEMOVE&apos;, &apos;EVENT_MOUSEWHEEL&apos;, &apos;EVENT_RBUTTONDBLCLK&apos;, &apos;EVENT_RBUTTONDOWN&apos;, &apos;EVENT_RBUTTONUP&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åœ¨æˆ‘ä»¬åŒå‡»çš„åœ°æ–¹ç»˜åˆ¶ä¸€ä¸ªåœ†åœˆ</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># é¼ æ ‡å›è°ƒå‡½æ•°</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_circle</span><span class="params">(event,x,y,flags,param)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> event == cv.EVENT_LBUTTONDBLCLK:</span><br><span class="line">        cv.circle(img,(x,y),<span class="number">100</span>,(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># åˆ›å»ºä¸€ä¸ªé»‘è‰²çš„å›¾åƒï¼Œä¸€ä¸ªçª—å£ï¼Œå¹¶ç»‘å®šåˆ°çª—å£çš„åŠŸèƒ½</span></span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line">cv.setMouseCallback(<span class="string">'image'</span>,draw_circle)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æŒ‰mä¹‹å‰æ‹–æ‹½ç”»é’è‰²çŸ©å½¢</span></span><br><span class="line"><span class="comment"># æŒ‰mä¹‹åæ‹–æ‹½ç”»çº¢è‰²ç›´çº¿</span></span><br><span class="line"><span class="comment"># æŒ‰escé€€å‡º</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">drawing = <span class="literal">False</span> <span class="comment"># å¦‚æœæŒ‰ä¸‹é¼ æ ‡ï¼Œåˆ™ä¸ºçœŸ</span></span><br><span class="line">mode = <span class="literal">True</span> <span class="comment"># å¦‚æœä¸ºçœŸï¼Œç»˜åˆ¶çŸ©å½¢ã€‚æŒ‰ m é”®å¯ä»¥åˆ‡æ¢åˆ°æ›²çº¿</span></span><br><span class="line">ix,iy = <span class="number">-1</span>,<span class="number">-1</span></span><br><span class="line"><span class="comment"># é¼ æ ‡å›è°ƒå‡½æ•°</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_circle</span><span class="params">(event,x,y,flags,param)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ix,iy,drawing,mode</span><br><span class="line">    <span class="keyword">if</span> event == cv.EVENT_LBUTTONDOWN:</span><br><span class="line">        drawing = <span class="literal">True</span></span><br><span class="line">        ix,iy = x,y</span><br><span class="line">    <span class="keyword">elif</span> event == cv.EVENT_MOUSEMOVE:</span><br><span class="line">        <span class="keyword">if</span> drawing == <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="literal">True</span>:</span><br><span class="line">                cv.rectangle(img,(ix,iy),(x,y),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cv.circle(img,(x,y),<span class="number">25</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">elif</span> event == cv.EVENT_LBUTTONUP:</span><br><span class="line">        drawing = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="literal">True</span>:</span><br><span class="line">            cv.rectangle(img,(ix,iy),(x,y),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cv.circle(img,(x,y),<span class="number">25</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">-1</span>)</span><br><span class="line">            </span><br><span class="line">img = np.zeros((<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line">cv.setMouseCallback(<span class="string">'image'</span>,draw_circle)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; cv.waitKey(<span class="number">20</span>) == ord(<span class="string">'m'</span>):</span><br><span class="line">        mode = ~mode</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">20</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<h1 id="è½¨è¿¹æ ä½œä¸ºè°ƒè‰²æ¿"><a href="#è½¨è¿¹æ ä½œä¸ºè°ƒè‰²æ¿" class="headerlink" title="è½¨è¿¹æ ä½œä¸ºè°ƒè‰²æ¿"></a><span id="header5">è½¨è¿¹æ ä½œä¸ºè°ƒè‰²æ¿</span></h1><h2 id="ç›®æ ‡-4"><a href="#ç›®æ ‡-4" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><ul>
<li>äº†è§£å°†è½¨è¿¹æ å›ºå®šåˆ°OpenCVçª—å£</li>
<li>æ‚¨å°†å­¦ä¹ ä»¥ä¸‹åŠŸèƒ½ï¼šcv.getTrackbarPosï¼Œ<strong>cv.createTrackbar</strong>ç­‰ã€‚</li>
</ul>
<p>å¯¹äºcv.getTrackbarPos()å‡½æ•°ï¼Œ</p>
<p>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è½¨è¿¹æ åç§°ï¼Œ</p>
<p>ç¬¬äºŒä¸ªå‚æ•°æ˜¯å®ƒé™„åŠ åˆ°çš„çª—å£åç§°ï¼Œ</p>
<p>ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯é»˜è®¤å€¼ï¼Œ</p>
<p>ç¬¬å››ä¸ªå‚æ•°æ˜¯æœ€å¤§å€¼ï¼Œ</p>
<p>ç¬¬äº”ä¸ªæ˜¯æ‰§è¡Œçš„å›è°ƒå‡½æ•°æ¯æ¬¡è·Ÿè¸ªæ å€¼æ›´æ”¹ã€‚</p>
<p>å›è°ƒå‡½æ•°å§‹ç»ˆå…·æœ‰é»˜è®¤å‚æ•°ï¼Œå³è½¨è¿¹æ ä½ç½®ã€‚</p>
<p>åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå‡½æ•°ä»€ä¹ˆéƒ½ä¸åšï¼Œæ‰€ä»¥æˆ‘ä»¬ç®€å•åœ°é€šè¿‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åºï¼Œä»¥æ˜¾ç¤ºæ‚¨æŒ‡å®šçš„é¢œè‰²ã€‚</span></span><br><span class="line"><span class="comment"># æ‚¨æœ‰ä¸€ä¸ªæ˜¾ç¤ºé¢œè‰²çš„çª—å£ï¼Œä»¥åŠä¸‰ä¸ªç”¨äºæŒ‡å®šBã€Gã€Ré¢œè‰²çš„è·Ÿè¸ªæ ã€‚æ»‘åŠ¨è½¨è¿¹æ ï¼Œå¹¶ç›¸åº”åœ°æ›´æ”¹çª—å£é¢œè‰²ã€‚</span></span><br><span class="line"><span class="comment"># é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆå§‹é¢œè‰²å°†è®¾ç½®ä¸ºé»‘è‰²ã€‚åªæœ‰åœ¨è¯¥å¼€å…³ä¸ºONçš„æƒ…å†µä¸‹ï¼Œè¯¥åº”ç”¨ç¨‹åºæ‰èƒ½åœ¨å…¶ä¸­è¿è¡Œï¼Œå¦åˆ™å±å¹•å§‹ç»ˆä¸ºé»‘è‰²ã€‚</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nothing</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># åˆ›å»ºä¸€ä¸ªé»‘è‰²çš„å›¾åƒï¼Œä¸€ä¸ªçª—å£</span></span><br><span class="line">img = np.zeros((<span class="number">300</span>,<span class="number">512</span>,<span class="number">3</span>), np.uint8)</span><br><span class="line">cv.namedWindow(<span class="string">'image'</span>)</span><br><span class="line"><span class="comment"># åˆ›å»ºé¢œè‰²å˜åŒ–çš„è½¨è¿¹æ </span></span><br><span class="line">cv.createTrackbar(<span class="string">'R'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line">cv.createTrackbar(<span class="string">'G'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line">cv.createTrackbar(<span class="string">'B'</span>,<span class="string">'image'</span>,<span class="number">0</span>,<span class="number">255</span>,nothing)</span><br><span class="line"><span class="comment"># ä¸º ON/OFF åŠŸèƒ½åˆ›å»ºå¼€å…³</span></span><br><span class="line">switch = <span class="string">'0 : OFF \n1 : ON'</span></span><br><span class="line">cv.createTrackbar(switch, <span class="string">'image'</span>,<span class="number">0</span>,<span class="number">1</span>,nothing)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    cv.imshow(<span class="string">'image'</span>,img)</span><br><span class="line">    k = cv.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># å¾—åˆ°å››æ¡è½¨è¿¹çš„å½“å‰ä½ç½®</span></span><br><span class="line">    r = cv.getTrackbarPos(<span class="string">'R'</span>,<span class="string">'image'</span>)</span><br><span class="line">    g = cv.getTrackbarPos(<span class="string">'G'</span>,<span class="string">'image'</span>)</span><br><span class="line">    b = cv.getTrackbarPos(<span class="string">'B'</span>,<span class="string">'image'</span>)</span><br><span class="line">    s = cv.getTrackbarPos(switch,<span class="string">'image'</span>)</span><br><span class="line">    <span class="keyword">if</span> s == <span class="number">0</span>:</span><br><span class="line">        img[:] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img[:] = [b,g,r]</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>å›¾åƒ</tag>
      </tags>
  </entry>
  <entry>
    <title>å²­å›å½’</title>
    <url>/2020/07/11/%E5%B2%AD%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p><a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/longley.csv" target="_blank" rel="noopener">longley.csv</a></p>
<a id="more"></a>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%991.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%992.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%993.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%994.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%995.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%996.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%997.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%998.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%999.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9910.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9911.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9912.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9913.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E5%88%9914.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = genfromtxt(<span class="string">'./data/longley.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>

<pre><code>[[     nan      nan      nan      nan      nan      nan      nan      nan]
 [     nan   83.     234.289  235.6    159.     107.608 1947.      60.323]
 [     nan   88.5    259.426  232.5    145.6    108.632 1948.      61.122]
 [     nan   88.2    258.054  368.2    161.6    109.773 1949.      60.171]
 [     nan   89.5    284.599  335.1    165.     110.929 1950.      61.187]
 [     nan   96.2    328.975  209.9    309.9    112.075 1951.      63.221]
 [     nan   98.1    346.999  193.2    359.4    113.27  1952.      63.639]
 [     nan   99.     365.385  187.     354.7    115.094 1953.      64.989]
 [     nan  100.     363.112  357.8    335.     116.219 1954.      63.761]
 [     nan  101.2    397.469  290.4    304.8    117.388 1955.      66.019]
 [     nan  104.6    419.18   282.2    285.7    118.734 1956.      67.857]
 [     nan  108.4    442.769  293.6    279.8    120.445 1957.      68.169]
 [     nan  110.8    444.546  468.1    263.7    121.95  1958.      66.513]
 [     nan  112.6    482.704  381.3    255.2    123.366 1959.      68.655]
 [     nan  114.2    502.601  393.1    251.4    125.368 1960.      69.564]
 [     nan  115.7    518.173  480.6    257.2    127.852 1961.      69.331]
 [     nan  116.9    554.894  400.7    282.7    130.081 1962.      70.551]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = data[<span class="number">1</span>:,<span class="number">2</span>:]</span><br><span class="line">y_data = data[<span class="number">1</span>:,<span class="number">1</span>,np.newaxis]</span><br><span class="line">print(x_data)</span><br><span class="line">print(y_data)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 234.289  235.6    159.     107.608 1947.      60.323]
 [ 259.426  232.5    145.6    108.632 1948.      61.122]
 [ 258.054  368.2    161.6    109.773 1949.      60.171]
 [ 284.599  335.1    165.     110.929 1950.      61.187]
 [ 328.975  209.9    309.9    112.075 1951.      63.221]
 [ 346.999  193.2    359.4    113.27  1952.      63.639]
 [ 365.385  187.     354.7    115.094 1953.      64.989]
 [ 363.112  357.8    335.     116.219 1954.      63.761]
 [ 397.469  290.4    304.8    117.388 1955.      66.019]
 [ 419.18   282.2    285.7    118.734 1956.      67.857]
 [ 442.769  293.6    279.8    120.445 1957.      68.169]
 [ 444.546  468.1    263.7    121.95  1958.      66.513]
 [ 482.704  381.3    255.2    123.366 1959.      68.655]
 [ 502.601  393.1    251.4    125.368 1960.      69.564]
 [ 518.173  480.6    257.2    127.852 1961.      69.331]
 [ 554.894  400.7    282.7    130.081 1962.      70.551]]
[[ 83. ]
 [ 88.5]
 [ 88.2]
 [ 89.5]
 [ 96.2]
 [ 98.1]
 [ 99. ]
 [100. ]
 [101.2]
 [104.6]
 [108.4]
 [110.8]
 [112.6]
 [114.2]
 [115.7]
 [116.9]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.mat(x_data).shape)</span><br><span class="line">print(np.mat(y_data).shape)</span><br><span class="line">X_data = np.concatenate((np.ones((<span class="number">16</span>,<span class="number">1</span>)),x_data),axis=<span class="number">1</span>)</span><br><span class="line">print(X_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(16, 6)
(16, 1)
(16, 7)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(X_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[[1.00000e+00 2.34289e+02 2.35600e+02 1.59000e+02 1.07608e+02 1.94700e+03
  6.03230e+01]
 [1.00000e+00 2.59426e+02 2.32500e+02 1.45600e+02 1.08632e+02 1.94800e+03
  6.11220e+01]
 [1.00000e+00 2.58054e+02 3.68200e+02 1.61600e+02 1.09773e+02 1.94900e+03
  6.01710e+01]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights</span><span class="params">(xArr,yArr,lam=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    rxTx = xTx +np.eye(xMat.shape[<span class="number">1</span>])*lam</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(rxTx) == <span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'This martix cannot do inverse'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = rxTx.I*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = weights(X_data,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 7.38107363e-04]
 [ 2.07703836e-01]
 [ 2.10076376e-02]
 [ 5.05385441e-03]
 [-1.59173066e+00]
 [ 1.10442920e-01]
 [-2.42280461e-01]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.mat(X_data)* np.mat(ws)</span><br></pre></td></tr></table></figure>




<pre><code>matrix([[ 83.55075226],
        [ 86.92588689],
        [ 88.09720227],
        [ 90.95677622],
        [ 96.06951002],
        [ 97.81955375],
        [ 98.36444357],
        [ 99.99814266],
        [103.26832266],
        [105.03165135],
        [107.45224671],
        [109.52190685],
        [112.91863666],
        [113.98357055],
        [115.29845063],
        [117.64279933]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>çº¿æ€§å›å½’</tag>
      </tags>
  </entry>
  <entry>
    <title>æ¢¯åº¦ä¸‹é™æ³•</title>
    <url>/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</url>
    <content><![CDATA[<p>çº¿æ€§å›å½’æ¢¯åº¦ä¸‹é™ä»£ç ç®€å•æ¼”ç¤º</p>
<p>æ•°æ®é›†ä¸‹è½½:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/data.csv" target="_blank" rel="noopener">data.csv</a></p>
<a id="more"></a>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%951.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%952.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%953.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%954.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%955.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%956.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%957.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%958.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%959.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9510.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9511.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9512.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9513.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9514.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9515.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9516.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%9517.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/data.csv'</span>,delimiter=<span class="string">","</span>)</span><br><span class="line">x_data = data[:,<span class="number">0</span>]</span><br><span class="line">y_data = data[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">k = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span><span class="params">(b,k,x_data,y_data)</span>:</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        totalError += (y_data[i]-(k*x_data[i]+b))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/float(len(x_data))/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span><span class="params">(x_data,y_data,b,k,lr,epochs)</span>:</span></span><br><span class="line">    m = float(len(x_data))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        b_grad = <span class="number">0</span></span><br><span class="line">        k_grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,len(x_data)):</span><br><span class="line">            b_grad += (<span class="number">1</span>/m)*(((k*x_data[j])+b)-y_data[j])</span><br><span class="line">            k_grad += (<span class="number">1</span>/m)*x_data[j]*(((k*x_data[j])+b)-y_data[j])</span><br><span class="line">        b = b-(lr*b_grad)</span><br><span class="line">        k = k-(lr*k_grad)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epochs:"</span>,i)</span><br><span class="line">            plt.plot(x_data,y_data,<span class="string">'b.'</span>)</span><br><span class="line">            plt.plot(x_data, k*x_data+b, <span class="string">'r'</span>)</span><br><span class="line">            plt.show()</span><br><span class="line">    <span class="keyword">return</span> b,k</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Starting b = &#123;0&#125;, k = &#123;1&#125;, error = &#123;2&#125;"</span>.format(b, k, compute_error(b, k, x_data, y_data)))</span><br><span class="line">print(<span class="string">"Running..."</span>)</span><br><span class="line">b, k = gradient_descent_runner(x_data, y_data, b, k, lr, epochs)</span><br><span class="line">print(<span class="string">"After &#123;0&#125; iterations b = &#123;1&#125;, k = &#123;2&#125;, error = &#123;3&#125;"</span>.format(epochs, b, k, compute_error(b, k, x_data, y_data)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç”»å›¾</span></span><br><span class="line">plt.plot(x_data, y_data, <span class="string">'b.'</span>)</span><br><span class="line">plt.plot(x_data, k*x_data + b, <span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Starting b = 0, k = 0, error = 2782.5539172416056
Running...
epochs: 0</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_1.png" alt="png"></p>
<pre><code>epochs: 10</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_3.png" alt="png"></p>
<pre><code>epochs: 20</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_5.png" alt="png"></p>
<pre><code>epochs: 30</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_7.png" alt="png"></p>
<pre><code>epochs: 40</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_9.png" alt="png"></p>
<pre><code>After 50 iterations b = 0.030569950649287983, k = 1.4788903781318357, error = 56.32488184238028</code></pre><p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/output_4_11.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x0,x1 = np.meshgrid([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x0</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3, 4],
       [1, 2, 3, 4]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1</span><br></pre></td></tr></table></figure>




<pre><code>array([[4, 4, 4, 4],
       [5, 5, 5, 5]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç¥ç»ç½‘ç»œ</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>æ ‡å‡†æ–¹ç¨‹æ³•</title>
    <url>/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/</url>
    <content><![CDATA[<p>æ ‡å‡†æ–¹ç¨‹æ³•æ±‚çº¿æ€§å›å½’ä»£ç å±•ç¤º:<br>æ•°æ®é›†ä¸‹è½½<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/data.csv" target="_blank" rel="noopener">data</a></p>
<a id="more"></a>

<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%951.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%952.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%953.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%954.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%955.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%956.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%957.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%958.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data =np.genfromtxt(<span class="string">'./data/data.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">x_data = data[:,<span class="number">0</span>,np.newaxis]</span><br><span class="line">y_data = data[:,<span class="number">1</span>,np.newaxis]</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/output_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.mat(x_data).shape)</span><br><span class="line">print(np.mat(y_data).shape)</span><br><span class="line">X_data = np.concatenate((np.ones((<span class="number">100</span>,<span class="number">1</span>)),x_data),axis=<span class="number">1</span>)</span><br><span class="line">print(X_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(100, 1)
(100, 1)
(100, 2)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(X_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[[ 1.         32.50234527]
 [ 1.         53.42680403]
 [ 1.         61.53035803]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) ==<span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'This martix cannot do inverse'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = xTx.I*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = weights(X_data,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>

<pre><code>[[7.99102098]
 [1.32243102]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test = np.array([[<span class="number">20</span>],[<span class="number">80</span>]])</span><br><span class="line">y_test = ws[<span class="number">0</span>]+x_test*ws[<span class="number">1</span>]</span><br><span class="line">plt.plot(x_data,y_data,<span class="string">'b.'</span>)</span><br><span class="line">plt.plot(x_test,y_test,<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/11/%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B%E6%B3%95/output_7_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>çº¿æ€§å›å½’</tag>
      </tags>
  </entry>
  <entry>
    <title>æ¢¯åº¦ä¸‹é™æ³•â€”â€”â€”å¤šå…ƒ</title>
    <url>/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E2%80%94%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83/</url>
    <content><![CDATA[<p>çº¿æ€§å›å½’å¤šå…ƒæ¢¯åº¦ä¸‹é™ä»£ç ç®€å•æ¼”ç¤º</p>
<p>æ•°æ®é›†ä¸‹è½½:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/Delivery.csv" target="_blank" rel="noopener">Delivery.csv</a></p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = genfromtxt(<span class="string">'./data/Delivery.csv'</span>, delimiter=<span class="string">","</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>

<pre><code>[[100.    4.    9.3]
 [ 50.    3.    4.8]
 [100.    4.    8.9]
 [100.    2.    6.5]
 [ 50.    2.    4.2]
 [ 80.    2.    6.2]
 [ 75.    3.    7.4]
 [ 65.    4.    6. ]
 [ 90.    3.    7.6]
 [ 90.    2.    6.1]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = data[:,:<span class="number">-1</span>]</span><br><span class="line">y_data = data[:,<span class="number">-1</span>]</span><br><span class="line">print(x_data)</span><br><span class="line">print(y_data)</span><br></pre></td></tr></table></figure>

<pre><code>[[100.   4.]
 [ 50.   3.]
 [100.   4.]
 [100.   2.]
 [ 50.   2.]
 [ 80.   2.]
 [ 75.   3.]
 [ 65.   4.]
 [ 90.   3.]
 [ 90.   2.]]
[9.3 4.8 8.9 6.5 4.2 6.2 7.4 6.  7.6 6.1]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">theta0 = <span class="number">0</span></span><br><span class="line">theta1 = <span class="number">0</span></span><br><span class="line">theta2 = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span><span class="params">(theta0,theta1,theta2,x_data,y_data)</span>:</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        totalError += (y_data[i]-(theta1*x_data[i,<span class="number">0</span>]+theta2*x_data[i,<span class="number">1</span>]+theta0))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/float(len(x_data))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span><span class="params">(x_data,y_data,theta0,theta1,theta2,lr,epochs)</span>:</span></span><br><span class="line">    m = float(len(x_data))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        theta0_grad = <span class="number">0</span></span><br><span class="line">        theta1_grad = <span class="number">0</span></span><br><span class="line">        theta2_grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,len(x_data)):</span><br><span class="line">            theta0_grad+=(<span class="number">1</span>/m)*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]))</span><br><span class="line">            theta1_grad+=(<span class="number">1</span>/m)*x_data[j,<span class="number">0</span>]*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]+theta0)-y_data[j])</span><br><span class="line">            theta2_grad+=(<span class="number">1</span>/m)*x_data[j,<span class="number">1</span>]*((theta1*x_data[j,<span class="number">0</span>]+theta2*x_data[j,<span class="number">1</span>]+theta0)-y_data[j])</span><br><span class="line">            theta0 = theta0-(lr*theta0_grad)</span><br><span class="line">            theta1 = theta1-(lr*theta1_grad)</span><br><span class="line">            theta2 = theta2-(lr*theta2_grad)</span><br><span class="line">    <span class="keyword">return</span> theta0,theta1,theta2</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = plt.figure().add_subplot(<span class="number">111</span>,projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],y_data,c=<span class="string">'r'</span>,marker=<span class="string">'o'</span>,s=<span class="number">100</span>)</span><br><span class="line">x0=x_data[:,<span class="number">0</span>]</span><br><span class="line">x1=x_data[:,<span class="number">1</span>]</span><br><span class="line">x0,x1 = np.meshgrid(x0,x1)</span><br><span class="line">z = theta0+x0*theta1+x1*theta2</span><br><span class="line"></span><br><span class="line">ax.plot_surface(x0,x1,z)</span><br><span class="line">ax.set_xlabel(<span class="string">'Miles'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Num of Deliveries'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E2%80%94%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç¥ç»ç½‘ç»œ</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>è´å¶æ–¯å•è¯æ‹¼å†™å™¨</title>
    <url>/2020/07/11/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8D%95%E8%AF%8D%E6%8B%BC%E5%86%99%E5%99%A8/</url>
    <content><![CDATA[<p>è´å¶æ–¯å•è¯æ‹¼å†™å™¨ä»£ç :<br>æ•°æ®é›†ä¸‹è½½:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/big.txt" target="_blank" rel="noopener">big.txt</a></p>
<a id="more"></a>
<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF7.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF9.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF10.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF11.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF12.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF13.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF14.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF15.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF16.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF17.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF18.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF19.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = open(<span class="string">'./data/big.txt'</span>).read()</span><br><span class="line">text = re.findall(<span class="string">'[a-z]+'</span>, text.lower())</span><br><span class="line">dic_words = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> text:</span><br><span class="line">    dic_words[t]=dic_words.get(t,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">dic_words</span><br></pre></td></tr></table></figure>




<pre><code>{&apos;the&apos;: 80030,
 &apos;project&apos;: 288,
 &apos;gutenberg&apos;: 263,
 &apos;ebook&apos;: 87,
 &apos;of&apos;: 40025,
 &apos;adventures&apos;: 17,
 &apos;sherlock&apos;: 101,
 &apos;holmes&apos;: 467,
 &apos;by&apos;: 6738,
 &apos;sir&apos;: 177,
 &apos;arthur&apos;: 34,
 &apos;conan&apos;: 4,
 &apos;doyle&apos;: 5,
 &apos;in&apos;: 22047,
 &apos;our&apos;: 1066,
 &apos;series&apos;: 128,
 &apos;copyright&apos;: 69,
 &apos;laws&apos;: 233,
 &apos;are&apos;: 3630,
 &apos;changing&apos;: 44,
 &apos;all&apos;: 4144,
 &apos;over&apos;: 1282,
 &apos;world&apos;: 362,
 &apos;be&apos;: 6155,
 &apos;sure&apos;: 123,
 &apos;to&apos;: 28766,
 &apos;check&apos;: 38,
 &apos;for&apos;: 6939,
 &apos;your&apos;: 1279,
 &apos;country&apos;: 423,
 &apos;before&apos;: 1363,
 &apos;downloading&apos;: 5,
 &apos;or&apos;: 5352,
 &apos;redistributing&apos;: 7,
 &apos;this&apos;: 4063,
 &apos;any&apos;: 1204,
 &apos;other&apos;: 1502,
 &apos;header&apos;: 7,
 &apos;should&apos;: 1297,
 &apos;first&apos;: 1177,
 &apos;thing&apos;: 303,
 &apos;seen&apos;: 444,
 &apos;when&apos;: 2923,
 &apos;viewing&apos;: 7,
 &apos;file&apos;: 21,
 &apos;please&apos;: 172,
 &apos;do&apos;: 1503,
 &apos;not&apos;: 6625,
 &apos;remove&apos;: 53,
 &apos;it&apos;: 10681,
 &apos;change&apos;: 150,
 &apos;edit&apos;: 4,
 &apos;without&apos;: 1015,
 &apos;written&apos;: 117,
 &apos;permission&apos;: 52,
 &apos;read&apos;: 218,
 &apos;legal&apos;: 52,
 &apos;small&apos;: 527,
 &apos;print&apos;: 47,
 &apos;and&apos;: 38312,
 &apos;information&apos;: 73,
 &apos;about&apos;: 1497,
 &apos;at&apos;: 6791,
 &apos;bottom&apos;: 42,
 &apos;included&apos;: 43,
 &apos;is&apos;: 9774,
 &apos;important&apos;: 285,
 &apos;specific&apos;: 37,
 &apos;rights&apos;: 168,
 &apos;restrictions&apos;: 23,
 &apos;how&apos;: 1315,
 &apos;may&apos;: 2551,
 &apos;used&apos;: 276,
 &apos;you&apos;: 5622,
 &apos;can&apos;: 1095,
 &apos;also&apos;: 778,
 &apos;find&apos;: 294,
 &apos;out&apos;: 1987,
 &apos;make&apos;: 504,
 &apos;a&apos;: 21155,
 &apos;donation&apos;: 10,
 &apos;get&apos;: 468,
 &apos;involved&apos;: 107,
 &apos;welcome&apos;: 18,
 &apos;free&apos;: 421,
 &apos;plain&apos;: 108,
 &apos;vanilla&apos;: 6,
 &apos;electronic&apos;: 58,
 &apos;texts&apos;: 7,
 &apos;ebooks&apos;: 54,
 &apos;readable&apos;: 13,
 &apos;both&apos;: 529,
 &apos;humans&apos;: 2,
 &apos;computers&apos;: 7,
 &apos;since&apos;: 260,
 &apos;these&apos;: 1231,
 &apos;were&apos;: 4289,
 &apos;prepared&apos;: 138,
 &apos;thousands&apos;: 93,
 &apos;volunteers&apos;: 22,
 &apos;title&apos;: 39,
 &apos;author&apos;: 29,
 &apos;release&apos;: 28,
 &apos;date&apos;: 48,
 &apos;march&apos;: 135,
 &apos;most&apos;: 908,
 &apos;recently&apos;: 30,
 &apos;updated&apos;: 4,
 &apos;november&apos;: 41,
 &apos;edition&apos;: 21,
 &apos;language&apos;: 61,
 &apos;english&apos;: 211,
 &apos;character&apos;: 174,
 &apos;set&apos;: 324,
 &apos;encoding&apos;: 5,
 &apos;ascii&apos;: 11,
 &apos;start&apos;: 67,
 &apos;additional&apos;: 30,
 &apos;editing&apos;: 6,
 &apos;jose&apos;: 1,
 &apos;menendez&apos;: 1,
 &apos;contents&apos;: 50,
 &apos;i&apos;: 7682,
 &apos;scandal&apos;: 19,
 &apos;bohemia&apos;: 15,
 &apos;ii&apos;: 77,
 &apos;red&apos;: 288,
 &apos;headed&apos;: 37,
 &apos;league&apos;: 53,
 &apos;iii&apos;: 91,
 &apos;case&apos;: 438,
 &apos;identity&apos;: 11,
 &apos;iv&apos;: 55,
 &apos;boscombe&apos;: 16,
 &apos;valley&apos;: 78,
 &apos;mystery&apos;: 39,
 &apos;v&apos;: 51,
 &apos;five&apos;: 279,
 &apos;orange&apos;: 23,
 &apos;pips&apos;: 12,
 &apos;vi&apos;: 37,
 &apos;man&apos;: 1652,
 &apos;with&apos;: 9740,
 &apos;twisted&apos;: 21,
 &apos;lip&apos;: 56,
 &apos;vii&apos;: 34,
 &apos;adventure&apos;: 34,
 &apos;blue&apos;: 143,
 &apos;carbuncle&apos;: 17,
 &apos;viii&apos;: 39,
 &apos;speckled&apos;: 5,
 &apos;band&apos;: 54,
 &apos;ix&apos;: 28,
 &apos;engineer&apos;: 12,
 &apos;s&apos;: 5631,
 &apos;thumb&apos;: 51,
 &apos;x&apos;: 136,
 &apos;noble&apos;: 48,
 &apos;bachelor&apos;: 18,
 &apos;xi&apos;: 28,
 &apos;beryl&apos;: 4,
 &apos;coronet&apos;: 29,
 &apos;xii&apos;: 28,
 &apos;copper&apos;: 26,
 &apos;beeches&apos;: 12,
 &apos;she&apos;: 3946,
 &apos;always&apos;: 608,
 &apos;woman&apos;: 325,
 &apos;have&apos;: 3493,
 &apos;seldom&apos;: 76,
 &apos;heard&apos;: 636,
 &apos;him&apos;: 5230,
 &apos;mention&apos;: 46,
 &apos;her&apos;: 5284,
 &apos;under&apos;: 963,
 &apos;name&apos;: 262,
 &apos;his&apos;: 10034,
 &apos;eyes&apos;: 939,
 &apos;eclipses&apos;: 2,
 &apos;predominates&apos;: 3,
 &apos;whole&apos;: 744,
 &apos;sex&apos;: 11,
 &apos;was&apos;: 11410,
 &apos;that&apos;: 12512,
 &apos;he&apos;: 12401,
 &apos;felt&apos;: 697,
 &apos;emotion&apos;: 36,
 &apos;akin&apos;: 14,
 &apos;love&apos;: 484,
 &apos;irene&apos;: 18,
 &apos;adler&apos;: 16,
 &apos;emotions&apos;: 10,
 &apos;one&apos;: 3371,
 &apos;particularly&apos;: 174,
 &apos;abhorrent&apos;: 1,
 &apos;cold&apos;: 257,
 &apos;precise&apos;: 13,
 &apos;but&apos;: 5653,
 &apos;admirably&apos;: 7,
 &apos;balanced&apos;: 6,
 &apos;mind&apos;: 341,
 &apos;take&apos;: 616,
 &apos;perfect&apos;: 39,
 &apos;reasoning&apos;: 41,
 &apos;observing&apos;: 21,
 &apos;machine&apos;: 39,
 &apos;has&apos;: 1603,
 &apos;as&apos;: 8064,
 &apos;lover&apos;: 26,
 &apos;would&apos;: 1953,
 &apos;placed&apos;: 182,
 &apos;himself&apos;: 1158,
 &apos;false&apos;: 64,
 &apos;position&apos;: 432,
 &apos;never&apos;: 593,
 &apos;spoke&apos;: 218,
 &apos;softer&apos;: 10,
 &apos;passions&apos;: 29,
 &apos;save&apos;: 110,
 &apos;gibe&apos;: 2,
 &apos;sneer&apos;: 6,
 &apos;they&apos;: 3938,
 &apos;admirable&apos;: 14,
 &apos;things&apos;: 321,
 &apos;observer&apos;: 13,
 &apos;excellent&apos;: 62,
 &apos;drawing&apos;: 240,
 &apos;veil&apos;: 16,
 &apos;from&apos;: 5709,
 &apos;men&apos;: 1145,
 &apos;motives&apos;: 14,
 &apos;actions&apos;: 77,
 &apos;trained&apos;: 23,
 &apos;reasoner&apos;: 6,
 &apos;admit&apos;: 65,
 &apos;such&apos;: 1436,
 &apos;intrusions&apos;: 1,
 &apos;into&apos;: 2124,
 &apos;own&apos;: 785,
 &apos;delicate&apos;: 54,
 &apos;finely&apos;: 11,
 &apos;adjusted&apos;: 16,
 &apos;temperament&apos;: 5,
 &apos;introduce&apos;: 23,
 &apos;distracting&apos;: 1,
 &apos;factor&apos;: 41,
 &apos;which&apos;: 4842,
 &apos;might&apos;: 536,
 &apos;throw&apos;: 48,
 &apos;doubt&apos;: 152,
 &apos;upon&apos;: 1111,
 &apos;mental&apos;: 37,
 &apos;results&apos;: 229,
 &apos;grit&apos;: 1,
 &apos;sensitive&apos;: 35,
 &apos;instrument&apos;: 35,
 &apos;crack&apos;: 20,
 &apos;high&apos;: 290,
 &apos;power&apos;: 548,
 &apos;lenses&apos;: 1,
 &apos;more&apos;: 1997,
 &apos;disturbing&apos;: 9,
 &apos;than&apos;: 1206,
 &apos;strong&apos;: 168,
 &apos;nature&apos;: 170,
 &apos;yet&apos;: 488,
 &apos;there&apos;: 2972,
 &apos;late&apos;: 165,
 &apos;dubious&apos;: 1,
 &apos;questionable&apos;: 3,
 &apos;memory&apos;: 55,
 &apos;had&apos;: 7383,
 &apos;little&apos;: 1001,
 &apos;lately&apos;: 22,
 &apos;my&apos;: 2249,
 &apos;marriage&apos;: 96,
 &apos;drifted&apos;: 5,
 &apos;us&apos;: 684,
 &apos;away&apos;: 838,
 &apos;each&apos;: 411,
 &apos;complete&apos;: 145,
 &apos;happiness&apos;: 143,
 &apos;home&apos;: 295,
 &apos;centred&apos;: 2,
 &apos;interests&apos;: 118,
 &apos;rise&apos;: 240,
 &apos;up&apos;: 2284,
 &apos;around&apos;: 271,
 &apos;who&apos;: 3050,
 &apos;finds&apos;: 23,
 &apos;master&apos;: 141,
 &apos;establishment&apos;: 40,
 &apos;sufficient&apos;: 75,
 &apos;absorb&apos;: 4,
 &apos;attention&apos;: 191,
 &apos;while&apos;: 768,
 &apos;loathed&apos;: 1,
 &apos;every&apos;: 650,
 &apos;form&apos;: 507,
 &apos;society&apos;: 169,
 &apos;bohemian&apos;: 8,
 &apos;soul&apos;: 168,
 &apos;remained&apos;: 231,
 &apos;lodgings&apos;: 11,
 &apos;baker&apos;: 49,
 &apos;street&apos;: 180,
 &apos;buried&apos;: 21,
 &apos;among&apos;: 451,
 &apos;old&apos;: 1180,
 &apos;books&apos;: 59,
 &apos;alternating&apos;: 2,
 &apos;week&apos;: 95,
 &apos;between&apos;: 654,
 &apos;cocaine&apos;: 4,
 &apos;ambition&apos;: 13,
 &apos;drowsiness&apos;: 4,
 &apos;drug&apos;: 21,
 &apos;fierce&apos;: 12,
 &apos;energy&apos;: 45,
 &apos;keen&apos;: 32,
 &apos;still&apos;: 922,
 &apos;ever&apos;: 274,
 &apos;deeply&apos;: 77,
 &apos;attracted&apos;: 36,
 &apos;study&apos;: 144,
 &apos;crime&apos;: 61,
 &apos;occupied&apos;: 116,
 &apos;immense&apos;: 77,
 &apos;faculties&apos;: 8,
 &apos;extraordinary&apos;: 74,
 &apos;powers&apos;: 149,
 &apos;observation&apos;: 39,
 &apos;following&apos;: 208,
 &apos;those&apos;: 1201,
 &apos;clues&apos;: 3,
 &apos;clearing&apos;: 29,
 &apos;mysteries&apos;: 9,
 &apos;been&apos;: 2599,
 &apos;abandoned&apos;: 72,
 &apos;hopeless&apos;: 17,
 &apos;official&apos;: 91,
 &apos;police&apos;: 94,
 &apos;time&apos;: 1529,
 &apos;some&apos;: 1536,
 &apos;vague&apos;: 39,
 &apos;account&apos;: 177,
 &apos;doings&apos;: 11,
 &apos;summons&apos;: 11,
 &apos;odessa&apos;: 3,
 &apos;trepoff&apos;: 1,
 &apos;murder&apos;: 30,
 &apos;singular&apos;: 36,
 &apos;tragedy&apos;: 9,
 &apos;atkinson&apos;: 1,
 &apos;brothers&apos;: 50,
 &apos;trincomalee&apos;: 1,
 &apos;finally&apos;: 156,
 &apos;mission&apos;: 34,
 &apos;accomplished&apos;: 39,
 &apos;so&apos;: 3017,
 &apos;delicately&apos;: 3,
 &apos;successfully&apos;: 25,
 &apos;reigning&apos;: 3,
 &apos;family&apos;: 210,
 &apos;holland&apos;: 12,
 &apos;beyond&apos;: 225,
 &apos;signs&apos;: 98,
 &apos;activity&apos;: 131,
 &apos;however&apos;: 430,
 &apos;merely&apos;: 189,
 &apos;shared&apos;: 25,
 &apos;readers&apos;: 11,
 &apos;daily&apos;: 44,
 &apos;press&apos;: 81,
 &apos;knew&apos;: 496,
 &apos;former&apos;: 177,
 &apos;friend&apos;: 283,
 &apos;companion&apos;: 81,
 &apos;night&apos;: 385,
 &apos;on&apos;: 6643,
 &apos;twentieth&apos;: 19,
 &apos;returning&apos;: 68,
 &apos;journey&apos;: 69,
 &apos;patient&apos;: 383,
 &apos;now&apos;: 1697,
 &apos;returned&apos;: 194,
 &apos;civil&apos;: 177,
 &apos;practice&apos;: 95,
 &apos;way&apos;: 859,
 &apos;led&apos;: 196,
 &apos;me&apos;: 1920,
 &apos;through&apos;: 815,
 &apos;passed&apos;: 367,
 &apos;well&apos;: 1198,
 &apos;remembered&apos;: 120,
 &apos;door&apos;: 498,
 &apos;must&apos;: 955,
 &apos;associated&apos;: 196,
 &apos;wooing&apos;: 2,
 &apos;dark&apos;: 181,
 &apos;incidents&apos;: 14,
 &apos;scarlet&apos;: 22,
 &apos;seized&apos;: 114,
 &apos;desire&apos;: 96,
 &apos;see&apos;: 1101,
 &apos;again&apos;: 866,
 &apos;know&apos;: 1048,
 &apos;employing&apos;: 7,
 &apos;rooms&apos;: 86,
 &apos;brilliantly&apos;: 5,
 &apos;lit&apos;: 74,
 &apos;even&apos;: 946,
 &apos;looked&apos;: 760,
 &apos;saw&apos;: 599,
 &apos;tall&apos;: 74,
 &apos;spare&apos;: 27,
 &apos;figure&apos;: 103,
 &apos;pass&apos;: 154,
 &apos;twice&apos;: 84,
 &apos;silhouette&apos;: 1,
 &apos;against&apos;: 660,
 &apos;blind&apos;: 23,
 &apos;pacing&apos;: 26,
 &apos;room&apos;: 960,
 &apos;swiftly&apos;: 38,
 &apos;eagerly&apos;: 39,
 &apos;head&apos;: 725,
 &apos;sunk&apos;: 27,
 &apos;chest&apos;: 81,
 &apos;hands&apos;: 455,
 &apos;clasped&apos;: 11,
 &apos;behind&apos;: 401,
 &apos;mood&apos;: 51,
 &apos;habit&apos;: 55,
 &apos;attitude&apos;: 72,
 &apos;manner&apos;: 135,
 &apos;told&apos;: 490,
 &apos;their&apos;: 2955,
 &apos;story&apos;: 133,
 &apos;work&apos;: 382,
 &apos;risen&apos;: 30,
 &apos;created&apos;: 62,
 &apos;dreams&apos;: 16,
 &apos;hot&apos;: 119,
 &apos;scent&apos;: 17,
 &apos;new&apos;: 1211,
 &apos;problem&apos;: 76,
 &apos;rang&apos;: 29,
 &apos;bell&apos;: 65,
 &apos;shown&apos;: 113,
 &apos;chamber&apos;: 35,
 &apos;formerly&apos;: 77,
 &apos;part&apos;: 704,
 &apos;effusive&apos;: 2,
 &apos;glad&apos;: 150,
 &apos;think&apos;: 557,
 &apos;hardly&apos;: 173,
 &apos;word&apos;: 298,
 &apos;spoken&apos;: 92,
 &apos;kindly&apos;: 86,
 &apos;eye&apos;: 110,
 &apos;waved&apos;: 29,
 &apos;an&apos;: 3423,
 &apos;armchair&apos;: 49,
 &apos;threw&apos;: 96,
 &apos;across&apos;: 222,
 &apos;cigars&apos;: 7,
 &apos;indicated&apos;: 88,
 &apos;spirit&apos;: 167,
 &apos;gasogene&apos;: 1,
 &apos;corner&apos;: 128,
 &apos;then&apos;: 1558,
 &apos;stood&apos;: 383,
 &apos;fire&apos;: 274,
 &apos;introspective&apos;: 3,
 &apos;fashion&apos;: 49,
 &apos;wedlock&apos;: 1,
 &apos;suits&apos;: 8,
 &apos;remarked&apos;: 169,
 &apos;watson&apos;: 83,
 &apos;put&apos;: 435,
 &apos;seven&apos;: 132,
 &apos;half&apos;: 318,
 &apos;pounds&apos;: 26,
 &apos;answered&apos;: 226,
 &apos;indeed&apos;: 139,
 &apos;thought&apos;: 902,
 &apos;just&apos;: 767,
 &apos;trifle&apos;: 11,
 &apos;fancy&apos;: 50,
 &apos;observe&apos;: 37,
 &apos;did&apos;: 1875,
 &apos;tell&apos;: 492,
 &apos;intended&apos;: 58,
 &apos;go&apos;: 905,
 &apos;harness&apos;: 27,
 &apos;deduce&apos;: 14,
 &apos;getting&apos;: 92,
 &apos;yourself&apos;: 162,
 &apos;very&apos;: 1340,
 &apos;wet&apos;: 60,
 &apos;clumsy&apos;: 8,
 &apos;careless&apos;: 14,
 &apos;servant&apos;: 46,
 &apos;girl&apos;: 166,
 &apos;dear&apos;: 449,
 &apos;said&apos;: 3464,
 &apos;too&apos;: 548,
 &apos;much&apos;: 671,
 &apos;certainly&apos;: 119,
 &apos;burned&apos;: 77,
 &apos;lived&apos;: 113,
 &apos;few&apos;: 458,
 &apos;centuries&apos;: 12,
 &apos;ago&apos;: 108,
 &apos;true&apos;: 205,
 &apos;walk&apos;: 75,
 &apos;thursday&apos;: 7,
 &apos;came&apos;: 979,
 &apos;dreadful&apos;: 68,
 &apos;mess&apos;: 10,
 &apos;changed&apos;: 134,
 &apos;clothes&apos;: 62,
 &apos;t&apos;: 1318,
 &apos;imagine&apos;: 96,
 &apos;mary&apos;: 705,
 &apos;jane&apos;: 2,
 &apos;incorrigible&apos;: 2,
 &apos;wife&apos;: 367,
 &apos;given&apos;: 364,
 &apos;notice&apos;: 98,
 &apos;fail&apos;: 40,
 &apos;chuckled&apos;: 7,
 &apos;rubbed&apos;: 32,
 &apos;long&apos;: 991,
 &apos;nervous&apos;: 54,
 &apos;together&apos;: 260,
 &apos;simplicity&apos;: 30,
 &apos;itself&apos;: 273,
 &apos;inside&apos;: 43,
 &apos;left&apos;: 834,
 &apos;shoe&apos;: 11,
 &apos;where&apos;: 977,
 &apos;firelight&apos;: 2,
 &apos;strikes&apos;: 19,
 &apos;leather&apos;: 35,
 &apos;scored&apos;: 4,
 &apos;six&apos;: 176,
 &apos;almost&apos;: 325,
 &apos;parallel&apos;: 17,
 &apos;cuts&apos;: 5,
 &apos;obviously&apos;: 38,
 &apos;caused&apos;: 102,
 &apos;someone&apos;: 160,
 &apos;carelessly&apos;: 14,
 &apos;scraped&apos;: 21,
 &apos;round&apos;: 556,
 &apos;edges&apos;: 70,
 &apos;sole&apos;: 70,
 &apos;order&apos;: 404,
 &apos;crusted&apos;: 2,
 &apos;mud&apos;: 36,
 &apos;hence&apos;: 32,
 &apos;double&apos;: 49,
 &apos;deduction&apos;: 12,
 &apos;vile&apos;: 16,
 &apos;weather&apos;: 42,
 &apos;malignant&apos;: 88,
 &apos;boot&apos;: 22,
 &apos;slitting&apos;: 2,
 &apos;specimen&apos;: 14,
 &apos;london&apos;: 76,
 &apos;slavey&apos;: 1,
 &apos;if&apos;: 2372,
 &apos;gentleman&apos;: 99,
 &apos;walks&apos;: 10,
 &apos;smelling&apos;: 5,
 &apos;iodoform&apos;: 43,
 &apos;black&apos;: 235,
 &apos;mark&apos;: 38,
 &apos;nitrate&apos;: 7,
 &apos;silver&apos;: 128,
 &apos;right&apos;: 710,
 &apos;forefinger&apos;: 7,
 &apos;bulge&apos;: 2,
 &apos;side&apos;: 511,
 &apos;top&apos;: 42,
 &apos;hat&apos;: 105,
 &apos;show&apos;: 213,
 &apos;secreted&apos;: 2,
 &apos;stethoscope&apos;: 2,
 &apos;dull&apos;: 74,
 &apos;pronounce&apos;: 9,
 &apos;active&apos;: 96,
 &apos;member&apos;: 50,
 &apos;medical&apos;: 22,
 &apos;profession&apos;: 22,
 &apos;could&apos;: 1700,
 &apos;help&apos;: 230,
 &apos;laughing&apos;: 115,
 &apos;ease&apos;: 44,
 &apos;explained&apos;: 60,
 &apos;process&apos;: 219,
 &apos;hear&apos;: 183,
 &apos;give&apos;: 523,
 &apos;reasons&apos;: 64,
 &apos;appears&apos;: 108,
 &apos;ridiculously&apos;: 1,
 &apos;simple&apos;: 139,
 &apos;easily&apos;: 114,
 &apos;myself&apos;: 227,
 &apos;though&apos;: 650,
 &apos;successive&apos;: 17,
 &apos;instance&apos;: 50,
 &apos;am&apos;: 746,
 &apos;baffled&apos;: 8,
 &apos;until&apos;: 325,
 &apos;explain&apos;: 123,
 &apos;believe&apos;: 183,
 &apos;good&apos;: 744,
 &apos;yours&apos;: 46,
 &apos;quite&apos;: 502,
 &apos;lighting&apos;: 16,
 &apos;cigarette&apos;: 6,
 &apos;throwing&apos;: 46,
 &apos;down&apos;: 1128,
 &apos;distinction&apos;: 19,
 &apos;clear&apos;: 233,
 &apos;example&apos;: 286,
 &apos;frequently&apos;: 218,
 &apos;steps&apos;: 188,
 &apos;lead&apos;: 137,
 &apos;hall&apos;: 83,
 &apos;often&apos;: 443,
 &apos;hundreds&apos;: 48,
 &apos;times&apos;: 236,
 &apos;many&apos;: 609,
 &apos;don&apos;: 581,
 &apos;observed&apos;: 131,
 &apos;point&apos;: 223,
 &apos;seventeen&apos;: 10,
 &apos;because&apos;: 630,
 &apos;interested&apos;: 65,
 &apos;problems&apos;: 78,
 &apos;enough&apos;: 175,
 &apos;chronicle&apos;: 7,
 &apos;two&apos;: 1138,
 &apos;trifling&apos;: 12,
 &apos;experiences&apos;: 11,
 &apos;sheet&apos;: 29,
 &apos;thick&apos;: 77,
 &apos;pink&apos;: 27,
 &apos;tinted&apos;: 9,
 &apos;notepaper&apos;: 2,
 &apos;lying&apos;: 118,
 &apos;open&apos;: 325,
 &apos;table&apos;: 296,
 &apos;last&apos;: 565,
 &apos;post&apos;: 117,
 &apos;aloud&apos;: 28,
 &apos;note&apos;: 115,
 &apos;undated&apos;: 1,
 &apos;either&apos;: 293,
 &apos;signature&apos;: 9,
 &apos;address&apos;: 76,
 &apos;will&apos;: 1577,
 &apos;call&apos;: 197,
 &apos;quarter&apos;: 46,
 &apos;eight&apos;: 128,
 &apos;o&apos;: 257,
 &apos;clock&apos;: 120,
 &apos;desires&apos;: 22,
 &apos;consult&apos;: 19,
 &apos;matter&apos;: 365,
 &apos;deepest&apos;: 15,
 &apos;moment&apos;: 487,
 &apos;recent&apos;: 54,
 &apos;services&apos;: 38,
 &apos;royal&apos;: 111,
 &apos;houses&apos;: 117,
 &apos;europe&apos;: 153,
 &apos;safely&apos;: 11,
 &apos;trusted&apos;: 16,
 &apos;matters&apos;: 136,
 &apos;importance&apos;: 117,
 &apos;exaggerated&apos;: 28,
 &apos;we&apos;: 1906,
 &apos;quarters&apos;: 72,
 &apos;received&apos;: 280,
 &apos;hour&apos;: 157,
 &apos;amiss&apos;: 6,
 &apos;visitor&apos;: 74,
 &apos;wear&apos;: 30,
 &apos;mask&apos;: 12,
 &apos;what&apos;: 3011,
 &apos;means&apos;: 253,
 &apos;no&apos;: 2348,
 &apos;data&apos;: 17,
 &apos;capital&apos;: 144,
 &apos;mistake&apos;: 39,
 &apos;theorise&apos;: 1,
 &apos;insensibly&apos;: 2,
 &apos;begins&apos;: 47,
 &apos;twist&apos;: 14,
 &apos;facts&apos;: 72,
 &apos;suit&apos;: 25,
 &apos;theories&apos;: 21,
 &apos;instead&apos;: 137,
 &apos;carefully&apos;: 72,
 &apos;examined&apos;: 49,
 &apos;writing&apos;: 69,
 &apos;paper&apos;: 177,
 &apos;wrote&apos;: 149,
 &apos;presumably&apos;: 8,
 &apos;endeavouring&apos;: 8,
 &apos;imitate&apos;: 7,
 &apos;processes&apos;: 35,
 &apos;bought&apos;: 55,
 &apos;crown&apos;: 61,
 &apos;packet&apos;: 11,
 &apos;peculiarly&apos;: 14,
 &apos;stiff&apos;: 20,
 &apos;peculiar&apos;: 84,
 &apos;hold&apos;: 114,
 &apos;light&apos;: 278,
 &apos;large&apos;: 483,
 &apos;e&apos;: 136,
 &apos;g&apos;: 55,
 &apos;p&apos;: 66,
 &apos;woven&apos;: 5,
 &apos;texture&apos;: 6,
 &apos;asked&apos;: 777,
 &apos;maker&apos;: 4,
 &apos;monogram&apos;: 4,
 &apos;rather&apos;: 219,
 &apos;stands&apos;: 19,
 &apos;gesellschaft&apos;: 1,
 &apos;german&apos;: 196,
 &apos;company&apos;: 192,
 &apos;customary&apos;: 19,
 &apos;contraction&apos;: 61,
 &apos;like&apos;: 1080,
 &apos;co&apos;: 30,
 &apos;course&apos;: 389,
 &apos;papier&apos;: 1,
 &apos;eg&apos;: 1,
 &apos;let&apos;: 506,
 &apos;glance&apos;: 91,
 &apos;continental&apos;: 46,
 &apos;gazetteer&apos;: 1,
 &apos;took&apos;: 573,
 &apos;heavy&apos;: 139,
 &apos;brown&apos;: 71,
 &apos;volume&apos;: 30,
 &apos;shelves&apos;: 3,
 &apos;eglow&apos;: 1,
 &apos;eglonitz&apos;: 1,
 &apos;here&apos;: 691,
 &apos;egria&apos;: 1,
 &apos;speaking&apos;: 185,
 &apos;far&apos;: 408,
 &apos;carlsbad&apos;: 1,
 &apos;remarkable&apos;: 77,
 &apos;being&apos;: 918,
 &apos;scene&apos;: 49,
 &apos;death&apos;: 330,
 &apos;wallenstein&apos;: 1,
 &apos;its&apos;: 1635,
 &apos;numerous&apos;: 50,
 &apos;glass&apos;: 116,
 &apos;factories&apos;: 29,
 &apos;mills&apos;: 39,
 &apos;ha&apos;: 75,
 &apos;boy&apos;: 169,
 &apos;sparkled&apos;: 5,
 &apos;sent&apos;: 319,
 &apos;great&apos;: 792,
 &apos;triumphant&apos;: 16,
 &apos;cloud&apos;: 30,
 &apos;made&apos;: 1007,
 &apos;precisely&apos;: 24,
 &apos;construction&apos;: 25,
 &apos;sentence&apos;: 26,
 &apos;frenchman&apos;: 102,
 &apos;russian&apos;: 461,
 &apos;uncourteous&apos;: 1,
 &apos;verbs&apos;: 1,
 &apos;only&apos;: 1873,
 &apos;remains&apos;: 73,
 &apos;therefore&apos;: 186,
 &apos;discover&apos;: 28,
 &apos;wanted&apos;: 213,
 &apos;writes&apos;: 20,
 &apos;prefers&apos;: 2,
 &apos;wearing&apos;: 87,
 &apos;showing&apos;: 104,
 &apos;face&apos;: 1125,
 &apos;comes&apos;: 91,
 &apos;mistaken&apos;: 59,
 &apos;resolve&apos;: 14,
 &apos;doubts&apos;: 39,
 &apos;sharp&apos;: 83,
 &apos;sound&apos;: 219,
 &apos;horses&apos;: 262,
 &apos;hoofs&apos;: 24,
 &apos;grating&apos;: 10,
 &apos;wheels&apos;: 47,
 &apos;curb&apos;: 4,
 &apos;followed&apos;: 329,
 &apos;pull&apos;: 23,
 &apos;whistled&apos;: 13,
 &apos;pair&apos;: 40,
 &apos;yes&apos;: 688,
 &apos;continued&apos;: 291,
 &apos;glancing&apos;: 98,
 &apos;window&apos;: 186,
 &apos;nice&apos;: 53,
 &apos;brougham&apos;: 4,
 &apos;beauties&apos;: 2,
 &apos;hundred&apos;: 229,
 &apos;fifty&apos;: 94,
 &apos;guineas&apos;: 3,
 &apos;apiece&apos;: 7,
 &apos;money&apos;: 326,
 &apos;nothing&apos;: 646,
 &apos;else&apos;: 201,
 &apos;better&apos;: 266,
 &apos;bit&apos;: 63,
 &apos;doctor&apos;: 183,
 &apos;stay&apos;: 74,
 &apos;lost&apos;: 224,
 &apos;boswell&apos;: 1,
 &apos;promises&apos;: 15,
 &apos;interesting&apos;: 71,
 &apos;pity&apos;: 75,
 &apos;miss&apos;: 112,
 &apos;client&apos;: 33,
 &apos;want&apos;: 323,
 &apos;sit&apos;: 89,
 &apos;best&apos;: 268,
 &apos;slow&apos;: 65,
 &apos;step&apos;: 139,
 &apos;stairs&apos;: 31,
 &apos;passage&apos;: 110,
 &apos;paused&apos;: 79,
 &apos;immediately&apos;: 182,
 &apos;outside&apos;: 110,
 &apos;loud&apos;: 64,
 &apos;authoritative&apos;: 2,
 &apos;tap&apos;: 10,
 &apos;come&apos;: 934,
 &apos;entered&apos;: 282,
 &apos;less&apos;: 367,
 &apos;feet&apos;: 179,
 &apos;inches&apos;: 16,
 &apos;height&apos;: 36,
 &apos;limbs&apos;: 67,
 &apos;hercules&apos;: 4,
 &apos;dress&apos;: 138,
 &apos;rich&apos;: 92,
 &apos;richness&apos;: 2,
 &apos;england&apos;: 311,
 &apos;bad&apos;: 155,
 &apos;taste&apos;: 23,
 &apos;bands&apos;: 27,
 &apos;astrakhan&apos;: 1,
 &apos;slashed&apos;: 3,
 &apos;sleeves&apos;: 30,
 &apos;fronts&apos;: 1,
 &apos;breasted&apos;: 1,
 &apos;coat&apos;: 172,
 &apos;deep&apos;: 215,
 &apos;cloak&apos;: 62,
 &apos;thrown&apos;: 92,
 &apos;shoulders&apos;: 125,
 &apos;lined&apos;: 32,
 &apos;flame&apos;: 15,
 &apos;coloured&apos;: 21,
 &apos;silk&apos;: 50,
 &apos;secured&apos;: 48,
 &apos;neck&apos;: 203,
 &apos;brooch&apos;: 1,
 &apos;consisted&apos;: 38,
 &apos;single&apos;: 173,
 &apos;flaming&apos;: 8,
 &apos;boots&apos;: 91,
 &apos;extended&apos;: 75,
 &apos;halfway&apos;: 19,
 &apos;calves&apos;: 3,
 &apos;trimmed&apos;: 8,
 &apos;tops&apos;: 3,
 &apos;fur&apos;: 38,
 &apos;completed&apos;: 25,
 &apos;impression&apos;: 67,
 &apos;barbaric&apos;: 2,
 &apos;opulence&apos;: 3,
 &apos;suggested&apos;: 69,
 &apos;appearance&apos;: 135,
 &apos;carried&apos;: 282,
 &apos;broad&apos;: 92,
 &apos;brimmed&apos;: 4,
 &apos;hand&apos;: 834,
 &apos;wore&apos;: 58,
 &apos;upper&apos;: 130,
 &apos;extending&apos;: 35,
 &apos;past&apos;: 223,
 &apos;cheekbones&apos;: 4,
 &apos;vizard&apos;: 1,
 &apos;apparently&apos;: 68,
 &apos;raised&apos;: 212,
 &apos;lower&apos;: 196,
 &apos;appeared&apos;: 197,
 &apos;hanging&apos;: 42,
 &apos;straight&apos;: 124,
 &apos;chin&apos;: 30,
 &apos;suggestive&apos;: 11,
 &apos;resolution&apos;: 57,
 &apos;pushed&apos;: 81,
 &apos;length&apos;: 63,
 &apos;obstinacy&apos;: 7,
 &apos;harsh&apos;: 22,
 &apos;voice&apos;: 462,
 &apos;strongly&apos;: 41,
 &apos;marked&apos;: 138,
 &apos;accent&apos;: 18,
 &apos;uncertain&apos;: 30,
 &apos;pray&apos;: 79,
 &apos;seat&apos;: 170,
 &apos;colleague&apos;: 7,
 &apos;dr&apos;: 48,
 &apos;occasionally&apos;: 89,
 &apos;cases&apos;: 453,
 &apos;whom&apos;: 489,
 &apos;honour&apos;: 16,
 &apos;count&apos;: 748,
 &apos;von&apos;: 11,
 &apos;kramm&apos;: 2,
 &apos;nobleman&apos;: 11,
 &apos;understand&apos;: 412,
 &apos;discretion&apos;: 13,
 &apos;trust&apos;: 68,
 &apos;extreme&apos;: 72,
 &apos;prefer&apos;: 21,
 &apos;communicate&apos;: 15,
 &apos;alone&apos;: 337,
 &apos;rose&apos;: 243,
 &apos;caught&apos;: 90,
 &apos;wrist&apos;: 68,
 &apos;back&apos;: 746,
 &apos;chair&apos;: 135,
 &apos;none&apos;: 110,
 &apos;say&apos;: 755,
 &apos;anything&apos;: 379,
 &apos;shrugged&apos;: 35,
 &apos;begin&apos;: 97,
 &apos;binding&apos;: 18,
 &apos;absolute&apos;: 56,
 &apos;secrecy&apos;: 18,
 &apos;years&apos;: 571,
 &apos;end&apos;: 465,
 &apos;present&apos;: 329,
 &apos;weight&apos;: 70,
 &apos;influence&apos;: 138,
 &apos;european&apos;: 99,
 &apos;history&apos;: 439,
 &apos;promise&apos;: 67,
 &apos;excuse&apos;: 53,
 &apos;strange&apos;: 220,
 &apos;august&apos;: 70,
 &apos;person&apos;: 185,
 &apos;employs&apos;: 2,
 &apos;wishes&apos;: 42,
 &apos;agent&apos;: 25,
 &apos;unknown&apos;: 87,
 &apos;confess&apos;: 36,
 &apos;once&apos;: 569,
 &apos;called&apos;: 450,
 &apos;exactly&apos;: 47,
 &apos;aware&apos;: 52,
 &apos;dryly&apos;: 5,
 &apos;circumstances&apos;: 107,
 &apos;delicacy&apos;: 11,
 &apos;precaution&apos;: 9,
 &apos;taken&apos;: 438,
 &apos;quench&apos;: 3,
 &apos;grow&apos;: 74,
 &apos;seriously&apos;: 63,
 &apos;compromise&apos;: 71,
 &apos;families&apos;: 45,
 &apos;speak&apos;: 255,
 &apos;plainly&apos;: 39,
 &apos;implicates&apos;: 5,
 &apos;house&apos;: 661,
 &apos;ormstein&apos;: 2,
 &apos;hereditary&apos;: 14,
 &apos;kings&apos;: 27,
 &apos;murmured&apos;: 18,
 &apos;settling&apos;: 16,
 &apos;closing&apos;: 35,
 &apos;glanced&apos;: 176,
 ...}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alphabet = <span class="string">'abcdefghigklmnopqrstuvwxyz'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span><span class="params">(word)</span>:</span></span><br><span class="line">    n = len(word)</span><br><span class="line">    <span class="keyword">return</span> set([word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]+</span><br><span class="line">              [word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>]+word[i]+word[i+<span class="number">2</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>)]+</span><br><span class="line">              [word[<span class="number">0</span>:i]+c+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet] +</span><br><span class="line">              [word[<span class="number">0</span>:i]+c+word[i:] <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> set(e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">e1 = edits1(<span class="string">'hello'</span>)</span><br><span class="line">e2 = edits2(<span class="string">'hello'</span>)</span><br><span class="line">len(e1)+len(e2)</span><br></pre></td></tr></table></figure>




<pre><code>33328</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span><span class="params">(words)</span>:</span></span><br><span class="line">    w = set()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dic_words:</span><br><span class="line">            w.add(word)</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correct</span><span class="params">(word)</span>:</span></span><br><span class="line">    candidates = known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known(edits2(word)) <span class="keyword">or</span> word</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> word == candidates:</span><br><span class="line">        <span class="keyword">return</span> word</span><br><span class="line">    max_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> candidates:</span><br><span class="line">        <span class="keyword">if</span> dic_words[c]&gt;=max_num:</span><br><span class="line">            max_num=dic_words[c]</span><br><span class="line">            candidate = c</span><br><span class="line">    <span class="keyword">return</span> candidate</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'ehell'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;shell&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'hel'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;he&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'heade'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;head&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'haved'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;have&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct(<span class="string">'adevnues'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&apos;avenues&apos;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>è´å¶æ–¯ç®—æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title>é›†æˆå­¦ä¹ bagging</title>
    <url>/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/</url>
    <content><![CDATA[<p>baggingä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>


<p>ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´ä¸å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œè£…è¢‹ï¼ˆbaggingï¼‰<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bagging3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data=iris.data[:,:<span class="number">2</span>]</span><br><span class="line">y_data=iris.target</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = neighbors.KNeighborsClassifier()</span><br><span class="line">knn.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="comment"># è·å–æ•°æ®å€¼æ‰€åœ¨çš„èŒƒå›´</span></span><br><span class="line">    x_min, x_max = x_data[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = x_data[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ç”Ÿæˆç½‘æ ¼çŸ©é˜µ</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                         np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line">    </span><br><span class="line">    z=model.predict(np.c_[xx.ravel(),yy.ravel()])</span><br><span class="line">    z = z.reshape(xx.shape)</span><br><span class="line">    cs = plt.contourf(xx,yy,z)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot(knn)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">knn.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_5_0.png" alt="png"></p>
<pre><code>0.631578947368421</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dtree = tree.DecisionTreeClassifier()</span><br><span class="line">dtree.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&apos;gini&apos;,
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=&apos;deprecated&apos;,
                       random_state=None, splitter=&apos;best&apos;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot(dtree)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">dtree.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_7_0.png" alt="png"></p>
<pre><code>0.6052631578947368</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bagging_knn = BaggingClassifier(knn,n_estimators=<span class="number">100</span>)</span><br><span class="line">bagging_knn.fit(x_train,y_train)</span><br><span class="line">plot(bagging_knn)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">bagging_knn.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_8_0.png" alt="png"></p>
<pre><code>0.631578947368421</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bagging_tree = BaggingClassifier(dtree,n_estimators=<span class="number">100</span>)</span><br><span class="line">bagging_tree.fit(x_train,y_train)</span><br><span class="line">plot(bagging_tree)</span><br><span class="line">plt.scatter(x_data[:,<span class="number">-0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br><span class="line">bagging_tree.score(x_test,y_test)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0bagging/output_9_0.png" alt="png"></p>
<pre><code>0.6578947368421053</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>é›†æˆå­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>é›†æˆå­¦ä¹ stacking</title>
    <url>/2020/07/11/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0stacking/</url>
    <content><![CDATA[<p>stackingä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>

<p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/stack3.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data,y_data=iris.data[:,<span class="number">1</span>:<span class="number">3</span>],iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = DecisionTreeClassifier()</span><br><span class="line">clf3 = LogisticRegression()</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1,clf2,clf3],meta_classifier=lr)</span><br><span class="line"><span class="keyword">for</span> clf,label <span class="keyword">in</span> zip([clf1,clf2,clf3,sclf],</span><br><span class="line">                    [<span class="string">'KNN'</span>,<span class="string">'Decision Tree'</span>,<span class="string">'LogisticRegression'</span>,<span class="string">'StackingClassifier'</span>]):</span><br><span class="line">    scores = model_selection.cross_val_score(clf,x_data,y_data,cv=<span class="number">3</span>,scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">'Accuracy:%0.2f [%s]'</span> %(scores.mean(),label))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy:0.91 [KNN]
Accuracy:0.93 [Decision Tree]
Accuracy:0.95 [LogisticRegression]
Accuracy:0.93 [StackingClassifier]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>é›†æˆå­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>é›†æˆå­¦ä¹ boosting</title>
    <url>/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/</url>
    <content><![CDATA[<p>boostingä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>


<p>ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œæå‡ï¼ˆboostingï¼‰<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting1.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting3.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting4.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting5.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/boosting7.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_gaussian_quantiles</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1,y1 = make_gaussian_quantiles(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,n_classes=<span class="number">2</span>)</span><br><span class="line">x2,y2 = make_gaussian_quantiles(mean=(<span class="number">3</span>,<span class="number">3</span>),n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,n_classes=<span class="number">2</span>)</span><br><span class="line">x_data = np.concatenate((x1,x2))</span><br><span class="line">y_data = np.concatenate((y1,<span class="number">1</span>-y2))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x_data[:,<span class="number">-0</span>],x_data[:,<span class="number">1</span>],c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_3_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tree.DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line">model.fit(x_data,y_data)</span><br><span class="line">x_min,x_max = x_data[:,<span class="number">0</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">0</span>].max()+<span class="number">1</span></span><br><span class="line">y_min,y_max = x_data[:,<span class="number">1</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">1</span>].max()+<span class="number">1</span></span><br><span class="line"><span class="comment"># ç”Ÿæˆç½‘æ ¼çŸ©é˜µ</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">z = model.predict(np.c_[xx.ravel(), yy.ravel()])<span class="comment"># ravelä¸flattenç±»ä¼¼ï¼Œå¤šç»´æ•°æ®è½¬ä¸€ç»´ã€‚flattenä¸ä¼šæ”¹å˜åŸå§‹æ•°æ®ï¼Œravelä¼šæ”¹å˜åŸå§‹æ•°æ®</span></span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># ç­‰é«˜çº¿å›¾</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># æ ·æœ¬æ•£ç‚¹å›¾</span></span><br><span class="line">plt.scatter(x_data[:, <span class="number">0</span>], x_data[:, <span class="number">1</span>], c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.score(x_data,y_data)</span><br></pre></td></tr></table></figure>




<pre><code>0.705</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=<span class="number">3</span>),n_estimators=<span class="number">10</span>)</span><br><span class="line">model.fit(x_data,y_data)</span><br><span class="line"><span class="comment"># è·å–æ•°æ®å€¼æ‰€åœ¨çš„èŒƒå›´</span></span><br><span class="line">x_min, x_max = x_data[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = x_data[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_data[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç”Ÿæˆç½‘æ ¼çŸ©é˜µ</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># è·å–é¢„æµ‹å€¼</span></span><br><span class="line">z = model.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"><span class="comment"># ç­‰é«˜çº¿å›¾</span></span><br><span class="line">cs = plt.contourf(xx, yy, z)</span><br><span class="line"><span class="comment"># æ ·æœ¬æ•£ç‚¹å›¾</span></span><br><span class="line">plt.scatter(x_data[:, <span class="number">0</span>], x_data[:, <span class="number">1</span>], c=y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0boosting/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.score(x_data,y_data)</span><br></pre></td></tr></table></figure>




<pre><code>0.988</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>é›†æˆå­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>é›†æˆå­¦ä¹ voting</title>
    <url>/2020/07/11/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0voting/</url>
    <content><![CDATA[<p>votingä»£ç ç®€å•æ¼”ç¤º</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_data,y_data = iris.data[:,<span class="number">1</span>:<span class="number">3</span>],iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = DecisionTreeClassifier()</span><br><span class="line">clf3 = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = VotingClassifier([(<span class="string">'knn'</span>,clf1),(<span class="string">'dtree'</span>,clf2),(<span class="string">'lr'</span>,clf3)])</span><br><span class="line"><span class="keyword">for</span> clf,label <span class="keyword">in</span> zip([clf1,clf2,clf3,sclf],</span><br><span class="line">                    [<span class="string">'KNN'</span>,<span class="string">'Decistion Tree'</span>,<span class="string">'LogisticRegression'</span>,<span class="string">'VotingClassifier'</span>]):</span><br><span class="line">    scores = model_selection.cross_val_score(clf,x_data,y_data,cv=<span class="number">3</span>,scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">'Accuracy: %0.2f[%s]'</span> %(scores.mean(),label))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy: 0.91[KNN]
Accuracy: 0.90[Decistion Tree]
Accuracy: 0.91[LogisticRegression]
Accuracy: 0.92[VotingClassifier]


C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\å¼ å¸…\AppData\Roaming\Python\Python36\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>é›†æˆå­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>éçº¿æ€§é€»è¾‘å›å½’</title>
    <url>/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>éçº¿æ€§é€»è¾‘å›å½’ä»£ç æ¼”ç¤º<br>æ•°æ®ä¸‹è½½<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/LR-testSet2.txt" target="_blank" rel="noopener">LR-testSet2.txt</a></p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">scale = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<ul>
<li>np.newaxisçš„åŠŸèƒ½â€”â€”æ’å…¥æ–°ç»´åº¦</li>
</ul>
<hr>
<p>ä¸¾ä¸ªç®€å•çš„ä¾‹å­ä»‹ç»ä¸€ä¸‹å§ã€‚</p>
<ul>
<li><p>æ —å­1ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">print(a.shape)</span><br><span class="line">print (a)</span><br></pre></td></tr></table></figure></li>
<li><p>è¾“å‡ºï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(5,)</span><br><span class="line">[1 2 3 4 5]</span><br></pre></td></tr></table></figure>
</li>
<li><p>æ —å­2ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">aa&#x3D;a[:,np.newaxis]</span><br><span class="line">print(aa.shape)</span><br><span class="line">print (aa)</span><br></pre></td></tr></table></figure>
</li>
<li><p>è¾“å‡ºï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(5, 1)</span><br><span class="line">[[1]</span><br><span class="line">[2]</span><br><span class="line">[3]</span><br><span class="line">[4]</span><br><span class="line">[5]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>æ —å­3ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a&#x3D;np.array([1,2,3,4,5])</span><br><span class="line">aa&#x3D;a[np.newaxis,:]</span><br><span class="line">print(aa.shape)</span><br><span class="line">print (aa)</span><br></pre></td></tr></table></figure></li>
<li><p>è¾“å‡º</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1, 5)</span><br><span class="line">[[1 2 3 4 5]]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<p>çœ‹æ˜ç™½äº†å§ï¼ŒåŸæ¥np.newaxisçš„ä½œç”¨æ˜¯å¢åŠ ä¸€ä¸ªç»´åº¦ã€‚</p>
<p>å¯¹äº[: , np.newaxis] å’Œ [np.newaxisï¼Œï¼š]<br>æ˜¯åœ¨np.newaxisè¿™é‡Œå¢åŠ 1ç»´ã€‚</p>
<p>è¿™æ ·æ”¹å˜ç»´åº¦çš„ä½œç”¨å¾€å¾€æ˜¯å°†ä¸€ç»´çš„æ•°æ®è½¬å˜æˆä¸€ä¸ªçŸ©é˜µ</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.genfromtxt(<span class="string">'./data/LR-testSet2.txt'</span>,delimiter=<span class="string">','</span>)</span><br><span class="line">x_data = data[:,:<span class="number">-1</span>] <span class="comment">#shape(118, 2)</span></span><br><span class="line">y_data = data[:,<span class="number">-1</span>,np.newaxis] <span class="comment">#shape(118, 1)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></span><br><span class="line">    x0 = []</span><br><span class="line">    x1 = []</span><br><span class="line">    y0 = []</span><br><span class="line">    y1 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        <span class="keyword">if</span> y_data[i]==<span class="number">0</span>:</span><br><span class="line">            x0.append(x_data[i,<span class="number">0</span>])</span><br><span class="line">            y0.append(x_data[i,<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x1.append(x_data[i,<span class="number">0</span>])</span><br><span class="line">            y1.append(x_data[i,<span class="number">1</span>])</span><br><span class="line">    scatter0 = plt.scatter(x0,y0,c=<span class="string">'b'</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">    scatter1 = plt.scatter(x1,y1,c=<span class="string">'r'</span>,marker=<span class="string">'x'</span>)</span><br><span class="line">    plt.legend(handles=[scatter0,scatter1],labels=[<span class="string">'labell0'</span>,<span class="string">'label1'</span>],loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plot()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/output_2_0.png" alt="png"></p>
<p>ä½¿ç”¨sklearn.preprocessing.PolynomialFeaturesæ¥è¿›è¡Œç‰¹å¾çš„æ„é€ ã€‚</p>
<p>å®ƒæ˜¯ä½¿ç”¨å¤šé¡¹å¼çš„æ–¹æ³•æ¥è¿›è¡Œçš„ï¼Œå¦‚æœæœ‰aï¼Œbä¸¤ä¸ªç‰¹å¾ï¼Œé‚£ä¹ˆå®ƒçš„2æ¬¡å¤šé¡¹å¼ä¸ºï¼ˆ1,a,b,a^2,ab, b^2ï¼‰ï¼Œè¿™ä¸ªå¤šé¡¹å¼çš„å½¢å¼æ˜¯ä½¿ç”¨polyçš„æ•ˆæœã€‚</p>
<p>PolynomialFeaturesæœ‰ä¸‰ä¸ªå‚æ•°</p>
<ul>
<li>degreeï¼šæ§åˆ¶å¤šé¡¹å¼çš„åº¦</li>
<li>interaction_onlyï¼š é»˜è®¤ä¸ºFalseï¼Œå¦‚æœæŒ‡å®šä¸ºTrueï¼Œé‚£ä¹ˆå°±ä¸ä¼šæœ‰ç‰¹å¾è‡ªå·±å’Œè‡ªå·±ç»“åˆçš„é¡¹ï¼Œä¸Šé¢çš„äºŒæ¬¡é¡¹ä¸­æ²¡æœ‰a^2å’Œb^2ã€‚</li>
<li>include_biasï¼šé»˜è®¤ä¸ºTrueã€‚å¦‚æœä¸ºTrueçš„è¯ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰ä¸Šé¢çš„ 1é‚£ä¸€é¡¹ã€‚</li>
</ul>
<p>ä¾‹å­1ï¼Œinteraction_onlyä¸ºé»˜è®¤çš„Falseæ—¶</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;[[5,10]] Â Â  #c&#x3D;[[a,b]],è¿™é‡Œè¦æ³¨æ„açš„shapeï¼Œå¦‚æœæ˜¯listå½¢å¼ï¼Œåˆ™å°†a.shape&#x3D;-1,1</span><br><span class="line">pl&#x3D;PolynomialFeatures()</span><br><span class="line">b&#x3D;pl.fit_transform(c)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<p>è¾“å‡ºï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[Â  1.,Â Â  5.,Â  10.,Â  25.,Â  50., 100.]])Â </span><br><span class="line">#ç¬¦åˆï¼ˆ1,a,b,a^2,ab, b^2ï¼‰</span><br></pre></td></tr></table></figure>

<p>ä¾‹å­2ï¼Œinteraction_only=Trueæ—¶</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;[[5,10]]</span><br><span class="line">pl&#x3D;PolynomialFeatures(interaction_only&#x3D;True)</span><br><span class="line">b&#x3D;pl.fit_transform(c)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<p>è¾“å‡ºï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 1.,Â  5., 10., 50.]]) Â Â </span><br><span class="line">#è¾“å‡ºä¸­ä¸åŒ…å«a^2å’Œb^2é¡¹</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly_reg = PolynomialFeatures(degree=<span class="number">3</span>)</span><br><span class="line">x_poly = poly_reg.fit_transform(x_data)</span><br><span class="line">x_poly</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 1.00000000e+00,  5.12670000e-02,  6.99560000e-01, ...,
         1.83865725e-03,  2.50892595e-02,  3.42353606e-01],
       [ 1.00000000e+00, -9.27420000e-02,  6.84940000e-01, ...,
         5.89122275e-03, -4.35092419e-02,  3.21334672e-01],
       [ 1.00000000e+00, -2.13710000e-01,  6.92250000e-01, ...,
         3.16164171e-02, -1.02411982e-01,  3.31733166e-01],
       ...,
       [ 1.00000000e+00, -4.84450000e-01,  9.99270000e-01, ...,
         2.34520477e-01, -4.83742961e-01,  9.97811598e-01],
       [ 1.00000000e+00, -6.33640000e-03,  9.99270000e-01, ...,
         4.01206555e-05, -6.32715223e-03,  9.97811598e-01],
       [ 1.00000000e+00,  6.32650000e-01, -3.06120000e-02, ...,
        -1.22523312e-02,  5.92852863e-04, -2.86863382e-05]])</code></pre><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%921.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%922.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%923.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%924.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(xMat,yMat,ws)</span>:</span></span><br><span class="line">    left = np.multiply(yMat,np.log(sigmoid(xMat*ws)))</span><br><span class="line">    right = np.multiply(<span class="number">1</span>-yMat,np.log(<span class="number">1</span>-sigmoid(xMat*ws)))</span><br><span class="line">    <span class="keyword">return</span> np.sum(left+right)/-(len(xMat))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scale == <span class="literal">True</span>:</span><br><span class="line">        xArr = preprocessing.scale(xArr)</span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr)</span><br><span class="line">    lr = <span class="number">0.03</span></span><br><span class="line">    epochs = <span class="number">50000</span></span><br><span class="line">    costList = []</span><br><span class="line">    m,n = np.shape(xMat)</span><br><span class="line">    ws = np.mat(np.ones((n,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs+<span class="number">1</span>):</span><br><span class="line">        h = sigmoid(xMat*ws)</span><br><span class="line">        ws_grad = xMat.T*(h-yMat)/m</span><br><span class="line">        ws = ws - lr*ws_grad</span><br><span class="line">        <span class="keyword">if</span> i %<span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            costList.append(cost(xMat,yMat,ws))</span><br><span class="line">    <span class="keyword">return</span> ws,costList</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws, costList = gradAscent(x_poly,y_data)</span><br><span class="line">print(ws)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 4.16787292]
 [ 2.72213524]
 [ 4.55120018]
 [-9.76109006]
 [-5.34880198]
 [-8.51458023]
 [-0.55950401]
 [-1.55418165]
 [-0.75929829]
 [-2.88573877]]</code></pre><p>np.c_æ˜¯æŒ‰è¡Œè¿æ¥ä¸¤ä¸ªçŸ©é˜µï¼Œå°±æ˜¯æŠŠä¸¤çŸ©é˜µå·¦å³ç›¸åŠ ï¼Œè¦æ±‚è¡Œæ•°ç›¸ç­‰ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; np.array([[1, 2, 3],[7,8,9]])</span><br><span class="line"> </span><br><span class="line">b&#x3D;np.array([[4,5,6],[1,2,3]])</span><br><span class="line"> </span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<p>Out[4]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1, 2, 3],</span><br><span class="line">       [7, 8, 9]])</span><br></pre></td></tr></table></figure>

<p>b<br>Out[5]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[4, 5, 6],</span><br><span class="line">       [1, 2, 3]])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c&#x3D;np.c_[a,b]</span><br><span class="line"> </span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<p>Out[7]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1, 2, 3, 4, 5, 6],</span><br><span class="line">       [7, 8, 9, 1, 2, 3]])</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">d&#x3D; np.array([7,8,9])</span><br><span class="line"> </span><br><span class="line">e&#x3D;np.array([1, 2, 3])</span><br><span class="line"> </span><br><span class="line">f&#x3D;np.c_[d,e]</span><br><span class="line"> </span><br><span class="line">f</span><br></pre></td></tr></table></figure>
<p>Out[12]: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[7, 1],</span><br><span class="line">       [8, 2],</span><br><span class="line">       [9, 3]])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_min,x_max = x_data[:,<span class="number">0</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">0</span>].max()+<span class="number">1</span></span><br><span class="line">y_min,y_max = x_data[:,<span class="number">1</span>].min()<span class="number">-1</span>,x_data[:,<span class="number">1</span>].max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">xx,yy = np.meshgrid(np.arange(x_min,y_max,<span class="number">0.02</span>),</span><br><span class="line">                   np.arange(y_min,y_max,<span class="number">0.02</span>))</span><br><span class="line"><span class="comment"># ravalå¤šç»´è½¬ä¸€ç»´</span></span><br><span class="line"><span class="comment"># dotçŸ©é˜µä¹˜ç§¯</span></span><br><span class="line"><span class="comment"># np.c_å·¦å³æ‹¼æ¥ä¸¤ä¸ªçŸ©é˜µ</span></span><br><span class="line">z = sigmoid(poly_reg.fit_transform(np.c_[xx.ravel(),yy.ravel()]).dot(np.array(ws)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">    <span class="keyword">if</span> z[i]&gt;<span class="number">0.5</span>:</span><br><span class="line">        z[i]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        z[i]=<span class="number">0</span></span><br><span class="line">z=z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">cs = plt.contourf(xx,yy,z)</span><br><span class="line">plot()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/09/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/output_9_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x_data,ws)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scale == <span class="literal">True</span>:</span><br><span class="line">        x_data = preprocessing.scale(x_data)</span><br><span class="line">    xMat = np.mat(x_data)</span><br><span class="line">    ws = np.mat(ws)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x&gt;=<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> sigmoid(xMat*ws)] </span><br><span class="line"></span><br><span class="line">predictions = predict(x_poly,ws)</span><br><span class="line">print(classification_report(y_data,predictions))</span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

         0.0       0.86      0.83      0.85        60
         1.0       0.83      0.86      0.85        58

    accuracy                           0.85       118
   macro avg       0.85      0.85      0.85       118
weighted avg       0.85      0.85      0.85       118</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>é€»è¾‘å›å½’</tag>
      </tags>
  </entry>
  <entry>
    <title>opencvä¸­çš„å›¾åƒå¤„ç†4-ç›´æ–¹å›¾</title>
    <url>/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%864-%E7%9B%B4%E6%96%B9%E5%9B%BE/</url>
    <content><![CDATA[<ul>
<li>10.1.<a href="#header1">ç›´æ–¹å›¾-1ï¼šæŸ¥æ‰¾ã€ç»˜åˆ¶å’Œåˆ†æ</a></li>
<li>10.2.<a href="#header2">ç›´æ–¹å›¾-2ï¼šç›´æ–¹å›¾å‡è¡¡</a></li>
<li>10.3.<a href="#header3">ç›´æ–¹å›¾-3ï¼šäºŒç»´ç›´æ–¹å›¾</a></li>
<li>10.4.<a href="#header4">ç›´æ–¹å›¾4ï¼šç›´æ–¹å›¾åæŠ•å½±</a><a id="more"></a>

</li>
</ul>
<h1 id="ç›´æ–¹å›¾-1ï¼šæŸ¥æ‰¾ã€ç»˜åˆ¶å’Œåˆ†æ"><a href="#ç›´æ–¹å›¾-1ï¼šæŸ¥æ‰¾ã€ç»˜åˆ¶å’Œåˆ†æ" class="headerlink" title="ç›´æ–¹å›¾-1ï¼šæŸ¥æ‰¾ã€ç»˜åˆ¶å’Œåˆ†æ"></a><span id="header1">ç›´æ–¹å›¾-1ï¼šæŸ¥æ‰¾ã€ç»˜åˆ¶å’Œåˆ†æ</span></h1><h2 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>å­¦ä¼š </p>
<ul>
<li>ä½¿ç”¨OpenCVå’ŒNumpyå‡½æ•°æŸ¥æ‰¾ç›´æ–¹å›¾ </li>
<li>ä½¿ç”¨OpenCVå’ŒMatplotlibå‡½æ•°ç»˜åˆ¶ç›´æ–¹å›¾ </li>
<li>ä½ å°†çœ‹åˆ°ä»¥ä¸‹å‡½æ•°ï¼šcv.calcHist()ï¼Œnp.histogram()ç­‰ã€‚</li>
</ul>
<h2 id="ç†è®º"><a href="#ç†è®º" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>é‚£ä¹ˆç›´æ–¹å›¾æ˜¯ä»€ä¹ˆï¼Ÿæ‚¨å¯ä»¥å°†ç›´æ–¹å›¾è§†ä¸ºå›¾å½¢æˆ–ç»˜å›¾ï¼Œä»è€Œå¯ä»¥æ€»ä½“äº†è§£å›¾åƒçš„å¼ºåº¦åˆ†å¸ƒã€‚</p>
<p>å®ƒæ˜¯åœ¨Xè½´ä¸Šå…·æœ‰åƒç´ å€¼ï¼ˆä¸æ€»æ˜¯ä»0åˆ°255çš„èŒƒå›´ï¼‰ï¼Œåœ¨Yè½´ä¸Šå…·æœ‰å›¾åƒä¸­ç›¸åº”åƒç´ æ•°çš„å›¾ã€‚</p>
<p>è¿™åªæ˜¯ç†è§£å›¾åƒçš„å¦ä¸€ç§æ–¹å¼ã€‚é€šè¿‡æŸ¥çœ‹å›¾åƒçš„ç›´æ–¹å›¾ï¼Œæ‚¨å¯ä»¥ç›´è§‚åœ°äº†è§£è¯¥å›¾åƒçš„å¯¹æ¯”åº¦ï¼Œäº®åº¦ï¼Œå¼ºåº¦åˆ†å¸ƒç­‰ã€‚</p>
<p>å½“ä»Šå‡ ä¹æ‰€æœ‰å›¾åƒå¤„ç†å·¥å…·éƒ½æä¾›ç›´æ–¹å›¾åŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯å‰‘æ¡¥å½©è‰²ç½‘ç«™çš„å›¾ç‰‡ï¼Œæˆ‘å»ºè®®æ‚¨è®¿é—®è¯¥ç½‘ç«™ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚</p>
<p><img src="http://qiniu.aihubs.net/histogram_sample.jpg" alt></p>
<p>æ‚¨å¯ä»¥çœ‹åˆ°å›¾åƒåŠå…¶ç›´æ–¹å›¾ã€‚ï¼ˆè¯·è®°ä½ï¼Œæ­¤ç›´æ–¹å›¾æ˜¯é’ˆå¯¹ç°åº¦å›¾åƒè€Œéå½©è‰²å›¾åƒç»˜åˆ¶çš„ï¼‰ã€‚</p>
<p>ç›´æ–¹å›¾çš„å·¦ä¾§åŒºåŸŸæ˜¾ç¤ºå›¾åƒä¸­è¾ƒæš—åƒç´ çš„æ•°é‡ï¼Œè€Œå³ä¾§åŒºåŸŸåˆ™æ˜¾ç¤ºæ˜äº®åƒç´ çš„æ•°é‡ã€‚</p>
<p>ä»ç›´æ–¹å›¾ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æš—åŒºåŸŸå¤šäºäº®åŒºåŸŸï¼Œè€Œä¸­é—´è°ƒçš„æ•°é‡ï¼ˆä¸­é—´å€¼çš„åƒç´ å€¼ï¼Œä¾‹å¦‚127é™„è¿‘ï¼‰åˆ™éå¸¸å°‘ã€‚</p>
<h2 id="å¯»æ‰¾ç›´æ–¹å›¾"><a href="#å¯»æ‰¾ç›´æ–¹å›¾" class="headerlink" title="å¯»æ‰¾ç›´æ–¹å›¾"></a>å¯»æ‰¾ç›´æ–¹å›¾</h2><p>ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå…³äºç›´æ–¹å›¾çš„æƒ³æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç ”ç©¶å¦‚ä½•æ‰¾åˆ°å®ƒã€‚OpenCVå’ŒNumpyéƒ½ä¸ºæ­¤å†…ç½®äº†åŠŸèƒ½ã€‚</p>
<p>åœ¨ä½¿ç”¨è¿™äº›åŠŸèƒ½ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›ä¸ç›´æ–¹å›¾æœ‰å…³çš„æœ¯è¯­ã€‚</p>
<p>BINSï¼šä¸Šé¢çš„ç›´æ–¹å›¾æ˜¾ç¤ºæ¯ä¸ªåƒç´ å€¼çš„åƒç´ æ•°ï¼Œå³ä»0åˆ°255ã€‚å³ï¼Œæ‚¨éœ€è¦256ä¸ªå€¼æ¥æ˜¾ç¤ºä¸Šé¢çš„ç›´æ–¹å›¾ã€‚ä½†æ˜¯è€ƒè™‘ä¸€ä¸‹ï¼Œå¦‚æœæ‚¨ä¸éœ€è¦åˆ†åˆ«æ‰¾åˆ°æ‰€æœ‰åƒç´ å€¼çš„åƒç´ æ•°ï¼Œè€Œæ˜¯æ‰¾åˆ°åƒç´ å€¼é—´éš”ä¸­çš„åƒç´ æ•°æ€ä¹ˆåŠï¼Ÿ ä¾‹å¦‚ï¼Œæ‚¨éœ€è¦æ‰¾åˆ°ä»‹äº0åˆ°15ä¹‹é—´çš„åƒç´ æ•°ï¼Œç„¶åæ‰¾åˆ°16åˆ°31ä¹‹é—´ï¼Œâ€¦ï¼Œ240åˆ°255ä¹‹é—´çš„åƒç´ æ•°ã€‚åªéœ€è¦16ä¸ªå€¼å³å¯è¡¨ç¤ºç›´æ–¹å›¾ã€‚è¿™å°±æ˜¯åœ¨OpenCVæ•™ç¨‹ä¸­æœ‰å…³ç›´æ–¹å›¾çš„ç¤ºä¾‹ä¸­æ˜¾ç¤ºçš„å†…å®¹ã€‚</p>
<p>å› æ­¤ï¼Œæ‚¨è¦åšçš„å°±æ˜¯å°†æ•´ä¸ªç›´æ–¹å›¾åˆ†æˆ16ä¸ªå­éƒ¨åˆ†ï¼Œæ¯ä¸ªå­éƒ¨åˆ†çš„å€¼å°±æ˜¯å…¶ä¸­æ‰€æœ‰åƒç´ æ•°çš„æ€»å’Œã€‚ æ¯ä¸ªå­éƒ¨åˆ†éƒ½ç§°ä¸ºâ€œ BINâ€ã€‚åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œbinçš„æ•°é‡ä¸º256ä¸ªï¼ˆæ¯ä¸ªåƒç´ ä¸€ä¸ªï¼‰ï¼Œè€Œåœ¨ç¬¬äºŒç§æƒ…å†µä¸‹ï¼Œbinçš„æ•°é‡ä»…ä¸º16ä¸ªã€‚BINSç”±OpenCVæ–‡æ¡£ä¸­çš„<strong>histSize</strong>æœ¯è¯­è¡¨ç¤ºã€‚</p>
<p>DIMSï¼šè¿™æ˜¯æˆ‘ä»¬ä¸ºå…¶æ”¶é›†æ•°æ®çš„å‚æ•°çš„æ•°é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»…æ”¶é›†å…³äºå¼ºåº¦å€¼çš„ä¸€ä»¶äº‹çš„æ•°æ®ã€‚æ‰€ä»¥è¿™é‡Œæ˜¯1ã€‚</p>
<p>RANGEï¼šè¿™æ˜¯æ‚¨è¦æµ‹é‡çš„å¼ºåº¦å€¼çš„èŒƒå›´ã€‚é€šå¸¸ï¼Œå®ƒæ˜¯[0,256]ï¼Œå³æ‰€æœ‰å¼ºåº¦å€¼ã€‚</p>
<h2 id="1-OpenCVä¸­çš„ç›´æ–¹å›¾è®¡ç®—"><a href="#1-OpenCVä¸­çš„ç›´æ–¹å›¾è®¡ç®—" class="headerlink" title="1. OpenCVä¸­çš„ç›´æ–¹å›¾è®¡ç®—"></a>1. OpenCVä¸­çš„ç›´æ–¹å›¾è®¡ç®—</h2><p>å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬ä½¿ç”¨<strong>cv.calcHist</strong>()å‡½æ•°æŸ¥æ‰¾ç›´æ–¹å›¾ã€‚è®©æˆ‘ä»¬ç†Ÿæ‚‰ä¸€ä¸‹è¯¥å‡½æ•°åŠå…¶å‚æ•°ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv.calcHistï¼ˆimagesï¼Œchannelsï¼Œmaskï¼ŒhistSizeï¼Œranges [ï¼Œhist [ï¼Œaccumulate]]ï¼‰</span><br><span class="line">imagesï¼šå®ƒæ˜¯uint8æˆ–float32ç±»å‹çš„æºå›¾åƒã€‚å®ƒåº”è¯¥æ”¾åœ¨æ–¹æ‹¬å·ä¸­ï¼Œå³â€œ [img]â€ã€‚</span><br><span class="line">channelsï¼šä¹Ÿä»¥æ–¹æ‹¬å·ç»™å‡ºã€‚å®ƒæ˜¯æˆ‘ä»¬è®¡ç®—ç›´æ–¹å›¾çš„é€šé“çš„ç´¢å¼•ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¾“å…¥ä¸ºç°åº¦å›¾åƒï¼Œåˆ™å…¶å€¼ä¸º[0]ã€‚å¯¹äºå½©è‰²å›¾åƒï¼Œæ‚¨å¯ä»¥ä¼ é€’[0]ï¼Œ[1]æˆ–[2]åˆ†åˆ«è®¡ç®—è“è‰²ï¼Œç»¿è‰²æˆ–çº¢è‰²é€šé“çš„ç›´æ–¹å›¾ã€‚</span><br><span class="line">maskï¼šå›¾åƒæ©ç ã€‚ä¸ºäº†æ‰¾åˆ°å®Œæ•´å›¾åƒçš„ç›´æ–¹å›¾ï¼Œå°†å…¶æŒ‡å®šä¸ºâ€œæ— â€ã€‚ä½†æ˜¯ï¼Œå¦‚æœè¦æŸ¥æ‰¾å›¾åƒç‰¹å®šåŒºåŸŸçš„ç›´æ–¹å›¾ï¼Œåˆ™å¿…é¡»ä¸ºæ­¤åˆ›å»ºä¸€ä¸ªæ©ç å›¾åƒå¹¶å°†å…¶ä½œä¸ºæ©ç ã€‚ï¼ˆæˆ‘å°†åœ¨åé¢æ˜¾ç¤ºä¸€ä¸ªç¤ºä¾‹ã€‚ï¼‰</span><br><span class="line">histSizeï¼šè¿™è¡¨ç¤ºæˆ‘ä»¬çš„BINè®¡æ•°ã€‚éœ€è¦æ”¾åœ¨æ–¹æ‹¬å·ä¸­ã€‚å¯¹äºå…¨å°ºå¯¸ï¼Œæˆ‘ä»¬é€šè¿‡[256]ã€‚</span><br><span class="line">rangesï¼šè¿™æ˜¯æˆ‘ä»¬çš„RANGEã€‚é€šå¸¸ä¸º[0,256]ã€‚</span><br></pre></td></tr></table></figure>
<p>å› æ­¤ï¼Œè®©æˆ‘ä»¬ä»ç¤ºä¾‹å›¾åƒå¼€å§‹ã€‚åªéœ€ä»¥ç°åº¦æ¨¡å¼åŠ è½½å›¾åƒå¹¶æ‰¾åˆ°å…¶å®Œæ•´ç›´æ–¹å›¾å³å¯ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line">hist &#x3D; cv.calcHist([img],[0],None,[256],[0,256])</span><br></pre></td></tr></table></figure>
<p>histæ˜¯256x1çš„æ•°ç»„ï¼Œæ¯ä¸ªå€¼å¯¹åº”äºè¯¥å›¾åƒä¸­å…·æœ‰ç›¸åº”åƒç´ å€¼çš„åƒç´ æ•°ã€‚</p>
<h2 id="2-numpyçš„ç›´æ–¹å›¾è®¡ç®—"><a href="#2-numpyçš„ç›´æ–¹å›¾è®¡ç®—" class="headerlink" title="2. numpyçš„ç›´æ–¹å›¾è®¡ç®—"></a>2. numpyçš„ç›´æ–¹å›¾è®¡ç®—</h2><p>Numpyè¿˜ä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªå‡½æ•°<strong>np.histogram</strong>()ã€‚å› æ­¤ï¼Œé™¤äº†<strong>calcHist</strong>()å‡½æ•°å¤–ï¼Œæ‚¨å¯ä»¥å°è¯•ä¸‹é¢çš„ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hist,bins &#x3D; np.histogram(img.ravel(),256,[0,256])</span><br></pre></td></tr></table></figure>
<p>histä¸æˆ‘ä»¬ä¹‹å‰è®¡ç®—çš„ç›¸åŒã€‚ä½†æ˜¯binå°†å…·æœ‰257ä¸ªå…ƒç´ ï¼Œå› ä¸ºNumpyè®¡ç®—å‡ºbinçš„èŒƒå›´ä¸º0-0.99ã€1-1.99ã€2-2.99ç­‰ã€‚å› æ­¤æœ€ç»ˆèŒƒå›´ä¸º255-255.99ã€‚ä¸ºäº†è¡¨ç¤ºè¿™ä¸€ç‚¹ï¼Œä»–ä»¬è¿˜åœ¨æœ€åæ·»åŠ äº†256ã€‚ä½†æ˜¯æˆ‘ä»¬ä¸éœ€è¦256ã€‚æœ€å¤š255å°±è¶³å¤Ÿäº†ã€‚</p>
<p>å¦å¤– Numpyè¿˜æœ‰å¦ä¸€ä¸ªå‡½æ•°<strong>np.bincount</strong>()ï¼Œå®ƒæ¯”np.histogram()å¿«10å€å·¦å³ã€‚å› æ­¤ï¼Œå¯¹äºä¸€ç»´ç›´æ–¹å›¾ï¼Œæ‚¨å¯ä»¥æ›´å¥½åœ°å°è¯•ä¸€ä¸‹ã€‚ä¸è¦å¿˜è®°åœ¨np.bincountä¸­è®¾ç½®minlength = 256ã€‚ä¾‹å¦‚ï¼Œhist = np.bincount(img.ravel()ï¼Œminlength = 256)<br>æ³¨æ„ OpenCVå‡½æ•°æ¯”np.histogram()å¿«å¤§çº¦40å€ã€‚å› æ­¤ï¼Œå°½å¯èƒ½ä½¿ç”¨OpenCVå‡½æ•°ã€‚</p>
<p>ç°åœ¨æˆ‘ä»¬åº”è¯¥ç»˜åˆ¶ç›´æ–¹å›¾ï¼Œä½†æ˜¯æ€ä¹ˆç»˜åˆ¶ï¼Ÿ</p>
<h2 id="ç»˜åˆ¶ç›´æ–¹å›¾"><a href="#ç»˜åˆ¶ç›´æ–¹å›¾" class="headerlink" title="ç»˜åˆ¶ç›´æ–¹å›¾"></a>ç»˜åˆ¶ç›´æ–¹å›¾</h2><p>æœ‰ä¸¤ç§æ–¹æ³•ï¼Œ 1. ç®€çŸ­çš„æ–¹æ³•ï¼šä½¿ç”¨Matplotlibç»˜å›¾åŠŸèƒ½ 2. ç¨é•¿çš„æ–¹æ³•ï¼šä½¿ç”¨OpenCVç»˜å›¾åŠŸèƒ½</p>
<h2 id="1-ä½¿ç”¨Matplotlib"><a href="#1-ä½¿ç”¨Matplotlib" class="headerlink" title="1. ä½¿ç”¨Matplotlib"></a>1. ä½¿ç”¨Matplotlib</h2><p>Matplotlibå¸¦æœ‰ç›´æ–¹å›¾ç»˜å›¾åŠŸèƒ½ï¼šmatplotlib.pyplot.hist() å®ƒç›´æ¥æ‰¾åˆ°ç›´æ–¹å›¾å¹¶å°†å…¶ç»˜åˆ¶ã€‚</p>
<p>æ‚¨æ— éœ€ä½¿ç”¨<strong>calcHist</strong>()æˆ–np.histogram()å‡½æ•°æ¥æŸ¥æ‰¾ç›´æ–¹å›¾ã€‚è¯·å‚è§ä¸‹é¢çš„ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line">plt.hist(img.ravel(),256,[0,256]); plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/histogram_matplotlib.jpg" alt></p>
<p>æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨matplotlibçš„æ³•çº¿å›¾ï¼Œè¿™å¯¹äºBGRå›¾æ˜¯å¾ˆå¥½çš„ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦é¦–å…ˆæ‰¾åˆ°ç›´æ–¹å›¾æ•°æ®ã€‚è¯•è¯•ä¸‹é¢çš„ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">color &#x3D; (&#39;b&#39;,&#39;g&#39;,&#39;r&#39;)</span><br><span class="line">for i,col in enumerate(color):</span><br><span class="line">    histr &#x3D; cv.calcHist([img],[i],None,[256],[0,256])</span><br><span class="line">    plt.plot(histr,color &#x3D; col)</span><br><span class="line">    plt.xlim([0,256])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/histogram_rgb_plot.jpg" alt><br>æ‚¨å¯ä»¥ä»ä¸Šå›¾ä¸­å¾—å‡ºï¼Œè“è‰²åœ¨å›¾åƒä¸­å…·æœ‰ä¸€äº›é«˜å€¼åŸŸï¼ˆæ˜¾ç„¶è¿™åº”è¯¥æ˜¯ç”±äºå¤©ç©ºï¼‰</p>
<h2 id="2-ä½¿ç”¨-OpenCV"><a href="#2-ä½¿ç”¨-OpenCV" class="headerlink" title="2. ä½¿ç”¨ OpenCV"></a>2. ä½¿ç”¨ OpenCV</h2><p>å¥½å§ï¼Œåœ¨è¿™é‡Œæ‚¨å¯ä»¥è°ƒæ•´ç›´æ–¹å›¾çš„å€¼åŠå…¶binå€¼ï¼Œä½¿å…¶çœ‹èµ·æ¥åƒxï¼Œyåæ ‡ï¼Œ</p>
<p>ä»¥ä¾¿æ‚¨å¯ä»¥ä½¿ç”¨<strong>cv.line</strong>()æˆ–cv.polyline()å‡½æ•°ç»˜åˆ¶å®ƒä»¥ç”Ÿæˆä¸ä¸Šè¿°ç›¸åŒçš„å›¾åƒã€‚</p>
<p>OpenCV-Python2å®˜æ–¹ç¤ºä¾‹å·²ç»æä¾›äº†æ­¤åŠŸèƒ½ã€‚æ£€æŸ¥ç¤ºä¾‹/python/hist.pyä¸­çš„ä»£ç ã€‚</p>
<p>æ©ç çš„åº”ç”¨<br>æˆ‘ä»¬ä½¿ç”¨äº†cv.calcHist()æ¥æŸ¥æ‰¾æ•´ä¸ªå›¾åƒçš„ç›´æ–¹å›¾ã€‚å¦‚æœä½ æƒ³æ‰¾åˆ°å›¾åƒæŸäº›åŒºåŸŸçš„ç›´æ–¹å›¾å‘¢?åªéœ€åˆ›å»ºä¸€ä¸ªæ©ç å›¾åƒï¼Œåœ¨ä½ è¦æ‰¾åˆ°ç›´æ–¹å›¾ä¸ºç™½è‰²ï¼Œå¦åˆ™é»‘è‰²ã€‚ç„¶åæŠŠè¿™ä¸ªä½œä¸ºæ©ç ä¼ é€’ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;,0)</span><br><span class="line"># create a mask</span><br><span class="line">mask &#x3D; np.zeros(img.shape[:2], np.uint8)</span><br><span class="line">mask[100:300, 100:400] &#x3D; 255</span><br><span class="line">masked_img &#x3D; cv.bitwise_and(img,img,mask &#x3D; mask)</span><br><span class="line"># è®¡ç®—æ©ç åŒºåŸŸå’Œéæ©ç åŒºåŸŸçš„ç›´æ–¹å›¾</span><br><span class="line"># æ£€æŸ¥ä½œä¸ºæ©ç çš„ç¬¬ä¸‰ä¸ªå‚æ•°</span><br><span class="line">hist_full &#x3D; cv.calcHist([img],[0],None,[256],[0,256])</span><br><span class="line">hist_mask &#x3D; cv.calcHist([img],[0],mask,[256],[0,256])</span><br><span class="line">plt.subplot(221), plt.imshow(img, &#39;gray&#39;)</span><br><span class="line">plt.subplot(222), plt.imshow(mask,&#39;gray&#39;)</span><br><span class="line">plt.subplot(223), plt.imshow(masked_img, &#39;gray&#39;)</span><br><span class="line">plt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)</span><br><span class="line">plt.xlim([0,256])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>æŸ¥çœ‹ç»“æœã€‚åœ¨ç›´æ–¹å›¾ä¸­ï¼Œè“çº¿è¡¨ç¤ºå®Œæ•´å›¾åƒçš„ç›´æ–¹å›¾ï¼Œç»¿çº¿è¡¨ç¤ºæ©ç åŒºåŸŸçš„ç›´æ–¹å›¾ã€‚<br><img src="http://qiniu.aihubs.net/histogram_masking.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="ç›´æ–¹å›¾-2ï¼šç›´æ–¹å›¾å‡è¡¡"><a href="#ç›´æ–¹å›¾-2ï¼šç›´æ–¹å›¾å‡è¡¡" class="headerlink" title="ç›´æ–¹å›¾-2ï¼šç›´æ–¹å›¾å‡è¡¡"></a><span id="header2">ç›´æ–¹å›¾-2ï¼šç›´æ–¹å›¾å‡è¡¡</span></h1><h2 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬èŠ‚ä¸­, - æˆ‘ä»¬å°†å­¦ä¹ ç›´æ–¹å›¾å‡è¡¡åŒ–çš„æ¦‚å¿µ,å¹¶åˆ©ç”¨å®ƒæ¥æé«˜å›¾åƒçš„å¯¹æ¯”åº¦ã€‚</p>
<h2 id="ç†è®º-1"><a href="#ç†è®º-1" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>è€ƒè™‘è¿™æ ·ä¸€ä¸ªå›¾åƒï¼Œå®ƒçš„åƒç´ å€¼ä»…å±€é™äºæŸä¸ªç‰¹å®šçš„å€¼èŒƒå›´ã€‚</p>
<p>ä¾‹å¦‚ï¼Œè¾ƒäº®çš„å›¾åƒå°†æŠŠæ‰€æœ‰åƒç´ é™åˆ¶åœ¨é«˜å€¼ä¸Šã€‚ä½†æ˜¯ä¸€å¹…å¥½çš„å›¾åƒä¼šæœ‰æ¥è‡ªå›¾åƒæ‰€æœ‰åŒºåŸŸçš„åƒç´ ã€‚</p>
<p>å› æ­¤ï¼Œæ‚¨éœ€è¦å°†è¿™ä¸ªç›´æ–¹å›¾æ‹‰ä¼¸åˆ°ä¸¤ç«¯(å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ¥è‡ªwikipedia)ï¼Œè¿™å°±æ˜¯ç›´æ–¹å›¾å‡è¡¡åŒ–çš„ä½œç”¨(ç®€å•æ¥è¯´)ã€‚è¿™é€šå¸¸ä¼šæé«˜å›¾åƒçš„å¯¹æ¯”åº¦ã€‚<br><img src="http://qiniu.aihubs.net/histogram_equalization.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;wiki.jpg&#39;,0)</span><br><span class="line">hist,bins &#x3D; np.histogram(img.flatten(),256,[0,256])</span><br><span class="line">cdf &#x3D; hist.cumsum()</span><br><span class="line">cdf_normalized &#x3D; cdf * float(hist.max()) &#x2F; cdf.max()</span><br><span class="line">plt.plot(cdf_normalized, color &#x3D; &#39;b&#39;)</span><br><span class="line">plt.hist(img.flatten(),256,[0,256], color &#x3D; &#39;r&#39;)</span><br><span class="line">plt.xlim([0,256])</span><br><span class="line">plt.legend((&#39;cdf&#39;,&#39;histogram&#39;), loc &#x3D; &#39;upper left&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/histeq_numpy1.jpg" alt></p>
<p>ä½ å¯ä»¥çœ‹åˆ°ç›´æ–¹å›¾ä½äºè¾ƒäº®çš„åŒºåŸŸã€‚æˆ‘ä»¬éœ€è¦å…¨é¢‘è°±ã€‚</p>
<p>ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè½¬æ¢å‡½æ•°ï¼Œå°†äº®åŒºåŸŸçš„è¾“å…¥åƒç´ æ˜ å°„åˆ°æ•´ä¸ªåŒºåŸŸçš„è¾“å‡ºåƒç´ ã€‚è¿™å°±æ˜¯ç›´æ–¹å›¾å‡è¡¡åŒ–çš„ä½œç”¨ã€‚</p>
<p>ç°åœ¨æˆ‘ä»¬æ‰¾åˆ°æœ€å°çš„ç›´æ–¹å›¾å€¼(ä¸åŒ…æ‹¬0)ï¼Œå¹¶åº”ç”¨wikié¡µé¢ä¸­ç»™å‡ºçš„ç›´æ–¹å›¾å‡è¡¡åŒ–æ–¹ç¨‹ã€‚</p>
<p>ä½†æˆ‘åœ¨è¿™é‡Œç”¨è¿‡ï¼Œæ¥è‡ªNumpyçš„æ©ç æ•°ç»„æ¦‚å¿µæ•°ç»„ã€‚å¯¹äºæ©ç æ•°ç»„ï¼Œæ‰€æœ‰æ“ä½œéƒ½åœ¨éæ©ç å…ƒç´ ä¸Šæ‰§è¡Œã€‚æ‚¨å¯ä»¥ä»Numpyæ–‡æ¡£ä¸­äº†è§£æ›´å¤šå…³äºæ©ç æ•°ç»„çš„ä¿¡æ¯ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cdf_m &#x3D; np.ma.masked_equal(cdf,0)</span><br><span class="line">cdf_m &#x3D; (cdf_m - cdf_m.min())*255&#x2F;(cdf_m.max()-cdf_m.min())</span><br><span class="line">cdf &#x3D; np.ma.filled(cdf_m,0).astype(&#39;uint8&#39;)</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨æˆ‘ä»¬æœ‰äº†æŸ¥æ‰¾è¡¨ï¼Œè¯¥è¡¨ä¸ºæˆ‘ä»¬æä¾›äº†æœ‰å…³æ¯ä¸ªè¾“å…¥åƒç´ å€¼çš„è¾“å‡ºåƒç´ å€¼æ˜¯ä»€ä¹ˆçš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»…åº”ç”¨å˜æ¢ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img2 &#x3D; cdf[img]</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬åƒä»¥å‰ä¸€æ ·è®¡ç®—å…¶ç›´æ–¹å›¾å’Œcdfï¼ˆæ‚¨è¿™æ ·åšï¼‰ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š<br><img src="http://qiniu.aihubs.net/histeq_numpy2.jpg" alt><br>å¦ä¸€ä¸ªé‡è¦çš„ç‰¹å¾æ˜¯ï¼Œå³ä½¿å›¾åƒæ˜¯ä¸€ä¸ªè¾ƒæš—çš„å›¾åƒ(è€Œä¸æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„ä¸€ä¸ªè¾ƒäº®çš„å›¾åƒ)ï¼Œç»è¿‡å‡è¡¡åï¼Œæˆ‘ä»¬å°†å¾—åˆ°å‡ ä¹ç›¸åŒçš„å›¾åƒã€‚å› æ­¤ï¼Œè¿™æ˜¯ä½œä¸ºä¸€ä¸ªâ€œå‚è€ƒå·¥å…·â€ï¼Œä½¿æ‰€æœ‰çš„å›¾åƒå…·æœ‰ç›¸åŒçš„ç…§æ˜æ¡ä»¶ã€‚è¿™åœ¨å¾ˆå¤šæƒ…å†µä¸‹éƒ½å¾ˆæœ‰ç”¨ã€‚</p>
<p>ä¾‹å¦‚ï¼Œåœ¨äººè„¸è¯†åˆ«ä¸­ï¼Œåœ¨å¯¹äººè„¸æ•°æ®è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œå¯¹äººè„¸å›¾åƒè¿›è¡Œç›´æ–¹å›¾å‡è¡¡åŒ–å¤„ç†ï¼Œä½¿å…¶å…·æœ‰ç›¸åŒçš„å…‰ç…§æ¡ä»¶ã€‚</p>
<h2 id="OpenCVä¸­çš„ç›´æ–¹å›¾å‡è¡¡"><a href="#OpenCVä¸­çš„ç›´æ–¹å›¾å‡è¡¡" class="headerlink" title="OpenCVä¸­çš„ç›´æ–¹å›¾å‡è¡¡"></a>OpenCVä¸­çš„ç›´æ–¹å›¾å‡è¡¡</h2><p>OpenCVå…·æœ‰æ‰§è¡Œæ­¤æ“ä½œçš„åŠŸèƒ½cv.equalizeHistï¼ˆï¼‰ã€‚</p>
<p>å®ƒçš„è¾“å…¥åªæ˜¯ç°åº¦å›¾åƒï¼Œè¾“å‡ºæ˜¯æˆ‘ä»¬çš„ç›´æ–¹å›¾å‡è¡¡å›¾åƒã€‚ ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä»£ç ç‰‡æ®µï¼Œæ˜¾ç¤ºäº†å®ƒä¸æˆ‘ä»¬ä½¿ç”¨çš„åŒä¸€å›¾åƒçš„ç”¨æ³•ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv.imread(&#39;wiki.jpg&#39;,0)</span><br><span class="line">equ &#x3D; cv.equalizeHist(img)</span><br><span class="line">res &#x3D; np.hstack((img,equ)) #stacking images side-by-side</span><br><span class="line">cv.imwrite(&#39;res.png&#39;,res)</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/equalization_opencv.jpg" alt><br>å› æ­¤ï¼Œç°åœ¨æ‚¨å¯ä»¥åœ¨ä¸åŒçš„å…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„ä¸åŒçš„å›¾åƒï¼Œå¯¹å…¶è¿›è¡Œå‡è¡¡å¹¶æ£€æŸ¥ç»“æœã€‚</p>
<p>å½“å›¾åƒçš„ç›´æ–¹å›¾é™åˆ¶åœ¨ç‰¹å®šåŒºåŸŸæ—¶ï¼Œç›´æ–¹å›¾å‡è¡¡åŒ–æ•ˆæœå¾ˆå¥½ã€‚åœ¨ç›´æ–¹å›¾è¦†ç›–è¾ƒå¤§åŒºåŸŸï¼ˆå³åŒæ—¶å­˜åœ¨äº®åƒç´ å’Œæš—åƒç´ ï¼‰çš„å¼ºåº¦å˜åŒ–è¾ƒå¤§çš„åœ°æ–¹ï¼Œæ•ˆæœä¸å¥½ã€‚è¯·æ£€æŸ¥å…¶ä»–èµ„æºä¸­çš„SOFé“¾æ¥ã€‚</p>
<h2 id="CLAHEï¼ˆå¯¹æ¯”åº¦å—é™çš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡ï¼‰"><a href="#CLAHEï¼ˆå¯¹æ¯”åº¦å—é™çš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡ï¼‰" class="headerlink" title="CLAHEï¼ˆå¯¹æ¯”åº¦å—é™çš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡ï¼‰"></a>CLAHEï¼ˆå¯¹æ¯”åº¦å—é™çš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡ï¼‰</h2><p>æˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„ç¬¬ä¸€ä¸ªç›´æ–¹å›¾å‡è¡¡åŒ–è€ƒè™‘äº†å›¾åƒçš„æ•´ä½“å¯¹æ¯”åº¦ã€‚</p>
<p>åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ä¾‹å¦‚ï¼Œä¸‹å›¾æ˜¾ç¤ºäº†è¾“å…¥å›¾åƒåŠå…¶åœ¨å…¨å±€ç›´æ–¹å›¾å‡è¡¡åçš„ç»“æœã€‚<br><img src="http://qiniu.aihubs.net/clahe_1.jpg" alt></p>
<p>ç›´æ–¹å›¾å‡è¡¡åï¼ŒèƒŒæ™¯å¯¹æ¯”åº¦ç¡®å®å¾—åˆ°äº†æ”¹å–„ã€‚ä½†æ˜¯åœ¨ä¸¤ä¸ªå›¾åƒä¸­æ¯”è¾ƒé›•åƒçš„è„¸ã€‚</p>
<p>ç”±äºäº®åº¦è¿‡é«˜ï¼Œæˆ‘ä»¬åœ¨é‚£é‡Œä¸¢å¤±äº†å¤§å¤šæ•°ä¿¡æ¯ã€‚</p>
<p>è¿™æ˜¯å› ä¸ºå®ƒçš„ç›´æ–¹å›¾ä¸åƒæˆ‘ä»¬åœ¨å‰é¢çš„æ¡ˆä¾‹ä¸­æ‰€çœ‹åˆ°çš„é‚£æ ·å±€é™äºç‰¹å®šåŒºåŸŸï¼ˆå°è¯•ç»˜åˆ¶è¾“å…¥å›¾åƒçš„ç›´æ–¹å›¾ï¼Œæ‚¨å°†è·å¾—æ›´å¤šçš„ç›´è§‰ï¼‰ã€‚</p>
<p>å› æ­¤ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½¿ç”¨äº†<strong>è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡</strong>ã€‚</p>
<p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›¾åƒè¢«åˆ†æˆç§°ä¸ºâ€œtilesâ€çš„å°å—ï¼ˆåœ¨OpenCVä¸­ï¼ŒtileSizeé»˜è®¤ä¸º8x8ï¼‰ã€‚</p>
<p>ç„¶åï¼Œåƒå¾€å¸¸ä¸€æ ·å¯¹è¿™äº›å—ä¸­çš„æ¯ä¸€ä¸ªè¿›è¡Œç›´æ–¹å›¾å‡è¡¡ã€‚</p>
<p>å› æ­¤ï¼Œåœ¨è¾ƒå°çš„åŒºåŸŸä¸­ï¼Œç›´æ–¹å›¾å°†é™åˆ¶åœ¨ä¸€ä¸ªè¾ƒå°çš„åŒºåŸŸä¸­ï¼ˆé™¤éå­˜åœ¨å™ªå£°ï¼‰ã€‚</p>
<p>å¦‚æœæœ‰å™ªéŸ³ï¼Œå®ƒå°†è¢«æ”¾å¤§ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œåº”ç”¨äº†å¯¹æ¯”åº¦é™åˆ¶ã€‚</p>
<p>å¦‚æœä»»ä½•ç›´æ–¹å›¾binè¶…å‡ºæŒ‡å®šçš„å¯¹æ¯”åº¦é™åˆ¶ï¼ˆåœ¨OpenCVä¸­é»˜è®¤ä¸º40ï¼‰ï¼Œåˆ™åœ¨åº”ç”¨ç›´æ–¹å›¾å‡è¡¡ä¹‹å‰ï¼Œå°†è¿™äº›åƒç´ è£å‰ªå¹¶å‡åŒ€åœ°åˆ†å¸ƒåˆ°å…¶ä»–binã€‚</p>
<p>å‡è¡¡åï¼Œè¦æ¶ˆé™¤å›¾å—è¾¹ç•Œä¸­çš„ä¼ªå½±ï¼Œè¯·åº”ç”¨åŒçº¿æ€§æ’å€¼ã€‚</p>
<p>ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ˜¾ç¤ºäº†å¦‚ä½•åœ¨OpenCVä¸­åº”ç”¨CLAHEï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;tsukuba_l.png&#39;,0)</span><br><span class="line"># create a CLAHE object (Arguments are optional).</span><br><span class="line">clahe &#x3D; cv.createCLAHE(clipLimit&#x3D;2.0, tileGridSize&#x3D;(8,8))</span><br><span class="line">cl1 &#x3D; clahe.apply(img)</span><br><span class="line">cv.imwrite(&#39;clahe_2.jpg&#39;,cl1)</span><br></pre></td></tr></table></figure>
<p>æŸ¥çœ‹ä¸‹é¢çš„ç»“æœï¼Œå¹¶å°†å…¶ä¸ä¸Šé¢çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œå°¤å…¶æ˜¯é›•åƒåŒºåŸŸï¼š<br><img src="http://qiniu.aihubs.net/clahe_2.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="ç›´æ–¹å›¾-3ï¼šäºŒç»´ç›´æ–¹å›¾"><a href="#ç›´æ–¹å›¾-3ï¼šäºŒç»´ç›´æ–¹å›¾" class="headerlink" title="ç›´æ–¹å›¾-3ï¼šäºŒç»´ç›´æ–¹å›¾"></a><span id="header3">ç›´æ–¹å›¾-3ï¼šäºŒç»´ç›´æ–¹å›¾</span></h1><h2 id="ç›®æ ‡-2"><a href="#ç›®æ ‡-2" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ æŸ¥æ‰¾å’Œç»˜åˆ¶2Dç›´æ–¹å›¾ã€‚è¿™å°†åœ¨ä»¥åçš„ç« èŠ‚ä¸­æœ‰æ‰€å¸®åŠ©ã€‚</p>
<h2 id="ä»‹ç»"><a href="#ä»‹ç»" class="headerlink" title="ä»‹ç»"></a>ä»‹ç»</h2><p>åœ¨ç¬¬ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—å¹¶ç»˜åˆ¶äº†ä¸€ç»´ç›´æ–¹å›¾ã€‚ ä¹‹æ‰€ä»¥ç§°ä¸ºä¸€ç»´ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬ä»…è€ƒè™‘ä¸€ä¸ªç‰¹å¾ï¼Œå³åƒç´ çš„ç°åº¦å¼ºåº¦å€¼ã€‚ </p>
<p>ä½†æ˜¯åœ¨äºŒç»´ç›´æ–¹å›¾ä¸­ï¼Œæ‚¨è¦è€ƒè™‘ä¸¤ä¸ªç‰¹å¾ã€‚ é€šå¸¸ï¼Œå®ƒç”¨äºæŸ¥æ‰¾é¢œè‰²ç›´æ–¹å›¾ï¼Œå…¶ä¸­ä¸¤ä¸ªç‰¹å¾æ˜¯æ¯ä¸ªåƒç´ çš„è‰²ç›¸å’Œé¥±å’Œåº¦å€¼ã€‚</p>
<p>æˆ‘ä»¬å°†å°è¯•äº†è§£å¦‚ä½•åˆ›å»ºè¿™ç§é¢œè‰²ç›´æ–¹å›¾ï¼Œè¿™å¯¹äºç†è§£è¯¸å¦‚ç›´æ–¹å›¾åå‘æŠ•å½±ä¹‹ç±»çš„æ›´å¤šä¸»é¢˜å°†å¾ˆæœ‰ç”¨ã€‚</p>
<h2 id="OpenCVä¸­çš„äºŒç»´ç›´æ–¹å›¾"><a href="#OpenCVä¸­çš„äºŒç»´ç›´æ–¹å›¾" class="headerlink" title="OpenCVä¸­çš„äºŒç»´ç›´æ–¹å›¾"></a>OpenCVä¸­çš„äºŒç»´ç›´æ–¹å›¾</h2><p>å®ƒéå¸¸ç®€å•ï¼Œå¹¶ä¸”ä½¿ç”¨ç›¸åŒçš„å‡½æ•°<strong>cv.calcHist</strong>()è¿›è¡Œè®¡ç®—ã€‚ </p>
<p>å¯¹äºé¢œè‰²ç›´æ–¹å›¾ï¼Œæˆ‘ä»¬éœ€è¦å°†å›¾åƒä»BGRè½¬æ¢ä¸ºHSVã€‚ï¼ˆè¯·è®°ä½ï¼Œå¯¹äºä¸€ç»´ç›´æ–¹å›¾ï¼Œæˆ‘ä»¬ä»BGRè½¬æ¢ä¸ºç°åº¦ï¼‰ã€‚å¯¹äºäºŒç»´ç›´æ–¹å›¾ï¼Œå…¶å‚æ•°å°†è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">channel &#x3D; [0,1]ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åŒæ—¶å¤„ç†Hå’ŒSå¹³é¢ã€‚</span><br><span class="line">bins &#x3D; [180,256] å¯¹äºHå¹³é¢ä¸º180ï¼Œå¯¹äºSå¹³é¢ä¸º256ã€‚</span><br><span class="line">range &#x3D; [0,180,0,256] è‰²ç›¸å€¼ä»‹äº0å’Œ180ä¹‹é—´ï¼Œé¥±å’Œåº¦ä»‹äº0å’Œ256ä¹‹é—´ã€‚</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨æ£€æŸ¥ä»¥ä¸‹ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist &#x3D; cv.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])</span><br></pre></td></tr></table></figure>
<p>å°±æ˜¯è¿™æ ·ã€‚</p>
<h2 id="Numpyä¸­çš„äºŒç»´ç›´æ–¹å›¾"><a href="#Numpyä¸­çš„äºŒç»´ç›´æ–¹å›¾" class="headerlink" title="Numpyä¸­çš„äºŒç»´ç›´æ–¹å›¾"></a>Numpyä¸­çš„äºŒç»´ç›´æ–¹å›¾</h2><p>Numpyè¿˜ä¸ºæ­¤æä¾›äº†ä¸€ä¸ªç‰¹å®šçš„å‡½æ•°:np.histogram2d()ã€‚(è®°ä½ï¼Œå¯¹äºä¸€ç»´ç›´æ–¹å›¾æˆ‘ä»¬ä½¿ç”¨äº†<strong>np.histogram</strong>())ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist, xbins, ybins &#x3D; np.histogram2d(h.ravel(),s.ravel(),[180,256],[[0,180],[0,256]])</span><br></pre></td></tr></table></figure>
<p>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯Hå¹³é¢ï¼Œç¬¬äºŒä¸ªæ˜¯Så¹³é¢ï¼Œç¬¬ä¸‰ä¸ªæ˜¯æ¯ä¸ªç®±å­çš„æ•°é‡ï¼Œç¬¬å››ä¸ªæ˜¯å®ƒä»¬çš„èŒƒå›´ã€‚</p>
<p>ç°åœ¨æˆ‘ä»¬å¯ä»¥æ£€æŸ¥å¦‚ä½•ç»˜åˆ¶è¿™ä¸ªé¢œè‰²ç›´æ–¹å›¾ã€‚</p>
<h2 id="ç»˜åˆ¶äºŒç»´ç›´æ–¹å›¾"><a href="#ç»˜åˆ¶äºŒç»´ç›´æ–¹å›¾" class="headerlink" title="ç»˜åˆ¶äºŒç»´ç›´æ–¹å›¾"></a>ç»˜åˆ¶äºŒç»´ç›´æ–¹å›¾</h2><h2 id="æ–¹æ³•1ï¼šä½¿ç”¨-cv-imshow"><a href="#æ–¹æ³•1ï¼šä½¿ç”¨-cv-imshow" class="headerlink" title="æ–¹æ³•1ï¼šä½¿ç”¨ cv.imshow()"></a>æ–¹æ³•1ï¼šä½¿ç”¨ cv.imshow()</h2><p>æˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯å°ºå¯¸ä¸º80x256çš„äºŒç»´æ•°ç»„ã€‚å› æ­¤ï¼Œå¯ä»¥ä½¿ç”¨<strong>cv.imshow</strong>()å‡½æ•°åƒå¹³å¸¸ä¸€æ ·æ˜¾ç¤ºå®ƒä»¬ã€‚</p>
<p>å®ƒå°†æ˜¯ä¸€å¹…ç°åº¦å›¾åƒï¼Œé™¤éæ‚¨çŸ¥é“ä¸åŒé¢œè‰²çš„è‰²ç›¸å€¼ï¼Œå¦åˆ™ä¸ä¼šå¯¹å…¶ä¸­çš„é¢œè‰²æœ‰å¤ªå¤šäº†è§£ã€‚</p>
<h2 id="æ–¹æ³•2ï¼šä½¿ç”¨Matplotlib"><a href="#æ–¹æ³•2ï¼šä½¿ç”¨Matplotlib" class="headerlink" title="æ–¹æ³•2ï¼šä½¿ç”¨Matplotlib"></a>æ–¹æ³•2ï¼šä½¿ç”¨Matplotlib</h2><p>æˆ‘ä»¬å¯ä»¥ä½¿ç”¨matplotlib.pyplot.imshow()å‡½æ•°ç»˜åˆ¶å…·æœ‰ä¸åŒé¢œè‰²å›¾çš„2Dç›´æ–¹å›¾ã€‚</p>
<p>å®ƒä½¿æˆ‘ä»¬å¯¹ä¸åŒçš„åƒç´ å¯†åº¦æœ‰äº†æ›´å¥½çš„äº†è§£ã€‚ä½†æ˜¯ï¼Œé™¤éæ‚¨çŸ¥é“ä¸åŒé¢œè‰²çš„è‰²ç›¸å€¼ï¼Œå¦åˆ™ä¹ä¸€çœ‹å¹¶ä¸èƒ½ä½¿æˆ‘ä»¬çŸ¥é“åˆ°åº•æ˜¯ä»€ä¹ˆé¢œè‰²ã€‚</p>
<p>æ³¨æ„ ä½¿ç”¨æ­¤åŠŸèƒ½æ—¶ï¼Œè¯·è®°ä½ï¼Œæ’å€¼æ³•åº”é‡‡ç”¨æœ€è¿‘é‚»ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚</p>
<p>è€ƒè™‘ä¸‹é¢çš„ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;home.jpg&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(img,cv.COLOR_BGR2HSV)</span><br><span class="line">hist &#x3D; cv.calcHist( [hsv], [0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line">plt.imshow(hist,interpolation &#x3D; &#39;nearest&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>ä¸‹é¢æ˜¯è¾“å…¥å›¾åƒåŠå…¶é¢œè‰²ç›´æ–¹å›¾ã€‚Xè½´æ˜¾ç¤ºSå€¼ï¼ŒYè½´æ˜¾ç¤ºè‰²ç›¸ã€‚<br><img src="http://qiniu.aihubs.net/2dhist_matplotlib.jpg" alt></p>
<p>åœ¨ç›´æ–¹å›¾ä¸­ï¼Œæ‚¨å¯ä»¥åœ¨H = 100å’ŒS = 200é™„è¿‘çœ‹åˆ°ä¸€äº›è¾ƒé«˜çš„å€¼ã€‚</p>
<p>å®ƒå¯¹åº”äºå¤©ç©ºçš„è“è‰²ã€‚åŒæ ·ï¼Œåœ¨H = 25å’ŒS = 100é™„è¿‘å¯ä»¥çœ‹åˆ°å¦ä¸€ä¸ªå³°å€¼ã€‚å®ƒå¯¹åº”äºå®«æ®¿çš„é»„è‰²ã€‚æ‚¨å¯ä»¥ä½¿ç”¨GIMPç­‰ä»»ä½•å›¾åƒç¼–è¾‘å·¥å…·è¿›è¡ŒéªŒè¯ã€‚</p>
<h2 id="æ–¹æ³•3ï¼šOpenCVç¤ºä¾‹æ ·å¼"><a href="#æ–¹æ³•3ï¼šOpenCVç¤ºä¾‹æ ·å¼" class="headerlink" title="æ–¹æ³•3ï¼šOpenCVç¤ºä¾‹æ ·å¼"></a>æ–¹æ³•3ï¼šOpenCVç¤ºä¾‹æ ·å¼</h2><p>OpenCV-Python2ç¤ºä¾‹ä¸­æœ‰ä¸€ä¸ªé¢œè‰²ç›´æ–¹å›¾çš„ç¤ºä¾‹ä»£ç (samples / python / color_histogram.py)ã€‚</p>
<p>å¦‚æœè¿è¡Œä»£ç ï¼Œåˆ™å¯ä»¥çœ‹åˆ°ç›´æ–¹å›¾ä¹Ÿæ˜¾ç¤ºäº†ç›¸åº”çš„é¢œè‰²ã€‚æˆ–è€…ç®€å•åœ°ï¼Œå®ƒè¾“å‡ºé¢œè‰²ç¼–ç çš„ç›´æ–¹å›¾ã€‚å…¶ç»“æœéå¸¸å¥½ï¼ˆå°½ç®¡æ‚¨éœ€è¦æ·»åŠ é¢å¤–çš„çº¿æŸï¼‰ã€‚</p>
<p>åœ¨è¯¥ä»£ç ä¸­ï¼Œä½œè€…åœ¨HSVä¸­åˆ›å»ºäº†ä¸€ä¸ªé¢œè‰²å›¾ã€‚ç„¶åå°†å…¶è½¬æ¢ä¸ºBGRã€‚å°†æ‰€å¾—çš„ç›´æ–¹å›¾å›¾åƒä¸æ­¤é¢œè‰²å›¾ç›¸ä¹˜ã€‚ä»–è¿˜ä½¿ç”¨ä¸€äº›é¢„å¤„ç†æ­¥éª¤æ¥åˆ é™¤å°çš„å­¤ç«‹åƒç´ ï¼Œä»è€Œè·å¾—è‰¯å¥½çš„ç›´æ–¹å›¾ã€‚</p>
<p>æˆ‘å°†å…¶ç•™ç»™è¯»è€…æ¥è¿è¡Œä»£ç ï¼Œå¯¹å…¶è¿›è¡Œåˆ†æå¹¶æ‹¥æœ‰è‡ªå·±çš„è§£å†³æ–¹æ³•ã€‚ä¸‹é¢æ˜¯ä¸ä¸Šé¢ç›¸åŒçš„å›¾åƒçš„ä»£ç è¾“å‡ºï¼š<br><img src="http://qiniu.aihubs.net/2dhist_opencv.jpg" alt></p>
<p>æ‚¨å¯ä»¥åœ¨ç›´æ–¹å›¾ä¸­æ¸…æ¥šåœ°çœ‹åˆ°å­˜åœ¨ä»€ä¹ˆé¢œè‰²ï¼Œé‚£é‡Œæ˜¯è“è‰²ï¼Œé‚£é‡Œæ˜¯é»„è‰²ï¼Œå¹¶ä¸”ç”±äºæ£‹ç›˜çš„å­˜åœ¨è€Œæœ‰äº›ç™½è‰²ã€‚å¾ˆå¥½ï¼</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="ç›´æ–¹å›¾4ï¼šç›´æ–¹å›¾åæŠ•å½±"><a href="#ç›´æ–¹å›¾4ï¼šç›´æ–¹å›¾åæŠ•å½±" class="headerlink" title="ç›´æ–¹å›¾4ï¼šç›´æ–¹å›¾åæŠ•å½±"></a><span id="header4">ç›´æ–¹å›¾4ï¼šç›´æ–¹å›¾åæŠ•å½±</span></h1><h2 id="ç›®æ ‡-3"><a href="#ç›®æ ‡-3" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ç›´æ–¹å›¾åæŠ•å½±ã€‚</p>
<h2 id="ç†è®º-2"><a href="#ç†è®º-2" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>è¿™æ˜¯ç”±<strong>Michael J. Swain</strong>å’Œ<strong>Dana H. Ballard</strong>åœ¨ä»–ä»¬çš„è®ºæ–‡ã€Šé€šè¿‡é¢œè‰²ç›´æ–¹å›¾ç´¢å¼•ã€‹ä¸­æå‡ºçš„ã€‚</p>
<p>ç”¨ç®€å•çš„è¯è¯´æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿå®ƒç”¨äºå›¾åƒåˆ†å‰²æˆ–åœ¨å›¾åƒä¸­æŸ¥æ‰¾æ„Ÿå…´è¶£çš„å¯¹è±¡ã€‚</p>
<p>ç®€è€Œè¨€ä¹‹ï¼Œå®ƒåˆ›å»ºçš„å›¾åƒå¤§å°ä¸è¾“å…¥å›¾åƒç›¸åŒï¼ˆä½†åªæœ‰ä¸€ä¸ªé€šé“ï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ å¯¹åº”äºè¯¥åƒç´ å±äºæˆ‘ä»¬ç‰©ä½“çš„æ¦‚ç‡ã€‚</p>
<p>ç”¨æ›´ç®€å•çš„è¯æ¥è¯´ï¼Œä¸å…¶ä½™éƒ¨åˆ†ç›¸æ¯”ï¼Œè¾“å‡ºå›¾åƒå°†åœ¨å¯èƒ½æœ‰å¯¹è±¡çš„åŒºåŸŸå…·æœ‰æ›´å¤šçš„ç™½è‰²å€¼ã€‚</p>
<p>å¥½å§ï¼Œè¿™æ˜¯ä¸€ä¸ªç›´è§‚çš„è§£é‡Šã€‚ï¼ˆæˆ‘æ— æ³•ä½¿å…¶æ›´ç®€å•ï¼‰ã€‚ç›´æ–¹å›¾åæŠ•å½±ä¸camshiftç®—æ³•ç­‰é…åˆä½¿ç”¨ã€‚</p>
<p>æˆ‘ä»¬è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå›¾åƒçš„ç›´æ–¹å›¾ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬æ„Ÿå…´è¶£çš„å¯¹è±¡ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­æ˜¯èƒŒæ™¯ï¼Œç¦»å¼€æ’­æ”¾å™¨ç­‰ï¼‰ã€‚</p>
<p>å¯¹è±¡åº”å°½å¯èƒ½å¡«å……å›¾åƒä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚è€Œä¸”é¢œè‰²ç›´æ–¹å›¾æ¯”ç°åº¦ç›´æ–¹å›¾æ›´å¯å–ï¼Œå› ä¸ºå¯¹è±¡çš„é¢œè‰²å¯¹æ¯”ç°åº¦å¼ºåº¦æ˜¯å®šä¹‰å¯¹è±¡çš„å¥½æ–¹æ³•ã€‚</p>
<p>ç„¶åï¼Œæˆ‘ä»¬å°†è¯¥ç›´æ–¹å›¾â€œåæŠ•å½±â€åˆ°éœ€è¦æ‰¾åˆ°å¯¹è±¡çš„æµ‹è¯•å›¾åƒä¸Šï¼Œ</p>
<p>æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬è®¡ç®—å‡ºå±äºèƒŒæ™¯çš„æ¯ä¸ªåƒç´ çš„æ¦‚ç‡å¹¶å°†å…¶æ˜¾ç¤ºå‡ºæ¥ã€‚åœ¨é€‚å½“çš„é˜ˆå€¼ä¸‹äº§ç”Ÿçš„è¾“å‡ºä½¿æˆ‘ä»¬ä»…è·å¾—èƒŒæ™¯ã€‚</p>
<h2 id="Numpyä¸­çš„ç®—æ³•"><a href="#Numpyä¸­çš„ç®—æ³•" class="headerlink" title="Numpyä¸­çš„ç®—æ³•"></a>Numpyä¸­çš„ç®—æ³•</h2><p>é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æˆ‘ä»¬è¦æŸ¥æ‰¾çš„å¯¹è±¡ï¼ˆä½¿å…¶ä¸ºâ€œ Mâ€ï¼‰å’Œè¦æœç´¢çš„å›¾åƒï¼ˆä½¿å…¶ä¸ºâ€œ Iâ€ï¼‰çš„é¢œè‰²ç›´æ–¹å›¾ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cvfrom matplotlib import pyplot as plt</span><br><span class="line">#roiæ˜¯æˆ‘ä»¬éœ€è¦æ‰¾åˆ°çš„å¯¹è±¡æˆ–å¯¹è±¡åŒºåŸŸ</span><br><span class="line">roi &#x3D; cv.imread(&#39;rose_red.png&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(roi,cv.COLOR_BGR2HSV)</span><br><span class="line">#ç›®æ ‡æ˜¯æˆ‘ä»¬æœç´¢çš„å›¾åƒ</span><br><span class="line">target &#x3D; cv.imread(&#39;rose.png&#39;)</span><br><span class="line">hsvt &#x3D; cv.cvtColor(target,cv.COLOR_BGR2HSV)</span><br><span class="line"># ä½¿ç”¨calcHistæŸ¥æ‰¾ç›´æ–¹å›¾ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨np.histogram2då®Œæˆ</span><br><span class="line">M &#x3D; cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line">I &#x3D; cv.calcHist([hsvt],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br></pre></td></tr></table></figure>

<p>æ±‚å‡ºæ¯”å€¼R=$\frac{M}{I}$ã€‚ç„¶ååå‘æŠ•å½±Rï¼Œå³ä½¿ç”¨Rä½œä¸ºè°ƒè‰²æ¿ï¼Œå¹¶ä»¥æ¯ä¸ªåƒç´ ä½œä¸ºå…¶å¯¹åº”çš„ç›®æ ‡æ¦‚ç‡åˆ›å»ºä¸€ä¸ªæ–°å›¾åƒã€‚å³B(x,y) = R[h(x,y),s(x,y)] å…¶ä¸­hæ˜¯è‰²è°ƒï¼Œsæ˜¯åƒç´ åœ¨(xï¼Œy)çš„é¥±å’Œåº¦ã€‚ä¹‹åï¼Œåº”ç”¨æ¡ä»¶B(x,y)=min[B(x,y),1]ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">h,s,v &#x3D; cv.split(hsvt)</span><br><span class="line">B &#x3D; R[h.ravel(),s.ravel()]</span><br><span class="line">B &#x3D; np.minimum(B,1)</span><br><span class="line">B &#x3D; B.reshape(hsvt.shape[:2])</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨å¯¹åœ†ç›˜åº”ç”¨å·ç§¯ï¼ŒB=Dâˆ—Bï¼Œå…¶ä¸­Dæ˜¯åœ†ç›˜å†…æ ¸ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">disc &#x3D; cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))</span><br><span class="line">cv.filter2D(B,-1,disc,B)</span><br><span class="line">B &#x3D; np.uint8(B)</span><br><span class="line">cv.normalize(B,B,0,255,cv.NORM_MINMAX)</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨æœ€å¤§å¼ºåº¦çš„ä½ç½®ç»™äº†æˆ‘ä»¬ç‰©ä½“çš„ä½ç½®ã€‚å¦‚æœæˆ‘ä»¬æœŸæœ›å›¾åƒä¸­æœ‰ä¸€ä¸ªåŒºåŸŸï¼Œåˆ™å¯¹åˆé€‚çš„å€¼è¿›è¡Œé˜ˆå€¼å¤„ç†å°†è·å¾—ä¸é”™çš„ç»“æœã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ret,thresh &#x3D; cv.threshold(B,50,255,0)</span><br></pre></td></tr></table></figure>
<p>å°±æ˜¯è¿™æ ·</p>
<h2 id="OpenCVçš„åæŠ•å½±"><a href="#OpenCVçš„åæŠ•å½±" class="headerlink" title="OpenCVçš„åæŠ•å½±"></a>OpenCVçš„åæŠ•å½±</h2><p>OpenCVæä¾›äº†ä¸€ä¸ªå†…å»ºçš„å‡½æ•°<strong>cv.calcBackProject</strong>()ã€‚</p>
<p>å®ƒçš„å‚æ•°å‡ ä¹ä¸<strong>cv.calchist</strong>()å‡½æ•°ç›¸åŒã€‚å®ƒçš„ä¸€ä¸ªå‚æ•°æ˜¯ç›´æ–¹å›¾ï¼Œä¹Ÿå°±æ˜¯ç‰©ä½“çš„ç›´æ–¹å›¾ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°å®ƒã€‚</p>
<p>å¦å¤–ï¼Œåœ¨ä¼ é€’ç»™backprojectå‡½æ•°ä¹‹å‰ï¼Œåº”è¯¥å¯¹å¯¹è±¡ç›´æ–¹å›¾è¿›è¡Œå½’ä¸€åŒ–ã€‚å®ƒè¿”å›æ¦‚ç‡å›¾åƒã€‚</p>
<p>ç„¶åæˆ‘ä»¬ç”¨åœ†ç›˜å†…æ ¸å¯¹å›¾åƒè¿›è¡Œå·ç§¯å¹¶åº”ç”¨é˜ˆå€¼ã€‚ä¸‹é¢æ˜¯æˆ‘çš„ä»£ç å’Œç»“æœ:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">roi &#x3D; cv.imread(&#39;rose_red.png&#39;)</span><br><span class="line">hsv &#x3D; cv.cvtColor(roi,cv.COLOR_BGR2HSV)</span><br><span class="line">target &#x3D; cv.imread(&#39;rose.png&#39;)</span><br><span class="line">hsvt &#x3D; cv.cvtColor(target,cv.COLOR_BGR2HSV)</span><br><span class="line"># è®¡ç®—å¯¹è±¡çš„ç›´æ–¹å›¾</span><br><span class="line">roihist &#x3D; cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )</span><br><span class="line"># ç›´æ–¹å›¾å½’ä¸€åŒ–å¹¶åˆ©ç”¨åä¼ ç®—æ³•</span><br><span class="line">cv.normalize(roihist,roihist,0,255,cv.NORM_MINMAX)</span><br><span class="line">dst &#x3D; cv.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)</span><br><span class="line"># ç”¨åœ†ç›˜è¿›è¡Œå·ç§¯</span><br><span class="line">disc &#x3D; cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))</span><br><span class="line">cv.filter2D(dst,-1,disc,dst)</span><br><span class="line"># åº”ç”¨é˜ˆå€¼ä½œä¸æ“ä½œ</span><br><span class="line">ret,thresh &#x3D; cv.threshold(dst,50,255,0)</span><br><span class="line">thresh &#x3D; cv.merge((thresh,thresh,thresh))</span><br><span class="line">res &#x3D; cv.bitwise_and(target,thresh)</span><br><span class="line">res &#x3D; np.vstack((target,thresh,res))</span><br><span class="line">cv.imwrite(&#39;res.jpg&#39;,res)</span><br></pre></td></tr></table></figure>
<p>ä»¥ä¸‹æ˜¯æˆ‘å¤„ç†è¿‡çš„ä¸€ä¸ªç¤ºä¾‹ã€‚æˆ‘å°†è“è‰²çŸ©å½¢å†…çš„åŒºåŸŸç”¨ä½œç¤ºä¾‹å¯¹è±¡ï¼Œæˆ‘æƒ³æå–æ•´ä¸ªåœ°é¢ã€‚<br><img src="http://qiniu.aihubs.net/backproject_opencv.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>å›¾åƒ</tag>
      </tags>
  </entry>
  <entry>
    <title>opencvä¸­çš„å›¾åƒå¤„ç†5</title>
    <url>/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%865/</url>
    <content><![CDATA[<ul>
<li>11.<a href="#header1">å‚…é‡Œå¶å˜æ¢</a></li>
<li>12.<a href="#header2">æ¨¡æ¿åŒ¹é…</a></li>
<li>13.<a href="#header3">éœå¤«çº¿å˜æ¢</a></li>
<li>14.<a href="#header4">éœå¤«åœˆå˜æ¢</a></li>
<li>15.<a href="#header5">å›¾åƒåˆ†å‰²ä¸Watershedç®—æ³•</a></li>
<li>16.<a href="#header6">äº¤äº’å¼å‰æ™¯æå–ä½¿ç”¨GrabCutç®—æ³•</a><a id="more"></a>

</li>
</ul>
<h1 id="å‚…é‡Œå¶å˜æ¢"><a href="#å‚…é‡Œå¶å˜æ¢" class="headerlink" title="å‚…é‡Œå¶å˜æ¢"></a><span id="header1">å‚…é‡Œå¶å˜æ¢</span></h1><h2 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  </p>
<ul>
<li>ä½¿ç”¨OpenCVæŸ¥æ‰¾å›¾åƒçš„å‚…ç«‹å¶å˜æ¢ </li>
<li>åˆ©ç”¨Numpyä¸­å¯ç”¨çš„FFTå‡½æ•° </li>
<li>å‚…ç«‹å¶å˜æ¢çš„æŸäº›åº”ç”¨ç¨‹åº </li>
<li>æˆ‘ä»¬å°†çœ‹åˆ°ä»¥ä¸‹å‡½æ•°ï¼šcv.dft()ï¼Œcv.idft()ç­‰</li>
</ul>
<h2 id="ç†è®º"><a href="#ç†è®º" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>å‚…ç«‹å¶å˜æ¢ç”¨äºåˆ†æå„ç§æ»¤æ³¢å™¨çš„é¢‘ç‡ç‰¹æ€§ã€‚</p>
<p>å¯¹äºå›¾åƒï¼Œä½¿ç”¨<strong>2Dç¦»æ•£å‚…é‡Œå¶å˜æ¢</strong>(DFT)æŸ¥æ‰¾é¢‘åŸŸã€‚</p>
<p>ä¸€ç§ç§°ä¸º<strong>å¿«é€Ÿå‚…ç«‹å¶å˜æ¢</strong>(FFT)çš„å¿«é€Ÿç®—æ³•ç”¨äºDFTçš„è®¡ç®—ã€‚</p>
<p>å…³äºè¿™äº›çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨ä»»ä½•å›¾åƒå¤„ç†æˆ–ä¿¡å·å¤„ç†æ•™ç§‘ä¹¦ä¸­æ‰¾åˆ°ã€‚è¯·å‚é˜…å…¶ä»–èµ„æºéƒ¨åˆ†ã€‚</p>
<p>å¯¹äºæ­£å¼¦ä¿¡å·x(t)=Asin(2Ï€ft)ï¼Œæˆ‘ä»¬å¯ä»¥è¯´fæ˜¯ä¿¡å·çš„é¢‘ç‡ï¼Œå¦‚æœé‡‡ç”¨å…¶é¢‘åŸŸï¼Œåˆ™å¯ä»¥çœ‹åˆ°fçš„å°–å³°ã€‚</p>
<p>å¦‚æœå¯¹ä¿¡å·è¿›è¡Œé‡‡æ ·ä»¥å½¢æˆç¦»æ•£ä¿¡å·ï¼Œæˆ‘ä»¬å°†è·å¾—ç›¸åŒçš„é¢‘åŸŸï¼Œä½†æ˜¯åœ¨[âˆ’Ï€ï¼ŒÏ€]æˆ–[0,2Ï€]èŒƒå›´å†…ï¼ˆå¯¹äºNç‚¹DFTä¸º[0ï¼ŒN]ï¼‰æ˜¯å‘¨æœŸæ€§çš„ã€‚</p>
<p>æ‚¨å¯ä»¥å°†å›¾åƒè§†ä¸ºåœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šé‡‡æ ·çš„ä¿¡å·ã€‚å› æ­¤ï¼Œåœ¨Xå’ŒYæ–¹å‘éƒ½è¿›è¡Œå‚…ç«‹å¶å˜æ¢ï¼Œå¯ä»¥å¾—åˆ°å›¾åƒçš„é¢‘ç‡è¡¨ç¤ºã€‚</p>
<p>æ›´ç›´è§‚åœ°è¯´ï¼Œå¯¹äºæ­£å¼¦ä¿¡å·ï¼Œå¦‚æœå¹…åº¦åœ¨çŸ­æ—¶é—´å†…å˜åŒ–å¦‚æ­¤ä¹‹å¿«ï¼Œåˆ™å¯ä»¥è¯´å®ƒæ˜¯é«˜é¢‘ä¿¡å·ã€‚</p>
<p>å¦‚æœå˜åŒ–ç¼“æ…¢ï¼Œåˆ™ä¸ºä½é¢‘ä¿¡å·ã€‚æ‚¨å¯ä»¥å°†ç›¸åŒçš„æƒ³æ³•æ‰©å±•åˆ°å›¾åƒã€‚</p>
<p>å›¾åƒä¸­çš„æŒ¯å¹…åœ¨å“ªé‡Œæ€¥å‰§å˜åŒ–ï¼Ÿåœ¨è¾¹ç¼˜ç‚¹æˆ–å™ªå£°ã€‚å› æ­¤ï¼Œå¯ä»¥è¯´è¾¹ç¼˜å’Œå™ªå£°æ˜¯å›¾åƒä¸­çš„é«˜é¢‘å†…å®¹ã€‚</p>
<p>å¦‚æœå¹…åº¦æ²¡æœ‰å¤ªå¤§å˜åŒ–ï¼Œåˆ™å®ƒæ˜¯ä½é¢‘åˆ†é‡ã€‚ï¼ˆä¸€äº›é“¾æ¥å·²æ·»åŠ åˆ°â€œå…¶ä»–èµ„æºâ€ï¼Œå…¶ä¸­é€šè¿‡ç¤ºä¾‹ç›´è§‚åœ°è¯´æ˜äº†é¢‘ç‡å˜æ¢ï¼‰ã€‚</p>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•æ‰¾åˆ°å‚…ç«‹å¶å˜æ¢ã€‚</p>
<h2 id="Numpyä¸­çš„å‚…é‡Œå¶å˜æ¢"><a href="#Numpyä¸­çš„å‚…é‡Œå¶å˜æ¢" class="headerlink" title="Numpyä¸­çš„å‚…é‡Œå¶å˜æ¢"></a>Numpyä¸­çš„å‚…é‡Œå¶å˜æ¢</h2><p>é¦–å…ˆï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨NumpyæŸ¥æ‰¾å‚…ç«‹å¶å˜æ¢ã€‚</p>
<p>Numpyå…·æœ‰FFTè½¯ä»¶åŒ…æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚np.fft.fft2()ä¸ºæˆ‘ä»¬æä¾›äº†é¢‘ç‡è½¬æ¢ï¼Œå®ƒå°†æ˜¯ä¸€ä¸ªå¤æ‚çš„æ•°ç»„ã€‚</p>
<ul>
<li>å®ƒçš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¾“å…¥å›¾åƒï¼Œå³ç°åº¦å›¾åƒã€‚</li>
<li>ç¬¬äºŒä¸ªå‚æ•°æ˜¯å¯é€‰çš„ï¼Œå®ƒå†³å®šè¾“å‡ºæ•°ç»„çš„å¤§å°ã€‚å¦‚æœå®ƒå¤§äºè¾“å…¥å›¾åƒçš„å¤§å°ï¼Œåˆ™åœ¨è®¡ç®—FFTä¹‹å‰ç”¨é›¶å¡«å……è¾“å…¥å›¾åƒã€‚å¦‚æœå°äºè¾“å…¥å›¾åƒï¼Œå°†è£åˆ‡è¾“å…¥å›¾åƒã€‚å¦‚æœæœªä¼ é€’ä»»ä½•å‚æ•°ï¼Œåˆ™è¾“å‡ºæ•°ç»„çš„å¤§å°å°†ä¸è¾“å…¥çš„å¤§å°ç›¸åŒã€‚</li>
</ul>
<p>ç°åœ¨ï¼Œä¸€æ—¦è·å¾—ç»“æœï¼Œé›¶é¢‘ç‡åˆ†é‡ï¼ˆDCåˆ†é‡ï¼‰å°†ä½äºå·¦ä¸Šè§’ã€‚</p>
<p>å¦‚æœè¦ä½¿å…¶å±…ä¸­ï¼Œåˆ™éœ€è¦åœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šå°†ç»“æœéƒ½ç§»åŠ¨N2ã€‚</p>
<p>åªéœ€é€šè¿‡å‡½æ•°<strong>np.fft.fftshift</strong>()å³å¯å®Œæˆã€‚ï¼ˆå®ƒæ›´å®¹æ˜“åˆ†æï¼‰ã€‚æ‰¾åˆ°é¢‘ç‡å˜æ¢åï¼Œå°±å¯ä»¥æ‰¾åˆ°å¹…åº¦è°±ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">f &#x3D; np.fft.fft2(img)</span><br><span class="line">fshift &#x3D; np.fft.fftshift(f)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(np.abs(fshift))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Result look like below:<br>ç»“æœçœ‹èµ·æ¥åƒä¸‹é¢è¿™æ ·:<br><img src="http://qiniu.aihubs.net/fft1.jpg" alt></p>
<p>çœ‹ï¼Œæ‚¨å¯ä»¥åœ¨ä¸­å¿ƒçœ‹åˆ°æ›´å¤šç™½è‰²åŒºåŸŸï¼Œè¿™è¡¨æ˜ä½é¢‘å†…å®¹æ›´å¤šã€‚</p>
<p>å› æ­¤ï¼Œæ‚¨å‘ç°äº†é¢‘ç‡å˜æ¢ç°åœ¨ï¼Œæ‚¨å¯ä»¥åœ¨é¢‘åŸŸä¸­è¿›è¡Œä¸€äº›æ“ä½œï¼Œä¾‹å¦‚é«˜é€šæ»¤æ³¢å’Œé‡å»ºå›¾åƒï¼Œå³æ‰¾åˆ°é€†DFTã€‚</p>
<p>ä¸ºæ­¤ï¼Œæ‚¨åªéœ€ç”¨å°ºå¯¸ä¸º60x60çš„çŸ©å½¢çª—å£é®ç½©å³å¯æ¶ˆé™¤ä½é¢‘ã€‚</p>
<p>ç„¶åï¼Œä½¿ç”¨<strong>np.fft.ifftshift</strong>()åº”ç”¨åå‘ç§»ä½ï¼Œä»¥ä½¿DCåˆ†é‡å†æ¬¡å‡ºç°åœ¨å·¦ä¸Šè§’ã€‚</p>
<p>ç„¶åä½¿ç”¨<strong>np.ifft2</strong>()å‡½æ•°æ‰¾åˆ°é€†FFTã€‚åŒæ ·ï¼Œç»“æœå°†æ˜¯ä¸€ä¸ªå¤æ•°ã€‚æ‚¨å¯ä»¥é‡‡ç”¨å…¶ç»å¯¹å€¼ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;&#x2F;2 , cols&#x2F;&#x2F;2</span><br><span class="line">fshift[crow-30:crow+31, ccol-30:ccol+31] &#x3D; 0</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; np.fft.ifft2(f_ishift)</span><br><span class="line">img_back &#x3D; np.real(img_back)</span><br><span class="line">plt.subplot(131),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(132),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Image after HPF&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(133),plt.imshow(img_back)</span><br><span class="line">plt.title(&#39;Result in JET&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>ç»“æœçœ‹èµ·æ¥åƒä¸‹é¢è¿™æ ·ï¼š<br><img src="http://qiniu.aihubs.net/fft2.jpg" alt></p>
<p>ç»“æœè¡¨æ˜é«˜é€šæ»¤æ³¢æ˜¯è¾¹ç¼˜æ£€æµ‹æ“ä½œã€‚</p>
<p>è¿™å°±æ˜¯æˆ‘ä»¬åœ¨â€œå›¾åƒæ¸å˜â€ä¸€ç« ä¸­çœ‹åˆ°çš„ã€‚</p>
<p>è¿™ä¹Ÿè¡¨æ˜å¤§å¤šæ•°å›¾åƒæ•°æ®éƒ½å­˜åœ¨äºé¢‘è°±çš„ä½é¢‘åŒºåŸŸã€‚</p>
<p>æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•åœ¨Numpyä¸­æ‰¾åˆ°DFTï¼ŒIDFTç­‰ã€‚</p>
<p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨OpenCVä¸­è¿›è¡Œæ“ä½œã€‚ </p>
<p>å¦‚æœæ‚¨ä»”ç»†è§‚å¯Ÿç»“æœï¼Œå°¤å…¶æ˜¯æœ€åä¸€å¼ JETé¢œè‰²çš„å›¾åƒï¼Œæ‚¨ä¼šçœ‹åˆ°ä¸€äº›ä¼ªåƒï¼ˆæˆ‘ç”¨çº¢è‰²ç®­å¤´æ ‡è®°çš„ä¸€ä¸ªå®ä¾‹ï¼‰ã€‚</p>
<p>å®ƒåœ¨é‚£é‡Œæ˜¾ç¤ºå‡ºä¸€äº›æ³¢çº¹çŠ¶ç»“æ„ï¼Œç§°ä¸º<strong>æŒ¯é“ƒæ•ˆåº”</strong>ã€‚</p>
<p>è¿™æ˜¯ç”±æˆ‘ä»¬ç”¨äºé®ç½©çš„çŸ©å½¢çª—å£å¼•èµ·çš„ã€‚æ­¤æ©ç è½¬æ¢ä¸ºæ­£å¼¦å½¢çŠ¶ï¼Œä»è€Œå¯¼è‡´æ­¤é—®é¢˜ã€‚</p>
<p>å› æ­¤ï¼ŒçŸ©å½¢çª—å£ä¸ç”¨äºè¿‡æ»¤ã€‚æ›´å¥½çš„é€‰æ‹©æ˜¯é«˜æ–¯çª—å£ã€‚</p>
<h2 id="OpenCVä¸­çš„å‚…é‡Œå¶å˜æ¢"><a href="#OpenCVä¸­çš„å‚…é‡Œå¶å˜æ¢" class="headerlink" title="OpenCVä¸­çš„å‚…é‡Œå¶å˜æ¢"></a>OpenCVä¸­çš„å‚…é‡Œå¶å˜æ¢</h2><p>OpenCVä¸ºæ­¤æä¾›äº†<strong>cv.dft</strong>()å’Œ<strong>cv.idft</strong>()å‡½æ•°ã€‚å®ƒè¿”å›ä¸å‰ä¸€ä¸ªç›¸åŒçš„ç»“æœï¼Œä½†æ˜¯æœ‰ä¸¤ä¸ªé€šé“ã€‚</p>
<p>ç¬¬ä¸€ä¸ªé€šé“æ˜¯ç»“æœçš„å®éƒ¨ï¼Œç¬¬äºŒä¸ªé€šé“æ˜¯ç»“æœçš„è™šéƒ¨ã€‚è¾“å…¥å›¾åƒé¦–å…ˆåº”è½¬æ¢ä¸ºnp.float32ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹æ€ä¹ˆåšã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">dft &#x3D; cv.dft(np.float32(img),flags &#x3D; cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">dft_shift &#x3D; np.fft.fftshift(dft)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(cv.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>æ³¨æ„ æ‚¨è¿˜å¯ä»¥ä½¿ç”¨<strong>cv.cartToPolar</strong>()ï¼Œå®ƒåœ¨å•ä¸ªé•œå¤´ä¸­åŒæ—¶è¿”å›å¹…å€¼å’Œç›¸ä½</p>
<p>ç°åœ¨æˆ‘ä»¬è¦åšDFTçš„é€†å˜æ¢ã€‚åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªHPFï¼Œè¿™æ¬¡æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åˆ é™¤å›¾åƒä¸­çš„é«˜é¢‘å†…å®¹ï¼Œå³æˆ‘ä»¬å°†LPFåº”ç”¨åˆ°å›¾åƒä¸­ã€‚</p>
<p>å®ƒå®é™…ä¸Šæ¨¡ç³Šäº†å›¾åƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªé«˜å€¼(1)åœ¨ä½é¢‘éƒ¨åˆ†ï¼Œå³æˆ‘ä»¬è¿‡æ»¤ä½é¢‘å†…å®¹ï¼Œ0åœ¨é«˜é¢‘åŒºã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;2 , cols&#x2F;2</span><br><span class="line"># é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ©ç ï¼Œä¸­å¿ƒæ­£æ–¹å½¢ä¸º1ï¼Œå…¶ä½™å…¨ä¸ºé›¶</span><br><span class="line">mask &#x3D; np.zeros((rows,cols,2),np.uint8)</span><br><span class="line">mask[crow-30:crow+30, ccol-30:ccol+30] &#x3D; 1</span><br><span class="line"># åº”ç”¨æ©ç å’Œé€†DFT</span><br><span class="line">fshift &#x3D; dft_shift*mask</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; cv.idft(f_ishift)</span><br><span class="line">img_back &#x3D; cv.magnitude(img_back[:,:,0],img_back[:,:,1])</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/fft4.jpg" alt></p>
<p>æ³¨æ„ é€šå¸¸ï¼ŒOpenCVå‡½æ•°<strong>cv.dft</strong>()å’Œ<strong>cv.idft</strong>()æ¯”Numpyå‡½æ•°æ›´å¿«ã€‚ä½†æ˜¯Numpyå‡½æ•°æ›´å®¹æ˜“ä½¿ç”¨ã€‚æœ‰å…³æ€§èƒ½é—®é¢˜çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è§ä¸‹é¢çš„éƒ¨åˆ†ã€‚</p>
<h2 id="DFTçš„æ€§èƒ½ä¼˜åŒ–"><a href="#DFTçš„æ€§èƒ½ä¼˜åŒ–" class="headerlink" title="DFTçš„æ€§èƒ½ä¼˜åŒ–"></a>DFTçš„æ€§èƒ½ä¼˜åŒ–</h2><p>å¯¹äºæŸäº›æ•°ç»„å°ºå¯¸ï¼ŒDFTçš„è®¡ç®—æ€§èƒ½è¾ƒå¥½ã€‚å½“æ•°ç»„å¤§å°ä¸º2çš„å¹‚æ—¶ï¼Œé€Ÿåº¦æœ€å¿«ã€‚</p>
<p>å¯¹äºå¤§å°ä¸º2ã€3å’Œ5çš„ä¹˜ç§¯çš„æ•°ç»„ï¼Œä¹Ÿå¯ä»¥éå¸¸æœ‰æ•ˆåœ°è¿›è¡Œå¤„ç†ã€‚</p>
<p>å› æ­¤ï¼Œå¦‚æœæ‚¨æ‹…å¿ƒä»£ç çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨æ‰¾åˆ°DFTä¹‹å‰å°†æ•°ç»„çš„å¤§å°ä¿®æ”¹ä¸ºä»»ä½•æœ€ä½³å¤§å°(é€šè¿‡å¡«å……é›¶)ã€‚å¯¹</p>
<p>äºOpenCVï¼Œæ‚¨å¿…é¡»æ‰‹åŠ¨å¡«å……é›¶ã€‚ä½†æ˜¯å¯¹äºNumpyï¼Œæ‚¨æŒ‡å®šFFTè®¡ç®—çš„æ–°å¤§å°ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨å¡«å……é›¶ã€‚</p>
<p>é‚£ä¹ˆå¦‚ä½•æ‰¾åˆ°æœ€ä¼˜çš„å¤§å°å‘¢?OpenCVä¸ºæ­¤æä¾›äº†ä¸€ä¸ªå‡½æ•°ï¼Œcv.getOptimalDFTSize()ã€‚å®ƒåŒæ—¶é€‚ç”¨äº<strong>cv.dft</strong>()å’Œ<strong>np.fft.fft2</strong>()ã€‚è®©æˆ‘ä»¬ä½¿ç”¨IPythoné­”æœ¯å‘½ä»¤timeitæ¥æ£€æŸ¥å®ƒä»¬çš„æ€§èƒ½ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [16]: img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">In [17]: rows,cols &#x3D; img.shape</span><br><span class="line">In [18]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(rows,cols))</span><br><span class="line">342 548</span><br><span class="line">In [19]: nrows &#x3D; cv.getOptimalDFTSize(rows)</span><br><span class="line">In [20]: ncols &#x3D; cv.getOptimalDFTSize(cols)</span><br><span class="line">In [21]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(nrows,ncols))</span><br><span class="line">360 576</span><br></pre></td></tr></table></figure>
<p>å‚è§ï¼Œå°†å¤§å°(342,548)ä¿®æ”¹ä¸º(360ï¼Œ576)ã€‚ç°åœ¨è®©æˆ‘ä»¬ç”¨é›¶å¡«å……ï¼ˆå¯¹äºOpenCVï¼‰ï¼Œå¹¶æ‰¾åˆ°å…¶DFTè®¡ç®—æ€§èƒ½ã€‚æ‚¨å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªæ–°çš„é›¶æ•°ç»„å¹¶å°†æ•°æ®å¤åˆ¶åˆ°å…¶ä¸­æ¥å®Œæˆæ­¤æ“ä½œï¼Œæˆ–è€…ä½¿ç”¨<strong>cv.copyMakeBorder</strong>()ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nimg &#x3D; np.zeros((nrows,ncols))</span><br><span class="line">nimg[:rows,:cols] &#x3D; img</span><br></pre></td></tr></table></figure>
<p>æˆ–è€…:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">right &#x3D; ncols - cols</span><br><span class="line">bottom &#x3D; nrows - rows</span><br><span class="line">bordertype &#x3D; cv.BORDER_CONSTANT ï¼ƒåªæ˜¯ä¸ºäº†é¿å…PDFæ–‡ä»¶ä¸­çš„è¡Œä¸­æ–­</span><br><span class="line">nimg &#x3D; cv.copyMakeBorder(img,0,bottom,0,right,bordertype, value &#x3D; 0)</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬è®¡ç®—Numpyå‡½æ•°çš„DFTæ€§èƒ½æ¯”è¾ƒï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [22]: %timeit fft1 &#x3D; np.fft.fft2(img)</span><br><span class="line">10 loops, best of 3: 40.9 ms per loop</span><br><span class="line">In [23]: %timeit fft2 &#x3D; np.fft.fft2(img,[nrows,ncols])</span><br><span class="line">100 loops, best of 3: 10.4 ms per loop</span><br></pre></td></tr></table></figure>
<p>å®ƒæ˜¾ç¤ºäº†4å€çš„åŠ é€Ÿã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†å°è¯•ä½¿ç”¨OpenCVå‡½æ•°ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [24]: %timeit dft1&#x3D; cv.dft(np.float32(img),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 13.5 ms per loop</span><br><span class="line">In [27]: %timeit dft2&#x3D; cv.dft(np.float32(nimg),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 3.11 ms per loop</span><br></pre></td></tr></table></figure>
<p>å®ƒè¿˜æ˜¾ç¤ºäº†4å€çš„åŠ é€Ÿã€‚æ‚¨è¿˜å¯ä»¥çœ‹åˆ°OpenCVå‡½æ•°æ¯”Numpyå‡½æ•°å¿«3å€å·¦å³ã€‚ä¹Ÿå¯ä»¥å¯¹é€†FFTè¿›è¡Œæµ‹è¯•ï¼Œè¿™ç•™ç»™æ‚¨ç»ƒä¹ ã€‚</p>
<h2 id="ä¸ºä»€ä¹ˆæ‹‰æ™®æ‹‰æ–¯ç®—å­æ˜¯é«˜é€šæ»¤æ³¢å™¨ï¼Ÿ"><a href="#ä¸ºä»€ä¹ˆæ‹‰æ™®æ‹‰æ–¯ç®—å­æ˜¯é«˜é€šæ»¤æ³¢å™¨ï¼Ÿ" class="headerlink" title="ä¸ºä»€ä¹ˆæ‹‰æ™®æ‹‰æ–¯ç®—å­æ˜¯é«˜é€šæ»¤æ³¢å™¨ï¼Ÿ"></a>ä¸ºä»€ä¹ˆæ‹‰æ™®æ‹‰æ–¯ç®—å­æ˜¯é«˜é€šæ»¤æ³¢å™¨ï¼Ÿ</h2><p>åœ¨ä¸€ä¸ªè®ºå›ä¸Šä¹Ÿæœ‰äººæå‡ºäº†ç±»ä¼¼çš„é—®é¢˜ã€‚é—®é¢˜æ˜¯ï¼Œä¸ºä»€ä¹ˆæ‹‰æ™®æ‹‰æ–¯å˜æ¢æ˜¯é«˜é€šæ»¤æ³¢å™¨?</p>
<p>ä¸ºä»€ä¹ˆSobelæ˜¯HPF?ç­‰ã€‚ç¬¬ä¸€ä¸ªç­”æ¡ˆæ˜¯å…³äºå‚…é‡Œå¶å˜æ¢çš„ã€‚å¯¹äºæ›´å¤§çš„FFTåªéœ€è¦æ‹‰æ™®æ‹‰æ–¯å˜æ¢ã€‚åˆ†æä¸‹é¢çš„ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"># æ²¡æœ‰ç¼©æ”¾å‚æ•°çš„ç®€å•å‡å€¼æ»¤æ³¢å™¨</span><br><span class="line">mean_filter &#x3D; np.ones((3,3))</span><br><span class="line"># åˆ›å»ºé«˜æ–¯æ»¤æ³¢å™¨</span><br><span class="line">x &#x3D; cv.getGaussianKernel(5,10)</span><br><span class="line">gaussian &#x3D; x*x.T</span><br><span class="line"># ä¸åŒçš„è¾¹ç¼˜æ£€æµ‹æ»¤æ³¢å™¨</span><br><span class="line"># xæ–¹å‘ä¸Šçš„scharr</span><br><span class="line">scharr &#x3D; np.array([[-3, 0, 3],</span><br><span class="line">                   [-10,0,10],</span><br><span class="line">                   [-3, 0, 3]])</span><br><span class="line"># xæ–¹å‘ä¸Šçš„sobel</span><br><span class="line">sobel_x&#x3D; np.array([[-1, 0, 1],</span><br><span class="line">                   [-2, 0, 2],</span><br><span class="line">                   [-1, 0, 1]])</span><br><span class="line"># yæ–¹å‘ä¸Šçš„sobel</span><br><span class="line">sobel_y&#x3D; np.array([[-1,-2,-1],</span><br><span class="line">                   [0, 0, 0],</span><br><span class="line">                   [1, 2, 1]])</span><br><span class="line"># æ‹‰æ™®æ‹‰æ–¯å˜æ¢</span><br><span class="line">laplacian&#x3D;np.array([[0, 1, 0],</span><br><span class="line">                    [1,-4, 1],</span><br><span class="line">                    [0, 1, 0]])</span><br><span class="line">filters &#x3D; [mean_filter, gaussian, laplacian, sobel_x, sobel_y, scharr]</span><br><span class="line">filter_name &#x3D; [&#39;mean_filter&#39;, &#39;gaussian&#39;,&#39;laplacian&#39;, &#39;sobel_x&#39;, \</span><br><span class="line">                &#39;sobel_y&#39;, &#39;scharr_x&#39;]</span><br><span class="line">fft_filters &#x3D; [np.fft.fft2(x) for x in filters]</span><br><span class="line">fft_shift &#x3D; [np.fft.fftshift(y) for y in fft_filters]</span><br><span class="line">mag_spectrum &#x3D; [np.log(np.abs(z)+1) for z in fft_shift]</span><br><span class="line">for i in range(6):</span><br><span class="line">    plt.subplot(2,3,i+1),plt.imshow(mag_spectrum[i],cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(filter_name[i]), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>çœ‹çœ‹ç»“æœï¼š<br><img src="http://qiniu.aihubs.net/fft5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="æ¨¡æ¿åŒ¹é…"><a href="#æ¨¡æ¿åŒ¹é…" class="headerlink" title="æ¨¡æ¿åŒ¹é…"></a><span id="header2">æ¨¡æ¿åŒ¹é…</span></h1><h2 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œæ‚¨å°†å­¦ä¹  - ä½¿ç”¨æ¨¡æ¿åŒ¹é…åœ¨å›¾åƒä¸­æŸ¥æ‰¾å¯¹è±¡ - ä½ å°†çœ‹åˆ°ä»¥ä¸‹åŠŸèƒ½ï¼šcv.matchTemplate()ï¼Œcv.minMaxLoc()</p>
<h2 id="ç†è®º-1"><a href="#ç†è®º-1" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>æ¨¡æ¿åŒ¹é…æ˜¯ä¸€ç§ç”¨äºåœ¨è¾ƒå¤§å›¾åƒä¸­æœç´¢å’ŒæŸ¥æ‰¾æ¨¡æ¿å›¾åƒä½ç½®çš„æ–¹æ³•ã€‚</p>
<p>ä¸ºæ­¤ï¼ŒOpenCVå¸¦æœ‰ä¸€ä¸ªå‡½æ•°<strong>cv.matchTemplate</strong>()ã€‚ </p>
<p>å®ƒåªæ˜¯å°†æ¨¡æ¿å›¾â€‹â€‹åƒæ»‘åŠ¨åˆ°è¾“å…¥å›¾åƒä¸Šï¼ˆå°±åƒåœ¨2Då·ç§¯ä¸­ä¸€æ ·ï¼‰ï¼Œç„¶ååœ¨æ¨¡æ¿å›¾åƒä¸‹æ¯”è¾ƒæ¨¡æ¿å’Œè¾“å…¥å›¾åƒçš„æ‹¼å›¾ã€‚ </p>
<p>OpenCVä¸­å®ç°äº†å‡ ç§æ¯”è¾ƒæ–¹æ³•ã€‚ï¼ˆæ‚¨å¯ä»¥æ£€æŸ¥æ–‡æ¡£ä»¥äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼‰ã€‚å®ƒè¿”å›ä¸€ä¸ªç°åº¦å›¾åƒï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¡¨ç¤ºè¯¥åƒç´ çš„é‚»åŸŸä¸æ¨¡æ¿åŒ¹é…çš„ç¨‹åº¦ã€‚</p>
<p>å¦‚æœè¾“å…¥å›¾åƒçš„å¤§å°ä¸º(WxH)ï¼Œè€Œæ¨¡æ¿å›¾åƒçš„å¤§å°ä¸º(wxh)ï¼Œåˆ™è¾“å‡ºå›¾åƒçš„å¤§å°å°†ä¸º(W-w + 1ï¼ŒH-h + 1)ã€‚å¾—åˆ°ç»“æœåï¼Œå¯ä»¥ä½¿ç”¨<strong>cv.minMaxLoc</strong>()å‡½æ•°æŸ¥æ‰¾æœ€å¤§/æœ€å°å€¼åœ¨å“ªã€‚å°†å…¶ä½œä¸ºçŸ©å½¢çš„å·¦ä¸Šè§’ï¼Œå¹¶ä»¥(wï¼Œh)ä½œä¸ºçŸ©å½¢çš„å®½åº¦å’Œé«˜åº¦ã€‚è¯¥çŸ©å½¢æ˜¯æ‚¨æ¨¡æ¿çš„åŒºåŸŸã€‚</p>
<p>æ³¨æ„ å¦‚æœä½¿ç”¨<strong>cv.TM_SQDIFF</strong>ä½œä¸ºæ¯”è¾ƒæ–¹æ³•ï¼Œåˆ™æœ€å°å€¼æä¾›æœ€ä½³åŒ¹é…ã€‚</p>
<h2 id="OpenCVä¸­çš„æ¨¡æ¿åŒ¹é…"><a href="#OpenCVä¸­çš„æ¨¡æ¿åŒ¹é…" class="headerlink" title="OpenCVä¸­çš„æ¨¡æ¿åŒ¹é…"></a>OpenCVä¸­çš„æ¨¡æ¿åŒ¹é…</h2><p>ä½œä¸ºç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†åœ¨æ¢…è¥¿çš„ç…§ç‰‡ä¸­æœç´¢ä»–çš„è„¸ã€‚æ‰€ä»¥æˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ¨¡æ¿ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š <img src="http://qiniu.aihubs.net/messi_face.jpg" alt> æˆ‘ä»¬å°†å°è¯•æ‰€æœ‰æ¯”è¾ƒæ–¹æ³•ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬çš„ç»“æœå¦‚ä½•ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">img2 &#x3D; img.copy()</span><br><span class="line">template &#x3D; cv.imread(&#39;template.jpg&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line"># åˆ—è¡¨ä¸­æ‰€æœ‰çš„6ç§æ¯”è¾ƒæ–¹æ³•</span><br><span class="line">methods &#x3D; [&#39;cv.TM_CCOEFF&#39;, &#39;cv.TM_CCOEFF_NORMED&#39;, &#39;cv.TM_CCORR&#39;,</span><br><span class="line">            &#39;cv.TM_CCORR_NORMED&#39;, &#39;cv.TM_SQDIFF&#39;, &#39;cv.TM_SQDIFF_NORMED&#39;]</span><br><span class="line">for meth in methods:</span><br><span class="line">    img &#x3D; img2.copy()</span><br><span class="line">    method &#x3D; eval(meth)</span><br><span class="line">    # åº”ç”¨æ¨¡æ¿åŒ¹é…</span><br><span class="line">    res &#x3D; cv.matchTemplate(img,template,method)</span><br><span class="line">    min_val, max_val, min_loc, max_loc &#x3D; cv.minMaxLoc(res)</span><br><span class="line">    # å¦‚æœæ–¹æ³•æ˜¯TM_SQDIFFæˆ–TM_SQDIFF_NORMEDï¼Œåˆ™å–æœ€å°å€¼</span><br><span class="line">    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:</span><br><span class="line">        top_left &#x3D; min_loc</span><br><span class="line">    else:</span><br><span class="line">        top_left &#x3D; max_loc</span><br><span class="line">    bottom_right &#x3D; (top_left[0] + w, top_left[1] + h)</span><br><span class="line">    cv.rectangle(img,top_left, bottom_right, 255, 2)</span><br><span class="line">    plt.subplot(121),plt.imshow(res,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Matching Result&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(122),plt.imshow(img,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Detected Point&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.suptitle(meth)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>æŸ¥çœ‹ä»¥ä¸‹ç»“æœï¼š</p>
<ul>
<li>cv.TM_CCOEFF<img src="http://qiniu.aihubs.net/template_ccoeff_1.jpg" alt></li>
<li>cv.TM_CCOEFF_NORMED<img src="http://qiniu.aihubs.net/template_ccoeffn_2.jpg" alt></li>
<li>cv.TM_CCORR<img src="http://qiniu.aihubs.net/template_ccorr_3.jpg" alt></li>
<li>cv.TM_CCORR_NORMED<img src="http://qiniu.aihubs.net/template_ccorrn_4.jpg" alt></li>
<li>cv.TM_SQDIFF<img src="http://qiniu.aihubs.net/template_sqdiff_5.jpg" alt></li>
<li>cv.TM_SQDIFF_NORMED<img src="http://qiniu.aihubs.net/template_sqdiffn_6.jpg" alt><br>ä½¿ç”¨<strong>cv.TM_CCORR</strong>çš„ç»“æœå¹¶ä¸ç†æƒ³ã€‚</li>
</ul>
<h2 id="å¤šå¯¹è±¡çš„æ¨¡æ¿åŒ¹é…"><a href="#å¤šå¯¹è±¡çš„æ¨¡æ¿åŒ¹é…" class="headerlink" title="å¤šå¯¹è±¡çš„æ¨¡æ¿åŒ¹é…"></a>å¤šå¯¹è±¡çš„æ¨¡æ¿åŒ¹é…</h2><p>åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬åœ¨å›¾åƒä¸­æœç´¢äº†æ¢…è¥¿çš„è„¸ï¼Œè¯¥è„¸åœ¨å›¾åƒä¸­ä»…å‡ºç°ä¸€æ¬¡ã€‚</p>
<p>å‡è®¾æ‚¨æ­£åœ¨æœç´¢å…·æœ‰å¤šæ¬¡å‡ºç°çš„å¯¹è±¡ï¼Œåˆ™<strong>cv.minMaxLoc</strong>()ä¸ä¼šä¸ºæ‚¨æä¾›æ‰€æœ‰ä½ç½®ã€‚</p>
<p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é˜ˆå€¼åŒ–ã€‚å› æ­¤ï¼Œåœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‘—åæ¸¸æˆ<strong>Mario</strong>çš„å±å¹•æˆªå›¾ï¼Œå¹¶åœ¨å…¶ä¸­æ‰¾åˆ°ç¡¬å¸ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img_rgb &#x3D; cv.imread(&#39;mario.png&#39;)</span><br><span class="line">img_gray &#x3D; cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)</span><br><span class="line">template &#x3D; cv.imread(&#39;mario_coin.png&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line">res &#x3D; cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)</span><br><span class="line">threshold &#x3D; 0.8</span><br><span class="line">loc &#x3D; np.where( res &gt;&#x3D; threshold)</span><br><span class="line">for pt in zip(*loc[::-1]):</span><br><span class="line">    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)</span><br><span class="line">cv.imwrite(&#39;res.png&#39;,img_rgb)</span><br></pre></td></tr></table></figure>
<p>ç»“æœ:<br><img src="http://qiniu.aihubs.net/res_mario.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="éœå¤«çº¿å˜æ¢"><a href="#éœå¤«çº¿å˜æ¢" class="headerlink" title="éœå¤«çº¿å˜æ¢"></a><span id="header3">éœå¤«çº¿å˜æ¢</span></h1><h2 id="ç›®æ ‡-2"><a href="#ç›®æ ‡-2" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨è¿™ä¸€ç« å½“ä¸­ï¼Œ </p>
<ul>
<li>æˆ‘ä»¬å°†äº†è§£éœå¤«å˜æ¢çš„æ¦‚å¿µã€‚ </li>
<li>æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨å®ƒæ¥æ£€æµ‹å›¾åƒä¸­çš„çº¿æ¡ã€‚ </li>
<li>æˆ‘ä»¬å°†çœ‹åˆ°ä»¥ä¸‹å‡½æ•°ï¼šcv.HoughLines()ï¼Œcv.HoughLinesP()</li>
</ul>
<h2 id="ç†è®º-2"><a href="#ç†è®º-2" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>å¦‚æœå¯ä»¥ç”¨æ•°å­¦å½¢å¼è¡¨ç¤ºå½¢çŠ¶ï¼Œåˆ™éœå¤«å˜æ¢æ˜¯ä¸€ç§æ£€æµ‹ä»»ä½•å½¢çŠ¶çš„æµè¡ŒæŠ€æœ¯ã€‚</p>
<p>å³ä½¿å½¢çŠ¶æœ‰äº›ç ´æŸæˆ–å˜å½¢ï¼Œä¹Ÿå¯ä»¥æ£€æµ‹å‡ºå½¢çŠ¶ã€‚æˆ‘ä»¬å°†çœ‹åˆ°å®ƒå¦‚ä½•ä½œç”¨äºä¸€æ¡çº¿ã€‚</p>
<p>ä¸€æ¡çº¿å¯ä»¥è¡¨ç¤ºä¸ºy=mx+cæˆ–ä»¥å‚æ•°å½¢å¼è¡¨ç¤ºä¸ºÏ=xcosÎ¸+ysinÎ¸ï¼Œ</p>
<p>å…¶ä¸­Ïæ˜¯ä»åŸç‚¹åˆ°è¯¥çº¿çš„å‚ç›´è·ç¦»ï¼Œ</p>
<p>è€ŒÎ¸æ˜¯ç”±è¯¥å‚ç›´çº¿å’Œæ°´å¹³è½´å½¢æˆçš„è§’åº¦ä»¥é€†æ—¶é’ˆæ–¹å‘æµ‹é‡ï¼ˆè¯¥æ–¹å‘éšæ‚¨å¦‚ä½•è¡¨ç¤ºåæ ‡ç³»è€Œå˜åŒ–ã€‚æ­¤è¡¨ç¤ºå½¢å¼åœ¨OpenCVä¸­ä½¿ç”¨ï¼‰ã€‚</p>
<p>æŸ¥çœ‹ä¸‹é¢çš„å›¾ç‰‡ï¼š<br><img src="http://qiniu.aihubs.net/1.png" alt></p>
<p>å› æ­¤ï¼Œå¦‚æœçº¿åœ¨åŸç‚¹ä¸‹æ–¹é€šè¿‡ï¼Œåˆ™å®ƒå°†å…·æœ‰æ­£çš„Ïä¸”è§’åº¦å°äº180ã€‚å¦‚æœçº¿åœ¨åŸç‚¹ä¸Šæ–¹ï¼Œåˆ™å°†è§’åº¦å–ä¸ºå°äº180ï¼Œè€Œä¸æ˜¯å¤§äº180çš„è§’åº¦ã€‚</p>
<p>Ïå–è´Ÿå€¼ã€‚ä»»ä½•å‚ç›´çº¿å°†å…·æœ‰0åº¦ï¼Œæ°´å¹³çº¿å°†å…·æœ‰90åº¦ã€‚</p>
<p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹éœå¤«å˜æ¢å¦‚ä½•å¤„ç†çº¿æ¡ã€‚ä»»ä½•ä¸€æ¡çº¿éƒ½å¯ä»¥ç”¨(Ïï¼ŒÎ¸)è¿™ä¸¤ä¸ªæœ¯è¯­è¡¨ç¤ºã€‚</p>
<p>å› æ­¤ï¼Œé¦–å…ˆåˆ›å»º2Dæ•°ç»„æˆ–ç´¯åŠ å™¨ï¼ˆä»¥ä¿å­˜ä¸¤ä¸ªå‚æ•°çš„å€¼ï¼‰ï¼Œå¹¶å°†å…¶åˆå§‹è®¾ç½®ä¸º0ã€‚</p>
<p>è®©è¡Œè¡¨ç¤ºÏï¼Œåˆ—è¡¨ç¤ºÎ¸ã€‚é˜µåˆ—çš„å¤§å°å–å†³äºæ‰€éœ€çš„ç²¾åº¦ã€‚å‡è®¾æ‚¨å¸Œæœ›è§’åº¦çš„ç²¾åº¦ä¸º1åº¦ï¼Œåˆ™éœ€è¦180åˆ—ã€‚</p>
<p>å¯¹äºÏï¼Œæœ€å¤§è·ç¦»å¯èƒ½æ˜¯å›¾åƒçš„å¯¹è§’çº¿é•¿åº¦ã€‚å› æ­¤ï¼Œä»¥ä¸€ä¸ªåƒç´ ç²¾åº¦ä¸ºå‡†ï¼Œè¡Œæ•°å¯ä»¥æ˜¯å›¾åƒçš„å¯¹è§’çº¿é•¿åº¦ã€‚</p>
<p>è€ƒè™‘ä¸€ä¸ª100x100çš„å›¾åƒï¼Œä¸­é—´æœ‰ä¸€æ¡æ°´å¹³çº¿ã€‚</p>
<p>å–ç›´çº¿çš„ç¬¬ä¸€ç‚¹ã€‚æ‚¨çŸ¥é“å®ƒçš„(xï¼Œy)å€¼ã€‚</p>
<p>ç°åœ¨åœ¨çº¿æ€§æ–¹ç¨‹å¼ä¸­ï¼Œå°†å€¼Î¸= 0,1,2ï¼Œâ€¦.. 180æ”¾è¿›å»ï¼Œç„¶åæ£€æŸ¥å¾—åˆ°Ïã€‚</p>
<p>å¯¹äºæ¯å¯¹(Ïï¼ŒÎ¸)ï¼Œåœ¨ç´¯åŠ å™¨ä¸­å¯¹åº”çš„(Ïï¼ŒÎ¸)å•å…ƒæ ¼å°†å€¼å¢åŠ 1ã€‚æ‰€ä»¥ç°åœ¨åœ¨ç´¯åŠ å™¨ä¸­ï¼Œå•å…ƒæ ¼(50,90)= 1ä»¥åŠå…¶ä»–ä¸€äº›å•å…ƒæ ¼ã€‚</p>
<p>ç°åœ¨ï¼Œå¯¹è¡Œçš„ç¬¬äºŒä¸ªç‚¹ã€‚æ‰§è¡Œä¸ä¸Šè¿°ç›¸åŒçš„æ“ä½œã€‚é€’å¢(Ïï¼ŒÎ¸)å¯¹åº”çš„å•å…ƒæ ¼ä¸­çš„å€¼ã€‚</p>
<p>è¿™æ¬¡ï¼Œå•å…ƒæ ¼(50,90)=2ã€‚å®é™…ä¸Šï¼Œæ‚¨æ­£åœ¨å¯¹(Ïï¼ŒÎ¸)å€¼è¿›è¡ŒæŠ•ç¥¨ã€‚</p>
<p>æ‚¨å¯¹çº¿è·¯ä¸Šçš„æ¯ä¸ªç‚¹éƒ½ç»§ç»­æ‰§è¡Œæ­¤è¿‡ç¨‹ã€‚</p>
<p>åœ¨æ¯ä¸ªç‚¹ä¸Šï¼Œå•å…ƒæ ¼(50,90)éƒ½ä¼šå¢åŠ æˆ–æŠ•ç¥¨ï¼Œè€Œå…¶ä»–å•å…ƒæ ¼å¯èƒ½ä¼šæˆ–å¯èƒ½ä¸ä¼šæŠ•ç¥¨ã€‚</p>
<p>è¿™æ ·ä¸€æ¥ï¼Œæœ€åï¼Œå•å…ƒæ ¼(50,90)çš„æŠ•ç¥¨æ•°å°†æœ€é«˜ã€‚</p>
<p>å› æ­¤ï¼Œå¦‚æœæ‚¨åœ¨ç´¯åŠ å™¨ä¸­æœç´¢æœ€å¤§ç¥¨æ•°ï¼Œåˆ™å°†è·å¾—(50,90)å€¼ï¼Œè¯¥å€¼è¡¨ç¤ºè¯¥å›¾åƒä¸­çš„ä¸€æ¡çº¿ä¸åŸç‚¹çš„è·ç¦»ä¸º50ï¼Œè§’åº¦ä¸º90åº¦ã€‚</p>
<p>åœ¨ä¸‹é¢çš„åŠ¨ç”»ä¸­å¾ˆå¥½åœ°æ˜¾ç¤ºäº†è¯¥å›¾ç‰‡(å›¾ç‰‡æä¾›ï¼šAmos Storkey)<br><img src="http://qiniu.aihubs.net/houghlinesdemo.gif" alt></p>
<p>è¿™å°±æ˜¯éœå¤«å˜æ¢å¯¹çº¿æ¡çš„å·¥ä½œæ–¹å¼ã€‚å®ƒå¾ˆç®€å•ï¼Œä¹Ÿè®¸æ‚¨å¯â€‹â€‹ä»¥è‡ªå·±ä½¿ç”¨Numpyæ¥å®ç°å®ƒã€‚</p>
<p>ä¸‹å›¾æ˜¾ç¤ºäº†ç´¯åŠ å™¨ã€‚æŸäº›ä½ç½®çš„äº®ç‚¹è¡¨ç¤ºå®ƒä»¬æ˜¯å›¾åƒä¸­å¯èƒ½çš„çº¿æ¡çš„å‚æ•°ã€‚</p>
<p><img src="http://qiniu.aihubs.net/houghlines2.jpg" alt></p>
<h2 id="OpenCVä¸­çš„éœå¤«æ›¼å˜æ¢"><a href="#OpenCVä¸­çš„éœå¤«æ›¼å˜æ¢" class="headerlink" title="OpenCVä¸­çš„éœå¤«æ›¼å˜æ¢"></a>OpenCVä¸­çš„éœå¤«æ›¼å˜æ¢</h2><p>ä¸Šé¢è¯´æ˜çš„æ‰€æœ‰å†…å®¹éƒ½å°è£…åœ¨OpenCVå‡½æ•°<strong>cv.HoughLines</strong>()ä¸­ã€‚</p>
<p>å®ƒåªæ˜¯è¿”å›ä¸€ä¸ªï¼šmath:(rhoï¼Œtheta)å€¼çš„æ•°ç»„ã€‚Ïä»¥åƒç´ ä¸ºå•ä½ï¼ŒÎ¸ä»¥å¼§åº¦ä¸ºå•ä½ã€‚</p>
<p>ç¬¬ä¸€ä¸ªå‚æ•°ï¼Œè¾“å…¥å›¾åƒåº”è¯¥æ˜¯äºŒè¿›åˆ¶å›¾åƒï¼Œå› æ­¤åœ¨åº”ç”¨éœå¤«å˜æ¢ä¹‹å‰ï¼Œè¯·åº”ç”¨é˜ˆå€¼æˆ–ä½¿ç”¨Cannyè¾¹ç¼˜æ£€æµ‹ã€‚</p>
<p>ç¬¬äºŒå’Œç¬¬ä¸‰å‚æ•°åˆ†åˆ«æ˜¯Ïå’ŒÎ¸ç²¾åº¦ã€‚</p>
<p>ç¬¬å››ä¸ªå‚æ•°æ˜¯é˜ˆå€¼ï¼Œè¿™æ„å‘³ç€åº”è¯¥å°†å…¶è§†ä¸ºè¡Œçš„æœ€ä½æŠ•ç¥¨ã€‚</p>
<p>è¯·è®°ä½ï¼Œç¥¨æ•°å–å†³äºçº¿ä¸Šçš„ç‚¹æ•°ã€‚å› æ­¤ï¼Œå®ƒè¡¨ç¤ºåº”æ£€æµ‹åˆ°çš„æœ€å°çº¿é•¿ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLines(edges,1,np.pi&#x2F;180,200)</span><br><span class="line">for line in lines:</span><br><span class="line">    rho,theta &#x3D; line[0]</span><br><span class="line">    a &#x3D; np.cos(theta)</span><br><span class="line">    b &#x3D; np.sin(theta)</span><br><span class="line">    x0 &#x3D; a*rho</span><br><span class="line">    y0 &#x3D; b*rho</span><br><span class="line">    x1 &#x3D; int(x0 + 1000*(-b))</span><br><span class="line">    y1 &#x3D; int(y0 + 1000*(a))</span><br><span class="line">    x2 &#x3D; int(x0 - 1000*(-b))</span><br><span class="line">    y2 &#x3D; int(y0 - 1000*(a))</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,0,255),2)</span><br><span class="line">cv.imwrite(&#39;houghlines3.jpg&#39;,img)</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.aihubs.net/houghlines3.jpg" alt></p>
<h2 id="æ¦‚ç‡éœå¤«å˜æ¢"><a href="#æ¦‚ç‡éœå¤«å˜æ¢" class="headerlink" title="æ¦‚ç‡éœå¤«å˜æ¢"></a>æ¦‚ç‡éœå¤«å˜æ¢</h2><p>åœ¨éœå¤«å˜æ¢ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ï¼Œå³ä½¿å¯¹äºå¸¦æœ‰ä¸¤ä¸ªå‚æ•°çš„è¡Œï¼Œä¹Ÿéœ€è¦å¤§é‡è®¡ç®—ã€‚</p>
<p>æ¦‚ç‡éœå¤«å˜æ¢æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„éœå¤«å˜æ¢çš„ä¼˜åŒ–ã€‚å®ƒæ²¡æœ‰è€ƒè™‘æ‰€æœ‰è¦ç‚¹ã€‚</p>
<p>å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œå®ƒä»…é‡‡ç”¨éšæœºçš„ç‚¹å­é›†ï¼Œè¶³ä»¥è¿›è¡Œçº¿æ£€æµ‹ã€‚</p>
<p>åªæ˜¯æˆ‘ä»¬å¿…é¡»é™ä½é˜ˆå€¼ã€‚å‚è§ä¸‹å›¾ï¼Œæ¯”è¾ƒäº†éœå¤«ç©ºé—´ä¸­çš„éœå¤«å˜æ¢å’Œæ¦‚ç‡éœå¤«å˜æ¢ã€‚<br><img src="http://qiniu.aihubs.net/houghlines4.png" alt></p>
<p>OpenCVçš„å®ç°åŸºäºMatas,J.å’ŒGalambos,C.å’ŒKittler, J.V.ä½¿ç”¨æ¸è¿›æ¦‚ç‡éœå¤«å˜æ¢å¯¹è¡Œè¿›è¡Œçš„ç¨³å¥æ£€æµ‹[145]ã€‚</p>
<p>ä½¿ç”¨çš„å‡½æ•°æ˜¯<strong>cv.HoughLinesP</strong>()ã€‚å®ƒæœ‰ä¸¤ä¸ªæ–°çš„è®ºç‚¹ã€‚ </p>
<ul>
<li>minLineLength - æœ€å°è¡Œé•¿ã€‚å°äºæ­¤é•¿åº¦çš„çº¿æ®µå°†è¢«æ‹’ç»ã€‚ </li>
<li>maxLineGap - çº¿æ®µä¹‹é—´å…è®¸å°†å®ƒä»¬è§†ä¸ºä¸€æ¡çº¿çš„æœ€å¤§é—´éš™ã€‚</li>
</ul>
<p>æœ€å¥½çš„æ˜¯ï¼Œå®ƒç›´æ¥è¿”å›è¡Œçš„ä¸¤ä¸ªç«¯ç‚¹ã€‚åœ¨ä»¥å‰çš„æƒ…å†µä¸‹ï¼Œæ‚¨ä»…è·å¾—çº¿çš„å‚æ•°ï¼Œå¹¶ä¸”å¿…é¡»æ‰¾åˆ°æ‰€æœ‰ç‚¹ã€‚åœ¨è¿™é‡Œï¼Œä¸€åˆ‡éƒ½æ˜¯ç›´æ¥è€Œç®€å•çš„ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLinesP(edges,1,np.pi&#x2F;180,100,minLineLength&#x3D;100,maxLineGap&#x3D;10)</span><br><span class="line">for line in lines:</span><br><span class="line">    x1,y1,x2,y2 &#x3D; line[0]</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)</span><br><span class="line">cv.imwrite(&#39;houghlines5.jpg&#39;,img)</span><br></pre></td></tr></table></figure>
<p>çœ‹åˆ°å¦‚ä¸‹ç»“æœï¼š<br><img src="http://qiniu.aihubs.net/houghlines5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="éœå¤«åœˆå˜æ¢"><a href="#éœå¤«åœˆå˜æ¢" class="headerlink" title="éœå¤«åœˆå˜æ¢"></a><span id="header4">éœå¤«åœˆå˜æ¢</span></h1><h2 id="å­¦ä¹ ç›®æ ‡"><a href="#å­¦ä¹ ç›®æ ‡" class="headerlink" title="å­¦ä¹ ç›®æ ‡"></a>å­¦ä¹ ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œ </p>
<ul>
<li>æˆ‘ä»¬å°†å­¦ä¹ ä½¿ç”¨éœå¤«å˜æ¢æ¥æŸ¥æ‰¾å›¾åƒä¸­çš„åœ†ã€‚ </li>
<li>æˆ‘ä»¬å°†çœ‹åˆ°ä»¥ä¸‹å‡½æ•°ï¼šcv.HoughCircles()</li>
</ul>
<p>ç†è®º<br>åœ†åœ¨æ•°å­¦ä¸Šè¡¨ç¤ºä¸º$(xâˆ’x_{center})^2+(yâˆ’y_{center})^2=r^2$ï¼Œå…¶ä¸­$(x_{center},y_{center})$æ˜¯åœ†çš„ä¸­å¿ƒï¼Œræ˜¯åœ†çš„åŠå¾„ã€‚ä»ç­‰å¼ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æœ‰3ä¸ªå‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦3Dç´¯åŠ å™¨è¿›è¡Œéœå¤«å˜æ¢ï¼Œè¿™å°†éå¸¸ä½æ•ˆã€‚å› æ­¤ï¼ŒOpenCVä½¿ç”¨æ›´åŠ æŠ€å·§æ€§çš„æ–¹æ³•ï¼Œå³ä½¿ç”¨è¾¹ç¼˜çš„æ¢¯åº¦ä¿¡æ¯çš„<strong>Houghæ¢¯åº¦æ–¹æ³•</strong>ã€‚</p>
<p>æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨çš„å‡½æ•°æ˜¯<strong>cv.HoughCircles</strong>()ã€‚å®ƒæœ‰å¾ˆå¤šå‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨æ–‡æ¡£ä¸­æœ‰å¾ˆå¥½çš„è§£é‡Šã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›´æ¥è½¬åˆ°ä»£ç ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;opencv-logo-white.png&#39;,0)</span><br><span class="line">img &#x3D; cv.medianBlur(img,5)</span><br><span class="line">cimg &#x3D; cv.cvtColor(img,cv.COLOR_GRAY2BGR)</span><br><span class="line">circles &#x3D; cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20,</span><br><span class="line">                            param1&#x3D;50,param2&#x3D;30,minRadius&#x3D;0,maxRadius&#x3D;0)</span><br><span class="line">circles &#x3D; np.uint16(np.around(circles))</span><br><span class="line">for i in circles[0,:]:</span><br><span class="line">    # ç»˜åˆ¶å¤–åœ†</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2)</span><br><span class="line">    # ç»˜åˆ¶åœ†å¿ƒ</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),2,(0,0,255),3)</span><br><span class="line">cv.imshow(&#39;detected circles&#39;,cimg)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/houghcircles2.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="å›¾åƒåˆ†å‰²ä¸Watershedç®—æ³•"><a href="#å›¾åƒåˆ†å‰²ä¸Watershedç®—æ³•" class="headerlink" title="å›¾åƒåˆ†å‰²ä¸Watershedç®—æ³•"></a><span id="header5">å›¾åƒåˆ†å‰²ä¸Watershedç®—æ³•</span></h1><h2 id="ç›®æ ‡-3"><a href="#ç›®æ ‡-3" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œ - æˆ‘ä»¬å°†å­¦ä¹ ä½¿ç”¨åˆ†æ°´å²­ç®—æ³•å®ç°åŸºäºæ ‡è®°çš„å›¾åƒåˆ†å‰² - æˆ‘ä»¬å°†çœ‹åˆ°ï¼šcv.watershed()</p>
<h2 id="ç†è®º-3"><a href="#ç†è®º-3" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>ä»»ä½•ç°åº¦å›¾åƒéƒ½å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªåœ°å½¢è¡¨é¢ï¼Œå…¶ä¸­é«˜å¼ºåº¦è¡¨ç¤ºå±±å³°ï¼Œä½å¼ºåº¦è¡¨ç¤ºå±±è°·ã€‚</p>
<p>ä½ å¼€å§‹ç”¨ä¸åŒé¢œè‰²çš„æ°´(æ ‡ç­¾)å¡«å……æ¯ä¸ªå­¤ç«‹çš„å±±è°·(å±€éƒ¨æœ€å°å€¼)ã€‚</p>
<p>éšç€æ°´ä½çš„ä¸Šå‡ï¼Œæ ¹æ®é™„è¿‘çš„å±±å³°(å¡åº¦)ï¼Œæ¥è‡ªä¸åŒå±±è°·çš„æ°´æ˜æ˜¾ä¼šå¼€å§‹åˆå¹¶ï¼Œé¢œè‰²ä¹Ÿä¸åŒã€‚</p>
<p>ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œä½ è¦åœ¨æ°´èåˆçš„åœ°æ–¹å»ºé€ å±éšœã€‚ä½ ç»§ç»­å¡«æ»¡æ°´ï¼Œå»ºé€ éšœç¢ï¼Œç›´åˆ°æ‰€æœ‰çš„å±±å³°éƒ½åœ¨æ°´ä¸‹ã€‚</p>
<p>ç„¶åä½ åˆ›å»ºçš„å±éšœå°†è¿”å›ä½ çš„åˆ†å‰²ç»“æœã€‚è¿™å°±æ˜¯WatershedèƒŒåçš„â€œæ€æƒ³â€ã€‚</p>
<p>ä½ å¯ä»¥è®¿é—®Watershedçš„CMMç½‘é¡µï¼Œäº†è§£å®ƒä¸ä¸€äº›åŠ¨ç”»çš„å¸®åŠ©ã€‚</p>
<p>ä½†æ˜¯è¿™ç§æ–¹æ³•ä¼šç”±äºå›¾åƒä¸­çš„å™ªå£°æˆ–å…¶ä»–ä¸è§„åˆ™æ€§è€Œäº§ç”Ÿè¿‡åº¦åˆ†å‰²çš„ç»“æœã€‚</p>
<p>å› æ­¤OpenCVå®ç°äº†ä¸€ä¸ªåŸºäºæ ‡è®°çš„åˆ†æ°´å²­ç®—æ³•ï¼Œä½ å¯ä»¥æŒ‡å®šå“ªäº›æ˜¯è¦åˆå¹¶çš„å±±è°·ç‚¹ï¼Œå“ªäº›ä¸æ˜¯ã€‚</p>
<p>è¿™æ˜¯ä¸€ä¸ªäº¤äº’å¼çš„å›¾åƒåˆ†å‰²ã€‚æˆ‘ä»¬æ‰€åšçš„æ˜¯ç»™æˆ‘ä»¬çŸ¥é“çš„å¯¹è±¡èµ‹äºˆä¸åŒçš„æ ‡ç­¾ã€‚</p>
<p>ç”¨ä¸€ç§é¢œè‰²(æˆ–å¼ºåº¦)æ ‡è®°æˆ‘ä»¬ç¡®å®šä¸ºå‰æ™¯æˆ–å¯¹è±¡çš„åŒºåŸŸï¼Œç”¨å¦ä¸€ç§é¢œè‰²æ ‡è®°æˆ‘ä»¬ç¡®å®šä¸ºèƒŒæ™¯æˆ–éå¯¹è±¡çš„åŒºåŸŸï¼Œæœ€åç”¨0æ ‡è®°æˆ‘ä»¬ä¸ç¡®å®šçš„åŒºåŸŸã€‚</p>
<p>è¿™æ˜¯æˆ‘ä»¬çš„æ ‡è®°ã€‚ç„¶ååº”ç”¨åˆ†æ°´å²­ç®—æ³•ã€‚ç„¶åæˆ‘ä»¬çš„æ ‡è®°å°†ä½¿ç”¨æˆ‘ä»¬ç»™å‡ºçš„æ ‡ç­¾è¿›è¡Œæ›´æ–°ï¼Œå¯¹è±¡çš„è¾¹ç•Œå€¼å°†ä¸º-1ã€‚</p>
<h2 id="ä»£ç "><a href="#ä»£ç " class="headerlink" title="ä»£ç "></a>ä»£ç </h2><p>ä¸‹é¢æˆ‘ä»¬å°†çœ‹åˆ°ä¸€ä¸ªæœ‰å…³å¦‚ä½•ä½¿ç”¨è·ç¦»å˜æ¢å’Œåˆ†æ°´å²­æ¥åˆ†å‰²ç›¸äº’æ¥è§¦çš„å¯¹è±¡çš„ç¤ºä¾‹ã€‚</p>
<p>è€ƒè™‘ä¸‹é¢çš„ç¡¬å¸å›¾åƒï¼Œç¡¬å¸å½¼æ­¤æ¥è§¦ã€‚å³ä½¿ä½ è®¾ç½®é˜ˆå€¼ï¼Œå®ƒä¹Ÿä¼šå½¼æ­¤æ¥è§¦ã€‚<br><img src="http://qiniu.aihubs.net/water_coins.jpg" alt></p>
<p>æˆ‘ä»¬å…ˆä»å¯»æ‰¾ç¡¬å¸çš„è¿‘ä¼¼ä¼°è®¡å¼€å§‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Otsuçš„äºŒå€¼åŒ–ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;coins.png&#39;)</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh &#x3D; cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/water_thresh.jpg" alt></p>
<p>ç°åœ¨æˆ‘ä»¬éœ€è¦å»é™¤å›¾åƒä¸­çš„ä»»ä½•ç™½ç‚¹å™ªå£°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å½¢æ€å­¦æ‰©å¼ ã€‚</p>
<p>è¦å»é™¤å¯¹è±¡ä¸­çš„ä»»ä½•å°å­”ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å½¢æ€å­¦ä¾µèš€ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ç¡®å®šï¼Œé è¿‘å¯¹è±¡ä¸­å¿ƒçš„åŒºåŸŸæ˜¯å‰æ™¯ï¼Œè€Œç¦»å¯¹è±¡ä¸­å¿ƒå¾ˆè¿œçš„åŒºåŸŸæ˜¯èƒŒæ™¯ã€‚</p>
<p>æˆ‘ä»¬ä¸ç¡®å®šçš„å”¯ä¸€åŒºåŸŸæ˜¯ç¡¬å¸çš„è¾¹ç•ŒåŒºåŸŸã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æå–æˆ‘ä»¬å¯ç¡®å®šä¸ºç¡¬å¸çš„åŒºåŸŸã€‚ä¾µèš€ä¼šå»é™¤è¾¹ç•Œåƒç´ ã€‚</p>
<p>å› æ­¤ï¼Œæ— è®ºå‰©ä½™å¤šå°‘ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è‚¯å®šå®ƒæ˜¯ç¡¬å¸ã€‚å¦‚æœç‰©ä½“å½¼æ­¤ä¸æ¥è§¦ï¼Œé‚£å°†èµ·ä½œç”¨ã€‚</p>
<p>ä½†æ˜¯ï¼Œç”±äºå®ƒä»¬å½¼æ­¤æ¥è§¦ï¼Œå› æ­¤å¦ä¸€ä¸ªå¥½é€‰æ‹©æ˜¯æ‰¾åˆ°è·ç¦»å˜æ¢å¹¶åº”ç”¨é€‚å½“çš„é˜ˆå€¼ã€‚</p>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æˆ‘ä»¬ç¡®å®šå®ƒä»¬ä¸æ˜¯ç¡¬å¸çš„åŒºåŸŸã€‚</p>
<p>ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ‰©å¼ äº†ç»“æœã€‚è†¨èƒ€å°†å¯¹è±¡è¾¹ç•Œå¢åŠ åˆ°èƒŒæ™¯ã€‚</p>
<p>è¿™æ ·ï¼Œç”±äºè¾¹ç•ŒåŒºåŸŸå·²åˆ é™¤ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç¡®ä¿ç»“æœä¸­èƒŒæ™¯ä¸­çš„ä»»ä½•åŒºåŸŸå®é™…ä¸Šéƒ½æ˜¯èƒŒæ™¯ã€‚å‚è§ä¸‹å›¾ã€‚<br><img src="http://qiniu.aihubs.net/water_fgbg.jpg" alt></p>
<p>å‰©ä¸‹çš„åŒºåŸŸæ˜¯æˆ‘ä»¬ä¸çŸ¥é“çš„åŒºåŸŸï¼Œæ— è®ºæ˜¯ç¡¬å¸è¿˜æ˜¯èƒŒæ™¯ã€‚åˆ†æ°´å²­ç®—æ³•åº”è¯¥æ‰¾åˆ°å®ƒã€‚</p>
<p>è¿™äº›åŒºåŸŸé€šå¸¸ä½äºå‰æ™¯å’ŒèƒŒæ™¯ç›¸é‡ï¼ˆç”šè‡³ä¸¤ä¸ªä¸åŒçš„ç¡¬å¸ç›¸é‡ï¼‰çš„ç¡¬å¸è¾¹ç•Œé™„è¿‘ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºè¾¹ç•Œã€‚å¯ä»¥é€šè¿‡ä»sure_bgåŒºåŸŸä¸­å‡å»sure_fgåŒºåŸŸæ¥è·å¾—ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># å™ªå£°å»é™¤</span><br><span class="line">kernel &#x3D; np.ones((3,3),np.uint8)</span><br><span class="line">opening &#x3D; cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations &#x3D; 2)</span><br><span class="line"># ç¡®å®šèƒŒæ™¯åŒºåŸŸ</span><br><span class="line">sure_bg &#x3D; cv.dilate(opening,kernel,iterations&#x3D;3)</span><br><span class="line"># å¯»æ‰¾å‰æ™¯åŒºåŸŸ</span><br><span class="line">dist_transform &#x3D; cv.distanceTransform(opening,cv.DIST_L2,5)</span><br><span class="line">ret, sure_fg &#x3D; cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)</span><br><span class="line"># æ‰¾åˆ°æœªçŸ¥åŒºåŸŸ</span><br><span class="line">sure_fg &#x3D; np.uint8(sure_fg)</span><br><span class="line">unknown &#x3D; cv.subtract(sure_bg,sure_fg)</span><br></pre></td></tr></table></figure>
<p>æŸ¥çœ‹ç»“æœã€‚åœ¨é˜ˆå€¼å›¾åƒä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€äº›ç¡¬å¸åŒºåŸŸï¼Œæˆ‘ä»¬ç¡®å®šå®ƒä»¬æ˜¯ç¡¬å¸ï¼Œå¹¶ä¸”ç°åœ¨å·²åˆ†ç¦»å®ƒä»¬ã€‚ï¼ˆåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½åªå¯¹å‰æ™¯åˆ†å‰²æ„Ÿå…´è¶£ï¼Œè€Œä¸å¯¹åˆ†ç¦»ç›¸äº’æ¥è§¦çš„å¯¹è±¡æ„Ÿå…´è¶£ã€‚åœ¨é‚£ç§æƒ…å†µä¸‹ï¼Œä½ æ— éœ€ä½¿ç”¨è·ç¦»å˜æ¢ï¼Œåªéœ€ä¾µèš€å°±è¶³å¤Ÿäº†ã€‚ä¾µèš€åªæ˜¯æå–ç¡®å®šå‰æ™¯åŒºåŸŸçš„å¦ä¸€ç§æ–¹æ³•ã€‚ï¼‰</p>
<p><img src="http://qiniu.aihubs.net/water_dt.jpg" alt></p>
<p>ç°åœ¨æˆ‘ä»¬å¯ä»¥ç¡®å®šå“ªäº›æ˜¯ç¡¬å¸çš„åŒºåŸŸï¼Œå“ªäº›æ˜¯èƒŒæ™¯ã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†æ ‡è®°ï¼ˆå®ƒçš„å¤§å°ä¸åŸå§‹å›¾åƒçš„å¤§å°ç›¸åŒï¼Œä½†å…·æœ‰int32æ•°æ®ç±»å‹ï¼‰ï¼Œå¹¶æ ‡è®°å…¶ä¸­çš„åŒºåŸŸã€‚</p>
<p>æˆ‘ä»¬è‚¯å®šçŸ¥é“çš„åŒºåŸŸï¼ˆæ— è®ºæ˜¯å‰æ™¯è¿˜æ˜¯èƒŒæ™¯ï¼‰éƒ½æ ‡æœ‰ä»»ä½•æ­£æ•´æ•°ï¼Œä½†æ˜¯å¸¦æœ‰ä¸åŒçš„æ•´æ•°ï¼Œè€Œæˆ‘ä»¬ä¸ç¡®å®šçš„åŒºåŸŸåˆ™ä¿ç•™ä¸ºé›¶ã€‚</p>
<p>ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>cv.connectedComponents</strong>()ã€‚å®ƒç”¨0æ ‡è®°å›¾åƒçš„èƒŒæ™¯ï¼Œç„¶åå…¶ä»–å¯¹è±¡ç”¨ä»1å¼€å§‹çš„æ•´æ•°æ ‡è®°ã€‚</p>
<p>ä½†æ˜¯æˆ‘ä»¬çŸ¥é“ï¼Œå¦‚æœèƒŒæ™¯æ ‡è®°ä¸º0ï¼Œåˆ™åˆ†æ°´å²­ä¼šå°†å…¶è§†ä¸ºæœªçŸ¥åŒºåŸŸã€‚æ‰€ä»¥æˆ‘ä»¬æƒ³ç”¨ä¸åŒçš„æ•´æ•°æ¥æ ‡è®°å®ƒã€‚ç›¸åï¼Œæˆ‘ä»¬å°†æœªçŸ¥å®šä¹‰çš„æœªçŸ¥åŒºåŸŸæ ‡è®°ä¸º0ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ç±»åˆ«æ ‡è®°</span><br><span class="line">ret, markers &#x3D; cv.connectedComponents(sure_fg)</span><br><span class="line"># ä¸ºæ‰€æœ‰çš„æ ‡è®°åŠ 1ï¼Œä¿è¯èƒŒæ™¯æ˜¯0è€Œä¸æ˜¯1</span><br><span class="line">markers &#x3D; markers+1</span><br><span class="line"># ç°åœ¨è®©æ‰€æœ‰çš„æœªçŸ¥åŒºåŸŸä¸º0</span><br><span class="line">markers[unknown&#x3D;&#x3D;255] &#x3D; 0</span><br></pre></td></tr></table></figure>
<p>å‚è§JET colormapä¸­æ˜¾ç¤ºçš„ç»“æœã€‚æ·±è“è‰²åŒºåŸŸæ˜¾ç¤ºæœªçŸ¥åŒºåŸŸã€‚å½“ç„¶,ç¡¬å¸çš„é¢œè‰²ä¸åŒã€‚å‰©ä¸‹,è‚¯å®šä¸ºèƒŒæ™¯çš„åŒºåŸŸæ˜¾ç¤ºåœ¨è¾ƒæµ…çš„è“è‰²ï¼Œè·ŸæœªçŸ¥åŒºåŸŸç›¸æ¯”ã€‚<br><img src="http://qiniu.aihubs.net/water_marker.jpg" alt></p>
<p>ç°åœ¨æˆ‘ä»¬çš„æ ‡è®°å·²å‡†å¤‡å°±ç»ªã€‚ç°åœ¨æ˜¯æœ€åä¸€æ­¥çš„æ—¶å€™äº†ï¼Œä½¿ç”¨åˆ†æ°´å²­ç®—æ³•ã€‚ç„¶åæ ‡è®°å›¾åƒå°†è¢«ä¿®æ”¹ã€‚è¾¹ç•ŒåŒºåŸŸå°†æ ‡è®°ä¸º-1ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">markers &#x3D; cv.watershed(img,markers) </span><br><span class="line">img[markers &#x3D;&#x3D; -1] &#x3D; [255,0,0]</span><br></pre></td></tr></table></figure>
<p>è¯·å‚é˜…ä¸‹é¢çš„ç»“æœã€‚å¯¹æŸäº›ç¡¬å¸ï¼Œå®ƒä»¬æ¥è§¦çš„åŒºåŸŸè¢«æ­£ç¡®åœ°åˆ†å‰²ï¼Œè€Œå¯¹äºæŸäº›ç¡¬å¸ï¼Œå´ä¸æ˜¯ã€‚</p>
<p><img src="http://qiniu.aihubs.net/water_result.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="äº¤äº’å¼å‰æ™¯æå–ä½¿ç”¨GrabCutç®—æ³•"><a href="#äº¤äº’å¼å‰æ™¯æå–ä½¿ç”¨GrabCutç®—æ³•" class="headerlink" title="äº¤äº’å¼å‰æ™¯æå–ä½¿ç”¨GrabCutç®—æ³•"></a><span id="header6">äº¤äº’å¼å‰æ™¯æå–ä½¿ç”¨GrabCutç®—æ³•</span></h1><h2 id="ç›®æ ‡-4"><a href="#ç›®æ ‡-4" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h2><p>åœ¨æœ¬ç« ä¸­ï¼Œ - æˆ‘ä»¬å°†çœ‹åˆ°GrabCutç®—æ³•æ¥æå–å›¾åƒä¸­çš„å‰æ™¯ - æˆ‘ä»¬å°†ä¸ºæ­¤åˆ›å»ºä¸€ä¸ªäº¤äº’å¼åº”ç”¨ç¨‹åºã€‚</p>
<h2 id="ç†è®º-4"><a href="#ç†è®º-4" class="headerlink" title="ç†è®º"></a>ç†è®º</h2><p>GrabCutç®—æ³•ç”±è‹±å›½å¾®è½¯ç ”ç©¶é™¢çš„Carsten Rotherï¼ŒVladimir Kolmogorovå’ŒAndrew Blakeè®¾è®¡ã€‚</p>
<p>åœ¨ä»–ä»¬çš„è®ºæ–‡â€œGrabCutâ€ä¸­ï¼šä½¿ç”¨è¿­ä»£å›¾å‰²çš„äº¤äº’å¼å‰æ™¯æå–ã€‚éœ€è¦ç”¨æœ€å°‘çš„ç”¨æˆ·äº¤äº’è¿›è¡Œå‰æ™¯æå–çš„ç®—æ³•ï¼Œç»“æœæ˜¯GrabCutã€‚</p>
<p>ä»ç”¨æˆ·è§’åº¦æ¥çœ‹ï¼Œå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ</p>
<p>æœ€åˆï¼Œç”¨æˆ·åœ¨å‰æ™¯åŒºåŸŸå‘¨å›´ç»˜åˆ¶ä¸€ä¸ªçŸ©å½¢ï¼ˆå‰æ™¯åŒºåŸŸåº”å®Œå…¨ä½äºçŸ©å½¢å†…éƒ¨ï¼‰ã€‚</p>
<p>ç„¶åï¼Œç®—æ³•ä¼šå¯¹å…¶è¿›è¡Œè¿­ä»£åˆ†å‰²ï¼Œä»¥è·å¾—æœ€ä½³ç»“æœã€‚</p>
<p>åšå®Œäº†ä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œåˆ†å‰²å¯èƒ½ä¸ä¼šå¾ˆå¥½ï¼Œä¾‹å¦‚ï¼Œå¯èƒ½å·²å°†æŸäº›å‰æ™¯åŒºåŸŸæ ‡è®°ä¸ºèƒŒæ™¯ï¼Œåä¹‹äº¦ç„¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦ç”¨æˆ·è¿›è¡Œç²¾ä¿®ã€‚</p>
<p>åªéœ€åœ¨å›¾åƒé”™è¯¯åˆ†å‰²åŒºåŸŸä¸Šç”»äº›ç¬”ç”»ã€‚ç¬”ç”»åŸºæœ¬ä¸Šè¯´ â€œå˜¿ï¼Œè¯¥åŒºåŸŸåº”è¯¥æ˜¯å‰æ™¯ï¼Œä½ å°†å…¶æ ‡è®°ä¸ºèƒŒæ™¯ï¼Œåœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­å¯¹å…¶è¿›è¡Œæ ¡æ­£â€æˆ–ä¸èƒŒæ™¯ç›¸åã€‚</p>
<p>ç„¶ååœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œä½ å°†è·å¾—æ›´å¥½çš„ç»“æœã€‚</p>
<p>å‚è§ä¸‹å›¾ã€‚</p>
<p>ç¬¬ä¸€åçƒå‘˜å’Œæ©„æ¦„çƒè¢«å°é—­åœ¨ä¸€ä¸ªè“è‰²çŸ©å½¢ä¸­ã€‚ç„¶åç”¨ç™½è‰²ç¬”åˆ’ï¼ˆè¡¨ç¤ºå‰æ™¯ï¼‰å’Œé»‘è‰²ç¬”åˆ’ï¼ˆè¡¨ç¤ºèƒŒæ™¯ï¼‰è¿›è¡Œæœ€åçš„ä¿®é¥°ã€‚è€Œä¸”æˆ‘ä»¬å¾—åˆ°äº†ä¸é”™çš„ç»“æœã€‚</p>
<p><img src="http://qiniu.aihubs.net/grabcut_output1.jpg" alt></p>
<p>é‚£ä¹ˆèƒŒæ™¯å‘ç”Ÿäº†ä»€ä¹ˆå‘¢ï¼Ÿ </p>
<ul>
<li>ç”¨æˆ·è¾“å…¥çŸ©å½¢ã€‚æ­¤çŸ©å½¢å¤–éƒ¨çš„æ‰€æœ‰å†…å®¹éƒ½å°†ä½œä¸ºèƒŒæ™¯ï¼ˆè¿™æ˜¯åœ¨çŸ©å½¢åº”åŒ…å«æ‰€æœ‰å¯¹è±¡ä¹‹å‰æåˆ°çš„åŸå› ï¼‰ã€‚çŸ©å½¢å†…çš„æ‰€æœ‰å†…å®¹éƒ½æ˜¯æœªçŸ¥çš„ã€‚åŒæ ·ï¼Œä»»ä½•æŒ‡å®šå‰æ™¯å’ŒèƒŒæ™¯çš„ç”¨æˆ·è¾“å…¥éƒ½è¢«è§†ä¸ºç¡¬æ ‡ç­¾ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åœ¨æ­¤è¿‡ç¨‹ä¸­ä¸ä¼šæ›´æ”¹ã€‚ </li>
<li>è®¡ç®—æœºæ ¹æ®æˆ‘ä»¬æä¾›çš„æ•°æ®è¿›è¡Œåˆå§‹æ ‡è®°ã€‚å®ƒæ ‡è®°å‰æ™¯å’ŒèƒŒæ™¯åƒç´ ï¼ˆæˆ–å¯¹å…¶è¿›è¡Œç¡¬æ ‡è®°ï¼‰ï¼Œç°åœ¨ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹(GMM)å¯¹å‰æ™¯å’ŒèƒŒæ™¯è¿›è¡Œå»ºæ¨¡ã€‚ </li>
<li>æ ¹æ®æˆ‘ä»¬æä¾›çš„æ•°æ®ï¼ŒGMMå¯ä»¥å­¦ä¹ å¹¶åˆ›å»ºæ–°çš„åƒç´ åˆ†å¸ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒæœªçŸ¥åƒç´ æ ¹æ®é¢œè‰²ç»Ÿè®¡ä¸Šä¸å…¶ä»–ç¡¬æ ‡è®°åƒç´ çš„å…³ç³»è€Œè¢«æ ‡è®°ä¸ºå¯èƒ½çš„å‰æ™¯æˆ–å¯èƒ½çš„èƒŒæ™¯ï¼ˆå°±åƒèšç±»ä¸€æ ·ï¼‰ã€‚ </li>
<li>æ ¹æ®æ­¤åƒç´ åˆ†å¸ƒæ„å»ºå›¾å½¢ã€‚å›¾ä¸­çš„èŠ‚ç‚¹ä¸ºåƒç´ ã€‚æ·»åŠ äº†å¦å¤–ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œå³â€œæºâ€èŠ‚ç‚¹å’Œâ€œæ¥æ”¶å™¨â€èŠ‚ç‚¹ã€‚æ¯ä¸ªå‰æ™¯åƒç´ éƒ½è¿æ¥åˆ°æºèŠ‚ç‚¹ï¼Œæ¯ä¸ªèƒŒæ™¯åƒç´ éƒ½è¿æ¥åˆ°æ¥æ”¶å™¨èŠ‚ç‚¹ã€‚ </li>
<li>é€šè¿‡åƒç´ æ˜¯å‰æ™¯/èƒŒæ™¯çš„æ¦‚ç‡æ¥å®šä¹‰å°†åƒç´ è¿æ¥åˆ°æºèŠ‚ç‚¹/æœ«ç«¯èŠ‚ç‚¹çš„è¾¹ç¼˜çš„æƒé‡ã€‚åƒç´ ä¹‹é—´çš„æƒé‡ç”±è¾¹ç¼˜ä¿¡æ¯æˆ–åƒç´ ç›¸ä¼¼åº¦å®šä¹‰ã€‚å¦‚æœåƒç´ é¢œè‰²å·®å¼‚å¾ˆå¤§ï¼Œåˆ™å®ƒä»¬ä¹‹é—´çš„è¾¹ç¼˜å°†å˜ä½ã€‚ </li>
<li>ç„¶åä½¿ç”¨mincutç®—æ³•å¯¹å›¾è¿›è¡Œåˆ†å‰²ã€‚å®ƒå°†å›¾åˆ‡æˆå…·æœ‰æœ€å°æˆæœ¬å‡½æ•°çš„ä¸¤ä¸ªåˆ†ç¦»çš„æºèŠ‚ç‚¹å’Œå®¿èŠ‚ç‚¹ã€‚æˆæœ¬å‡½æ•°æ˜¯è¢«åˆ‡å‰²è¾¹ç¼˜çš„æ‰€æœ‰æƒé‡çš„æ€»å’Œã€‚å‰ªåˆ‡åï¼Œè¿æ¥åˆ°â€œæºâ€èŠ‚ç‚¹çš„æ‰€æœ‰åƒç´ éƒ½å˜ä¸ºå‰æ™¯ï¼Œè€Œè¿æ¥åˆ°â€œæ¥æ”¶å™¨â€èŠ‚ç‚¹çš„åƒç´ éƒ½å˜ä¸ºèƒŒæ™¯ã€‚ </li>
<li>ç»§ç»­è¯¥è¿‡ç¨‹ï¼Œç›´åˆ°åˆ†ç±»æ”¶æ•›ä¸ºæ­¢ã€‚</li>
</ul>
<p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ˆå›¾ç‰‡æä¾›ï¼š<a href="http://www.cs.ru.ac.za/research/g02m1682/ï¼‰" target="_blank" rel="noopener">http://www.cs.ru.ac.za/research/g02m1682/ï¼‰</a><br><img src="http://qiniu.aihubs.net/grabcut_scheme.jpg" alt></p>
<p>ç¤ºä¾‹<br>ç°åœ¨æˆ‘ä»¬ä½¿ç”¨OpenCVè¿›è¡ŒæŠ“å–ç®—æ³•ã€‚OpenCVä¸ºæ­¤å…·æœ‰åŠŸèƒ½<strong>cv.grabCut</strong>()ï¼Œæˆ‘ä»¬å°†é¦–å…ˆçœ‹åˆ°å…¶å‚æ•°ï¼š </p>
<ul>
<li>img - è¾“å…¥å›¾åƒ </li>
<li>mask - è¿™æ˜¯ä¸€ä¸ªæ©ç å›¾åƒï¼Œåœ¨å…¶ä¸­æˆ‘ä»¬æŒ‡å®šå“ªäº›åŒºåŸŸæ˜¯èƒŒæ™¯ï¼Œå‰æ™¯æˆ–å¯èƒ½çš„èƒŒæ™¯/å‰æ™¯ç­‰ã€‚è¿™æ˜¯é€šè¿‡ä»¥ä¸‹æ ‡å¿—å®Œæˆçš„ï¼šcv.GC_BGD,cv.GC_FGD, cv.GC_PR_BGD,cv.GC_PR_FGDï¼Œæˆ–ç›´æ¥å°†0,1,2,3ä¼ é€’ç»™å›¾åƒã€‚ </li>
<li>rect - å®ƒæ˜¯çŸ©å½¢çš„åæ ‡ï¼Œå…¶ä¸­åŒ…æ‹¬å‰æ™¯å¯¹è±¡ï¼Œæ ¼å¼ä¸º(x,y,w,h) - bdgModel, fgdModel - è¿™äº›æ˜¯ç®—æ³•å†…éƒ¨ä½¿ç”¨çš„æ•°ç»„ã€‚ä½ åªéœ€åˆ›å»ºä¸¤ä¸ªå¤§å°ä¸º(1,65)çš„np.float64ç±»å‹é›¶æ•°ç»„ã€‚ </li>
<li>iterCount - ç®—æ³•åº”è¿è¡Œçš„è¿­ä»£æ¬¡æ•°ã€‚ </li>
<li>model - åº”è¯¥æ˜¯<strong>cv.GC_INIT_WITH_RECT</strong>æˆ–<strong>cv.GC_INIT_WITH_MASK</strong>æˆ–ä¸¤è€…ç»“åˆï¼Œå†³å®šæˆ‘ä»¬è¦ç»˜åˆ¶çŸ©å½¢è¿˜æ˜¯æœ€ç»ˆçš„ä¿®é¥°ç¬”è§¦ã€‚</li>
</ul>
<p>é¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹çŸ©å½¢æ¨¡å¼ã€‚æˆ‘ä»¬åŠ è½½å›¾åƒï¼Œåˆ›å»ºç±»ä¼¼çš„maskå›¾åƒã€‚ </p>
<p>æˆ‘ä»¬åˆ›å»º<em>fgdModel</em>å’Œ<em>bgdModel</em>ã€‚æˆ‘ä»¬ç»™å‡ºçŸ©å½¢å‚æ•°ã€‚ä¸€åˆ‡éƒ½æ˜¯ç›´æˆªäº†å½“çš„ã€‚</p>
<p>è®©ç®—æ³•è¿è¡Œ5æ¬¡è¿­ä»£ã€‚æ¨¡å¼åº”ä¸º<strong>cv.GC_INIT_WITH_RECT</strong>, å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯çŸ©å½¢ã€‚ </p>
<p>ç„¶åè¿è¡Œgrabcutã€‚ä¿®æ”¹maskå›¾åƒã€‚åœ¨æ–°çš„maskå›¾åƒä¸­ï¼Œåƒç´ å°†è¢«æ ‡è®°æœ‰å››ä¸ªæ ‡è®°ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸Šé¢æŒ‡å®šçš„èƒŒæ™¯/å‰æ™¯ã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬ä¿®æ”¹maskï¼Œä½¿æ‰€æœ‰0åƒç´ å’Œ2åƒç´ éƒ½ç½®ä¸º0ï¼ˆå³èƒŒæ™¯ï¼‰ï¼Œè€Œæ‰€æœ‰1åƒç´ å’Œ3åƒç´ å‡ç½®ä¸º1ï¼ˆå³å‰æ™¯åƒç´ ï¼‰ã€‚</p>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆmaskå·²ç»å‡†å¤‡å°±ç»ªã€‚åªéœ€å°†å…¶ä¸è¾“å…¥å›¾åƒç›¸ä¹˜å³å¯å¾—åˆ°åˆ†å‰²çš„å›¾åƒã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;)</span><br><span class="line">mask &#x3D; np.zeros(img.shape[:2],np.uint8)</span><br><span class="line">bgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">fgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">rect &#x3D; (50,50,450,290)</span><br><span class="line">cv.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv.GC_INIT_WITH_RECT)</span><br><span class="line">mask2 &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask2[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://qiniu.aihubs.net/grabcut_rect.jpg" alt></p>
<p>ç³Ÿç³•ï¼Œæ¢…è¥¿çš„å¤´å‘ä¸è§äº†ã€‚è°ä¼šå–œæ¬¢æ²¡æœ‰å¤´å‘çš„æ¢…è¥¿ï¼Ÿæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ‰¾å›æ¥ã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨1åƒç´ ï¼ˆç¡®ä¿å‰æ™¯ï¼‰è¿›è¡Œç²¾ç»†ä¿®é¥°ã€‚</p>
<p>åŒæ—¶ï¼Œä¸€äº›ä¸éœ€è¦çš„åœ°é¢ä¹Ÿå‡ºç°åœ¨å›¾ç‰‡é‡Œã€‚æˆ‘ä»¬éœ€è¦åˆ é™¤å®ƒä»¬ã€‚</p>
<p>åœ¨é‚£é‡Œï¼Œæˆ‘ä»¬ç»™å‡ºäº†ä¸€äº›0åƒç´ çš„ä¿®é¥°ï¼ˆç¡®ä¿èƒŒæ™¯ï¼‰ã€‚</p>
<p>å› æ­¤ï¼Œå¦‚ç°åœ¨æ‰€è¯´ï¼Œæˆ‘ä»¬åœ¨ä»¥å‰çš„æƒ…å†µä¸‹ä¿®æ”¹ç”Ÿæˆçš„maskã€‚</p>
<p>æˆ‘å®é™…ä¸Šæ‰€åšçš„æ˜¯ï¼Œæˆ‘åœ¨paintåº”ç”¨ç¨‹åºä¸­æ‰“å¼€äº†è¾“å…¥å›¾åƒï¼Œå¹¶åœ¨å›¾åƒä¸­æ·»åŠ äº†å¦ä¸€å±‚ã€‚</p>
<p>ä½¿ç”¨ç”»ç¬”ä¸­çš„ç”»ç¬”å·¥å…·ï¼Œæˆ‘åœ¨æ–°å›¾å±‚ä¸Šç”¨ç™½è‰²æ ‡è®°äº†é”™è¿‡çš„å‰æ™¯ï¼ˆå¤´å‘ï¼Œé‹å­ï¼Œçƒç­‰ï¼‰ï¼Œè€Œç”¨ç™½è‰²æ ‡è®°äº†ä¸éœ€è¦çš„èƒŒæ™¯ï¼ˆä¾‹å¦‚logoï¼Œåœ°é¢ç­‰ï¼‰ã€‚</p>
<p>ç„¶åç”¨ç°è‰²å¡«å……å‰©ä½™çš„èƒŒæ™¯ã€‚</p>
<p>ç„¶åå°†è¯¥maskå›¾åƒåŠ è½½åˆ°OpenCVä¸­ï¼Œç¼–è¾‘æˆ‘ä»¬åœ¨æ–°æ·»åŠ çš„maskå›¾åƒä¸­å…·æœ‰ç›¸åº”å€¼çš„åŸå§‹maskå›¾åƒã€‚</p>
<p>æ£€æŸ¥ä»¥ä¸‹ä»£ç ï¼š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ï¼ƒnewmaskæ˜¯æˆ‘æ‰‹åŠ¨æ ‡è®°è¿‡çš„maskå›¾åƒ</span><br><span class="line">newmask &#x3D; cv.imread(&#39;newmask.png&#39;,0)</span><br><span class="line"># æ ‡è®°ä¸ºç™½è‰²ï¼ˆç¡®ä¿å‰æ™¯ï¼‰çš„åœ°æ–¹ï¼Œæ›´æ”¹mask &#x3D; 1</span><br><span class="line"># æ ‡è®°ä¸ºé»‘è‰²ï¼ˆç¡®ä¿èƒŒæ™¯ï¼‰çš„åœ°æ–¹ï¼Œæ›´æ”¹mask &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 0] &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 255] &#x3D; 1</span><br><span class="line">mask, bgdModel, fgdModel &#x3D; cv.grabCut(img,mask,None,bgdModel,fgdModel,5,cv.GC_INIT_WITH_MASK)</span><br><span class="line">mask &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure>
<p>å°±æ˜¯è¿™æ ·äº†ã€‚åœ¨è¿™é‡Œï¼Œä½ æ— éœ€ç›´æ¥åœ¨rectæ¨¡å¼ä¸‹åˆå§‹åŒ–ï¼Œè€Œå¯ä»¥ç›´æ¥è¿›å…¥maskæ¨¡å¼ã€‚</p>
<p>åªéœ€ç”¨2åƒç´ æˆ–3åƒç´ ï¼ˆå¯èƒ½çš„èƒŒæ™¯/å‰æ™¯ï¼‰æ ‡è®°maskå›¾åƒä¸­çš„çŸ©å½¢åŒºåŸŸã€‚</p>
<p>ç„¶ååƒåœ¨ç¬¬äºŒä¸ªç¤ºä¾‹ä¸­ä¸€æ ·ï¼Œå°†æˆ‘ä»¬çš„sure_foregroundæ ‡è®°ä¸º1åƒç´ ã€‚ç„¶åç›´æ¥åœ¨maskæ¨¡å¼ä¸‹åº”ç”¨grabCutåŠŸèƒ½ã€‚<br><img src="http://qiniu.aihubs.net/grabcut_mask.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>å›¾åƒ</tag>
      </tags>
  </entry>
  <entry>
    <title>æœºå™¨å­¦ä¹ å®æˆ˜-æ³°å¦å°¼å…‹å·è·æ•‘é¢„æµ‹</title>
    <url>/2020/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B/</url>
    <content><![CDATA[<p>æ•°æ®é›†:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/titanic_train.csv" target="_blank" rel="noopener">titanic_train.csv</a></p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">titianic = pandas.read_csv(<span class="string">'../data/titanic_train.csv'</span>)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[<span class="string">'Age'</span>]=titianic[<span class="string">'Age'</span>].fillna(titianic[<span class="string">'Age'</span>].median())</span><br><span class="line">print(titianic.describe())</span><br></pre></td></tr></table></figure>

<pre><code>       PassengerId    Survived      Pclass         Age       SibSp  \
count   891.000000  891.000000  891.000000  891.000000  891.000000   
mean    446.000000    0.383838    2.308642   29.361582    0.523008   
std     257.353842    0.486592    0.836071   13.019697    1.102743   
min       1.000000    0.000000    1.000000    0.420000    0.000000   
25%     223.500000    0.000000    2.000000   22.000000    0.000000   
50%     446.000000    0.000000    3.000000   28.000000    0.000000   
75%     668.500000    1.000000    3.000000   35.000000    1.000000   
max     891.000000    1.000000    3.000000   80.000000    8.000000   

            Parch        Fare  
count  891.000000  891.000000  
mean     0.381594   32.204208  
std      0.806057   49.693429  
min      0.000000    0.000000  
25%      0.000000    7.910400  
50%      0.000000   14.454200  
75%      0.000000   31.000000  
max      6.000000  512.329200  </code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(titianic[<span class="string">'Sex'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Sex'</span>] = titianic[<span class="string">'Sex'</span>].map(&#123;<span class="string">'male'</span>:<span class="number">0</span>,<span class="string">'female'</span>:<span class="number">1</span>&#125;)</span><br><span class="line"><span class="comment"># æŠŠmaleå˜æˆ0ï¼ŒæŠŠfemaleå˜æˆ1</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "male", "Sex"] = 0</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "female", "Sex"] = 1</span></span><br></pre></td></tr></table></figure>

<pre><code>[&apos;male&apos; &apos;female&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(titianic[<span class="string">'Embarked'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>)</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].map(&#123;<span class="string">'S'</span>:<span class="number">0</span>,<span class="string">'C'</span>:<span class="number">1</span>,<span class="string">'Q'</span>:<span class="number">2</span>&#125;)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;S&apos; &apos;C&apos; &apos;Q&apos; nan]</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">predictors = [<span class="string">'Pclass'</span>,<span class="string">'Sex'</span>,<span class="string">'Age'</span>,<span class="string">'SibSp'</span>,<span class="string">'Parch'</span>,<span class="string">'Fare'</span>,<span class="string">'Embarked'</span>]</span><br><span class="line">x_data = titianic[predictors]</span><br><span class="line">y_data = titianic[<span class="string">'Survived'</span>]</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">x_data = scaler.fit_transform(x_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">LR = LogisticRegression()</span><br><span class="line">scores = model_selection.cross_val_score(LR,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.7901234567901234</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">20</span>,<span class="number">10</span>),max_iter=<span class="number">1000</span>)</span><br><span class="line">scores = model_selection.cross_val_score(mlp,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.8002244668911335</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line">knn = neighbors.KNeighborsClassifier(<span class="number">21</span>)</span><br><span class="line">scores=model_selection.cross_val_score(knn,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8125701459034792</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">dtree = tree.DecisionTreeClassifier(max_depth=<span class="number">5</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(dtree,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8080808080808081</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">RF1 = RandomForestClassifier(random_state=<span class="number">1</span>,n_estimators=<span class="number">10</span>,min_samples_split=<span class="number">2</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF1,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.7991021324354657</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RF2 = RandomForestClassifier(n_estimators=<span class="number">100</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF2,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8125701459034792</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line">bagging_clf = BaggingClassifier(RF2,n_estimators=<span class="number">20</span>)</span><br><span class="line">scores=model_selection.cross_val_score(bagging_clf,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.819304152637486</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">adaboost = AdaBoostClassifier(bagging_clf,n_estimators=<span class="number">10</span>)</span><br><span class="line">scores=model_selection.cross_val_score(adaboost,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>0.8181818181818182</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line">sclf = StackingClassifier(classifiers=[bagging_clf,mlp,LR],</span><br><span class="line">                         meta_classifier=LogisticRegression())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.819304152637486</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sclf2 = VotingClassifier([(<span class="string">'adaboost'</span>,adaboost),(<span class="string">'mlp'</span>,mlp),(<span class="string">'LR'</span>,LR),(<span class="string">'knn'</span>,knn),(<span class="string">'dtree'</span>,dtree)])</span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf2,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>

<pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)
c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.
  % self.max_iter, ConvergenceWarning)


0.8159371492704826</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>æœºå™¨å­¦ä¹ å®æˆ˜</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-cifar10tutorial-visualizing</title>
    <url>/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/</url>
    <content><![CDATA[<p>Pytorch-Learning-cifar10tutorial-visualizing</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="Training-a-Classifier"><a href="#Training-a-Classifier" class="headerlink" title="Training a Classifier"></a>Training a Classifier</h1><p>This is it. You have seen how to define neural networks, compute loss and make<br>updates to the weights of the network.</p>
<p>Now you might be thinking,</p>
<h2 id="What-about-data"><a href="#What-about-data" class="headerlink" title="What about data?"></a>What about data?</h2><p>Generally, when you have to deal with image, text, audio or video data,<br>you can use standard python packages that load data into a numpy array.<br>Then you can convert this array into a <code>torch.*Tensor</code>.</p>
<ul>
<li>For images, packages such as Pillow, OpenCV are useful</li>
<li>For audio, packages such as scipy and librosa</li>
<li>For text, either raw Python or Cython based loading, or NLTK and<br>SpaCy are useful</li>
</ul>
<p>Specifically for vision, we have created a package called<br><code>torchvision</code>, that has data loaders for common datasets such as<br>Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,<br><code>torchvision.datasets</code> and <code>torch.utils.data.DataLoader</code>.</p>
<p>This provides a huge convenience and avoids writing boilerplate code.</p>
<p>For this tutorial, we will use the CIFAR10 dataset.<br>It has the classes: â€˜airplaneâ€™, â€˜automobileâ€™, â€˜birdâ€™, â€˜catâ€™, â€˜deerâ€™,<br>â€˜dogâ€™, â€˜frogâ€™, â€˜horseâ€™, â€˜shipâ€™, â€˜truckâ€™. The images in CIFAR-10 are of<br>size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.</p>
<p><img src="https://pytorch.org/tutorials/_images/cifar10.png" alt></p>
<p>   cifar10</p>
<h2 id="Training-an-image-classifier"><a href="#Training-an-image-classifier" class="headerlink" title="Training an image classifier"></a>Training an image classifier</h2><p>We will do the following steps in order:</p>
<ol>
<li><p>Load and normalizing the CIFAR10 training and test datasets using<br><code>torchvision</code></p>
</li>
<li><p>Define a Convolutional Neural Network</p>
</li>
<li><p>Define a loss function</p>
</li>
<li><p>Train the network on the training data</p>
</li>
<li><p>Test the network on the test data</p>
</li>
<li><p>Loading and normalizing CIFAR10</p>
</li>
</ol>
<hr>
<p>Using <code>torchvision</code>, itâ€™s extremely easy to load CIFAR10.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>

<p>The output of torchvision datasets are PILImage images of range [0, 1].<br>We transform them to Tensors of normalized range [-1, 1].</p>
<div class="alert alert-info"><h4>Note</h4><p>If running on Windows and you get a BrokenPipeError, try setting
    the num_worker of torch.utils.data.DataLoader() to 0.</p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Files already downloaded and verified
Files already downloaded and verified</code></pre><p>Let us show some of the training images, for fun.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_6_0.png" alt="png"></p>
<pre><code>horse   cat  deer   cat</code></pre><ol start="2">
<li>Define a Convolutional Neural Network</li>
</ol>
<hr>
<p>Copy the neural network from the Neural Networks section before and modify it to<br>take 3-channel images (instead of 1-channel images as it was defined).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Define a Loss function and optimizer</li>
</ol>
<hr>
<p>Letâ€™s use a Classification Cross-Entropy loss and SGD with momentum.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>Train the network</li>
</ol>
<hr>
<p>This is when things start to get interesting.<br>We simply have to loop over our data iterator, and feed the inputs to the<br>network and optimize.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[1,  2000] loss: 2.173
[1,  4000] loss: 1.818
[1,  6000] loss: 1.647
[1,  8000] loss: 1.545
[1, 10000] loss: 1.490
[1, 12000] loss: 1.436
[2,  2000] loss: 1.384
[2,  4000] loss: 1.348
[2,  6000] loss: 1.341
[2,  8000] loss: 1.306
[2, 10000] loss: 1.292
[2, 12000] loss: 1.283
Finished Training</code></pre><p>Letâ€™s quickly save our trained model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">'./cifar_net.pth'</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<p>See <code>here &lt;https://pytorch.org/docs/stable/notes/serialization.html&gt;</code>_<br>for more details on saving PyTorch models.</p>
<ol start="5">
<li>Test the network on the test data</li>
</ol>
<hr>
<p>We have trained the network for 2 passes over the training dataset.<br>But we need to check if the network has learnt anything at all.</p>
<p>We will check this by predicting the class label that the neural network<br>outputs, and checking it against the ground-truth. If the prediction is<br>correct, we add the sample to the list of correct predictions.</p>
<p>Okay, first step. Let us display an image from the test set to get familiar.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_16_0.png" alt="png"></p>
<pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p>Next, letâ€™s load back in our saved model (note: saving and re-loading the model<br>wasnâ€™t necessary here, we only did it to illustrate how to do so):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>




<pre><code>IncompatibleKeys(missing_keys=[], unexpected_keys=[])</code></pre><p>Okay, now let us see what the neural network thinks these examples above are:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure>

<p>The outputs are energies for the 10 classes.<br>The higher the energy for a class, the more the network<br>thinks that the image is of the particular class.<br>So, letâ€™s get the index of the highest energy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>Predicted:    cat plane plane plane</code></pre><p>The results seem pretty good.</p>
<p>Let us look at how the network performs on the whole dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of the network on the 10000 test images: 53 %</code></pre><p>That looks way better than chance, which is 10% accuracy (randomly picking<br>a class out of 10 classes).<br>Seems like the network learnt something.</p>
<p>Hmmm, what are the classes that performed well, and the classes that did<br>not perform well:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of plane : 71 %
Accuracy of   car : 57 %
Accuracy of  bird : 26 %
Accuracy of   cat : 32 %
Accuracy of  deer : 52 %
Accuracy of   dog : 40 %
Accuracy of  frog : 72 %
Accuracy of horse : 74 %
Accuracy of  ship : 57 %
Accuracy of truck : 53 %</code></pre><p>Okay, so what next?</p>
<p>How do we run these neural networks on the GPU?</p>
<h2 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h2><p>Just like how you transfer a Tensor onto the GPU, you transfer the neural<br>net onto the GPU.</p>
<p>Letâ€™s first define our device as the first visible cuda device if we have<br>CUDA available:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<pre><code>cpu</code></pre><p>The rest of this section assumes that <code>device</code> is a CUDA device.</p>
<p>Then these methods will recursively go over all modules and convert their<br>parameters and buffers to CUDA tensors:</p>
<p>.. code:: python</p>
<pre><code>net.to(device)</code></pre><p>Remember that you will have to send the inputs and targets at every step<br>to the GPU too:</p>
<p>.. code:: python</p>
<pre><code>inputs, labels = data[0].to(device), data[1].to(device)</code></pre><p>Why dont I notice MASSIVE speedup compared to CPU? Because your network<br>is really small.</p>
<p><strong>Exercise:</strong> Try increasing the width of your network (argument 2 of<br>the first <code>nn.Conv2d</code>, and argument 1 of the second <code>nn.Conv2d</code> â€“<br>they need to be the same number), see what kind of speedup you get.</p>
<p><strong>Goals achieved</strong>:</p>
<ul>
<li>Understanding PyTorchâ€™s Tensor library and neural networks at a high level.</li>
<li>Train a small neural network to classify images</li>
</ul>
<h2 id="Training-on-multiple-GPUs"><a href="#Training-on-multiple-GPUs" class="headerlink" title="Training on multiple GPUs"></a>Training on multiple GPUs</h2><p>If you want to see even more MASSIVE speedup using all of your GPUs,<br>please check out :doc:<code>data_parallel_tutorial</code>.</p>
<h2 id="Where-do-I-go-next"><a href="#Where-do-I-go-next" class="headerlink" title="Where do I go next?"></a>Where do I go next?</h2><ul>
<li>:doc:<code>Train neural nets to play video games &lt;/intermediate/reinforcement_q_learning&gt;</code></li>
<li><code>Train a state-of-the-art ResNet network on imagenet</code>_</li>
<li><code>Train a face generator using Generative Adversarial Networks</code>_</li>
<li><code>Train a word-level language model using Recurrent LSTM networks</code>_</li>
<li><code>More examples</code>_</li>
<li><code>More tutorials</code>_</li>
<li><code>Discuss PyTorch on the Forums</code>_</li>
<li><code>Chat with other users on Slack</code>_</li>
</ul>
<h2 id="VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD"><a href="#VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD" class="headerlink" title="VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD"></a>VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD</h2><p>In the 60 Minute Blitz, we show you how to load in data, feed it through a model we define as a subclass of nn.Module, train this model on training data, and test it on test data. To see whatâ€™s happening, we print out some statistics as the model is training to get a sense for whether training is progressing. However, we can do much better than that: PyTorch integrates with TensorBoard, a tool designed for visualizing the results of neural network training runs. This tutorial illustrates some of its functionality, using the Fashion-MNIST dataset which can be read into PyTorch using torchvision.datasets.</p>
<p>In this tutorial, weâ€™ll learn how to:</p>
<ul>
<li>Read in data and with appropriate transforms (nearly identical to the prior tutorial).</li>
<li>Set up TensorBoard.</li>
<li>Write to TensorBoard.</li>
<li>Inspect a model architecture using TensorBoard.</li>
</ul>
<p>Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code<br>Specifically, on point #5, weâ€™ll see:</p>
<ul>
<li>A couple of ways to inspect our training data</li>
<li>How to track our modelâ€™s performance as it trains</li>
<li>How to assess our modelâ€™s performance once it is trained.</li>
<li>Weâ€™ll begin with similar boilerplate code as in the CIFAR-10 tutorial:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># transforms</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># datasets</span></span><br><span class="line">trainset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line">testset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataloaders</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># constant for classes</span></span><br><span class="line">classes = (<span class="string">'T-shirt/top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,</span><br><span class="line">        <span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle Boot'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function to show an image</span></span><br><span class="line"><span class="comment"># (used in the `plot_classes_preds` function below)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matplotlib_imshow</span><span class="params">(img, one_channel=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        img = img.mean(dim=<span class="number">0</span>)</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        plt.imshow(npimg, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\train-images-idx3-ubyte.gz


100.0%

Extracting ./data\FashionMNIST\raw\train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gz


111.0%

Extracting ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz


100.0%

Extracting ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz


159.1%

Extracting ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz
Processing...
Done!</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. TensorBoard setup</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">'runs/fashion_mnist_experiment_1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Writing to TensorBoard</span></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">matplotlib_imshow(img_grid, one_channel=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># write to tensorboard</span></span><br><span class="line">writer.add_image(<span class="string">'four_fashion_mnist_images'</span>, img_grid)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Now running: tensorboard --logdir=runs</span></span><br><span class="line"><span class="comment"># from the command line and then navigating to https://localhost:6006</span></span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_32_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3. Inspect the model using TensorBoard</span></span><br><span class="line">writer.add_graph(net, images)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4. Adding a â€œProjectorâ€ to TensorBoard</span></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_n_random</span><span class="params">(data, labels, n=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Selects n random datapoints and their corresponding labels from a dataset</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> len(data) == len(labels)</span><br><span class="line"></span><br><span class="line">    perm = torch.randperm(len(data))</span><br><span class="line">    <span class="keyword">return</span> data[perm][:n], labels[perm][:n]</span><br><span class="line"></span><br><span class="line"><span class="comment"># select random images and their target indices</span></span><br><span class="line">images, labels = select_n_random(trainset.data, trainset.targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the class labels for each image</span></span><br><span class="line">class_labels = [classes[lab] <span class="keyword">for</span> lab <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="comment"># log embeddings</span></span><br><span class="line">features = images.view(<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>)</span><br><span class="line">writer.add_embedding(features,</span><br><span class="line">                    metadata=class_labels,</span><br><span class="line">                    label_img=images.unsqueeze(<span class="number">1</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5. Tracking model training with TensorBoard</span></span><br><span class="line"><span class="comment"># helper functions</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">images_to_probs</span><span class="params">(net, images)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates predictions and corresponding probabilities from a trained</span></span><br><span class="line"><span class="string">    network and a list of images</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    output = net(images)</span><br><span class="line">    <span class="comment"># convert output probabilities to predicted class</span></span><br><span class="line">    _, preds_tensor = torch.max(output, <span class="number">1</span>)</span><br><span class="line">    preds = np.squeeze(preds_tensor.numpy())</span><br><span class="line">    <span class="keyword">return</span> preds, [F.softmax(el, dim=<span class="number">0</span>)[i].item() <span class="keyword">for</span> i, el <span class="keyword">in</span> zip(preds, output)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_classes_preds</span><span class="params">(net, images, labels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates matplotlib Figure using a trained network, along with images</span></span><br><span class="line"><span class="string">    and labels from a batch, that shows the network's top prediction along</span></span><br><span class="line"><span class="string">    with its probability, alongside the actual label, coloring this</span></span><br><span class="line"><span class="string">    information based on whether the prediction was correct or not.</span></span><br><span class="line"><span class="string">    Uses the "images_to_probs" function.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    preds, probs = images_to_probs(net, images)</span><br><span class="line">    <span class="comment"># plot the images in the batch, along with predicted and true labels</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">48</span>))</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> np.arange(<span class="number">4</span>):</span><br><span class="line">        ax = fig.add_subplot(<span class="number">1</span>, <span class="number">4</span>, idx+<span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        matplotlib_imshow(images[idx], one_channel=<span class="literal">True</span>)</span><br><span class="line">        ax.set_title(<span class="string">"&#123;0&#125;, &#123;1:.1f&#125;%\n(label: &#123;2&#125;)"</span>.format(</span><br><span class="line">            classes[preds[idx]],</span><br><span class="line">            probs[idx] * <span class="number">100.0</span>,</span><br><span class="line">            classes[labels[idx]]),</span><br><span class="line">                    color=(<span class="string">"green"</span> <span class="keyword">if</span> preds[idx]==labels[idx].item() <span class="keyword">else</span> <span class="string">"red"</span>))</span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line">running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">999</span>:    <span class="comment"># every 1000 mini-batches...</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log the running loss</span></span><br><span class="line">            writer.add_scalar(<span class="string">'training loss'</span>,</span><br><span class="line">                            running_loss / <span class="number">1000</span>,</span><br><span class="line">                            epoch * len(trainloader) + i)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log a Matplotlib Figure showing the model's predictions on a</span></span><br><span class="line">            <span class="comment"># random mini-batch</span></span><br><span class="line">            writer.add_figure(<span class="string">'predictions vs. actuals'</span>,</span><br><span class="line">                            plot_classes_preds(net, inputs, labels),</span><br><span class="line">                            global_step=epoch * len(trainloader) + i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Finished Training</code></pre><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_1.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_2.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_3.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_4.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_5.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_6.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_7.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_8.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_9.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_10.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_11.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_12.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_13.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_14.png" alt="png"></p>
<p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_15.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 6. Assessing trained models with TensorBoard</span></span><br><span class="line"><span class="comment"># 1. gets the probability predictions in a test_size x num_classes Tensor</span></span><br><span class="line"><span class="comment"># 2. gets the preds in a test_size Tensor</span></span><br><span class="line"><span class="comment"># takes ~10 seconds to run</span></span><br><span class="line">class_probs = []</span><br><span class="line">class_preds = []</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        output = net(images)</span><br><span class="line">        class_probs_batch = [F.softmax(el, dim=<span class="number">0</span>) <span class="keyword">for</span> el <span class="keyword">in</span> output]</span><br><span class="line">        _, class_preds_batch = torch.max(output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        class_probs.append(class_probs_batch)</span><br><span class="line">        class_preds.append(class_preds_batch)</span><br><span class="line"></span><br><span class="line">test_probs = torch.cat([torch.stack(batch) <span class="keyword">for</span> batch <span class="keyword">in</span> class_probs])</span><br><span class="line">test_preds = torch.cat(class_preds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_pr_curve_tensorboard</span><span class="params">(class_index, test_probs, test_preds, global_step=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Takes in a "class_index" from 0 to 9 and plots the corresponding</span></span><br><span class="line"><span class="string">    precision-recall curve</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tensorboard_preds = test_preds == class_index</span><br><span class="line">    tensorboard_probs = test_probs[:, class_index]</span><br><span class="line"></span><br><span class="line">    writer.add_pr_curve(classes[class_index],</span><br><span class="line">                        tensorboard_preds,</span><br><span class="line">                        tensorboard_probs,</span><br><span class="line">                        global_step=global_step)</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot all the pr curves</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">    add_pr_curve_tensorboard(i, test_probs, test_preds)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-examples</title>
    <url>/2020/07/23/Pytorch-Learning-examples/</url>
    <content><![CDATA[<p>Pytorch-Learning-examples</p>
<a id="more"></a>

<ul>
<li>1.<a href="#header1">warm-up:numpy</a></li>
<li>2.<a href="#header2">pytorch:tensors</a></li>
<li>3.<a href="#header3">pytorch:tensors and autograd</a></li>
<li>4.<a href="#header4">pytorch:defining new autograd functions</a></li>
<li>5.<a href="#header5">tensorflow1.x:static graphs</a></li>
<li>6.<a href="#header6">pytorch:nn</a></li>
<li>7.<a href="#header7">pytorch:optim</a></li>
<li>8.<a href="#header8">pytorch:custom nn modules</a></li>
<li>9.<a href="#header9">pytorch:control flow+weight sharing</a></li>
</ul>
<h1 id="warm-up-numpy"><a href="#warm-up-numpy" class="headerlink" title="warm-up:numpy"></a><span id="header1">warm-up:numpy</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="Warm-up-numpy"><a href="#Warm-up-numpy" class="headerlink" title="Warm-up: numpy"></a>Warm-up: numpy</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x using Euclidean error.</p>
<p>This implementation uses numpy to manually compute the forward pass, loss, and<br>backward pass.</p>
<p>A numpy array is a generic n-dimensional array; it does not know anything about<br>deep learning or gradients or computational graphs, and is just a way to perform<br>generic numeric computations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<pre><code>0 38502658.12178467
1 39021030.505150035
2 43783005.35946928
3 43513345.95754772
4 33130647.574302156
5 18494184.582708906
6 8309147.312092974
7 3709728.9004302337
8 1973222.0064019447
9 1292160.3020421679
10 969207.6696523366
11 777313.2166244711
12 642885.4458057204
13 539862.0964943856
14 457706.36910393136
15 390776.99980368273
16 335541.3660443882
17 289534.9698406697
18 250999.0891175047
19 218440.8922160021
20 190801.03505385082
21 167229.41111619392
22 147030.14995170798
23 129644.55669752343
24 114634.85573944401
25 101654.50910880938
26 90379.66910158034
27 80547.50420605141
28 71942.30234259031
29 64389.53897462465
30 57755.011608667795
31 51903.61478445082
32 46747.37821866218
33 42180.36670548356
34 38120.10143244453
35 34504.8367862085
36 31283.625825762603
37 28409.554826229883
38 25832.84410659271
39 23520.944901953713
40 21443.92872486158
41 19573.49619579054
42 17887.52961185431
43 16364.901917158142
44 14986.999074876061
45 13739.21782497934
46 12607.392910105698
47 11579.322653387639
48 10644.061828317645
49 9792.956168204091
50 9016.956906200578
51 8309.619387713636
52 7664.013985165039
53 7073.449800791166
54 6532.833920817042
55 6037.538464055722
56 5583.274822316345
57 5166.3150372957025
58 4783.438966887356
59 4431.265118218456
60 4107.306250671735
61 3809.173244284424
62 3534.3065526324035
63 3281.382487196609
64 3048.2720327779725
65 2832.9613638905826
66 2634.0552486543734
67 2450.1208680788013
68 2280.0038612106828
69 2122.5020348125718
70 1976.6140763769765
71 1841.480282193922
72 1716.1436144355846
73 1599.8498210515127
74 1491.925103606006
75 1391.74788163408
76 1298.700821661702
77 1212.212450782299
78 1131.7942696970626
79 1057.0430043302988
80 987.4925199287406
81 922.7460298895005
82 862.4637455820518
83 806.335290707932
84 754.0347890268129
85 705.3032590197868
86 659.8640583682109
87 617.4715043492135
88 577.9369320777199
89 541.0382711590596
90 506.62139688224494
91 474.46146460235906
92 444.4289850228761
93 416.3831714509241
94 390.1781262414818
95 365.68416156867966
96 342.7828243058609
97 321.37451033250693
98 301.34344345528007
99 282.6049616675032
100 265.0769405906856
101 248.66706624922423
102 233.31182982470165
103 218.93579583019107
104 205.47399586723316
105 192.8673639685237
106 181.05784775853311
107 169.9914161272534
108 159.62205201703807
109 149.90414548125696
110 140.7986893389537
111 132.2586907147042
112 124.25279696970381
113 116.74222194734885
114 109.70210950592357
115 103.09497001058955
116 96.89525543441061
117 91.07884065063246
118 85.6198765958778
119 80.49732520749234
120 75.6882961733577
121 71.1722154368565
122 66.93235677110607
123 62.95241596773887
124 59.21332801738676
125 55.70220136122839
126 52.40276978766286
127 49.30324491666052
128 46.39219465065453
129 43.6561631420701
130 41.08445097890545
131 38.66705716574056
132 36.39569090451589
133 34.2596896820596
134 32.25129366141559
135 30.36277332849072
136 28.58840749936592
137 26.91886404849481
138 25.348295684128104
139 23.871887568407733
140 22.482392230639924
141 21.1752612429013
142 19.94540814626695
143 18.78848359075504
144 17.699455611505552
145 16.674867891142245
146 15.710568962031482
147 14.802825886952098
148 13.94842074029503
149 13.144227723348116
150 12.387131550628439
151 11.674026481428772
152 11.002689146006661
153 10.370609949309603
154 9.775509369377593
155 9.215072226656517
156 8.687075426828947
157 8.189767782525552
158 7.7214576865624025
159 7.280233710481657
160 6.864642688917494
161 6.473008103216368
162 6.104105173833666
163 5.7564916113754085
164 5.428879681554832
165 5.120147993533515
166 4.829259142691539
167 4.555145740046442
168 4.29672213329475
169 4.053143156190888
170 3.8235214202376655
171 3.607138244094545
172 3.4031196927940224
173 3.210757574413539
174 3.0293765387034064
175 2.858454621478234
176 2.697234384156207
177 2.545177266862872
178 2.4017939117722404
179 2.2665997813870664
180 2.1391195167593566
181 2.0188693271562754
182 1.9054496369340477
183 1.7984763417415828
184 1.6975705115269373
185 1.6023729047397333
186 1.512573512380922
187 1.4278650471502181
188 1.347969970837449
189 1.272567002922919
190 1.2014204465432794
191 1.1342855863955332
192 1.0709568915888343
193 1.0111820179227746
194 0.9547746744541861
195 0.9015532749111042
196 0.8513302278674784
197 0.803922014568537
198 0.7591740066091374
199 0.7169375303267338
200 0.6770877114086349
201 0.6394607979618921
202 0.6039480138637082
203 0.5704178896870162
204 0.5387711941311117
205 0.5088920505641997
206 0.4806800135074349
207 0.45404935956164216
208 0.4289117734184146
209 0.40517486327826213
210 0.38275697069924214
211 0.36158944936278636
212 0.3416070794747825
213 0.3227344916114124
214 0.3049141167911339
215 0.28808300182326085
216 0.27219060149023244
217 0.2571773488343734
218 0.24299758077475503
219 0.22960605968706874
220 0.21696181922245522
221 0.20501866875786806
222 0.19373590402638502
223 0.1830764093023291
224 0.17300985254584955
225 0.1635004063397864
226 0.15451596252804456
227 0.14602859536330753
228 0.13801270444499963
229 0.1304395543791902
230 0.12328307118542486
231 0.1165215280650118
232 0.11013498048221826
233 0.10409915585381448
234 0.09839683048227854
235 0.09300849009893092
236 0.08791893479291592
237 0.08310796594355277
238 0.07856156604359649
239 0.07426508307950577
240 0.07020565731703925
241 0.06636946700613897
242 0.0627437660794708
243 0.059317781060088316
244 0.05608034093448906
245 0.05301955241121613
246 0.050126674069563906
247 0.04739256213401111
248 0.04480928906259292
249 0.04236691748155608
250 0.04005879599601969
251 0.03787661761938381
252 0.03581475257492655
253 0.033864928740949526
254 0.032021804201541854
255 0.030279858100986354
256 0.02863334928312954
257 0.02707658789186648
258 0.025604569332906155
259 0.024213249395320283
260 0.022897970661672578
261 0.02165425187746097
262 0.020478737432682297
263 0.01936719332160696
264 0.018316430865928372
265 0.017322619307502316
266 0.016383050617920083
267 0.015494734481326376
268 0.014655032299327592
269 0.013860819190983916
270 0.013109809002567857
271 0.0123997755042141
272 0.011728358290906498
273 0.011093355906296416
274 0.01049298268330884
275 0.009925290535664889
276 0.009388360895355642
277 0.008880549450207519
278 0.008400420664047465
279 0.007946325800609208
280 0.00751686776061394
281 0.007110688095051999
282 0.006726592064373132
283 0.00636331544306615
284 0.006019761179348237
285 0.005694800059913335
286 0.005387424438387055
287 0.005096730276146309
288 0.004821747870498604
289 0.004561741644547909
290 0.004315728687198646
291 0.004083088241466739
292 0.0038630342216639736
293 0.0036548280743543514
294 0.003457908894412565
295 0.0032716544897889873
296 0.003095480430783593
297 0.0029287903501173454
298 0.0027711283850350016
299 0.002622002726801867
300 0.0024808885834647024
301 0.0023473968087844807
302 0.00222113284310873
303 0.00210170542574699
304 0.0019887074497667136
305 0.0018817802500325769
306 0.00178065116010868
307 0.0016849552143364992
308 0.0015944091801416442
309 0.001508753706017779
310 0.0014277397746070208
311 0.0013510695821452332
312 0.0012785229777043136
313 0.0012098928983483854
314 0.0011449625641886667
315 0.0010835156717760649
316 0.001025377588941091
317 0.0009703758384736077
318 0.0009183361752611588
319 0.0008690850720838991
320 0.0008224940700348837
321 0.0007784029943592372
322 0.0007366791771033156
323 0.0006971989990605096
324 0.0006598426281655082
325 0.000624495741732001
326 0.0005910439292796761
327 0.0005593973773407745
328 0.0005294403512899198
329 0.0005010976939705757
330 0.00047427317778801455
331 0.0004488877470686361
332 0.0004248669939759654
333 0.0004021385047171281
334 0.0003806279081745162
335 0.0003602664168058468
336 0.00034099755680728995
337 0.0003227653070463328
338 0.00030550707403562225
339 0.0002891754204576711
340 0.000273721587865668
341 0.00025909406293294767
342 0.0002452471530517434
343 0.0002321429824444712
344 0.00021974540966147302
345 0.00020800723976222412
346 0.00019689755859322988
347 0.00018638458674544666
348 0.00017643522452308918
349 0.00016701530053816744
350 0.0001581003279303452
351 0.00014966407110653316
352 0.00014167707307891997
353 0.00013411790417482185
354 0.00012696356477874673
355 0.00012019230146127977
356 0.00011378157083006711
357 0.00010771346551538498
358 0.0001019705019676451
359 9.653404961343065e-05
360 9.138760257508527e-05
361 8.651709721760377e-05
362 8.19070094203869e-05
363 7.754226881202036e-05
364 7.341041687150536e-05
365 6.94995078743375e-05
366 6.579726835351665e-05
367 6.229309750532338e-05
368 5.897611236256165e-05
369 5.583565153701364e-05
370 5.286297313654044e-05
371 5.004834708849745e-05
372 4.738444170826265e-05
373 4.486241750382573e-05
374 4.2474866618345255e-05
375 4.0215019660048885e-05
376 3.807518640145299e-05
377 3.604963907599714e-05
378 3.4131911558009186e-05
379 3.231655671510322e-05
380 3.059770246224207e-05
381 2.8970757618958097e-05
382 2.743057310133526e-05
383 2.5972006538400987e-05
384 2.459140169431031e-05
385 2.3284245013094282e-05
386 2.2046767101248254e-05
387 2.0874974705148337e-05
388 1.976572632623195e-05
389 1.8715764940418173e-05
390 1.7721415862570632e-05
391 1.6779971953419194e-05
392 1.588871637152896e-05
393 1.5044836588187851e-05
394 1.4245828226986497e-05
395 1.3489410778399332e-05
396 1.2773364399028685e-05
397 1.2095139789484804e-05
398 1.145299443496586e-05
399 1.0845077903682161e-05
400 1.0269508212283454e-05
401 9.724527789368986e-06
402 9.208527492374383e-06
403 8.720045436070135e-06
404 8.2573575243078e-06
405 7.819257808270196e-06
406 7.404463436549593e-06
407 7.011821523303816e-06
408 6.639943046336263e-06
409 6.2878326555936834e-06
410 5.954457206733875e-06
411 5.638725815406013e-06
412 5.339791165439266e-06
413 5.056690012109548e-06
414 4.7886913275527965e-06
415 4.5348871524205895e-06
416 4.294550962428522e-06
417 4.0669989344986414e-06
418 3.851507524029607e-06
419 3.6474147842461815e-06
420 3.4541567135768535e-06
421 3.2712066098541254e-06
422 3.0979501309951946e-06
423 2.9338623570734417e-06
424 2.778486859288849e-06
425 2.6313579995304816e-06
426 2.492005877958706e-06
427 2.3600400344687442e-06
428 2.235117848833567e-06
429 2.1168180481419086e-06
430 2.0047497870126444e-06
431 1.8986381787503396e-06
432 1.7981531121369732e-06
433 1.7029874912083225e-06
434 1.6128687459205246e-06
435 1.5275476848984372e-06
436 1.4467329944515942e-06
437 1.3701865914534269e-06
438 1.2977020898043846e-06
439 1.2290629088171652e-06
440 1.164051733130194e-06
441 1.1024831847468408e-06
442 1.0441903271914875e-06
443 9.889751182032595e-07
444 9.366772125412096e-07
445 8.871553290035977e-07
446 8.402520019677647e-07
447 7.958308104042071e-07
448 7.53763518098307e-07
449 7.139244346971557e-07
450 6.761960655207478e-07
451 6.404595540956439e-07
452 6.066124906303541e-07
453 5.74556854302957e-07
454 5.442030621013854e-07
455 5.154497792919255e-07
456 4.882203577154002e-07
457 4.6243074105866146e-07
458 4.380049551012981e-07
459 4.1486836737937236e-07
460 3.929552794591328e-07
461 3.722082406282778e-07
462 3.525512434892848e-07
463 3.339350935587304e-07
464 3.163035075274776e-07
465 2.9960560558970377e-07
466 2.8378833766961356e-07
467 2.688083814724989e-07
468 2.5461985965457267e-07
469 2.4117912639785815e-07
470 2.2845076123088963e-07
471 2.1639404311547854e-07
472 2.049768154586423e-07
473 1.9416035947365062e-07
474 1.8391578287894605e-07
475 1.742122640542393e-07
476 1.6502075323866398e-07
477 1.5631602307949188e-07
478 1.4807025488750153e-07
479 1.4026124873333745e-07
480 1.3286339655833968e-07
481 1.2585529347481789e-07
482 1.1921849306789818e-07
483 1.1293151084301514e-07
484 1.069766166825837e-07
485 1.0133619692710931e-07
486 9.599472852904284e-08
487 9.093341825332011e-08
488 8.613939196599216e-08
489 8.159862515209676e-08
490 7.729751975546753e-08
491 7.322345528572764e-08
492 6.936479915203619e-08
493 6.57099781685289e-08
494 6.224692745752194e-08
495 5.896657665795129e-08
496 5.585950062511546e-08
497 5.291656163586972e-08
498 5.012926872369238e-08
499 4.748859133659835e-08</code></pre><h1 id="pytorch-tensors"><a href="#pytorch-tensors" class="headerlink" title="pytorch:tensors"></a><span id="header2">pytorch:tensors</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p>
<p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p>
<p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<pre><code>99 784.6785888671875
199 5.850834846496582
299 0.07988587021827698
399 0.0017072007758542895
499 0.00015852594515308738</code></pre><h1 id="pytorch-tensors-and-autograd"><a href="#pytorch-tensors-and-autograd" class="headerlink" title="pytorch:tensors and autograd"></a><span id="header3">pytorch:tensors and autograd</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Tensors-and-autograd"><a href="#PyTorch-Tensors-and-autograd" class="headerlink" title="PyTorch: Tensors and autograd"></a>PyTorch: Tensors and autograd</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation computes the forward pass using operations on PyTorch<br>Tensors, and uses PyTorch autograd to compute gradients.</p>
<p>A PyTorch Tensor represents a node in a computational graph. If <code>x</code> is a<br>Tensor that has <code>x.requires_grad=True</code> then <code>x.grad</code> is another Tensor<br>holding the gradient of <code>x</code> with respect to some scalar value.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></span><br><span class="line"><span class="comment"># with respect to these Tensors during the backward pass.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Tensors during the backward pass.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Tensors; these</span></span><br><span class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></span><br><span class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></span><br><span class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss using operations on Tensors.</span></span><br><span class="line">    <span class="comment"># Now loss is a Tensor of shape (1,)</span></span><br><span class="line">    <span class="comment"># loss.item() gets the scalar value held in the loss.</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></span><br><span class="line">    <span class="comment"># gradient of loss with respect to all Tensors with requires_grad=True.</span></span><br><span class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Tensors holding the gradient</span></span><br><span class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually update weights using gradient descent. Wrap in torch.no_grad()</span></span><br><span class="line">    <span class="comment"># because weights have requires_grad=True, but we don't need to track this</span></span><br><span class="line">    <span class="comment"># in autograd.</span></span><br><span class="line">    <span class="comment"># An alternative way is to operate on weight.data and weight.grad.data.</span></span><br><span class="line">    <span class="comment"># Recall that tensor.data gives a tensor that shares the storage with</span></span><br><span class="line">    <span class="comment"># tensor, but doesn't track history.</span></span><br><span class="line">    <span class="comment"># You can also use torch.optim.SGD to achieve this.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure>

<pre><code>99 850.6499633789062
199 5.497010231018066
299 0.0542689710855484
399 0.0009686618577688932
499 0.000102342150057666</code></pre><h1 id="pytorch-defining-new-autograd-functions"><a href="#pytorch-defining-new-autograd-functions" class="headerlink" title="pytorch:defining new autograd functions"></a><span id="header4">pytorch:defining new autograd functions</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Defining-New-autograd-Functions"><a href="#PyTorch-Defining-New-autograd-Functions" class="headerlink" title="PyTorch: Defining New autograd Functions"></a>PyTorch: Defining New autograd Functions</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation computes the forward pass using operations on PyTorch<br>Variables, and uses PyTorch autograd to compute gradients.</p>
<p>In this implementation we implement our own custom autograd function to perform<br>the ReLU function.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># To apply our Function, we use Function.apply method. We alias this as 'relu'.</span></span><br><span class="line">    relu = MyReLU.apply</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure>

<pre><code>99 173.57586669921875
199 0.16617316007614136
299 0.0004797253059223294
399 3.693650069180876e-05
499 1.2812281056540087e-05</code></pre><h1 id="pytorch-static-graphs"><a href="#pytorch-static-graphs" class="headerlink" title="pytorch:static graphs"></a><span id="header5">pytorch:static graphs</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Tensors-1"><a href="#PyTorch-Tensors-1" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p>
<p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p>
<p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p>
<p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<pre><code>99 784.6785888671875
199 5.850834846496582
299 0.07988587021827698
399 0.0017072007758542895
499 0.00015852594515308738</code></pre><h1 id="pytorch-nn"><a href="#pytorch-nn" class="headerlink" title="pytorch:nn"></a><span id="header6">pytorch:nn</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation uses the nn package from PyTorch to build the network.<br>PyTorch autograd makes it easy to define computational graphs and take gradients,<br>but raw autograd can be a bit too low-level for defining complex neural networks;<br>this is where the nn package can help. The nn package defines a set of Modules,<br>which you can think of as a neural network layer that has produces output from<br>input and may have some trainable weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></span><br><span class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></span><br><span class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></span><br><span class="line"><span class="comment"># linear function, and holds internal Tensors for its weight and bias.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></span><br><span class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></span><br><span class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></span><br><span class="line">    <span class="comment"># doing so you pass a Tensor of input data to the Module and it produces</span></span><br><span class="line">    <span class="comment"># a Tensor of output data.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss. We pass Tensors containing the predicted and true</span></span><br><span class="line">    <span class="comment"># values of y, and the loss function returns a Tensor containing the</span></span><br><span class="line">    <span class="comment"># loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></span><br><span class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></span><br><span class="line">    <span class="comment"># in Tensors with requires_grad=True, so this call will compute gradients for</span></span><br><span class="line">    <span class="comment"># all learnable parameters in the model.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Tensor, so</span></span><br><span class="line">    <span class="comment"># we can access its gradients like we did before.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param -= learning_rate * param.grad</span><br></pre></td></tr></table></figure>

<pre><code>99 2.456005573272705
199 0.04037925601005554
299 0.001298694172874093
399 5.4667791118845344e-05
499 2.6507393613428576e-06</code></pre><h1 id="pytorch-optim"><a href="#pytorch-optim" class="headerlink" title="pytorch:optim"></a><span id="header7">pytorch:optim</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation uses the nn package from PyTorch to build the network.</p>
<p>Rather than manually updating the weights of the model as we have been doing,<br>we use the optim package to define an Optimizer that will update the weights<br>for us. The optim package defines many optimization algorithms that are commonly<br>used for deep learning, including SGD+momentum, RMSProp, Adam, etc.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></span><br><span class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></span><br><span class="line"><span class="comment"># optimization algorithms. The first argument to the Adam constructor tells the</span></span><br><span class="line"><span class="comment"># optimizer which Tensors it should update.</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></span><br><span class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable</span></span><br><span class="line">    <span class="comment"># weights of the model). This is because by default, gradients are</span></span><br><span class="line">    <span class="comment"># accumulated in buffers( i.e, not overwritten) whenever .backward()</span></span><br><span class="line">    <span class="comment"># is called. Checkout docs of torch.autograd.backward for more details.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<pre><code>99 40.16399383544922
199 0.3977137506008148
299 0.001604456570930779
399 1.6438591046608053e-05
499 8.815557350771996e-08</code></pre><h1 id="pytorch-custom-nn-modules"><a href="#pytorch-custom-nn-modules" class="headerlink" title="pytorch:custom nn modules"></a><span id="header8">pytorch:custom nn modules</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch: Custom nn Modules"></a>PyTorch: Custom nn Modules</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p>
<p>This implementation defines the model as a custom Module subclass. Whenever you<br>want a model more complex than a simple sequence of existing Modules you will<br>need to define your model this way.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as</span></span><br><span class="line"><span class="string">        member variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward function we accept a Tensor of input data and we must return</span></span><br><span class="line"><span class="string">        a Tensor of output data. We can use Modules defined in the constructor as</span></span><br><span class="line"><span class="string">        well as arbitrary operators on Tensors.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.linear2(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></span><br><span class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></span><br><span class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>




<h1 id="pytorch-control-flow-weight-sharing"><a href="#pytorch-control-flow-weight-sharing" class="headerlink" title="pytorch:control flow+weight sharing"></a><span id="header9">pytorch:control flow+weight sharing</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h2 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch: Control Flow + Weight Sharing"></a>PyTorch: Control Flow + Weight Sharing</h2><p>To showcase the power of PyTorch dynamic graphs, we will implement a very strange<br>model: a fully-connected ReLU network that on each forward pass randomly chooses<br>a number between 1 and 4 and has that many hidden layers, reusing the same<br>weights multiple times to compute the innermost hidden layers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we construct three nn.Linear instances that we will use</span></span><br><span class="line"><span class="string">        in the forward pass.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(DynamicNet, self).__init__()</span><br><span class="line">        self.input_linear = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.middle_linear = torch.nn.Linear(H, H)</span><br><span class="line">        self.output_linear = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</span></span><br><span class="line"><span class="string">        and reuse the middle_linear Module that many times to compute hidden layer</span></span><br><span class="line"><span class="string">        representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Since each forward pass builds a dynamic computation graph, we can use normal</span></span><br><span class="line"><span class="string">        Python control-flow operators like loops or conditional statements when</span></span><br><span class="line"><span class="string">        defining the forward pass of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Here we also see that it is perfectly safe to reuse the same Module many</span></span><br><span class="line"><span class="string">        times when defining a computational graph. This is a big improvement from Lua</span></span><br><span class="line"><span class="string">        Torch, where each Module could be used only once.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</span><br><span class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.output_linear(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = DynamicNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></span><br><span class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<pre><code>99 18.901199340820312
199 7.5054731369018555
299 1.1003623008728027
399 0.8731748461723328
499 2.003668785095215</code></pre>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-neural_newworks</title>
    <url>/2020/07/23/Pytorch-Learning-neural-newworks/</url>
    <content><![CDATA[<p>Pytorch-Learning-autograd:</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><p>Neural networks can be constructed using the <code>torch.nn</code> package.</p>
<p>Now that you had a glimpse of <code>autograd</code>, <code>nn</code> depends on<br><code>autograd</code> to define models and differentiate them.<br>An <code>nn.Module</code> contains layers, and a method <code>forward(input)</code> that<br>returns the <code>output</code>.</p>
<p>For example, look at this network that classifies digit images:</p>
<p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt></p>
<p>   convnet</p>
<p>It is a simple feed-forward network. It takes the input, feeds it<br>through several layers one after the other, and then finally gives the<br>output.</p>
<p>A typical training procedure for a neural network is as follows:</p>
<ul>
<li>Define the neural network that has some learnable parameters (or<br>weights)</li>
<li>Iterate over a dataset of inputs</li>
<li>Process input through the network</li>
<li>Compute the loss (how far is the output from being correct)</li>
<li>Propagate gradients back into the networkâ€™s parameters</li>
<li>Update the weights of the network, typically using a simple update rule:<br><code>weight = weight - learning_rate * gradient</code></li>
</ul>
<h2 id="Define-the-network"><a href="#Define-the-network" class="headerlink" title="Define the network"></a>Define the network</h2><p>Letâ€™s define this network:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)  <span class="comment"># 6*6 from image dimension </span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>

<pre><code>Net(
  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=576, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)</code></pre><p>You just have to define the <code>forward</code> function, and the <code>backward</code><br>function (where gradients are computed) is automatically defined for you<br>using <code>autograd</code>.<br>You can use any of the Tensor operations in the <code>forward</code> function.</p>
<p>The learnable parameters of a model are returned by <code>net.parameters()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's .weight</span></span><br></pre></td></tr></table></figure>

<pre><code>10
torch.Size([6, 1, 3, 3])</code></pre><p>Letâ€™s try a random 32x32 input.<br>Note: expected input size of this net (LeNet) is 32x32. To use this net on<br>the MNIST dataset, please resize the images from the dataset to 32x32.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.0416,  0.0926, -0.0761, -0.0135, -0.0745, -0.0158,  0.0696, -0.0040,
         -0.0099, -0.1799]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>Zero the gradient buffers of all parameters and backprops with random<br>gradients:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<div class="alert alert-info">
    <h4>Note</h4>

<pre><code>torch.nn only supports mini-batches. The entire torch.nn
package only supports inputs that are a mini-batch of samples, and not
a single sample.

For example, nn.Conv2d will take in a 4D Tensor of
nSamples x nChannels x Height x Width.

If you have a single sample, just use input.unsqueeze(0) to add
a fake batch dimension.</code></pre></div>

<p>Before proceeding further, letâ€™s recap all the classes youâ€™ve seen so far.</p>
<p><strong>Recap:</strong></p>
<ul>
<li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd<br>operations like <code>backward()</code>. Also <em>holds the gradient</em> w.r.t. the<br>tensor.</li>
<li><code>nn.Module</code> - Neural network module. <em>Convenient way of<br>encapsulating parameters</em>, with helpers for moving them to GPU,<br>exporting, loading, etc.</li>
<li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically<br>registered as a parameter when assigned as an attribute to a</em><br><code>Module</code>.</li>
<li><code>autograd.Function</code> - Implements <em>forward and backward definitions<br>of an autograd operation</em>. Every <code>Tensor</code> operation creates at<br>least a single <code>Function</code> node that connects to functions that<br>created a <code>Tensor</code> and <em>encodes its history</em>.</li>
</ul>
<p><strong>At this point, we covered:</strong></p>
<ul>
<li>Defining a neural network</li>
<li>Processing inputs and calling backward</li>
</ul>
<p><strong>Still Left:</strong></p>
<ul>
<li>Computing the loss</li>
<li>Updating the weights of the network</li>
</ul>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>A loss function takes the (output, target) pair of inputs, and computes a<br>value that estimates how far away the output is from the target.</p>
<p>There are several different<br><code>loss functions &lt;https://pytorch.org/docs/nn.html#loss-functions&gt;</code>_ under the<br>nn package .<br>A simple loss is: <code>nn.MSELoss</code> which computes the mean-squared error<br>between the input and the target.</p>
<p>For example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.9128, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>Now, if you follow <code>loss</code> in the backward direction, using its<br><code>.grad_fn</code> attribute, you will see a graph of computations that looks<br>like this:</p>
<p>::</p>
<pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
      -&gt; MSELoss
      -&gt; loss</code></pre><p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated<br>w.r.t. the loss, and all Tensors in the graph that has <code>requires_grad=True</code><br>will have their <code>.grad</code> Tensor accumulated with the gradient.</p>
<p>For illustration, let us follow a few steps backward:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure>

<pre><code>&lt;MseLossBackward object at 0x000001F109346C88&gt;
&lt;AddmmBackward object at 0x000001F109346EB8&gt;
&lt;AccumulateGrad object at 0x000001F109346C88&gt;</code></pre><h2 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a>Backprop</h2><p>To backpropagate the error all we have to do is to <code>loss.backward()</code>.<br>You need to clear the existing gradients though, else gradients will be<br>accumulated to existing gradients.</p>
<p>Now we shall call <code>loss.backward()</code>, and have a look at conv1â€™s bias<br>gradients before and after the backward.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<pre><code>conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([-0.0032, -0.0131,  0.0148,  0.0334, -0.0327, -0.0073])</code></pre><p>Now, we have seen how to use loss functions.</p>
<p><strong>Read Later:</strong></p>
<p>  The neural network package contains various modules and loss functions<br>  that form the building blocks of deep neural networks. A full list with<br>  documentation is <code>here &lt;https://pytorch.org/docs/nn&gt;</code>_.</p>
<p><strong>The only thing left to learn is:</strong></p>
<ul>
<li>Updating the weights of the network</li>
</ul>
<h2 id="Update-the-weights"><a href="#Update-the-weights" class="headerlink" title="Update the weights"></a>Update the weights</h2><p>The simplest update rule used in practice is the Stochastic Gradient<br>Descent (SGD):</p>
<pre><code>``weight = weight - learning_rate * gradient``</code></pre><p>We can implement this using simple Python code:</p>
<p>.. code:: python</p>
<pre><code>learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)</code></pre><p>However, as you use neural networks, you want to use various different<br>update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.<br>To enable this, we built a small package: <code>torch.optim</code> that<br>implements all these methods. Using it is very simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>

<p>.. Note::</p>
<pre><code>Observe how gradient buffers had to be manually set to zero using
``optimizer.zero_grad()``. This is because gradients are accumulated
as explained in the `Backprop`_ section.</code></pre><h2 id="æˆ‘ä¸è®¤è¯†çš„å•è¯"><a href="#æˆ‘ä¸è®¤è¯†çš„å•è¯" class="headerlink" title="æˆ‘ä¸è®¤è¯†çš„å•è¯"></a>æˆ‘ä¸è®¤è¯†çš„å•è¯</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">feed-forward:å‰å‘</span><br><span class="line">glimpse:ä¸€ç¥</span><br><span class="line">proce:è¿›è¡Œ</span><br><span class="line">recap:å›é¡¾</span><br><span class="line">encapsulat:å°è£…</span><br><span class="line">assign:åˆ†é…</span><br><span class="line">For illustration:ä¸ºäº†è¯´æ˜</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-tensor</title>
    <url>/2020/07/23/Pytorch-Learning-tensor/</url>
    <content><![CDATA[<p>Pytorch-Learning-tensor:</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="What-is-PyTorch"><a href="#What-is-PyTorch" class="headerlink" title="What is PyTorch?"></a>What is PyTorch?</h1><p>Itâ€™s a Python-based scientific computing package targeted at two sets of<br>audiences:</p>
<ul>
<li>A replacement for NumPy to use the power of GPUs</li>
<li>a deep learning research platform that provides maximum flexibility<br>and speed</li>
</ul>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>Tensors</p>
<p>Tensors are similar to NumPyâ€™s ndarrays, with the addition being that<br>Tensors can also be used on a GPU to accelerate computing.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<div class="alert alert-info"><h4>Note</h4><p>An uninitialized matrix is declared,
    but does not contain definite known
    values before it is used. When an
    uninitialized matrix is created,
    whatever values were in the allocated
    memory at the time will appear as the initial values.</p></div>



<p>Construct a 5x3 matrix, uninitialized:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-3.4374e-14,  7.1046e-43, -3.4374e-14],
        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],
        [-3.4374e-14,  7.1046e-43, -3.4374e-14],
        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],
        [-3.4374e-14,  7.1046e-43, -3.4374e-14]])</code></pre><p>Construct a randomly initialized matrix:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0.6385, 0.8264, 0.0737],
        [0.1567, 0.5029, 0.7141],
        [0.8297, 0.3453, 0.2860],
        [0.0158, 0.3826, 0.7823],
        [0.3434, 0.0977, 0.1530]])</code></pre><p>Construct a matrix filled zeros and of dtype long:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])</code></pre><p>Construct a tensor directly from data:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.5000, 3.0000])</code></pre><p>or create a tensor based on an existing tensor. These methods<br>will reuse properties of the input tensor, e.g. dtype, unless<br>new values are provided by user</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)      <span class="comment"># new_* methods take in sizes</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)    <span class="comment"># override dtype!</span></span><br><span class="line">print(x)                                      <span class="comment"># result has the same size</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 2.0072,  0.0294,  0.1776],
        [-0.3961, -1.7436, -0.1741],
        [ 0.7820,  0.5535, -0.0059],
        [-1.9826, -0.7387, -0.3942],
        [ 0.3501,  0.5796, -1.3633]])</code></pre><p>Get its size:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 3])</code></pre><div class="alert alert-info"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div>

<p>Operations</p>
<p>There are multiple syntaxes for operations. In the following<br>example, we will take a look at the addition operation.</p>
<p>Addition: syntax 1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: syntax 2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: providing an output tensor as argument</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: in-place</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],
        [-1.7379e-01, -9.1149e-01,  7.2974e-01],
        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],
        [-1.4469e+00, -1.0496e-01, -1.7707e-03],
        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><div class="alert alert-info"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.
    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div>

<p>You can use standard NumPy-like indexing with all bells and whistles!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([ 0.0294, -1.7436,  0.5535, -0.7387,  0.5796])</code></pre><p>Resizing: If you want to resize/reshape tensor, you can use <code>torch.view</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>If you have a one element tensor, use <code>.item()</code> to get the value as a<br>Python number</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1.0191])
1.0191349983215332</code></pre><p><strong>Read later:</strong></p>
<p>  100+ Tensor operations, including transposing, indexing, slicing,<br>  mathematical operations, linear algebra, random numbers, etc.,<br>  are described<br>  <code>here &lt;https://pytorch.org/docs/torch&gt;</code>_.</p>
<h2 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h2><p>Converting a Torch Tensor to a NumPy array and vice versa is a breeze.</p>
<p>The Torch Tensor and NumPy array will share their underlying memory<br>locations (if the Torch Tensor is on CPU), and changing one will change<br>the other.</p>
<p>Converting a Torch Tensor to a NumPy Array</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1., 1., 1., 1., 1.])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>[1. 1. 1. 1. 1.]</code></pre><p>See how the numpy array changed in value.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]</code></pre><p>Converting NumPy Array to Torch Tensor</p>
<p>See how changing the np array changed the Torch Tensor automatically</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><p>All the Tensors on the CPU except a CharTensor support converting to<br>NumPy and back.</p>
<h2 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h2><p>Tensors can be moved onto any device using the <code>.to</code> method.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="comment"># We will use ``torch.device`` objects to move tensors in and out of GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># a CUDA device object</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` can also change dtype together!</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">allocate:åˆ†é…</span><br><span class="line">mutate:å˜å¼‚</span><br><span class="line">Bridge:æ¡¥</span><br><span class="line">vice versa:åä¹‹äº¦ç„¶</span><br><span class="line">underlying:åº•å±‚çš„</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-torch.nn</title>
    <url>/2020/07/23/Pytorch-Learning-torch-nn/</url>
    <content><![CDATA[<p>Pytorch-Learning-torch.nn</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="What-is-torch-nn-really"><a href="#What-is-torch-nn-really" class="headerlink" title="What is torch.nn really?"></a>What is <code>torch.nn</code> <em>really</em>?</h1><p>by Jeremy Howard, <code>fast.ai &lt;https://www.fast.ai&gt;</code>_. Thanks to Rachel Thomas and Francisco Ingham.</p>
<p>We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,<br>click the link at the top of the page.</p>
<p>PyTorch provides the elegantly designed modules and classes <code>torch.nn &lt;https://pytorch.org/docs/stable/nn.html&gt;</code>_ ,<br><code>torch.optim &lt;https://pytorch.org/docs/stable/optim.html&gt;</code>_ ,<br><code>Dataset &lt;https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset&gt;</code>_ ,<br>and <code>DataLoader &lt;https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader&gt;</code>_<br>to help you create and train neural networks.<br>In order to fully utilize their power and customize<br>them for your problem, you need to really understand exactly what theyâ€™re<br>doing. To develop this understanding, we will first train basic neural net<br>on the MNIST data set without using any features from these models; we will<br>initially only use the most basic PyTorch tensor functionality. Then, we will<br>incrementally add one feature from <code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, or<br><code>DataLoader</code> at a time, showing exactly what each piece does, and how it<br>works to make the code either more concise, or more flexible.</p>
<p><strong>This tutorial assumes you already have PyTorch installed, and are familiar<br>with the basics of tensor operations.</strong> (If youâ€™re familiar with Numpy array<br>operations, youâ€™ll find the PyTorch tensor operations used here nearly identical).</p>
<h2 id="MNIST-data-setup"><a href="#MNIST-data-setup" class="headerlink" title="MNIST data setup"></a>MNIST data setup</h2><p>We will use the classic <code>MNIST &lt;http://deeplearning.net/data/mnist/&gt;</code>_ dataset,<br>which consists of black-and-white images of hand-drawn digits (between 0 and 9).</p>
<p>We will use <code>pathlib &lt;https://docs.python.org/3/library/pathlib.html&gt;</code>_<br>for dealing with paths (part of the Python 3 standard library), and will<br>download the dataset using<br><code>requests &lt;http://docs.python-requests.org/en/master/&gt;</code>_. We will only<br>import modules when we use them, so you can see exactly whatâ€™s being<br>used at each point.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">"data"</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">"mnist"</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">"http://deeplearning.net/data/mnist/"</span></span><br><span class="line">FILENAME = <span class="string">"mnist.pkl.gz"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).open(<span class="string">"wb"</span>).write(content)</span><br></pre></td></tr></table></figure>

<p>This dataset is in numpy array format, and has been stored using pickle,<br>a python-specific format for serializing data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.open((PATH / FILENAME).as_posix(), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">"latin-1"</span>)</span><br></pre></td></tr></table></figure>

<p>Each image is 28 x 28, and is being stored as a flattened row of length<br>784 (=28x28). Letâ€™s take a look at one; we need to reshape it to 2d<br>first.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">"gray"</span>)</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(50000, 784)</code></pre><p><img src="/2020/07/23/Pytorch-Learning-torch-nn/output_7_1.png" alt="png"></p>
<p>PyTorch uses <code>torch.tensor</code>, rather than numpy arrays, so we need to<br>convert our data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = map(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line">x_train, x_train.shape, y_train.min(), y_train.max()</span><br><span class="line">print(x_train, y_train)</span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.min(), y_train.max())</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])
torch.Size([50000, 784])
tensor(0) tensor(9)</code></pre><h2 id="Neural-net-from-scratch-no-torch-nn"><a href="#Neural-net-from-scratch-no-torch-nn" class="headerlink" title="Neural net from scratch (no torch.nn)"></a>Neural net from scratch (no torch.nn)</h2><p>Letâ€™s first create a model using nothing but PyTorch tensor operations. Weâ€™re assuming<br>youâ€™re already familiar with the basics of neural networks. (If youâ€™re not, you can<br>learn them at <code>course.fast.ai &lt;https://course.fast.ai&gt;</code>_).</p>
<p>PyTorch provides methods to create random or zero-filled tensors, which we will<br>use to create our weights and bias for a simple linear model. These are just regular<br>tensors, with one very special addition: we tell PyTorch that they require a<br>gradient. This causes PyTorch to record all of the operations done on the tensor,<br>so that it can calculate the gradient during back-propagation <em>automatically</em>!</p>
<p>For the weights, we set <code>requires_grad</code> <strong>after</strong> the initialization, since we<br>donâ€™t want that step included in the gradient. (Note that a trailling <code>_</code> in<br>PyTorch signifies that the operation is performed in-place.)</p>
<div class="alert alert-info"><h4>Note</h4><p>We are initializing the weights here with
   `Xavier initialisation <http: proceedings.mlr.press v9 glorot10a glorot10a.pdf>`_
   (by multiplying with 1/sqrt(n)).</http:></p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>Thanks to PyTorchâ€™s ability to calculate gradients automatically, we can<br>use any standard Python function (or callable object) as a model! So<br>letâ€™s just write a plain matrix multiplication and broadcasted addition<br>to create a simple linear model. We also need an activation function, so<br>weâ€™ll write <code>log_softmax</code> and use it. Remember: although PyTorch<br>provides lots of pre-written loss functions, activation functions, and<br>so forth, you can easily write your own using plain python. PyTorch will<br>even create fast GPU or vectorized CPU code for your function<br>automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x - x.exp().sum(<span class="number">-1</span>).log().unsqueeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>

<p>In the above, the <code>@</code> stands for the dot product operation. We will call<br>our function on one batch of data (in this case, 64 images).  This is<br>one <em>forward pass</em>.  Note that our predictions wonâ€™t be any better than<br>random at this stage, since we start with random weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bs = <span class="number">64</span>  <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs]  <span class="comment"># a mini-batch from x</span></span><br><span class="line">preds = model(xb)  <span class="comment"># predictions</span></span><br><span class="line">preds[<span class="number">0</span>], preds.shape</span><br><span class="line">print(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-2.2669, -2.6024, -2.8454, -1.5665, -2.7687, -2.2455, -2.6885, -2.4918,
        -2.1065, -2.1682], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])</code></pre><p>As you see, the <code>preds</code> tensor contains not only the tensor values, but also a<br>gradient function. Weâ€™ll use this later to do backprop.</p>
<p>Letâ€™s implement negative log-likelihood to use as the loss function<br>(again, we can just use standard Python):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nll</span><span class="params">(input, target)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -input[range(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>

<p>Letâ€™s check our loss with our random model, so we can see if we improve<br>after a backprop pass later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line">print(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.2549, grad_fn=&lt;NegBackward&gt;)</code></pre><p>Letâ€™s also implement a function to calculate the accuracy of our model.<br>For each prediction, if the index with the largest value matches the<br>target value, then the prediction was correct.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(out, yb)</span>:</span></span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).float().mean()</span><br></pre></td></tr></table></figure>

<p>Letâ€™s check the accuracy of our random model, so we can see if our<br>accuracy improves as our loss improves.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.1562)</code></pre><p>We can now run a training loop.  For each iteration, we will:</p>
<ul>
<li>select a mini-batch of data (of size <code>bs</code>)</li>
<li>use the model to make predictions</li>
<li>calculate the loss</li>
<li><code>loss.backward()</code> updates the gradients of the model, in this case, <code>weights</code><br>and <code>bias</code>.</li>
</ul>
<p>We now use these gradients to update the weights and bias.  We do this<br>within the <code>torch.no_grad()</code> context manager, because we do not want these<br>actions to be recorded for our next calculation of the gradient.  You can read<br>more about how PyTorchâ€™s Autograd records operations<br><code>here &lt;https://pytorch.org/docs/stable/notes/autograd.html&gt;</code>_.</p>
<p>We then set the<br>gradients to zero, so that we are ready for the next loop.<br>Otherwise, our gradients would record a running tally of all the operations<br>that had happened (i.e. <code>loss.backward()</code> <em>adds</em> the gradients to whatever is<br>already stored, rather than replacing them).</p>
<p>.. tip:: You can use the standard python debugger to step through PyTorch<br>   code, allowing you to check the various variable values at each step.<br>   Uncomment <code>set_trace()</code> below to try it out.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># how many epochs to train for</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line"><span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>Thatâ€™s it: weâ€™ve created and trained a minimal neural network (in this case, a<br>logistic regression, since we have no hidden layers) entirely from scratch!</p>
<p>Letâ€™s check the loss and accuracy and compare those to what we got<br>earlier. We expect that the loss will have decreased and accuracy to<br>have increased, and they have.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0781, grad_fn=&lt;NegBackward&gt;) tensor(1.)</code></pre><h2 id="Using-torch-nn-functional"><a href="#Using-torch-nn-functional" class="headerlink" title="Using torch.nn.functional"></a>Using torch.nn.functional</h2><p>We will now refactor our code, so that it does the same thing as before, only<br>weâ€™ll start taking advantage of PyTorchâ€™s <code>nn</code> classes to make it more concise<br>and flexible. At each step from here, we should be making our code one or more<br>of: shorter, more understandable, and/or more flexible.</p>
<p>The first and easiest step is to make our code shorter by replacing our<br>hand-written activation and loss functions with those from <code>torch.nn.functional</code><br>(which is generally imported into the namespace <code>F</code> by convention). This module<br>contains all the functions in the <code>torch.nn</code> library (whereas other parts of the<br>library contain classes). As well as a wide range of loss and activation<br>functions, youâ€™ll also find here some convenient functions for creating neural<br>nets, such as pooling functions. (There are also functions for doing convolutions,<br>linear layers, etc, but as weâ€™ll see, these are usually better handled using<br>other parts of the library.)</p>
<p>If youâ€™re using negative log likelihood loss and log softmax activation,<br>then Pytorch provides a single function <code>F.cross_entropy</code> that combines<br>the two. So we can even remove the activation function from our model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure>

<p>Note that we no longer call <code>log_softmax</code> in the <code>model</code> function. Letâ€™s<br>confirm that our loss and accuracy are the same as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0781, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)</code></pre><h2 id="Refactor-using-nn-Module"><a href="#Refactor-using-nn-Module" class="headerlink" title="Refactor using nn.Module"></a>Refactor using nn.Module</h2><p>Next up, weâ€™ll use <code>nn.Module</code> and <code>nn.Parameter</code>, for a clearer and more<br>concise training loop. We subclass <code>nn.Module</code> (which itself is a class and<br>able to keep track of state).  In this case, we want to create a class that<br>holds our weights, bias, and method for the forward step.  <code>nn.Module</code> has a<br>number of attributes and methods (such as <code>.parameters()</code> and <code>.zero_grad()</code>)<br>which we will be using.</p>
<div class="alert alert-info"><h4>Note</h4><p>``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a
   class we'll be using a lot. ``nn.Module`` is not to be confused with the Python
   concept of a (lowercase ``m``) `module <https: 3 docs.python.org tutorial modules.html>`_,
   which is a file of Python code that can be imported.</https:></p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>

<p>Since weâ€™re now using an object instead of just using a function, we<br>first have to instantiate our model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure>

<p>Now we can calculate the loss in the same way as before. Note that<br><code>nn.Module</code> objects are used as if they are functions (i.e they are<br><em>callable</em>), but behind the scenes Pytorch will call our <code>forward</code><br>method automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.4768, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>Previously for our training loop we had to update the values for each parameter<br>by name, and manually zero out the grads for each parameter separately, like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    weights -&#x3D; weights.grad * lr</span><br><span class="line">    bias -&#x3D; bias.grad * lr</span><br><span class="line">    weights.grad.zero_()</span><br><span class="line">    bias.grad.zero_()</span><br></pre></td></tr></table></figure>


<p>Now we can take advantage of model.parameters() and model.zero_grad() (which<br>are both defined by PyTorch for <code>nn.Module</code>) to make those steps more concise<br>and less prone to the error of forgetting some of our parameters, particularly<br>if we had a more complicated model:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure>


<p>Weâ€™ll wrap our little training loop in a <code>fit</code> function so we can run it<br>again later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>

<p>Letâ€™s double-check that our loss has gone down:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0860, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-nn-Linear"><a href="#Refactor-using-nn-Linear" class="headerlink" title="Refactor using nn.Linear"></a>Refactor using nn.Linear</h2><p>We continue to refactor our code.  Instead of manually defining and<br>initializing <code>self.weights</code> and <code>self.bias</code>, and calculating <code>xb  @
self.weights + self.bias</code>, we will instead use the Pytorch class<br><code>nn.Linear &lt;https://pytorch.org/docs/stable/nn.html#linear-layers&gt;</code>_ for a<br>linear layer, which does all that for us. Pytorch has many types of<br>predefined layers that can greatly simplify our code, and often makes it<br>faster too.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure>

<p>We instantiate our model and calculate the loss in the same way as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.3752, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>We are still able to use our same <code>fit</code> method as before.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0814, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-optim"><a href="#Refactor-using-optim" class="headerlink" title="Refactor using optim"></a>Refactor using optim</h2><p>Pytorch also has a package with various optimization algorithms, <code>torch.optim</code>.<br>We can use the <code>step</code> method from our optimizer to take a forward step, instead<br>of manually updating each parameter.</p>
<p>This will let us replace our previous manually coded optimization step:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure>
<p>and instead use just:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">opt.step()</span><br><span class="line">opt.zero_grad()</span><br></pre></td></tr></table></figure>

<p>(<code>optim.zero_grad()</code> resets the gradient to 0 and we need to call it before<br>computing the gradient for the next minibatch.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>

<p>Weâ€™ll define a little function to create our model and optimizer so we<br>can reuse it in the future.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.2501, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.0822, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-Dataset"><a href="#Refactor-using-Dataset" class="headerlink" title="Refactor using Dataset"></a>Refactor using Dataset</h2><p>PyTorch has an abstract Dataset class.  A Dataset can be anything that has<br>a <code>__len__</code> function (called by Pythonâ€™s standard <code>len</code> function) and<br>a <code>__getitem__</code> function as a way of indexing into it.<br><code>This tutorial &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code>_<br>walks through a nice example of creating a custom <code>FacialLandmarkDataset</code> class<br>as a subclass of <code>Dataset</code>.</p>
<p>PyTorchâ€™s <code>TensorDataset &lt;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset&gt;</code>_<br>is a Dataset wrapping tensors. By defining a length and way of indexing,<br>this also gives us a way to iterate, index, and slice along the first<br>dimension of a tensor. This will make it easier to access both the<br>independent and dependent variables in the same line as we train.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br></pre></td></tr></table></figure>

<p>Both <code>x_train</code> and <code>y_train</code> can be combined in a single <code>TensorDataset</code>,<br>which will be easier to iterate over and slice.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>

<p>Previously, we had to iterate through minibatches of x and y values separately:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xb &#x3D; x_train[start_i:end_i]</span><br><span class="line">yb &#x3D; y_train[start_i:end_i]</span><br></pre></td></tr></table></figure>

<p>Now, we can do these two steps together:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xb,yb &#x3D; train_ds[i*bs : i*bs+bs]</span><br></pre></td></tr></table></figure>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0801, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-DataLoader"><a href="#Refactor-using-DataLoader" class="headerlink" title="Refactor using DataLoader"></a>Refactor using DataLoader</h2><p>Pytorchâ€™s <code>DataLoader</code> is responsible for managing batches. You can<br>create a <code>DataLoader</code> from any <code>Dataset</code>. <code>DataLoader</code> makes it easier<br>to iterate over batches. Rather than having to use <code>train_ds[i*bs : i*bs+bs]</code>,<br>the DataLoader gives us each minibatch automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure>

<p>Previously, our loop iterated over batches (xb, yb) like this:<br>::<br>      for i in range((n-1)//bs + 1):<br>          xb,yb = train_ds[i<em>bs : i</em>bs+bs]<br>          pred = model(xb)</p>
<p>Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:<br>::<br>      for xb,yb in train_dl:<br>          pred = model(xb)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0824, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>Thanks to Pytorchâ€™s <code>nn.Module</code>, <code>nn.Parameter</code>, <code>Dataset</code>, and <code>DataLoader</code>,<br>our training loop is now dramatically smaller and easier to understand. Letâ€™s<br>now try to add the basic features necessary to create effecive models in practice.</p>
<h2 id="Add-validation"><a href="#Add-validation" class="headerlink" title="Add validation"></a>Add validation</h2><p>In section 1, we were just trying to get a reasonable training loop set up for<br>use on our training data.  In reality, you <strong>always</strong> should also have<br>a <code>validation set &lt;https://www.fast.ai/2017/11/13/validation-sets/&gt;</code>_, in order<br>to identify if you are overfitting.</p>
<p>Shuffling the training data is<br><code>important &lt;https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks&gt;</code>_<br>to prevent correlation between batches and overfitting. On the other hand, the<br>validation loss will be identical whether we shuffle the validation set or not.<br>Since shuffling takes extra time, it makes no sense to shuffle the validation data.</p>
<p>Weâ€™ll use a batch size for the validation set that is twice as large as<br>that for the training set. This is because the validation set does not<br>need backpropagation and thus takes less memory (it doesnâ€™t need to<br>store the gradients). We take advantage of this to use a larger batch<br>size and compute the loss more quickly.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>We will calculate and print the validation loss at the end of each epoch.</p>
<p>(Note that we always call <code>model.train()</code> before training, and <code>model.eval()</code><br>before inference, because these are used by layers such as <code>nn.BatchNorm2d</code><br>and <code>nn.Dropout</code> to ensure appropriate behaviour for these different phases.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = sum(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line"></span><br><span class="line">    print(epoch, valid_loss / len(valid_dl))</span><br></pre></td></tr></table></figure>

<pre><code>0 tensor(0.3169)
1 tensor(0.4910)</code></pre><h2 id="Create-fit-and-get-data"><a href="#Create-fit-and-get-data" class="headerlink" title="Create fit() and get_data()"></a>Create fit() and get_data()</h2><p>Weâ€™ll now do a little refactoring of our own. Since we go through a similar<br>process twice of calculating the loss for both the training set and the<br>validation set, letâ€™s make that into its own function, <code>loss_batch</code>, which<br>computes the loss for one batch.</p>
<p>We pass an optimizer in for the training set, and use it to perform<br>backprop.  For the validation set, we donâ€™t pass an optimizer, so the<br>method doesnâ€™t perform backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_batch</span><span class="params">(model, loss_func, xb, yb, opt=None)</span>:</span></span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), len(xb)</span><br></pre></td></tr></table></figure>

<p><code>fit</code> runs the necessary operations to train our model and compute the<br>training and validation losses for each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(epochs, model, loss_func, opt, train_dl, valid_dl)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = zip(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)</span><br><span class="line"></span><br><span class="line">        print(epoch, val_loss)</span><br></pre></td></tr></table></figure>

<p><code>get_data</code> returns dataloaders for the training and validation sets.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(train_ds, valid_ds, bs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>Now, our whole process of obtaining the data loaders and fitting the<br>model can be run in 3 lines of code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.30705235414505005
1 0.31455935287475584</code></pre><p>You can use these basic 3 lines of code to train a wide variety of models.<br>Letâ€™s see if we can use them to train a convolutional neural network (CNN)!</p>
<h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>We are now going to build our neural network with three convolutional layers.<br>Because none of the functions in the previous section assume anything about<br>the model form, weâ€™ll be able to use them to train a CNN without any modification.</p>
<p>We will use Pytorchâ€™s predefined<br><code>Conv2d &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code>_ class<br>as our convolutional layer. We define a CNN with 3 convolutional layers.<br>Each convolution is followed by a ReLU.  At the end, we perform an<br>average pooling.  (Note that <code>view</code> is PyTorchâ€™s version of numpyâ€™s<br><code>reshape</code>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        xb = xb.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> xb.view(<span class="number">-1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>

<p><code>Momentum &lt;https://cs231n.github.io/neural-networks-3/#sgd&gt;</code>_ is a variation on<br>stochastic gradient descent that takes previous updates into account as well<br>and generally leads to faster training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.8898362560272217
1 0.7796085683822632</code></pre><h2 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h2><p><code>torch.nn</code> has another handy class we can use to simply our code:<br><code>Sequential &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential&gt;</code>_ .<br>A <code>Sequential</code> object runs each of the modules contained within it, in a<br>sequential manner. This is a simpler way of writing our neural network.</p>
<p>To take advantage of this, we need to be able to easily define a<br><strong>custom layer</strong> from a given function.  For instance, PyTorch doesnâ€™t<br>have a <code>view</code> layer, and we need to create one for our network. <code>Lambda</code><br>will create a layer that we can then use when defining a network with<br><code>Sequential</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure>

<p>The model created with <code>Sequential</code> is simply:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.4746176950454712
1 0.3015344765663147</code></pre><h2 id="Wrapping-DataLoader"><a href="#Wrapping-DataLoader" class="headerlink" title="Wrapping DataLoader"></a>Wrapping DataLoader</h2><p>Our CNN is fairly concise, but it only works with MNIST, because:</p>
<ul>
<li>It assumes the input is a 28*28 long vector</li>
<li>It assumes that the final CNN grid size is 4*4 (since thatâ€™s the average<br>pooling kernel size we used)</li>
</ul>
<p>Letâ€™s get rid of these two assumptions, so our model works with any 2d<br>single channel image. First, we can remove the initial Lambda layer but<br>moving the data preprocessing into a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WrappedDataLoader</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, func)</span>:</span></span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        batches = iter(self.dl)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> batches:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>

<p>Next, we can replace <code>nn.AvgPool2d</code> with <code>nn.AdaptiveAvgPool2d</code>, which<br>allows us to define the size of the <em>output</em> tensor we want, rather than<br>the <em>input</em> tensor we have. As a result, our model will work with any<br>size input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>Letâ€™s try it out:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.43946951394081113
1 0.2360862446308136</code></pre><h2 id="Using-your-GPU"><a href="#Using-your-GPU" class="headerlink" title="Using your GPU"></a>Using your GPU</h2><p>If youâ€™re lucky enough to have access to a CUDA-capable GPU (you can<br>rent one for about $0.50/hour from most cloud providers) you can<br>use it to speed up your code. First check that your GPU is working in<br>Pytorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>

<pre><code>False</code></pre><p>And then create a device object for it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dev = torch.device(</span><br><span class="line">    <span class="string">"cuda"</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>

<p>Letâ€™s update <code>preprocess</code> to move batches to the GPU:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(dev), y.to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>

<p>Finally, we can move our model to the GPU.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>You should find it runs faster now:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.20087482118606567
1 0.21629996614456176</code></pre><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>We now have a general data pipeline and training loop which you can use for<br>training many types of models using Pytorch. To see how simple training a model<br>can now be, take a look at the <code>mnist_sample</code> sample notebook.</p>
<p>Of course, there are many things youâ€™ll want to add, such as data augmentation,<br>hyperparameter tuning, monitoring training, transfer learning, and so forth.<br>These features are available in the fastai library, which has been developed<br>using the same design approach shown in this tutorial, providing a natural<br>next step for practitioners looking to take their models further.</p>
<p>We promised at the start of this tutorial weâ€™d explain through example each of<br><code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, and <code>DataLoader</code>. So letâ€™s summarize<br>what weâ€™ve seen:</p>
<ul>
<li><p><strong>torch.nn</strong></p>
<ul>
<li><code>Module</code>: creates a callable which behaves like a function, but can also<br>contain state(such as neural net layer weights). It knows what <code>Parameter</code> (s) it<br>contains and can zero all their gradients, loop through them for weight updates, etc.</li>
<li><code>Parameter</code>: a wrapper for a tensor that tells a <code>Module</code> that it has weights<br>that need updating during backprop. Only tensors with the <code>requires_grad</code> attribute set are updated</li>
<li><code>functional</code>: a module(usually imported into the <code>F</code> namespace by convention)<br>which contains activation functions, loss functions, etc, as well as non-stateful<br>versions of layers such as convolutional and linear layers.</li>
</ul>
</li>
<li><p><code>torch.optim</code>: Contains optimizers such as <code>SGD</code>, which update the weights<br>of <code>Parameter</code> during the backward step</p>
</li>
<li><p><code>Dataset</code>: An abstract interface of objects with a <code>__len__</code> and a <code>__getitem__</code>,<br>including classes provided with Pytorch such as <code>TensorDataset</code></p>
</li>
<li><p><code>DataLoader</code>: Takes any <code>Dataset</code> and creates an iterator which returns batches of data.</p>
</li>
</ul>
<h2 id="æˆ‘ä¸ä¼šçš„å•è¯"><a href="#æˆ‘ä¸ä¼šçš„å•è¯" class="headerlink" title="æˆ‘ä¸ä¼šçš„å•è¯"></a>æˆ‘ä¸ä¼šçš„å•è¯</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">utilize:åˆ©ç”¨</span><br><span class="line">customize:å®šåˆ¶</span><br><span class="line">refactor:é‡æ„</span><br><span class="line">identical:ç›¸åŒçš„</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Learning-autograd</title>
    <url>/2020/07/23/Pytorch-Learning-autograd/</url>
    <content><![CDATA[<p>Pytorch-Learning-autograd:<br><a id="more"></a></p>
<p><div style="text-align:center;font-size:30px">Pytorch1.5.1å®˜ç½‘æ•™ç¨‹ç›®å½•</div></p>
<ul>
<li>1.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/" target="_blank" rel="noopener">Learning</a><ul>
<li>1.1<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-tensor/" target="_blank" rel="noopener">Pytorch-Learning-tensor</a></li>
<li>1.2<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-autograd/" target="_blank" rel="noopener">Pytorch-Learning-autograd</a></li>
<li>1.3<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-neural-newworks/" target="_blank" rel="noopener">Pytorch-Learning-neural_newworks</a></li>
<li>1.4<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-examples/" target="_blank" rel="noopener">Pytorch-Learning-examples</a></li>
<li>1.5<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-torch-nn/" target="_blank" rel="noopener">Pytorch-Learning-torch.nn</a></li>
<li>1.6<a href="https://githubzhangshuai.github.io/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/" target="_blank" rel="noopener">Pytorch-Learning-cifar10tutorial-visualizing</a></li>
</ul>
</li>
<li>2.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/" target="_blank" rel="noopener">Image</a><ul>
<li>2.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">å¾®è°ƒTorchVisionå¯¹è±¡æ£€æµ‹</a></li>
<li>2.2<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">è®¡ç®—æœºè§†è§‰è¿ç§»å­¦ä¹ </a></li>
<li>2.3[å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ](<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" target="_blank" rel="noopener">https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</a></li>
<li>2.4<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">DCGANæ•™ç¨‹</a></li>
</ul>
</li>
<li>3.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/" target="_blank" rel="noopener">Audio</a><ul>
<li>3.1<a href="https://githubzhangshuai.github.io/2020/07/24/Pytorch-Audio-torchaudio/" target="_blank" rel="noopener">torchaudio</a></li>
</ul>
</li>
<li>4.<a href="https://githubzhangshuai.github.io/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/" target="_blank" rel="noopener">Text</a><ul>
<li>4.1<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" target="_blank" rel="noopener">ç”¨NN.TRANFORMERå’ŒTORCHTEXTè¿›è¡Œåºåˆ—åˆ°åºåˆ—å»ºæ¨¡</a></li>
<li>4.2<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">ä½¿ç”¨å­—ç¬¦çº§RNNå¯¹åç§°è¿›è¡Œåˆ†ç±»</a></li>
<li>4.3<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/" target="_blank" rel="noopener">ç”¨å­—ç¬¦çº§RNNç”Ÿæˆåç§°</a></li>
<li>4.4<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">ä½¿ç”¨Sequence2Sequenceç½‘ç»œå’Œæ³¨æ„åŠ›è¿›è¡Œç¿»è¯‘ä½¿ç”¨Sequence2Sequenceç½‘ç»œå’Œæ³¨æ„åŠ›è¿›è¡Œç¿»è¯‘</a></li>
<li>4.5<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">TORCHTEXTçš„æ–‡æœ¬åˆ†ç±»</a></li>
<li>4.6<a href="https://githubzhangshuai.github.io/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/" target="_blank" rel="noopener">TORCHTEXTçš„è¯­è¨€ç¿»è¯‘</a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1 id="Autograd-Automatic-Differentiation"><a href="#Autograd-Automatic-Differentiation" class="headerlink" title="Autograd: Automatic Differentiation"></a>Autograd: Automatic Differentiation</h1><p>Central to all neural networks in PyTorch is the <code>autograd</code> package.<br>Letâ€™s first briefly visit this, and we will then go to training our<br>first neural network.</p>
<p>The <code>autograd</code> package provides automatic differentiation for all operations<br>on Tensors. It is a define-by-run framework, which means that your backprop is<br>defined by how your code is run, and that every single iteration can be<br>different.</p>
<p>Let us see this in more simple terms with some examples.</p>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><code>torch.Tensor</code> is the central class of the package. If you set its attribute<br><code>.requires_grad</code> as <code>True</code>, it starts to track all operations on it. When<br>you finish your computation you can call <code>.backward()</code> and have all the<br>gradients computed automatically. The gradient for this tensor will be<br>accumulated into <code>.grad</code> attribute.</p>
<p>To stop a tensor from tracking history, you can call <code>.detach()</code> to detach<br>it from the computation history, and to prevent future computation from being<br>tracked.</p>
<p>To prevent tracking history (and using memory), you can also wrap the code block<br>in <code>with torch.no_grad():</code>. This can be particularly helpful when evaluating a<br>model because the model may have trainable parameters with<br><code>requires_grad=True</code>, but for which we donâ€™t need the gradients.</p>
<p>Thereâ€™s one more class which is very important for autograd<br>implementation - a <code>Function</code>.</p>
<p><code>Tensor</code> and <code>Function</code> are interconnected and build up an acyclic<br>graph, that encodes a complete history of computation. Each tensor has<br>a <code>.grad_fn</code> attribute that references a <code>Function</code> that has created<br>the <code>Tensor</code> (except for Tensors created by the user - their<br><code>grad_fn is None</code>).</p>
<p>If you want to compute the derivatives, you can call <code>.backward()</code> on<br>a <code>Tensor</code>. If <code>Tensor</code> is a scalar (i.e. it holds a one element<br>data), you donâ€™t need to specify any arguments to <code>backward()</code>,<br>however if it has more elements, you need to specify a <code>gradient</code><br>argument that is a tensor of matching shape.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>Create a tensor and set <code>requires_grad=True</code> to track computation with it</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
</code></pre><p>Do a tensor operation:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
</code></pre><p><code>y</code> was created as a result of an operation, so it has a <code>grad_fn</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;AddBackward0 object at 0x0000015370CAB438&gt;
</code></pre><p>Do more operations on <code>y</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)
</code></pre><p><code>.requires_grad_( ... )</code> changes an existing Tensorâ€™s <code>requires_grad</code><br>flag in-place. The input flag defaults to <code>False</code> if not given.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>False
True
&lt;SumBackward0 object at 0x000001536D5A24E0&gt;
</code></pre><h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>Letâ€™s backprop now.<br>Because <code>out</code> contains a single scalar, <code>out.backward()</code> is<br>equivalent to <code>out.backward(torch.tensor(1.))</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>
<p>Print gradients d(out)/dx</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
</code></pre><p>You should have got a matrix of <code>4.5</code>. Letâ€™s call the <code>out</code><br><em>Tensor</em> â€œ$o$â€.<br>We have that $o = \frac{1}{4}\sum_i z_i$,<br>$z_i = 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$.<br>Therefore,<br>$\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)$, hence<br>$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$.</p>
<p>Mathematically, if you have a vector valued function $\vec{y}=f(\vec{x})$,<br>then the gradient of $\vec{y}$ with respect to $\vec{x}$<br>is a Jacobian matrix:</p>
<p>\begin{align}J=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\<br>   \vdots &amp; \ddots &amp; \vdots\\<br>   \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p>
<p>Generally speaking, <code>torch.autograd</code> is an engine for computing<br>vector-Jacobian product. That is, given any vector<br>$v=\left(\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m}\end{array}\right)^{T}$,<br>compute the product $v^{T}\cdot J$. If $v$ happens to be<br>the gradient of a scalar function $l=g\left(\vec{y}\right)$,<br>that is,<br>$v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$,<br>then by the chain rule, the vector-Jacobian product would be the<br>gradient of $l$ with respect to $\vec{x}$:</p>
<p>\begin{align}J^{T}\cdot v=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}\\<br>   \vdots &amp; \ddots &amp; \vdots\\<br>   \frac{\partial y_{1}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\left(\begin{array}{c}<br>   \frac{\partial l}{\partial y_{1}}\\<br>   \vdots\\<br>   \frac{\partial l}{\partial y_{m}}<br>   \end{array}\right)=\left(\begin{array}{c}<br>   \frac{\partial l}{\partial x_{1}}\\<br>   \vdots\\<br>   \frac{\partial l}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p>
<p>(Note that $v^{T}\cdot J$ gives a row vector which can be<br>treated as a column vector by taking $J^{T}\cdot v$.)</p>
<p>This characteristic of vector-Jacobian product makes it very<br>convenient to feed external gradients into a model that has<br>non-scalar output.</p>
<p>Now letâ€™s take a look at an example of vector-Jacobian product:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([849.9269, 245.4694, 633.8903], grad_fn=&lt;MulBackward0&gt;)
</code></pre><p>Now in this case <code>y</code> is no longer a scalar. <code>torch.autograd</code><br>could not compute the full Jacobian directly, but if we just<br>want the vector-Jacobian product, simply pass the vector to<br><code>backward</code> as argument:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])
</code></pre><p>You can also stop autograd from tracking history on Tensors<br>with <code>.requires_grad=True</code> either by wrapping the code block in<br><code>with torch.no_grad():</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">	print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>
<pre><code>True
True
False
</code></pre><p>Or by using <code>.detach()</code> to get a new Tensor with the same<br>content but that does not require gradients:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">y = x.detach()</span><br><span class="line">print(y.requires_grad)</span><br><span class="line">print(x.eq(y).all())</span><br></pre></td></tr></table></figure>
<pre><code>True
False
tensor(1, dtype=torch.uint8)
</code></pre><p><strong>Read Later:</strong></p>
<p>Document about <code>autograd.Function</code> is at<br><a href="https://pytorch.org/docs/stable/autograd.html#function" target="_blank" rel="noopener">https://pytorch.org/docs/stable/autograd.html#function</a></p>
<h2 id="ä¸è®¤è¯†çš„å•è¯"><a href="#ä¸è®¤è¯†çš„å•è¯" class="headerlink" title="ä¸è®¤è¯†çš„å•è¯"></a>ä¸è®¤è¯†çš„å•è¯</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Automatic:è‡ªåŠ¨çš„</span><br><span class="line">briefly:çŸ­æš‚çš„</span><br><span class="line">track:è·Ÿè¸ª</span><br><span class="line">specify:æŒ‡æ˜</span><br><span class="line">accumulate:ç´¯åŠ </span><br><span class="line">particularly:å°¤å…¶,æ ¼å¤–çš„</span><br><span class="line">interconnect:äº’ç›¸è¿æ¥</span><br><span class="line">acyclic:éå¾ªç¯çš„,æ— ç¯çš„</span><br><span class="line">Mathematically:æ•°å­¦ä¸Š</span><br><span class="line">Jacobian:é›…å¯æ¯”</span><br><span class="line">scalar:æ ‡é‡</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-DCGANæ•™ç¨‹</title>
    <url>/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>Pytorch-Image-DCGANæ•™ç¨‹:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="DCGAN-Tutorial"><a href="#DCGAN-Tutorial" class="headerlink" title="DCGAN Tutorial"></a>DCGAN Tutorial</h1><p><strong>Author</strong>: <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This tutorial will give an introduction to DCGANs through an example. We<br>will train a generative adversarial network (GAN) to generate new<br>celebrities after showing it pictures of many real celebrities. Most of<br>the code here is from the dcgan implementation in<br><code>pytorch/examples &lt;https://github.com/pytorch/examples&gt;</code>__, and this<br>document will give a thorough explanation of the implementation and shed<br>light on how and why this model works. But donâ€™t worry, no prior<br>knowledge of GANs is required, but it may require a first-timer to spend<br>some time reasoning about what is actually happening under the hood.<br>Also, for the sake of time it will help to have a GPU, or two. Lets<br>start from the beginning.</p>
<h2 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h2><p>What is a GAN?</p>
<p>GANs are a framework for teaching a DL model to capture the training<br>dataâ€™s distribution so we can generate new data from that same<br>distribution. GANs were invented by Ian Goodfellow in 2014 and first<br>described in the paper <code>Generative Adversarial
Nets &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__.<br>They are made of two distinct models, a <em>generator</em> and a<br><em>discriminator</em>. The job of the generator is to spawn â€˜fakeâ€™ images that<br>look like the training images. The job of the discriminator is to look<br>at an image and output whether or not it is a real training image or a<br>fake image from the generator. During training, the generator is<br>constantly trying to outsmart the discriminator by generating better and<br>better fakes, while the discriminator is working to become a better<br>detective and correctly classify the real and fake images. The<br>equilibrium of this game is when the generator is generating perfect<br>fakes that look as if they came directly from the training data, and the<br>discriminator is left to always guess at 50% confidence that the<br>generator output is real or fake.</p>
<p>Now, lets define some notation to be used throughout tutorial starting<br>with the discriminator. Let $x$ be data representing an image.<br>$D(x)$ is the discriminator network which outputs the (scalar)<br>probability that $x$ came from training data rather than the<br>generator. Here, since we are dealing with images the input to<br>$D(x)$ is an image of CHW size 3x64x64. Intuitively, $D(x)$<br>should be HIGH when $x$ comes from training data and LOW when<br>$x$ comes from the generator. $D(x)$ can also be thought of<br>as a traditional binary classifier.</p>
<p>For the generatorâ€™s notation, let $z$ be a latent space vector<br>sampled from a standard normal distribution. $G(z)$ represents the<br>generator function which maps the latent vector $z$ to data-space.<br>The goal of $G$ is to estimate the distribution that the training<br>data comes from ($p_{data}$) so it can generate fake samples from<br>that estimated distribution ($p_g$).</p>
<p>So, $D(G(z))$ is the probability (scalar) that the output of the<br>generator $G$ is a real image. As described in <code>Goodfellowâ€™s
paper &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__,<br>$D$ and $G$ play a minimax game in which $D$ tries to<br>maximize the probability it correctly classifies reals and fakes<br>($logD(x)$), and $G$ tries to minimize the probability that<br>$D$ will predict its outputs are fake ($log(1-D(G(x)))$).<br>From the paper, the GAN loss function is</p>
<p>$\begin{align}\underset{G}{\text{min}} \underset{D}{\text{max}}V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}\big[logD(x)\big] + \mathbb{E}_{z\sim p_{z}(z)}\big[log(1-D(G(z)))\big]\end{align}$</p>
<p>In theory, the solution to this minimax game is where<br>$p_g = p_{data}$, and the discriminator guesses randomly if the<br>inputs are real or fake. However, the convergence theory of GANs is<br>still being actively researched and in reality models do not always<br>train to this point.</p>
<p>What is a DCGAN?</p>
<p>A DCGAN is a direct extension of the GAN described above, except that it<br>explicitly uses convolutional and convolutional-transpose layers in the<br>discriminator and generator, respectively. It was first described by<br>Radford et. al. in the paper <code>Unsupervised Representation Learning With
Deep Convolutional Generative Adversarial
Networks &lt;https://arxiv.org/pdf/1511.06434.pdf&gt;</code>. The discriminator<br>is made up of strided<br><code>convolution &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code><br>layers, <code>batch
norm &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d&gt;</code><br>layers, and<br><code>LeakyReLU &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU&gt;</code><br>activations. The input is a 3x64x64 input image and the output is a<br>scalar probability that the input is from the real data distribution.<br>The generator is comprised of<br><code>convolutional-transpose &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d&gt;</code><br>layers, batch norm layers, and<br><code>ReLU &lt;https://pytorch.org/docs/stable/nn.html#relu&gt;</code>__ activations. The<br>input is a latent vector, $z$, that is drawn from a standard<br>normal distribution and the output is a 3x64x64 RGB image. The strided<br>conv-transpose layers allow the latent vector to be transformed into a<br>volume with the same shape as an image. In the paper, the authors also<br>give some tips about how to setup the optimizers, how to calculate the<br>loss functions, and how to initialize the model weights, all of which<br>will be explained in the coming sections.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="comment">#%matplotlib inline</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed for reproducibility</span></span><br><span class="line">manualSeed = <span class="number">999</span></span><br><span class="line"><span class="comment">#manualSeed = random.randint(1, 10000) # use if you want new results</span></span><br><span class="line">print(<span class="string">"Random Seed: "</span>, manualSeed)</span><br><span class="line">random.seed(manualSeed)</span><br><span class="line">torch.manual_seed(manualSeed)</span><br></pre></td></tr></table></figure>
<pre><code>Random Seed:  999





&lt;torch._C.Generator at 0x268c24f1e50&gt;
</code></pre><h2 id="Inputs"><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h2><p>Letâ€™s define some inputs for the run:</p>
<ul>
<li><strong>dataroot</strong> - the path to the root of the dataset folder. We will<br>talk more about the dataset in the next section</li>
<li><strong>workers</strong> - the number of worker threads for loading the data with<br>the DataLoader</li>
<li><strong>batch_size</strong> - the batch size used in training. The DCGAN paper<br>uses a batch size of 128</li>
<li><strong>image_size</strong> - the spatial size of the images used for training.<br>This implementation defaults to 64x64. If another size is desired,<br>the structures of D and G must be changed. See<br><code>here &lt;https://github.com/pytorch/examples/issues/70&gt;</code>__ for more<br>details</li>
<li><strong>nc</strong> - number of color channels in the input images. For color<br>images this is 3</li>
<li><strong>nz</strong> - length of latent vector</li>
<li><strong>ngf</strong> - relates to the depth of feature maps carried through the<br>generator</li>
<li><strong>ndf</strong> - sets the depth of feature maps propagated through the<br>discriminator</li>
<li><strong>num_epochs</strong> - number of training epochs to run. Training for<br>longer will probably lead to better results but will also take much<br>longer</li>
<li><strong>lr</strong> - learning rate for training. As described in the DCGAN paper,<br>this number should be 0.0002</li>
<li><strong>beta1</strong> - beta1 hyperparameter for Adam optimizers. As described in<br>paper, this number should be 0.5</li>
<li><strong>ngpu</strong> - number of GPUs available. If this is 0, code will run in<br>CPU mode. If this number is greater than 0 it will run on that number<br>of GPUs</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Root directory for dataset</span></span><br><span class="line">dataroot = <span class="string">"data/celeba"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of workers for dataloader</span></span><br><span class="line">workers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch size during training</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spatial size of training images. All images will be resized to this</span></span><br><span class="line"><span class="comment">#   size using a transformer.</span></span><br><span class="line">image_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of channels in the training images. For color images this is 3</span></span><br><span class="line">nc = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of z latent vector (i.e. size of generator input)</span></span><br><span class="line">nz = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in generator</span></span><br><span class="line">ngf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in discriminator</span></span><br><span class="line">ndf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of training epochs</span></span><br><span class="line"><span class="comment"># num_epochs = 5</span></span><br><span class="line">num_epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate for optimizers</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Beta1 hyperparam for Adam optimizers</span></span><br><span class="line">beta1 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of GPUs available. Use 0 for CPU mode.</span></span><br><span class="line">ngpu = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>In this tutorial we will use the <code>Celeb-A Faces
dataset &lt;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&gt;</code><strong> which can<br>be downloaded at the linked site, or in <code>Google
Drive &lt;https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg&gt;</code></strong>.<br>The dataset will download as a file named <em>img_align_celeba.zip</em>. Once<br>downloaded, create a directory named <em>celeba</em> and extract the zip file<br>into that directory. Then, set the <em>dataroot</em> input for this notebook to<br>the <em>celeba</em> directory you just created. The resulting directory<br>structure should be:</p>
<p>::</p>
<p>   /path/to/celeba<br>       -&gt; img_align_celeba<br>           -&gt; 188242.jpg<br>           -&gt; 173822.jpg<br>           -&gt; 284702.jpg<br>           -&gt; 537394.jpg<br>              â€¦</p>
<p>This is an important step because we will be using the ImageFolder<br>dataset class, which requires there to be subdirectories in the<br>datasetâ€™s root folder. Now, we can create the dataset, create the<br>dataloader, set the device to run on, and finally visualize some of the<br>training data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We can use an image folder dataset the way we have it setup.</span></span><br><span class="line"><span class="comment"># Create the dataset</span></span><br><span class="line">dataset = dset.ImageFolder(root=dataroot,</span><br><span class="line">                           transform=transforms.Compose([</span><br><span class="line">                               transforms.Resize(image_size),</span><br><span class="line">                               transforms.CenterCrop(image_size),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">                           ]))</span><br><span class="line"><span class="comment"># Create the dataloader</span></span><br><span class="line">dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>, num_workers=workers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot some training images</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Training Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">2</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x268c49247f0&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/output_7_1.png" alt="png"></p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>With our input parameters set and the dataset prepared, we can now get<br>into the implementation. We will start with the weigth initialization<br>strategy, then talk about the generator, discriminator, loss functions,<br>and training loop in detail.</p>
<p>Weight Initialization</p>
<p>From the DCGAN paper, the authors specify that all model weights shall<br>be randomly initialized from a Normal distribution with mean=0,<br>stdev=0.02. The <code>weights_init</code> function takes an initialized model as<br>input and reinitializes all convolutional, convolutional-transpose, and<br>batch normalization layers to meet this criteria. This function is<br>applied to the models immediately after initialization.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># custom weights initialization called on netG and netD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        nn.init.constant_(m.bias.data, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>Generator</p>
<p>The generator, $G$, is designed to map the latent space vector<br>($z$) to data-space. Since our data are images, converting<br>$z$ to data-space means ultimately creating a RGB image with the<br>same size as the training images (i.e. 3x64x64). In practice, this is<br>accomplished through a series of strided two dimensional convolutional<br>transpose layers, each paired with a 2d batch norm layer and a relu<br>activation. The output of the generator is fed through a tanh function<br>to return it to the input data range of $[-1,1]$. It is worth<br>noting the existence of the batch norm functions after the<br>conv-transpose layers, as this is a critical contribution of the DCGAN<br>paper. These layers help with the flow of gradients during training. An<br>image of the generator from the DCGAN paper is shown below.</p>
<p>.. figure:: /_static/img/dcgan_generator.png<br>   :alt: dcgan_generator</p>
<p>Notice, the how the inputs we set in the input section (<em>nz</em>, <em>ngf</em>, and<br><em>nc</em>) influence the generator architecture in code. <em>nz</em> is the length<br>of the z input vector, <em>ngf</em> relates to the size of the feature maps<br>that are propagated through the generator, and <em>nc</em> is the number of<br>channels in the output image (set to 3 for RGB images). Below is the<br>code for the generator.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generator Code</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is Z, going into a convolution</span></span><br><span class="line">            nn.ConvTranspose2d( nz, ngf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">8</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*8) x 4 x 4</span></span><br><span class="line">            nn.ConvTranspose2d(ngf * <span class="number">8</span>, ngf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">4</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*4) x 8 x 8</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">4</span>, ngf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*2) x 16 x 16</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">2</span>, ngf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf) x 32 x 32</span></span><br><span class="line">            nn.ConvTranspose2d( ngf, nc, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">            <span class="comment"># state size. (nc) x 64 x 64</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure>
<p>Now, we can instantiate the generator and apply the <code>weights_init</code><br>function. Check out the printed model to see how the generator object is<br>structured.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the generator</span></span><br><span class="line">netG = Generator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netG = nn.DataParallel(netG, list(range(ngpu)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netG.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netG)</span><br></pre></td></tr></table></figure>
<pre><code>Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
</code></pre><p>Discriminator</p>
<p>As mentioned, the discriminator, $D$, is a binary classification<br>network that takes an image as input and outputs a scalar probability<br>that the input image is real (as opposed to fake). Here, $D$ takes<br>a 3x64x64 input image, processes it through a series of Conv2d,<br>BatchNorm2d, and LeakyReLU layers, and outputs the final probability<br>through a Sigmoid activation function. This architecture can be extended<br>with more layers if necessary for the problem, but there is significance<br>to the use of the strided convolution, BatchNorm, and LeakyReLUs. The<br>DCGAN paper mentions it is a good practice to use strided convolution<br>rather than pooling to downsample because it lets the network learn its<br>own pooling function. Also batch norm and leaky relu functions promote<br>healthy gradient flow which is critical for the learning process of both<br>$G$ and $D$.</p>
<p>Discriminator Code</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is (nc) x 64 x 64</span></span><br><span class="line">            nn.Conv2d(nc, ndf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf) x 32 x 32</span></span><br><span class="line">            nn.Conv2d(ndf, ndf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">2</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*2) x 16 x 16</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">2</span>, ndf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">4</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*4) x 8 x 8</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">4</span>, ndf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">8</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*8) x 4 x 4</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">8</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure>
<p>Now, as with the generator, we can create the discriminator, apply the<br><code>weights_init</code> function, and print the modelâ€™s structure.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the Discriminator</span></span><br><span class="line">netD = Discriminator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netD = nn.DataParallel(netD, list(range(ngpu)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netD.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netD)</span><br></pre></td></tr></table></figure>
<pre><code>Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (12): Sigmoid()
  )
)
</code></pre><p>Loss Functions and Optimizers</p>
<p>With $D$ and $G$ setup, we can specify how they learn<br>through the loss functions and optimizers. We will use the Binary Cross<br>Entropy loss<br>(<code>BCELoss &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss&gt;</code>__)<br>function which is defined in PyTorch as:</p>
<p>\begin{align}\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]\end{align}</p>
<p>Notice how this function provides the calculation of both log components<br>in the objective function (i.e. $log(D(x))$ and<br>$log(1-D(G(z)))$). We can specify what part of the BCE equation to<br>use with the $y$ input. This is accomplished in the training loop<br>which is coming up soon, but it is important to understand how we can<br>choose which component we wish to calculate just by changing $y$<br>(i.e. GT labels).</p>
<p>Next, we define our real label as 1 and the fake label as 0. These<br>labels will be used when calculating the losses of $D$ and<br>$G$, and this is also the convention used in the original GAN<br>paper. Finally, we set up two separate optimizers, one for $D$ and<br>one for $G$. As specified in the DCGAN paper, both are Adam<br>optimizers with learning rate 0.0002 and Beta1 = 0.5. For keeping track<br>of the generatorâ€™s learning progression, we will generate a fixed batch<br>of latent vectors that are drawn from a Gaussian distribution<br>(i.e. fixed_noise) . In the training loop, we will periodically input<br>this fixed_noise into $G$, and over the iterations we will see<br>images form out of the noise.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize BCELoss function</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create batch of latent vectors that we will use to visualize</span></span><br><span class="line"><span class="comment">#  the progression of the generator</span></span><br><span class="line">fixed_noise = torch.randn(<span class="number">64</span>, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Establish convention for real and fake labels during training</span></span><br><span class="line">real_label = <span class="number">1</span></span><br><span class="line">fake_label = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup Adam optimizers for both G and D</span></span><br><span class="line">optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure>
<p>Training</p>
<p>Finally, now that we have all of the parts of the GAN framework defined,<br>we can train it. Be mindful that training GANs is somewhat of an art<br>form, as incorrect hyperparameter settings lead to mode collapse with<br>little explanation of what went wrong. Here, we will closely follow<br>Algorithm 1 from Goodfellowâ€™s paper, while abiding by some of the best<br>practices shown in <code>ganhacks &lt;https://github.com/soumith/ganhacks&gt;</code>__.<br>Namely, we will â€œconstruct different mini-batches for real and fakeâ€<br>images, and also adjust Gâ€™s objective function to maximize<br>$logD(G(z))$. Training is split up into two main parts. Part 1<br>updates the Discriminator and Part 2 updates the Generator.</p>
<p><strong>Part 1 - Train the Discriminator</strong></p>
<p>Recall, the goal of training the discriminator is to maximize the<br>probability of correctly classifying a given input as real or fake. In<br>terms of Goodfellow, we wish to â€œupdate the discriminator by ascending<br>its stochastic gradientâ€. Practically, we want to maximize<br>$log(D(x)) + log(1-D(G(z)))$. Due to the separate mini-batch<br>suggestion from ganhacks, we will calculate this in two steps. First, we<br>will construct a batch of real samples from the training set, forward<br>pass through $D$, calculate the loss ($log(D(x))$), then<br>calculate the gradients in a backward pass. Secondly, we will construct<br>a batch of fake samples with the current generator, forward pass this<br>batch through $D$, calculate the loss ($log(1-D(G(z)))$),<br>and <em>accumulate</em> the gradients with a backward pass. Now, with the<br>gradients accumulated from both the all-real and all-fake batches, we<br>call a step of the Discriminatorâ€™s optimizer.</p>
<p><strong>Part 2 - Train the Generator</strong></p>
<p>As stated in the original paper, we want to train the Generator by<br>minimizing $log(1-D(G(z)))$ in an effort to generate better fakes.<br>As mentioned, this was shown by Goodfellow to not provide sufficient<br>gradients, especially early in the learning process. As a fix, we<br>instead wish to maximize $log(D(G(z)))$. In the code we accomplish<br>this by: classifying the Generator output from Part 1 with the<br>Discriminator, computing Gâ€™s loss <em>using real labels as GT</em>, computing<br>Gâ€™s gradients in a backward pass, and finally updating Gâ€™s parameters<br>with an optimizer step. It may seem counter-intuitive to use the real<br>labels as GT labels for the loss function, but this allows us to use the<br>$log(x)$ part of the BCELoss (rather than the $log(1-x)$<br>part) which is exactly what we want.</p>
<p>Finally, we will do some statistic reporting and at the end of each<br>epoch we will push our fixed_noise batch through the generator to<br>visually track the progress of Gâ€™s training. The training statistics<br>reported are:</p>
<ul>
<li><strong>Loss_D</strong> - discriminator loss calculated as the sum of losses for<br>the all real and all fake batches ($log(D(x)) + log(D(G(z)))$).</li>
<li><strong>Loss_G</strong> - generator loss calculated as $log(D(G(z)))$</li>
<li><strong>D(x)</strong> - the average output (across the batch) of the discriminator<br>for the all real batch. This should start close to 1 then<br>theoretically converge to 0.5 when G gets better. Think about why<br>this is.</li>
<li><strong>D(G(z))</strong> - average discriminator outputs for the all fake batch.<br>The first number is before D is updated and the second number is<br>after D is updated. These numbers should start near 0 and converge to<br>0.5 as G gets better. Think about why this is.</li>
</ul>
<p><strong>Note:</strong> This step might take a while, depending on how many epochs you<br>run and if you removed some data from the dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Training Loop</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Lists to keep track of progress</span></span><br><span class="line">img_list = []</span><br><span class="line">G_losses = []</span><br><span class="line">D_losses = []</span><br><span class="line">iters = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Starting Training Loop..."</span>)</span><br><span class="line"><span class="comment"># For each epoch</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># For each batch in the dataloader</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(dataloader, <span class="number">0</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        <span class="comment">## Train with all-real batch</span></span><br><span class="line">        netD.zero_grad()</span><br><span class="line">        <span class="comment"># Format batch</span></span><br><span class="line">        real_cpu = data[<span class="number">0</span>].to(device)</span><br><span class="line">        b_size = real_cpu.size(<span class="number">0</span>)</span><br><span class="line">        label = torch.full((b_size,), real_label, device=device)</span><br><span class="line">        <span class="comment"># Forward pass real batch through D</span></span><br><span class="line">        output = netD(real_cpu).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate loss on all-real batch</span></span><br><span class="line">        errD_real = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for D in backward pass</span></span><br><span class="line">        errD_real.backward()</span><br><span class="line">        D_x = output.mean().item()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Train with all-fake batch</span></span><br><span class="line">        <span class="comment"># Generate batch of latent vectors</span></span><br><span class="line">        noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        <span class="comment"># Generate fake image batch with G</span></span><br><span class="line">        fake = netG(noise)</span><br><span class="line">        label.fill_(fake_label)</span><br><span class="line">        <span class="comment"># Classify all fake batch with D</span></span><br><span class="line">        output = netD(fake.detach()).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate D's loss on the all-fake batch</span></span><br><span class="line">        errD_fake = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate the gradients for this batch</span></span><br><span class="line">        errD_fake.backward()</span><br><span class="line">        D_G_z1 = output.mean().item()</span><br><span class="line">        <span class="comment"># Add the gradients from the all-real and all-fake batches</span></span><br><span class="line">        errD = errD_real + errD_fake</span><br><span class="line">        <span class="comment"># Update D</span></span><br><span class="line">        optimizerD.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (2) Update G network: maximize log(D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        label.fill_(real_label)  <span class="comment"># fake labels are real for generator cost</span></span><br><span class="line">        <span class="comment"># Since we just updated D, perform another forward pass of all-fake batch through D</span></span><br><span class="line">        output = netD(fake).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate G's loss based on this output</span></span><br><span class="line">        errG = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for G</span></span><br><span class="line">        errG.backward()</span><br><span class="line">        D_G_z2 = output.mean().item()</span><br><span class="line">        <span class="comment"># Update G</span></span><br><span class="line">        optimizerG.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Output training stats</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'</span></span><br><span class="line">                  % (epoch, num_epochs, i, len(dataloader),</span><br><span class="line">                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Save Losses for plotting later</span></span><br><span class="line">        G_losses.append(errG.item())</span><br><span class="line">        D_losses.append(errD.item())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check how the generator is doing by saving G's output on fixed_noise</span></span><br><span class="line">        <span class="keyword">if</span> (iters % <span class="number">500</span> == <span class="number">0</span>) <span class="keyword">or</span> ((epoch == num_epochs<span class="number">-1</span>) <span class="keyword">and</span> (i == len(dataloader)<span class="number">-1</span>)):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = netG(fixed_noise).detach().cpu()</span><br><span class="line">            img_list.append(vutils.make_grid(fake, padding=<span class="number">2</span>, normalize=<span class="literal">True</span>))</span><br><span class="line">            </span><br><span class="line">        iters += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>Starting Training Loop...


..\aten\src\ATen\native\TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.


[0/2][0/1583]    Loss_D: 1.8664    Loss_G: 4.9949    D(x): 0.5050    D(G(z)): 0.5928 / 0.0106
[0/2][50/1583]    Loss_D: 0.1046    Loss_G: 7.1177    D(x): 0.9758    D(G(z)): 0.0124 / 0.0086
[0/2][100/1583]    Loss_D: 1.0915    Loss_G: 12.3498    D(x): 0.9553    D(G(z)): 0.4722 / 0.0000
[0/2][150/1583]    Loss_D: 1.7593    Loss_G: 5.9544    D(x): 0.3933    D(G(z)): 0.0053 / 0.0070
[0/2][200/1583]    Loss_D: 0.8020    Loss_G: 5.9020    D(x): 0.6117    D(G(z)): 0.0295 / 0.0069
[0/2][250/1583]    Loss_D: 0.6124    Loss_G: 4.1050    D(x): 0.7546    D(G(z)): 0.1926 / 0.0268
[0/2][300/1583]    Loss_D: 0.9607    Loss_G: 1.8866    D(x): 0.5101    D(G(z)): 0.0519 / 0.2133
[0/2][350/1583]    Loss_D: 0.5478    Loss_G: 3.3292    D(x): 0.7691    D(G(z)): 0.1973 / 0.0492
[0/2][400/1583]    Loss_D: 1.2509    Loss_G: 1.3775    D(x): 0.4456    D(G(z)): 0.1254 / 0.2993
[0/2][450/1583]    Loss_D: 0.4384    Loss_G: 5.5121    D(x): 0.7491    D(G(z)): 0.0298 / 0.0068
[0/2][500/1583]    Loss_D: 0.4170    Loss_G: 3.6389    D(x): 0.7676    D(G(z)): 0.0610 / 0.0396
[0/2][550/1583]    Loss_D: 0.5595    Loss_G: 4.2165    D(x): 0.8009    D(G(z)): 0.1784 / 0.0296
[0/2][600/1583]    Loss_D: 1.2295    Loss_G: 3.3066    D(x): 0.5244    D(G(z)): 0.0907 / 0.0752
[0/2][650/1583]    Loss_D: 0.5091    Loss_G: 4.7135    D(x): 0.7118    D(G(z)): 0.0292 / 0.0203
[0/2][700/1583]    Loss_D: 0.3912    Loss_G: 2.7194    D(x): 0.8198    D(G(z)): 0.1221 / 0.1098
[0/2][750/1583]    Loss_D: 0.7578    Loss_G: 7.3482    D(x): 0.9471    D(G(z)): 0.4270 / 0.0015
[0/2][800/1583]    Loss_D: 0.7080    Loss_G: 5.8282    D(x): 0.9395    D(G(z)): 0.4118 / 0.0053
[0/2][850/1583]    Loss_D: 0.5755    Loss_G: 3.2657    D(x): 0.6967    D(G(z)): 0.0784 / 0.0637
[0/2][900/1583]    Loss_D: 1.0720    Loss_G: 6.0690    D(x): 0.8734    D(G(z)): 0.5405 / 0.0047
[0/2][950/1583]    Loss_D: 0.6901    Loss_G: 5.7612    D(x): 0.9313    D(G(z)): 0.3993 / 0.0064
[0/2][1000/1583]    Loss_D: 0.2473    Loss_G: 5.5474    D(x): 0.8535    D(G(z)): 0.0574 / 0.0123
[0/2][1050/1583]    Loss_D: 0.6581    Loss_G: 5.6885    D(x): 0.9537    D(G(z)): 0.4081 / 0.0069
[0/2][1100/1583]    Loss_D: 0.5447    Loss_G: 2.8289    D(x): 0.7523    D(G(z)): 0.1252 / 0.0966
[0/2][1150/1583]    Loss_D: 0.3936    Loss_G: 3.4201    D(x): 0.8383    D(G(z)): 0.1639 / 0.0528
[0/2][1200/1583]    Loss_D: 0.7676    Loss_G: 5.3321    D(x): 0.8581    D(G(z)): 0.3643 / 0.0112
[0/2][1250/1583]    Loss_D: 0.4362    Loss_G: 3.9937    D(x): 0.8223    D(G(z)): 0.1671 / 0.0321
[0/2][1300/1583]    Loss_D: 0.6602    Loss_G: 5.5204    D(x): 0.9106    D(G(z)): 0.3806 / 0.0072
[0/2][1350/1583]    Loss_D: 0.6352    Loss_G: 3.2255    D(x): 0.7404    D(G(z)): 0.1788 / 0.0708
[0/2][1400/1583]    Loss_D: 0.7936    Loss_G: 3.5125    D(x): 0.7248    D(G(z)): 0.2855 / 0.0474
[0/2][1450/1583]    Loss_D: 1.0550    Loss_G: 1.2678    D(x): 0.7036    D(G(z)): 0.3793 / 0.3957
[0/2][1500/1583]    Loss_D: 0.4235    Loss_G: 2.8506    D(x): 0.7756    D(G(z)): 0.1001 / 0.0801
[0/2][1550/1583]    Loss_D: 0.4839    Loss_G: 4.1835    D(x): 0.8839    D(G(z)): 0.2465 / 0.0254
[1/2][0/1583]    Loss_D: 0.7391    Loss_G: 3.9735    D(x): 0.8016    D(G(z)): 0.3263 / 0.0284
[1/2][50/1583]    Loss_D: 0.5051    Loss_G: 3.8654    D(x): 0.7839    D(G(z)): 0.1756 / 0.0337
[1/2][100/1583]    Loss_D: 0.4857    Loss_G: 4.5489    D(x): 0.8677    D(G(z)): 0.2420 / 0.0209
[1/2][150/1583]    Loss_D: 0.6025    Loss_G: 4.4404    D(x): 0.8519    D(G(z)): 0.2920 / 0.0212
[1/2][200/1583]    Loss_D: 0.4301    Loss_G: 4.4767    D(x): 0.8909    D(G(z)): 0.2399 / 0.0190
[1/2][250/1583]    Loss_D: 1.2600    Loss_G: 7.6782    D(x): 0.9744    D(G(z)): 0.6415 / 0.0013
[1/2][300/1583]    Loss_D: 0.5044    Loss_G: 3.7002    D(x): 0.8408    D(G(z)): 0.2446 / 0.0375
[1/2][350/1583]    Loss_D: 0.4184    Loss_G: 3.2221    D(x): 0.7736    D(G(z)): 0.0924 / 0.0649
[1/2][400/1583]    Loss_D: 0.5320    Loss_G: 4.6695    D(x): 0.9051    D(G(z)): 0.3072 / 0.0150
[1/2][450/1583]    Loss_D: 0.3804    Loss_G: 3.3363    D(x): 0.7888    D(G(z)): 0.0978 / 0.0636
[1/2][500/1583]    Loss_D: 0.4293    Loss_G: 4.2911    D(x): 0.9014    D(G(z)): 0.2399 / 0.0226
[1/2][550/1583]    Loss_D: 0.3940    Loss_G: 2.7648    D(x): 0.7634    D(G(z)): 0.0777 / 0.0929
[1/2][600/1583]    Loss_D: 0.4044    Loss_G: 3.3666    D(x): 0.8438    D(G(z)): 0.1664 / 0.0598
[1/2][650/1583]    Loss_D: 0.3879    Loss_G: 3.4838    D(x): 0.8517    D(G(z)): 0.1754 / 0.0455
[1/2][700/1583]    Loss_D: 0.4487    Loss_G: 3.6364    D(x): 0.8773    D(G(z)): 0.2434 / 0.0370
[1/2][750/1583]    Loss_D: 0.7588    Loss_G: 2.0882    D(x): 0.6144    D(G(z)): 0.1150 / 0.1773
[1/2][800/1583]    Loss_D: 0.6134    Loss_G: 4.0046    D(x): 0.9100    D(G(z)): 0.3546 / 0.0278
[1/2][850/1583]    Loss_D: 0.5061    Loss_G: 2.2267    D(x): 0.7046    D(G(z)): 0.0860 / 0.1488
[1/2][900/1583]    Loss_D: 0.6032    Loss_G: 1.8834    D(x): 0.6518    D(G(z)): 0.0847 / 0.2023
[1/2][950/1583]    Loss_D: 1.1199    Loss_G: 2.2135    D(x): 0.4166    D(G(z)): 0.0332 / 0.1791
[1/2][1000/1583]    Loss_D: 0.8061    Loss_G: 2.2557    D(x): 0.5479    D(G(z)): 0.0442 / 0.1506
[1/2][1050/1583]    Loss_D: 0.7723    Loss_G: 2.7941    D(x): 0.5652    D(G(z)): 0.0532 / 0.0968
[1/2][1100/1583]    Loss_D: 0.6160    Loss_G: 1.4266    D(x): 0.6152    D(G(z)): 0.0460 / 0.2874
[1/2][1150/1583]    Loss_D: 1.1706    Loss_G: 5.4761    D(x): 0.9509    D(G(z)): 0.6143 / 0.0088
[1/2][1200/1583]    Loss_D: 0.5637    Loss_G: 2.2863    D(x): 0.7523    D(G(z)): 0.1901 / 0.1335
[1/2][1250/1583]    Loss_D: 0.4913    Loss_G: 2.1290    D(x): 0.7336    D(G(z)): 0.1155 / 0.1592
[1/2][1300/1583]    Loss_D: 0.4753    Loss_G: 2.9672    D(x): 0.8157    D(G(z)): 0.1986 / 0.0763
[1/2][1350/1583]    Loss_D: 0.6133    Loss_G: 2.9954    D(x): 0.8253    D(G(z)): 0.2826 / 0.0687
[1/2][1400/1583]    Loss_D: 0.4921    Loss_G: 3.1019    D(x): 0.8035    D(G(z)): 0.1985 / 0.0676
</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Finally, lets check out how we did. Here, we will look at three<br>different results. First, we will see how D and Gâ€™s losses changed<br>during training. Second, we will visualize Gâ€™s output on the fixed_noise<br>batch for every epoch. And third, we will look at a batch of real data<br>next to a batch of fake data from G.</p>
<p><strong>Loss versus training iteration</strong></p>
<p>Below is a plot of D &amp; Gâ€™s losses versus training iterations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">"Generator and Discriminator Loss During Training"</span>)</span><br><span class="line">plt.plot(G_losses,label=<span class="string">"G"</span>)</span><br><span class="line">plt.plot(D_losses,label=<span class="string">"D"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"iterations"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>Visualization of Gâ€™s progression</strong></p>
<p>Remember how we saved the generatorâ€™s output on the fixed_noise batch<br>after every epoch of training. Now, we can visualize the training<br>progression of G with an animation. Press the play button to start the<br>animation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#%%capture</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">ims = [[plt.imshow(np.transpose(i,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)), animated=<span class="literal">True</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> img_list]</span><br><span class="line">ani = animation.ArtistAnimation(fig, ims, interval=<span class="number">1000</span>, repeat_delay=<span class="number">1000</span>, blit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">HTML(ani.to_jshtml())</span><br></pre></td></tr></table></figure>
<p><strong>Real Images vs. Fake Images</strong></p>
<p>Finally, lets take a look at some real images and fake images side by<br>side.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Grab a batch of real images from the dataloader</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the real images</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Real Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">5</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the fake images from the last epoch</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Fake Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(img_list[<span class="number">-1</span>],(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Where-to-Go-Next"><a href="#Where-to-Go-Next" class="headerlink" title="Where to Go Next"></a>Where to Go Next</h2><p>We have reached the end of our journey, but there are several places you<br>could go from here. You could:</p>
<ul>
<li>Train for longer to see how good the results get</li>
<li>Modify this model to take a different dataset and possibly change the<br>size of the images and the model architecture</li>
<li>Check out some other cool GAN projects<br><code>here &lt;https://github.com/nashory/gans-awesome-applications&gt;</code>__</li>
<li>Create GANs that generate<br><code>music &lt;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&gt;</code>__</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## æˆ‘ä¸è®¤è¯†çš„å•è¯</span></span><br></pre></td></tr></table></figure>
<p>Intuitively:ç›´è§‚åœ°<br><code>
</code></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-è®¡ç®—æœºè§†è§‰è¿ç§»å­¦ä¹ </title>
    <url>/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>Pytorch-Image-è®¡ç®—æœºè§†è§‰è¿ç§»å­¦ä¹ :</p>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<h1 id="Transfer-Learning-for-Computer-Vision-Tutorial"><a href="#Transfer-Learning-for-Computer-Vision-Tutorial" class="headerlink" title="Transfer Learning for Computer Vision Tutorial"></a>Transfer Learning for Computer Vision Tutorial</h1><p><strong>Author</strong>: <code>Sasank Chilamkurthy &lt;https://chsasank.github.io&gt;</code>_</p>
<p>In this tutorial, you will learn how to train a convolutional neural network for<br>image classification using transfer learning. You can read more about the transfer<br>learning at <code>cs231n notes &lt;https://cs231n.github.io/transfer-learning/&gt;</code>__</p>
<p>Quoting these notes,</p>
<pre><code>In practice, very few people train an entire Convolutional Network
from scratch (with random initialization), because it is relatively
rare to have a dataset of sufficient size. Instead, it is common to
pretrain a ConvNet on a very large dataset (e.g. ImageNet, which
contains 1.2 million images with 1000 categories), and then use the
ConvNet either as an initialization or a fixed feature extractor for
the task of interest.</code></pre><p>These two major transfer learning scenarios look as follows:</p>
<ul>
<li><strong>Finetuning the convnet</strong>: Instead of random initializaion, we<br>initialize the network with a pretrained network, like the one that is<br>trained on imagenet 1000 dataset. Rest of the training looks as<br>usual.</li>
<li><strong>ConvNet as fixed feature extractor</strong>: Here, we will freeze the weights<br>for all of the network except that of the final fully connected<br>layer. This last fully connected layer is replaced with a new one<br>with random weights and only this layer is trained.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># License: BSD</span></span><br><span class="line"><span class="comment"># Author: Sasank Chilamkurthy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure>

<h2 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h2><p>We will use torchvision and torch.utils.data packages for loading the<br>data.</p>
<p>The problem weâ€™re going to solve today is to train a model to classify<br><strong>ants</strong> and <strong>bees</strong>. We have about 120 training images each for ants and bees.<br>There are 75 validation images for each class. Usually, this is a very<br>small dataset to generalize upon, if trained from scratch. Since we<br>are using transfer learning, we should be able to generalize reasonably<br>well.</p>
<p>This dataset is a very small subset of imagenet.</p>
<p>.. Note ::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/hymenoptera_data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Data augmentation and normalization for training</span></span><br><span class="line"><span class="comment"># Just normalization for validation</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">'val'</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">'data/hymenoptera_data'</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">                                          data_transforms[x])</span><br><span class="line">                  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataset_sizes = &#123;x: len(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">class_names = image_datasets[<span class="string">'train'</span>].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>

<p>Visualize a few images</p>
<p>Letâ€™s visualize a few training images so as to understand the data<br>augmentations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch of training data</span></span><br><span class="line">inputs, classes = next(iter(dataloaders[<span class="string">'train'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a grid from batch</span></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=[class_names[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_6_0.png" alt="png"></p>
<h2 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h2><p>Now, letâ€™s write a general function to train a model. Here, we will<br>illustrate:</p>
<ul>
<li>Scheduling the learning rate</li>
<li>Saving the best model</li>
</ul>
<p>In the following, parameter <code>scheduler</code> is an LR scheduler object from<br><code>torch.optim.lr_scheduler</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter gradients</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># statistics</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>Visualizing the model predictions</p>
<p>Generic function to display predictions for a few images</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span><span class="params">(model, num_images=<span class="number">6</span>)</span>:</span></span><br><span class="line">    was_training = model.training</span><br><span class="line">    model.eval()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(dataloaders[<span class="string">'val'</span>]):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(inputs.size()[<span class="number">0</span>]):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">'off'</span>)</span><br><span class="line">                ax.set_title(<span class="string">'predicted: &#123;&#125;'</span>.format(class_names[preds[j]]))</span><br><span class="line">                imshow(inputs.cpu().data[j])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train(mode=was_training)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train(mode=was_training)</span><br></pre></td></tr></table></figure>

<h2 id="Finetuning-the-convnet"><a href="#Finetuning-the-convnet" class="headerlink" title="Finetuning the convnet"></a>Finetuning the convnet</h2><p>Load a pretrained model and reset final fully connected layer.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"><span class="comment"># Here the size of each output sample is set to 2.</span></span><br><span class="line"><span class="comment"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 0/24
----------
train Loss: 0.5563 Acc: 0.7131
val Loss: 0.3475 Acc: 0.8693

Epoch 1/24
----------
train Loss: 0.4089 Acc: 0.8156
val Loss: 0.2445 Acc: 0.9085

Epoch 2/24
----------
train Loss: 0.5546 Acc: 0.7910
val Loss: 0.2798 Acc: 0.8758

Epoch 3/24
----------
train Loss: 0.4420 Acc: 0.8238
val Loss: 0.2220 Acc: 0.9085

Epoch 4/24
----------
train Loss: 0.5138 Acc: 0.8279
val Loss: 0.4449 Acc: 0.8497

Epoch 5/24
----------
train Loss: 0.4944 Acc: 0.8320
val Loss: 0.2964 Acc: 0.9281

Epoch 6/24
----------
train Loss: 0.6059 Acc: 0.7541
val Loss: 0.2792 Acc: 0.8889

Epoch 7/24
----------
train Loss: 0.4492 Acc: 0.8279
val Loss: 0.2148 Acc: 0.9085

Epoch 8/24
----------
train Loss: 0.3162 Acc: 0.8730
val Loss: 0.2214 Acc: 0.9281

Epoch 9/24
----------
train Loss: 0.2760 Acc: 0.8730
val Loss: 0.2317 Acc: 0.9281

Epoch 10/24
----------
train Loss: 0.2800 Acc: 0.8811
val Loss: 0.2063 Acc: 0.9216

Epoch 11/24
----------
train Loss: 0.2789 Acc: 0.8975
val Loss: 0.2132 Acc: 0.9281

Epoch 12/24
----------
train Loss: 0.2112 Acc: 0.9180
val Loss: 0.2114 Acc: 0.9346

Epoch 13/24
----------
train Loss: 0.3116 Acc: 0.8811
val Loss: 0.2009 Acc: 0.9346

Epoch 14/24
----------
train Loss: 0.2907 Acc: 0.8975
val Loss: 0.1990 Acc: 0.9346

Epoch 15/24
----------
train Loss: 0.2431 Acc: 0.9098
val Loss: 0.2149 Acc: 0.9346

Epoch 16/24
----------
train Loss: 0.2203 Acc: 0.9180
val Loss: 0.2014 Acc: 0.9346

Epoch 17/24
----------
train Loss: 0.2727 Acc: 0.8689
val Loss: 0.1924 Acc: 0.9346

Epoch 18/24
----------
train Loss: 0.2276 Acc: 0.9139
val Loss: 0.1987 Acc: 0.9281

Epoch 19/24
----------
train Loss: 0.1850 Acc: 0.9180
val Loss: 0.2287 Acc: 0.9346

Epoch 20/24
----------
train Loss: 0.2624 Acc: 0.8893
val Loss: 0.2368 Acc: 0.9281

Epoch 21/24
----------
train Loss: 0.2524 Acc: 0.8975
val Loss: 0.2231 Acc: 0.9346

Epoch 22/24
----------
train Loss: 0.2732 Acc: 0.8730
val Loss: 0.1966 Acc: 0.9346

Epoch 23/24
----------
train Loss: 0.3067 Acc: 0.8811
val Loss: 0.1995 Acc: 0.9346

Epoch 24/24
----------
train Loss: 0.2301 Acc: 0.8934
val Loss: 0.1958 Acc: 0.9281

Training complete in 80m 47s
Best val Acc: 0.934641</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_0.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_1.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_2.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_3.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_4.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_5.png" alt="png"></p>
<h2 id="ConvNet-as-fixed-feature-extractor"><a href="#ConvNet-as-fixed-feature-extractor" class="headerlink" title="ConvNet as fixed feature extractor"></a>ConvNet as fixed feature extractor</h2><p>Here, we need to freeze all the network except the final layer. We need<br>to set <code>requires_grad == False</code> to freeze the parameters so that the<br>gradients are not computed in <code>backward()</code>.</p>
<p>You can read more about this in the documentation<br><code>here &lt;https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></span><br><span class="line"><span class="comment"># opposed to before.</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p>Train and evaluate</p>
<p>On CPU this will take about half the time compared to previous scenario.<br>This is expected as gradients donâ€™t need to be computed for most of the<br>network. However, forward does need to be computed.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 0/24
----------
train Loss: 0.6604 Acc: 0.6516
val Loss: 0.4662 Acc: 0.7582

Epoch 1/24
----------
train Loss: 0.4774 Acc: 0.7910
val Loss: 0.2465 Acc: 0.9020

Epoch 2/24
----------
train Loss: 0.7743 Acc: 0.7049
val Loss: 0.3536 Acc: 0.8431

Epoch 3/24
----------
train Loss: 0.4459 Acc: 0.8033
val Loss: 0.1802 Acc: 0.9346

Epoch 4/24
----------
train Loss: 0.8349 Acc: 0.6803
val Loss: 0.3540 Acc: 0.8627

Epoch 5/24
----------
train Loss: 0.4686 Acc: 0.8033
val Loss: 0.1727 Acc: 0.9608

Epoch 6/24
----------
train Loss: 0.5511 Acc: 0.7623
val Loss: 0.3009 Acc: 0.8889

Epoch 7/24
----------
train Loss: 0.3496 Acc: 0.8525
val Loss: 0.1738 Acc: 0.9542

Epoch 8/24
----------
train Loss: 0.3696 Acc: 0.8484
val Loss: 0.1663 Acc: 0.9542

Epoch 9/24
----------
train Loss: 0.2564 Acc: 0.8770
val Loss: 0.1647 Acc: 0.9608

Epoch 10/24
----------
train Loss: 0.3623 Acc: 0.8402
val Loss: 0.1873 Acc: 0.9346

Epoch 11/24
----------
train Loss: 0.3846 Acc: 0.8320
val Loss: 0.1770 Acc: 0.9477

Epoch 12/24
----------
train Loss: 0.3871 Acc: 0.8238
val Loss: 0.1760 Acc: 0.9477

Epoch 13/24
----------
train Loss: 0.3481 Acc: 0.8525
val Loss: 0.1711 Acc: 0.9542

Epoch 14/24
----------
train Loss: 0.3504 Acc: 0.8402
val Loss: 0.1635 Acc: 0.9477

Epoch 15/24
----------
train Loss: 0.4247 Acc: 0.8279
val Loss: 0.1630 Acc: 0.9608

Epoch 16/24
----------
train Loss: 0.3036 Acc: 0.8607
val Loss: 0.1695 Acc: 0.9608

Epoch 17/24
----------
train Loss: 0.2761 Acc: 0.8934
val Loss: 0.1709 Acc: 0.9608

Epoch 18/24
----------
train Loss: 0.4223 Acc: 0.8238
val Loss: 0.1854 Acc: 0.9412

Epoch 19/24
----------
train Loss: 0.3503 Acc: 0.8402
val Loss: 0.1845 Acc: 0.9216

Epoch 20/24
----------
train Loss: 0.2934 Acc: 0.8811
val Loss: 0.1648 Acc: 0.9412

Epoch 21/24
----------
train Loss: 0.3156 Acc: 0.8402
val Loss: 0.1775 Acc: 0.9346

Epoch 22/24
----------
train Loss: 0.4119 Acc: 0.8115
val Loss: 0.1744 Acc: 0.9477

Epoch 23/24
----------
train Loss: 0.2424 Acc: 0.8893
val Loss: 0.1738 Acc: 0.9412

Epoch 24/24
----------
train Loss: 0.3547 Acc: 0.8361
val Loss: 0.1687 Acc: 0.9412

Training complete in 46m 23s
Best val Acc: 0.960784</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_0.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_1.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_2.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_3.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_4.png" alt="png"></p>
<p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_5.png" alt="png"></p>
<h2 id="Further-Learning"><a href="#Further-Learning" class="headerlink" title="Further Learning"></a>Further Learning</h2><p>If you would like to learn more about the applications of transfer learning,<br>checkout our <code>Quantized Transfer Learning for Computer Vision Tutorial &lt;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&gt;</code>_.</p>
<h2 id="æˆ‘ä¸è®¤è¯†çš„å•è¯"><a href="#æˆ‘ä¸è®¤è¯†çš„å•è¯" class="headerlink" title="æˆ‘ä¸è®¤è¯†çš„å•è¯"></a>æˆ‘ä¸è®¤è¯†çš„å•è¯</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sufficient:è¶³å¤Ÿçš„</span><br><span class="line">Finetuning:å¾®è°ƒ</span><br><span class="line">extractor:æå–å™¨</span><br><span class="line">generalize:æ¦‚æ‹¬</span><br><span class="line">flip:ç¿»è½¬</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ</title>
    <url>/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<p>Pytorch-Image-å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Adversarial-Example-Generation"><a href="#Adversarial-Example-Generation" class="headerlink" title="Adversarial Example Generation"></a>Adversarial Example Generation</h1><p><strong>Author:</strong> <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p>
<p>If you are reading this, hopefully you can appreciate how effective some<br>machine learning models are. Research is constantly pushing ML models to<br>be faster, more accurate, and more efficient. However, an often<br>overlooked aspect of designing and training models is security and<br>robustness, especially in the face of an adversary who wishes to fool<br>the model.</p>
<p>This tutorial will raise your awareness to the security vulnerabilities<br>of ML models, and will give insight into the hot topic of adversarial<br>machine learning. You may be surprised to find that adding imperceptible<br>perturbations to an image <em>can</em> cause drastically different model<br>performance. Given that this is a tutorial, we will explore the topic<br>via example on an image classifier. Specifically we will use one of the<br>first and most popular attack methods, the Fast Gradient Sign Attack<br>(FGSM), to fool an MNIST classifier.</p>
<h2 id="Threat-Model"><a href="#Threat-Model" class="headerlink" title="Threat Model"></a>Threat Model</h2><p>For context, there are many categories of adversarial attacks, each with<br>a different goal and assumption of the attackerâ€™s knowledge. However, in<br>general the overarching goal is to add the least amount of perturbation<br>to the input data to cause the desired misclassification. There are<br>several kinds of assumptions of the attackerâ€™s knowledge, two of which<br>are: <strong>white-box</strong> and <strong>black-box</strong>. A <em>white-box</em> attack assumes the<br>attacker has full knowledge and access to the model, including<br>architecture, inputs, outputs, and weights. A <em>black-box</em> attack assumes<br>the attacker only has access to the inputs and outputs of the model, and<br>knows nothing about the underlying architecture or weights. There are<br>also several types of goals, including <strong>misclassification</strong> and<br><strong>source/target misclassification</strong>. A goal of <em>misclassification</em> means<br>the adversary only wants the output classification to be wrong but does<br>not care what the new classification is. A <em>source/target<br>misclassification</em> means the adversary wants to alter an image that is<br>originally of a specific source class so that it is classified as a<br>specific target class.</p>
<p>In this case, the FGSM attack is a <em>white-box</em> attack with the goal of<br><em>misclassification</em>. With this background information, we can now<br>discuss the attack in detail.</p>
<p>Fast Gradient Sign Attack</p>
<p>One of the first and most popular adversarial attacks to date is<br>referred to as the <em>Fast Gradient Sign Attack (FGSM)</em> and is described<br>by Goodfellow et. al. in <code>Explaining and Harnessing Adversarial
Examples &lt;https://arxiv.org/abs/1412.6572&gt;</code>__. The attack is remarkably<br>powerful, and yet intuitive. It is designed to attack neural networks by<br>leveraging the way they learn, <em>gradients</em>. The idea is simple, rather<br>than working to minimize the loss by adjusting the weights based on the<br>backpropagated gradients, the attack <em>adjusts the input data to maximize<br>the loss</em> based on the same backpropagated gradients. In other words,<br>the attack uses the gradient of the loss w.r.t the input data, then<br>adjusts the input data to maximize the loss.</p>
<p>Before we jump into the code, letâ€™s look at the famous<br><code>FGSM &lt;https://arxiv.org/abs/1412.6572&gt;</code>__ panda example and extract<br>some notation.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_images/fgsm_panda_image.png" alt></p>
<p>From the figure, $\mathbf{x}$ is the original input image<br>correctly classified as a â€œpandaâ€, $y$ is the ground truth label<br>for $\mathbf{x}$, $\mathbf{\theta}$ represents the model<br>parameters, and $J(\mathbf{\theta}, \mathbf{x}, y)$ is the loss<br>that is used to train the network. The attack backpropagates the<br>gradient back to the input data to calculate<br>$\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$. Then, it adjusts<br>the input data by a small step ($\epsilon$ or $0.007$ in the<br>picture) in the direction (i.e.<br>$sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))$) that will<br>maximize the loss. The resulting perturbed image, $xâ€™$, is then<br><em>misclassified</em> by the target network as a â€œgibbonâ€ when it is still<br>clearly a â€œpandaâ€.</p>
<p>Hopefully now the motivation for this tutorial is clear, so lets jump<br>into the implementation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>In this section, we will discuss the input parameters for the tutorial,<br>define the model under attack, then code the attack and run some tests.</p>
<p>Inputs</p>
<p>There are only three inputs for this tutorial, and are defined as<br>follows:</p>
<ul>
<li><p><strong>epsilons</strong> - List of epsilon values to use for the run. It is<br>important to keep 0 in the list because it represents the model<br>performance on the original test set. Also, intuitively we would<br>expect the larger the epsilon, the more noticeable the perturbations<br>but the more effective the attack in terms of degrading model<br>accuracy. Since the data range here is $[0,1]$, no epsilon<br>value should exceed 1.</p>
</li>
<li><p><strong>pretrained_model</strong> - path to the pretrained MNIST model which was<br>trained with<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code><strong>.<br>For simplicity, download the pretrained model <code>here &lt;https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing&gt;</code></strong>.</p>
</li>
<li><p><strong>use_cuda</strong> - boolean flag to use CUDA if desired and available.<br>Note, a GPU with CUDA is not critical for this tutorial as a CPU will<br>not take much time.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilons = [<span class="number">0</span>, <span class="number">.05</span>, <span class="number">.1</span>, <span class="number">.15</span>, <span class="number">.2</span>, <span class="number">.25</span>, <span class="number">.3</span>]</span><br><span class="line">pretrained_model = <span class="string">"data/lenet_mnist_model.pth"</span></span><br><span class="line">use_cuda=<span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>Model Under Attack</p>
<p>As mentioned, the model under attack is the same MNIST model from<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code>__.<br>You may train and save your own MNIST model or you can download and use<br>the provided model. The <em>Net</em> definition and test dataloader here have<br>been copied from the MNIST example. The purpose of this section is to<br>define the model and dataloader, then initialize the model and load the<br>pretrained weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># LeNet Model definition</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MNIST Test dataset and dataloader declaration</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            ])), </span><br><span class="line">        batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define what device we are using</span></span><br><span class="line">print(<span class="string">"CUDA Available: "</span>,torch.cuda.is_available())</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> (use_cuda <span class="keyword">and</span> torch.cuda.is_available()) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the network</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pretrained model</span></span><br><span class="line">model.load_state_dict(torch.load(pretrained_model, map_location=<span class="string">'cpu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the model in evaluation mode. In this case this is for the Dropout layers</span></span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>
<pre><code>Using downloaded and verified file: data\MNIST\raw\train-images-idx3-ubyte.gz
Extracting data\MNIST\raw\train-images-idx3-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\train-labels-idx1-ubyte.gz
Extracting data\MNIST\raw\train-labels-idx1-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\t10k-images-idx3-ubyte.gz
Extracting data\MNIST\raw\t10k-images-idx3-ubyte.gz
Using downloaded and verified file: data\MNIST\raw\t10k-labels-idx1-ubyte.gz
Extracting data\MNIST\raw\t10k-labels-idx1-ubyte.gz
Processing...
Done!
CUDA Available:  False





Net(
  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))
  (conv2_drop): Dropout2d(p=0.5)
  (fc1): Linear(in_features=320, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=10, bias=True)
)
</code></pre><p>FGSM Attack</p>
<p>Now, we can define the function that creates the adversarial examples by<br>perturbing the original inputs. The <code>fgsm_attack</code> function takes three<br>inputs, <em>image</em> is the original clean image ($x$), <em>epsilon</em> is<br>the pixel-wise perturbation amount ($\epsilon$), and <em>data_grad</em><br>is gradient of the loss w.r.t the input image<br>($\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$). The function<br>then creates perturbed image as</p>
<p>\begin{align}perturbed_image = image + epsilon<em>sign(data_grad) = x + \epsilon </em> sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))\end{align}</p>
<p>Finally, in order to maintain the original range of the data, the<br>perturbed image is clipped to range $[0,1]$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># FGSM attack code</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fgsm_attack</span><span class="params">(image, epsilon, data_grad)</span>:</span></span><br><span class="line">    <span class="comment"># Collect the element-wise sign of the data gradient</span></span><br><span class="line">    sign_data_grad = data_grad.sign()</span><br><span class="line">    <span class="comment"># Create the perturbed image by adjusting each pixel of the input image</span></span><br><span class="line">    perturbed_image = image + epsilon*sign_data_grad</span><br><span class="line">    <span class="comment"># Adding clipping to maintain [0,1] range</span></span><br><span class="line">    perturbed_image = torch.clamp(perturbed_image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Return the perturbed image</span></span><br><span class="line">    <span class="keyword">return</span> perturbed_image</span><br></pre></td></tr></table></figure>
<p>Testing Function</p>
<p>Finally, the central result of this tutorial comes from the <code>test</code><br>function. Each call to this test function performs a full test step on<br>the MNIST test set and reports a final accuracy. However, notice that<br>this function also takes an <em>epsilon</em> input. This is because the<br><code>test</code> function reports the accuracy of a model that is under attack<br>from an adversary with strength $\epsilon$. More specifically, for<br>each sample in the test set, the function computes the gradient of the<br>loss w.r.t the input data ($data_grad$), creates a perturbed<br>image with <code>fgsm_attack</code> ($perturbed_data$), then checks to see<br>if the perturbed example is adversarial. In addition to testing the<br>accuracy of the model, the function also saves and returns some<br>successful adversarial examples to be visualized later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">( model, device, test_loader, epsilon )</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Accuracy counter</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    adv_examples = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop over all examples in test set</span></span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Send the data and label to the device</span></span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set requires_grad attribute of tensor. Important for Attack</span></span><br><span class="line">        data.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass the data through the model</span></span><br><span class="line">        output = model(data)</span><br><span class="line">        init_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the initial prediction is wrong, dont bother attacking, just move on</span></span><br><span class="line">        <span class="keyword">if</span> init_pred.item() != target.item():</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the loss</span></span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero all existing gradients</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate gradients of model in backward pass</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Collect datagrad</span></span><br><span class="line">        data_grad = data.grad.data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Call FGSM Attack</span></span><br><span class="line">        perturbed_data = fgsm_attack(data, epsilon, data_grad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Re-classify the perturbed image</span></span><br><span class="line">        output = model(perturbed_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for success</span></span><br><span class="line">        final_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">        <span class="keyword">if</span> final_pred.item() == target.item():</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Special case for saving 0 epsilon examples</span></span><br><span class="line">            <span class="keyword">if</span> (epsilon == <span class="number">0</span>) <span class="keyword">and</span> (len(adv_examples) &lt; <span class="number">5</span>):</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Save some adv examples for visualization later</span></span><br><span class="line">            <span class="keyword">if</span> len(adv_examples) &lt; <span class="number">5</span>:</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate final accuracy for this epsilon</span></span><br><span class="line">    final_acc = correct/float(len(test_loader))</span><br><span class="line">    print(<span class="string">"Epsilon: &#123;&#125;\tTest Accuracy = &#123;&#125; / &#123;&#125; = &#123;&#125;"</span>.format(epsilon, correct, len(test_loader), final_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the accuracy and an adversarial example</span></span><br><span class="line">    <span class="keyword">return</span> final_acc, adv_examples</span><br></pre></td></tr></table></figure>
<p>Run Attack</p>
<p>The last part of the implementation is to actually run the attack. Here,<br>we run a full test step for each epsilon value in the <em>epsilons</em> input.<br>For each epsilon we also save the final accuracy and some successful<br>adversarial examples to be plotted in the coming sections. Notice how<br>the printed accuracies decrease as the epsilon value increases. Also,<br>note the $\epsilon=0$ case represents the original test accuracy,<br>with no attack.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracies = []</span><br><span class="line">examples = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run test for each epsilon</span></span><br><span class="line"><span class="keyword">for</span> eps <span class="keyword">in</span> epsilons:</span><br><span class="line">    acc, ex = test(model, device, test_loader, eps)</span><br><span class="line">    accuracies.append(acc)</span><br><span class="line">    examples.append(ex)</span><br></pre></td></tr></table></figure>
<pre><code>Epsilon: 0    Test Accuracy = 9810 / 10000 = 0.981
Epsilon: 0.05    Test Accuracy = 9426 / 10000 = 0.9426
Epsilon: 0.1    Test Accuracy = 8510 / 10000 = 0.851
Epsilon: 0.15    Test Accuracy = 6826 / 10000 = 0.6826
Epsilon: 0.2    Test Accuracy = 4301 / 10000 = 0.4301
Epsilon: 0.25    Test Accuracy = 2082 / 10000 = 0.2082
Epsilon: 0.3    Test Accuracy = 869 / 10000 = 0.0869
</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Accuracy vs Epsilon</p>
<p>The first result is the accuracy versus epsilon plot. As alluded to<br>earlier, as epsilon increases we expect the test accuracy to decrease.<br>This is because larger epsilons mean we take a larger step in the<br>direction that will maximize the loss. Notice the trend in the curve is<br>not linear even though the epsilon values are linearly spaced. For<br>example, the accuracy at $\epsilon=0.05$ is only about 4% lower<br>than $\epsilon=0$, but the accuracy at $\epsilon=0.2$ is 25%<br>lower than $\epsilon=0.15$. Also, notice the accuracy of the model<br>hits random accuracy for a 10-class classifier between<br>$\epsilon=0.25$ and $\epsilon=0.3$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(epsilons, accuracies, <span class="string">"*-"</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">1.1</span>, step=<span class="number">0.1</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, <span class="number">.35</span>, step=<span class="number">0.05</span>))</span><br><span class="line">plt.title(<span class="string">"Accuracy vs Epsilon"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epsilon"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_15_0.png" alt="png"></p>
<p>Sample Adversarial Examples</p>
<p>Remember the idea of no free lunch? In this case, as epsilon increases<br>the test accuracy decreases <strong>BUT</strong> the perturbations become more easily<br>perceptible. In reality, there is a tradeoff between accuracy<br>degredation and perceptibility that an attacker must consider. Here, we<br>show some examples of successful adversarial examples at each epsilon<br>value. Each row of the plot shows a different epsilon value. The first<br>row is the $\epsilon=0$ examples which represent the original<br>â€œcleanâ€ images with no perturbation. The title of each image shows the<br>â€œoriginal classification -&gt; adversarial classification.â€ Notice, the<br>perturbations start to become evident at $\epsilon=0.15$ and are<br>quite evident at $\epsilon=0.3$. However, in all cases humans are<br>still capable of identifying the correct class despite the added noise.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Plot several examples of adversarial samples at each epsilon</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(epsilons)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(examples[i])):</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.subplot(len(epsilons),len(examples[<span class="number">0</span>]),cnt)</span><br><span class="line">        plt.xticks([], [])</span><br><span class="line">        plt.yticks([], [])</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            plt.ylabel(<span class="string">"Eps: &#123;&#125;"</span>.format(epsilons[i]), fontsize=<span class="number">14</span>)</span><br><span class="line">        orig,adv,ex = examples[i][j]</span><br><span class="line">        plt.title(<span class="string">"&#123;&#125; -&gt; &#123;&#125;"</span>.format(orig, adv))</span><br><span class="line">        plt.imshow(ex, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_17_0.png" alt="png"></p>
<h2 id="Where-to-go-next"><a href="#Where-to-go-next" class="headerlink" title="Where to go next?"></a>Where to go next?</h2><p>Hopefully this tutorial gives some insight into the topic of adversarial<br>machine learning. There are many potential directions to go from here.<br>This attack represents the very beginning of adversarial attack research<br>and since there have been many subsequent ideas for how to attack and<br>defend ML models from an adversary. In fact, at NIPS 2017 there was an<br>adversarial attack and defense competition and many of the methods used<br>in the competition are described in this paper: <code>Adversarial Attacks and
Defences Competition &lt;https://arxiv.org/pdf/1804.00097.pdf&gt;</code>__. The work<br>on defense also leads into the idea of making machine learning models<br>more <em>robust</em> in general, to both naturally perturbed and adversarially<br>crafted inputs.</p>
<p>Another direction to go is adversarial attacks and defense in different<br>domains. Adversarial research is not limited to the image domain, check<br>out <code>this &lt;https://arxiv.org/pdf/1801.01944.pdf&gt;</code>__ attack on<br>speech-to-text models. But perhaps the best way to learn more about<br>adversarial machine learning is to get your hands dirty. Try to<br>implement a different attack from the NIPS 2017 competition, and see how<br>it differs from FGSM. Then, try to defend the model from your own<br>attacks.</p>
<h2 id="æˆ‘ä¸è®¤è¯†çš„å•è¯"><a href="#æˆ‘ä¸è®¤è¯†çš„å•è¯" class="headerlink" title="æˆ‘ä¸è®¤è¯†çš„å•è¯"></a>æˆ‘ä¸è®¤è¯†çš„å•è¯</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">overlooked:è¢«å¿½è§†</span><br><span class="line">robustness:å¥å£®æ€§&#x2F;é²æ£’æ€§</span><br><span class="line">vulnerabilities:æ¼æ´</span><br><span class="line">imperceptible:ä¸å¯å¯Ÿè§‰çš„</span><br><span class="line">drastically:å‰§çƒˆåœ°</span><br><span class="line">via:é€šè¿‡</span><br><span class="line">remarkably:æ˜¾ç€åœ°</span><br><span class="line">intuitive:ç›´è§‰çš„</span><br><span class="line">tradeoff:äº¤æ˜“</span><br><span class="line">perturbations:æ‘„åŠ¨,æ‰°åŠ¨</span><br><span class="line">adversarial:å¯¹æŠ—çš„</span><br><span class="line">However, in general the overarching goal is to add the least amount of perturbation to the input data to cause the desired misclassification.:ä¸€èˆ¬æ¥è¯´ï¼Œæ€»ä½“ç›®æ ‡æ˜¯å‘è¾“å…¥æ•°æ®æ·»åŠ æœ€å°‘çš„æ‰°åŠ¨é‡ï¼Œä»è€Œå¯¼è‡´æ‰€éœ€çš„é”™è¯¯åˆ†ç±»</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Image-å¾®è°ƒTorchVisionå¯¹è±¡æ£€æµ‹</title>
    <url>/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>Pytorch-Image-å¾®è°ƒTorchVisionå¯¹è±¡æ£€æµ‹:<br><a id="more"></a></p>
<h1 id="TorchVision-0-3-Object-Detection-finetuning-tutorial"><a href="#TorchVision-0-3-Object-Detection-finetuning-tutorial" class="headerlink" title="TorchVision 0.3 Object Detection finetuning tutorial"></a>TorchVision 0.3 Object Detection finetuning tutorial</h1><p>For this tutorial, we will be finetuning a pre-trained <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a> model in the <a href="https://www.cis.upenn.edu/jshi/ped_html/" target="_blank" rel="noopener"><em>Penn-Fudan Database for Pedestrian Detection and Segmentation</em></a>. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.</p>
<p>First, we need to install <code>pycocotools</code>. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line">pip install cython</span><br><span class="line"><span class="comment"># Install pycocotools, the version by default in Colab</span></span><br><span class="line"><span class="comment"># has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354</span></span><br><span class="line">pip install -U <span class="string">'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</span></span><br></pre></td></tr></table></figure>
<pre><code>Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)
Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI
  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-h3isg2r5
  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-h3isg2r5
Requirement already satisfied, skipping upgrade: setuptools&gt;=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.1.0)
Requirement already satisfied, skipping upgrade: cython&gt;=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)
Requirement already satisfied, skipping upgrade: matplotlib&gt;=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)
Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.4.7)
Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.8.1)
Requirement already satisfied, skipping upgrade: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.18.5)
Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (0.10.0)
Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.2.0)
Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.15.0)
Building wheels for collected packages: pycocotools
  Building wheel for pycocotools (setup.py) ... [?25l[?25hdone
  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266460 sha256=3292fbae19c3df30ceb54183f71e1e7288d447743b1dcb8f88257833cf2f23e1
  Stored in directory: /tmp/pip-ephem-wheel-cache-y2jf5d3p/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a
Successfully built pycocotools
Installing collected packages: pycocotools
  Found existing installation: pycocotools 2.0.1
    Uninstalling pycocotools-2.0.1:
      Successfully uninstalled pycocotools-2.0.1
Successfully installed pycocotools-2.0
</code></pre><h2 id="Defining-the-Dataset"><a href="#Defining-the-Dataset" class="headerlink" title="Defining the Dataset"></a>Defining the Dataset</h2><p>The <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision reference scripts for training object detection, instance segmentation and person keypoint detection</a> allows for easily supporting adding new custom datasets.<br>The dataset should inherit from the standard <code>torch.utils.data.Dataset</code> class, and implement <code>__len__</code> and <code>__getitem__</code>.</p>
<p>The only specificity that we require is that the dataset <code>__getitem__</code> should return:</p>
<ul>
<li>image: a PIL Image of size (H, W)</li>
<li>target: a dict containing the following fields<ul>
<li><code>boxes</code> (<code>FloatTensor[N, 4]</code>): the coordinates of the <code>N</code> bounding boxes in <code>[x0, y0, x1, y1]</code> format, ranging from <code>0</code> to <code>W</code> and <code>0</code> to <code>H</code></li>
<li><code>labels</code> (<code>Int64Tensor[N]</code>): the label for each bounding box</li>
<li><code>image_id</code> (<code>Int64Tensor[1]</code>): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation</li>
<li><code>area</code> (<code>Tensor[N]</code>): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.</li>
<li><code>iscrowd</code> (<code>UInt8Tensor[N]</code>): instances with <code>iscrowd=True</code> will be ignored during evaluation.</li>
<li>(optionally) <code>masks</code> (<code>UInt8Tensor[N, H, W]</code>): The segmentation masks for each one of the objects</li>
<li>(optionally) <code>keypoints</code> (<code>FloatTensor[N, K, 3]</code>): For each one of the <code>N</code> objects, it contains the <code>K</code> keypoints in <code>[x, y, visibility]</code> format, defining the object. <code>visibility=0</code> means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt <code>references/detection/transforms.py</code> for your new keypoint representation</li>
</ul>
</li>
</ul>
<p>If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.</p>
<p>Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a <code>get_height_and_width</code> method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via <code>__getitem__</code> , which loads the image in memory and is slower than if a custom method is provided.</p>
<h3 id="Writing-a-custom-dataset-for-Penn-Fudan"><a href="#Writing-a-custom-dataset-for-Penn-Fudan" class="headerlink" title="Writing a custom dataset for Penn-Fudan"></a>Writing a custom dataset for Penn-Fudan</h3><p>Letâ€™s write a dataset for the Penn-Fudan dataset.</p>
<p>First, letâ€™s download and extract the data, present in a zip file at <a href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip" target="_blank" rel="noopener">https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the Penn-Fudan dataset</span></span><br><span class="line">wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .</span><br><span class="line"><span class="comment"># extract it in the current folder</span></span><br><span class="line">unzip PennFudanPed.zip</span><br></pre></td></tr></table></figure>
<pre><code>--2020-07-23 13:46:08--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip
Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d
Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 53723336 (51M) [application/zip]
Saving to: â€˜PennFudanPed.zipâ€™

PennFudanPed.zip    100%[===================&gt;]  51.23M  1009KB/s    in 48s     

2020-07-23 13:46:58 (1.06 MB/s) - â€˜PennFudanPed.zipâ€™ saved [53723336/53723336]

--2020-07-23 13:46:58--  http://./
Resolving . (.)... failed: No address associated with hostname.
wget: unable to resolve host address â€˜.â€™
FINISHED --2020-07-23 13:46:58--
Total wall clock time: 50s
Downloaded: 1 files, 51M in 48s (1.06 MB/s)
Archive:  PennFudanPed.zip
   creating: PennFudanPed/
  inflating: PennFudanPed/added-object-list.txt  
   creating: PennFudanPed/Annotation/
  inflating: PennFudanPed/Annotation/FudanPed00001.txt  
  inflating: PennFudanPed/Annotation/FudanPed00002.txt  
  inflating: PennFudanPed/Annotation/FudanPed00003.txt  
  inflating: PennFudanPed/Annotation/FudanPed00004.txt  
  inflating: PennFudanPed/Annotation/FudanPed00005.txt  
  inflating: PennFudanPed/Annotation/FudanPed00006.txt  
  inflating: PennFudanPed/Annotation/FudanPed00007.txt  
  inflating: PennFudanPed/Annotation/FudanPed00008.txt  
  inflating: PennFudanPed/Annotation/FudanPed00009.txt  
  inflating: PennFudanPed/Annotation/FudanPed00010.txt  
  inflating: PennFudanPed/Annotation/FudanPed00011.txt  
  inflating: PennFudanPed/Annotation/FudanPed00012.txt  
  inflating: PennFudanPed/Annotation/FudanPed00013.txt  
  inflating: PennFudanPed/Annotation/FudanPed00014.txt  
  inflating: PennFudanPed/Annotation/FudanPed00015.txt  
  inflating: PennFudanPed/Annotation/FudanPed00016.txt  
  inflating: PennFudanPed/Annotation/FudanPed00017.txt  
  inflating: PennFudanPed/Annotation/FudanPed00018.txt  
  inflating: PennFudanPed/Annotation/FudanPed00019.txt  
  inflating: PennFudanPed/Annotation/FudanPed00020.txt  
  inflating: PennFudanPed/Annotation/FudanPed00021.txt  
  inflating: PennFudanPed/Annotation/FudanPed00022.txt  
  inflating: PennFudanPed/Annotation/FudanPed00023.txt  
  inflating: PennFudanPed/Annotation/FudanPed00024.txt  
  inflating: PennFudanPed/Annotation/FudanPed00025.txt  
  inflating: PennFudanPed/Annotation/FudanPed00026.txt  
  inflating: PennFudanPed/Annotation/FudanPed00027.txt  
  inflating: PennFudanPed/Annotation/FudanPed00028.txt  
  inflating: PennFudanPed/Annotation/FudanPed00029.txt  
  inflating: PennFudanPed/Annotation/FudanPed00030.txt  
  inflating: PennFudanPed/Annotation/FudanPed00031.txt  
  inflating: PennFudanPed/Annotation/FudanPed00032.txt  
  inflating: PennFudanPed/Annotation/FudanPed00033.txt  
  inflating: PennFudanPed/Annotation/FudanPed00034.txt  
  inflating: PennFudanPed/Annotation/FudanPed00035.txt  
  inflating: PennFudanPed/Annotation/FudanPed00036.txt  
  inflating: PennFudanPed/Annotation/FudanPed00037.txt  
  inflating: PennFudanPed/Annotation/FudanPed00038.txt  
  inflating: PennFudanPed/Annotation/FudanPed00039.txt  
  inflating: PennFudanPed/Annotation/FudanPed00040.txt  
  inflating: PennFudanPed/Annotation/FudanPed00041.txt  
  inflating: PennFudanPed/Annotation/FudanPed00042.txt  
  inflating: PennFudanPed/Annotation/FudanPed00043.txt  
  inflating: PennFudanPed/Annotation/FudanPed00044.txt  
  inflating: PennFudanPed/Annotation/FudanPed00045.txt  
  inflating: PennFudanPed/Annotation/FudanPed00046.txt  
  inflating: PennFudanPed/Annotation/FudanPed00047.txt  
  inflating: PennFudanPed/Annotation/FudanPed00048.txt  
  inflating: PennFudanPed/Annotation/FudanPed00049.txt  
  inflating: PennFudanPed/Annotation/FudanPed00050.txt  
  inflating: PennFudanPed/Annotation/FudanPed00051.txt  
  inflating: PennFudanPed/Annotation/FudanPed00052.txt  
  inflating: PennFudanPed/Annotation/FudanPed00053.txt  
  inflating: PennFudanPed/Annotation/FudanPed00054.txt  
  inflating: PennFudanPed/Annotation/FudanPed00055.txt  
  inflating: PennFudanPed/Annotation/FudanPed00056.txt  
  inflating: PennFudanPed/Annotation/FudanPed00057.txt  
  inflating: PennFudanPed/Annotation/FudanPed00058.txt  
  inflating: PennFudanPed/Annotation/FudanPed00059.txt  
  inflating: PennFudanPed/Annotation/FudanPed00060.txt  
  inflating: PennFudanPed/Annotation/FudanPed00061.txt  
  inflating: PennFudanPed/Annotation/FudanPed00062.txt  
  inflating: PennFudanPed/Annotation/FudanPed00063.txt  
  inflating: PennFudanPed/Annotation/FudanPed00064.txt  
  inflating: PennFudanPed/Annotation/FudanPed00065.txt  
  inflating: PennFudanPed/Annotation/FudanPed00066.txt  
  inflating: PennFudanPed/Annotation/FudanPed00067.txt  
  inflating: PennFudanPed/Annotation/FudanPed00068.txt  
  inflating: PennFudanPed/Annotation/FudanPed00069.txt  
  inflating: PennFudanPed/Annotation/FudanPed00070.txt  
  inflating: PennFudanPed/Annotation/FudanPed00071.txt  
  inflating: PennFudanPed/Annotation/FudanPed00072.txt  
  inflating: PennFudanPed/Annotation/FudanPed00073.txt  
  inflating: PennFudanPed/Annotation/FudanPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00001.txt  
  inflating: PennFudanPed/Annotation/PennPed00002.txt  
  inflating: PennFudanPed/Annotation/PennPed00003.txt  
  inflating: PennFudanPed/Annotation/PennPed00004.txt  
  inflating: PennFudanPed/Annotation/PennPed00005.txt  
  inflating: PennFudanPed/Annotation/PennPed00006.txt  
  inflating: PennFudanPed/Annotation/PennPed00007.txt  
  inflating: PennFudanPed/Annotation/PennPed00008.txt  
  inflating: PennFudanPed/Annotation/PennPed00009.txt  
  inflating: PennFudanPed/Annotation/PennPed00010.txt  
  inflating: PennFudanPed/Annotation/PennPed00011.txt  
  inflating: PennFudanPed/Annotation/PennPed00012.txt  
  inflating: PennFudanPed/Annotation/PennPed00013.txt  
  inflating: PennFudanPed/Annotation/PennPed00014.txt  
  inflating: PennFudanPed/Annotation/PennPed00015.txt  
  inflating: PennFudanPed/Annotation/PennPed00016.txt  
  inflating: PennFudanPed/Annotation/PennPed00017.txt  
  inflating: PennFudanPed/Annotation/PennPed00018.txt  
  inflating: PennFudanPed/Annotation/PennPed00019.txt  
  inflating: PennFudanPed/Annotation/PennPed00020.txt  
  inflating: PennFudanPed/Annotation/PennPed00021.txt  
  inflating: PennFudanPed/Annotation/PennPed00022.txt  
  inflating: PennFudanPed/Annotation/PennPed00023.txt  
  inflating: PennFudanPed/Annotation/PennPed00024.txt  
  inflating: PennFudanPed/Annotation/PennPed00025.txt  
  inflating: PennFudanPed/Annotation/PennPed00026.txt  
  inflating: PennFudanPed/Annotation/PennPed00027.txt  
  inflating: PennFudanPed/Annotation/PennPed00028.txt  
  inflating: PennFudanPed/Annotation/PennPed00029.txt  
  inflating: PennFudanPed/Annotation/PennPed00030.txt  
  inflating: PennFudanPed/Annotation/PennPed00031.txt  
  inflating: PennFudanPed/Annotation/PennPed00032.txt  
  inflating: PennFudanPed/Annotation/PennPed00033.txt  
  inflating: PennFudanPed/Annotation/PennPed00034.txt  
  inflating: PennFudanPed/Annotation/PennPed00035.txt  
  inflating: PennFudanPed/Annotation/PennPed00036.txt  
  inflating: PennFudanPed/Annotation/PennPed00037.txt  
  inflating: PennFudanPed/Annotation/PennPed00038.txt  
  inflating: PennFudanPed/Annotation/PennPed00039.txt  
  inflating: PennFudanPed/Annotation/PennPed00040.txt  
  inflating: PennFudanPed/Annotation/PennPed00041.txt  
  inflating: PennFudanPed/Annotation/PennPed00042.txt  
  inflating: PennFudanPed/Annotation/PennPed00043.txt  
  inflating: PennFudanPed/Annotation/PennPed00044.txt  
  inflating: PennFudanPed/Annotation/PennPed00045.txt  
  inflating: PennFudanPed/Annotation/PennPed00046.txt  
  inflating: PennFudanPed/Annotation/PennPed00047.txt  
  inflating: PennFudanPed/Annotation/PennPed00048.txt  
  inflating: PennFudanPed/Annotation/PennPed00049.txt  
  inflating: PennFudanPed/Annotation/PennPed00050.txt  
  inflating: PennFudanPed/Annotation/PennPed00051.txt  
  inflating: PennFudanPed/Annotation/PennPed00052.txt  
  inflating: PennFudanPed/Annotation/PennPed00053.txt  
  inflating: PennFudanPed/Annotation/PennPed00054.txt  
  inflating: PennFudanPed/Annotation/PennPed00055.txt  
  inflating: PennFudanPed/Annotation/PennPed00056.txt  
  inflating: PennFudanPed/Annotation/PennPed00057.txt  
  inflating: PennFudanPed/Annotation/PennPed00058.txt  
  inflating: PennFudanPed/Annotation/PennPed00059.txt  
  inflating: PennFudanPed/Annotation/PennPed00060.txt  
  inflating: PennFudanPed/Annotation/PennPed00061.txt  
  inflating: PennFudanPed/Annotation/PennPed00062.txt  
  inflating: PennFudanPed/Annotation/PennPed00063.txt  
  inflating: PennFudanPed/Annotation/PennPed00064.txt  
  inflating: PennFudanPed/Annotation/PennPed00065.txt  
  inflating: PennFudanPed/Annotation/PennPed00066.txt  
  inflating: PennFudanPed/Annotation/PennPed00067.txt  
  inflating: PennFudanPed/Annotation/PennPed00068.txt  
  inflating: PennFudanPed/Annotation/PennPed00069.txt  
  inflating: PennFudanPed/Annotation/PennPed00070.txt  
  inflating: PennFudanPed/Annotation/PennPed00071.txt  
  inflating: PennFudanPed/Annotation/PennPed00072.txt  
  inflating: PennFudanPed/Annotation/PennPed00073.txt  
  inflating: PennFudanPed/Annotation/PennPed00074.txt  
  inflating: PennFudanPed/Annotation/PennPed00075.txt  
  inflating: PennFudanPed/Annotation/PennPed00076.txt  
  inflating: PennFudanPed/Annotation/PennPed00077.txt  
  inflating: PennFudanPed/Annotation/PennPed00078.txt  
  inflating: PennFudanPed/Annotation/PennPed00079.txt  
  inflating: PennFudanPed/Annotation/PennPed00080.txt  
  inflating: PennFudanPed/Annotation/PennPed00081.txt  
  inflating: PennFudanPed/Annotation/PennPed00082.txt  
  inflating: PennFudanPed/Annotation/PennPed00083.txt  
  inflating: PennFudanPed/Annotation/PennPed00084.txt  
  inflating: PennFudanPed/Annotation/PennPed00085.txt  
  inflating: PennFudanPed/Annotation/PennPed00086.txt  
  inflating: PennFudanPed/Annotation/PennPed00087.txt  
  inflating: PennFudanPed/Annotation/PennPed00088.txt  
  inflating: PennFudanPed/Annotation/PennPed00089.txt  
  inflating: PennFudanPed/Annotation/PennPed00090.txt  
  inflating: PennFudanPed/Annotation/PennPed00091.txt  
  inflating: PennFudanPed/Annotation/PennPed00092.txt  
  inflating: PennFudanPed/Annotation/PennPed00093.txt  
  inflating: PennFudanPed/Annotation/PennPed00094.txt  
  inflating: PennFudanPed/Annotation/PennPed00095.txt  
  inflating: PennFudanPed/Annotation/PennPed00096.txt  
   creating: PennFudanPed/PedMasks/
  inflating: PennFudanPed/PedMasks/FudanPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00014_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00017_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00027_mask.png  
 extracting: PennFudanPed/PedMasks/FudanPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00052_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00061_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00065_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00068_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00069_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00071_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00073_mask.png  
  inflating: PennFudanPed/PedMasks/FudanPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00001_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00002_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00003_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00004_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00005_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00006_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00007_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00008_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00009_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00010_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00011_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00012_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00013_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00014_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00015_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00016_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00017_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00018_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00019_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00020_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00021_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00022_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00023_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00024_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00025_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00026_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00027_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00028_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00029_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00030_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00031_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00032_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00033_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00034_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00035_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00036_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00037_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00038_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00039_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00040_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00041_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00042_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00043_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00044_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00045_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00046_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00047_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00048_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00049_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00050_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00051_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00052_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00053_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00054_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00055_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00056_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00057_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00058_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00059_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00060_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00061_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00062_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00063_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00064_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00065_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00066_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00067_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00068_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00069_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00070_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00071_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00072_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00073_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00074_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00075_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00076_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00077_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00078_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00079_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00080_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00081_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00082_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00083_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00084_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00085_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00086_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00087_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00088_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00089_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00090_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00091_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00092_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00093_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00094_mask.png  
  inflating: PennFudanPed/PedMasks/PennPed00095_mask.png  
 extracting: PennFudanPed/PedMasks/PennPed00096_mask.png  
   creating: PennFudanPed/PNGImages/
  inflating: PennFudanPed/PNGImages/FudanPed00001.png  
  inflating: PennFudanPed/PNGImages/FudanPed00002.png  
  inflating: PennFudanPed/PNGImages/FudanPed00003.png  
  inflating: PennFudanPed/PNGImages/FudanPed00004.png  
  inflating: PennFudanPed/PNGImages/FudanPed00005.png  
  inflating: PennFudanPed/PNGImages/FudanPed00006.png  
  inflating: PennFudanPed/PNGImages/FudanPed00007.png  
  inflating: PennFudanPed/PNGImages/FudanPed00008.png  
  inflating: PennFudanPed/PNGImages/FudanPed00009.png  
  inflating: PennFudanPed/PNGImages/FudanPed00010.png  
  inflating: PennFudanPed/PNGImages/FudanPed00011.png  
  inflating: PennFudanPed/PNGImages/FudanPed00012.png  
  inflating: PennFudanPed/PNGImages/FudanPed00013.png  
  inflating: PennFudanPed/PNGImages/FudanPed00014.png  
  inflating: PennFudanPed/PNGImages/FudanPed00015.png  
  inflating: PennFudanPed/PNGImages/FudanPed00016.png  
  inflating: PennFudanPed/PNGImages/FudanPed00017.png  
  inflating: PennFudanPed/PNGImages/FudanPed00018.png  
  inflating: PennFudanPed/PNGImages/FudanPed00019.png  
  inflating: PennFudanPed/PNGImages/FudanPed00020.png  
  inflating: PennFudanPed/PNGImages/FudanPed00021.png  
  inflating: PennFudanPed/PNGImages/FudanPed00022.png  
  inflating: PennFudanPed/PNGImages/FudanPed00023.png  
  inflating: PennFudanPed/PNGImages/FudanPed00024.png  
  inflating: PennFudanPed/PNGImages/FudanPed00025.png  
  inflating: PennFudanPed/PNGImages/FudanPed00026.png  
  inflating: PennFudanPed/PNGImages/FudanPed00027.png  
  inflating: PennFudanPed/PNGImages/FudanPed00028.png  
  inflating: PennFudanPed/PNGImages/FudanPed00029.png  
  inflating: PennFudanPed/PNGImages/FudanPed00030.png  
  inflating: PennFudanPed/PNGImages/FudanPed00031.png  
  inflating: PennFudanPed/PNGImages/FudanPed00032.png  
  inflating: PennFudanPed/PNGImages/FudanPed00033.png  
  inflating: PennFudanPed/PNGImages/FudanPed00034.png  
  inflating: PennFudanPed/PNGImages/FudanPed00035.png  
  inflating: PennFudanPed/PNGImages/FudanPed00036.png  
  inflating: PennFudanPed/PNGImages/FudanPed00037.png  
  inflating: PennFudanPed/PNGImages/FudanPed00038.png  
  inflating: PennFudanPed/PNGImages/FudanPed00039.png  
  inflating: PennFudanPed/PNGImages/FudanPed00040.png  
  inflating: PennFudanPed/PNGImages/FudanPed00041.png  
  inflating: PennFudanPed/PNGImages/FudanPed00042.png  
  inflating: PennFudanPed/PNGImages/FudanPed00043.png  
  inflating: PennFudanPed/PNGImages/FudanPed00044.png  
  inflating: PennFudanPed/PNGImages/FudanPed00045.png  
  inflating: PennFudanPed/PNGImages/FudanPed00046.png  
  inflating: PennFudanPed/PNGImages/FudanPed00047.png  
  inflating: PennFudanPed/PNGImages/FudanPed00048.png  
  inflating: PennFudanPed/PNGImages/FudanPed00049.png  
  inflating: PennFudanPed/PNGImages/FudanPed00050.png  
  inflating: PennFudanPed/PNGImages/FudanPed00051.png  
  inflating: PennFudanPed/PNGImages/FudanPed00052.png  
  inflating: PennFudanPed/PNGImages/FudanPed00053.png  
  inflating: PennFudanPed/PNGImages/FudanPed00054.png  
  inflating: PennFudanPed/PNGImages/FudanPed00055.png  
  inflating: PennFudanPed/PNGImages/FudanPed00056.png  
  inflating: PennFudanPed/PNGImages/FudanPed00057.png  
  inflating: PennFudanPed/PNGImages/FudanPed00058.png  
  inflating: PennFudanPed/PNGImages/FudanPed00059.png  
  inflating: PennFudanPed/PNGImages/FudanPed00060.png  
  inflating: PennFudanPed/PNGImages/FudanPed00061.png  
  inflating: PennFudanPed/PNGImages/FudanPed00062.png  
  inflating: PennFudanPed/PNGImages/FudanPed00063.png  
  inflating: PennFudanPed/PNGImages/FudanPed00064.png  
  inflating: PennFudanPed/PNGImages/FudanPed00065.png  
  inflating: PennFudanPed/PNGImages/FudanPed00066.png  
  inflating: PennFudanPed/PNGImages/FudanPed00067.png  
  inflating: PennFudanPed/PNGImages/FudanPed00068.png  
  inflating: PennFudanPed/PNGImages/FudanPed00069.png  
  inflating: PennFudanPed/PNGImages/FudanPed00070.png  
  inflating: PennFudanPed/PNGImages/FudanPed00071.png  
  inflating: PennFudanPed/PNGImages/FudanPed00072.png  
  inflating: PennFudanPed/PNGImages/FudanPed00073.png  
  inflating: PennFudanPed/PNGImages/FudanPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00001.png  
  inflating: PennFudanPed/PNGImages/PennPed00002.png  
  inflating: PennFudanPed/PNGImages/PennPed00003.png  
  inflating: PennFudanPed/PNGImages/PennPed00004.png  
  inflating: PennFudanPed/PNGImages/PennPed00005.png  
  inflating: PennFudanPed/PNGImages/PennPed00006.png  
  inflating: PennFudanPed/PNGImages/PennPed00007.png  
  inflating: PennFudanPed/PNGImages/PennPed00008.png  
  inflating: PennFudanPed/PNGImages/PennPed00009.png  
  inflating: PennFudanPed/PNGImages/PennPed00010.png  
  inflating: PennFudanPed/PNGImages/PennPed00011.png  
  inflating: PennFudanPed/PNGImages/PennPed00012.png  
  inflating: PennFudanPed/PNGImages/PennPed00013.png  
  inflating: PennFudanPed/PNGImages/PennPed00014.png  
  inflating: PennFudanPed/PNGImages/PennPed00015.png  
  inflating: PennFudanPed/PNGImages/PennPed00016.png  
  inflating: PennFudanPed/PNGImages/PennPed00017.png  
  inflating: PennFudanPed/PNGImages/PennPed00018.png  
  inflating: PennFudanPed/PNGImages/PennPed00019.png  
  inflating: PennFudanPed/PNGImages/PennPed00020.png  
  inflating: PennFudanPed/PNGImages/PennPed00021.png  
  inflating: PennFudanPed/PNGImages/PennPed00022.png  
  inflating: PennFudanPed/PNGImages/PennPed00023.png  
  inflating: PennFudanPed/PNGImages/PennPed00024.png  
  inflating: PennFudanPed/PNGImages/PennPed00025.png  
  inflating: PennFudanPed/PNGImages/PennPed00026.png  
  inflating: PennFudanPed/PNGImages/PennPed00027.png  
  inflating: PennFudanPed/PNGImages/PennPed00028.png  
  inflating: PennFudanPed/PNGImages/PennPed00029.png  
  inflating: PennFudanPed/PNGImages/PennPed00030.png  
  inflating: PennFudanPed/PNGImages/PennPed00031.png  
  inflating: PennFudanPed/PNGImages/PennPed00032.png  
  inflating: PennFudanPed/PNGImages/PennPed00033.png  
  inflating: PennFudanPed/PNGImages/PennPed00034.png  
  inflating: PennFudanPed/PNGImages/PennPed00035.png  
  inflating: PennFudanPed/PNGImages/PennPed00036.png  
  inflating: PennFudanPed/PNGImages/PennPed00037.png  
  inflating: PennFudanPed/PNGImages/PennPed00038.png  
  inflating: PennFudanPed/PNGImages/PennPed00039.png  
  inflating: PennFudanPed/PNGImages/PennPed00040.png  
  inflating: PennFudanPed/PNGImages/PennPed00041.png  
  inflating: PennFudanPed/PNGImages/PennPed00042.png  
  inflating: PennFudanPed/PNGImages/PennPed00043.png  
  inflating: PennFudanPed/PNGImages/PennPed00044.png  
  inflating: PennFudanPed/PNGImages/PennPed00045.png  
  inflating: PennFudanPed/PNGImages/PennPed00046.png  
  inflating: PennFudanPed/PNGImages/PennPed00047.png  
  inflating: PennFudanPed/PNGImages/PennPed00048.png  
  inflating: PennFudanPed/PNGImages/PennPed00049.png  
  inflating: PennFudanPed/PNGImages/PennPed00050.png  
  inflating: PennFudanPed/PNGImages/PennPed00051.png  
  inflating: PennFudanPed/PNGImages/PennPed00052.png  
  inflating: PennFudanPed/PNGImages/PennPed00053.png  
  inflating: PennFudanPed/PNGImages/PennPed00054.png  
  inflating: PennFudanPed/PNGImages/PennPed00055.png  
  inflating: PennFudanPed/PNGImages/PennPed00056.png  
  inflating: PennFudanPed/PNGImages/PennPed00057.png  
  inflating: PennFudanPed/PNGImages/PennPed00058.png  
  inflating: PennFudanPed/PNGImages/PennPed00059.png  
  inflating: PennFudanPed/PNGImages/PennPed00060.png  
  inflating: PennFudanPed/PNGImages/PennPed00061.png  
  inflating: PennFudanPed/PNGImages/PennPed00062.png  
  inflating: PennFudanPed/PNGImages/PennPed00063.png  
  inflating: PennFudanPed/PNGImages/PennPed00064.png  
  inflating: PennFudanPed/PNGImages/PennPed00065.png  
  inflating: PennFudanPed/PNGImages/PennPed00066.png  
  inflating: PennFudanPed/PNGImages/PennPed00067.png  
  inflating: PennFudanPed/PNGImages/PennPed00068.png  
  inflating: PennFudanPed/PNGImages/PennPed00069.png  
  inflating: PennFudanPed/PNGImages/PennPed00070.png  
  inflating: PennFudanPed/PNGImages/PennPed00071.png  
  inflating: PennFudanPed/PNGImages/PennPed00072.png  
  inflating: PennFudanPed/PNGImages/PennPed00073.png  
  inflating: PennFudanPed/PNGImages/PennPed00074.png  
  inflating: PennFudanPed/PNGImages/PennPed00075.png  
  inflating: PennFudanPed/PNGImages/PennPed00076.png  
  inflating: PennFudanPed/PNGImages/PennPed00077.png  
  inflating: PennFudanPed/PNGImages/PennPed00078.png  
  inflating: PennFudanPed/PNGImages/PennPed00079.png  
  inflating: PennFudanPed/PNGImages/PennPed00080.png  
  inflating: PennFudanPed/PNGImages/PennPed00081.png  
  inflating: PennFudanPed/PNGImages/PennPed00082.png  
  inflating: PennFudanPed/PNGImages/PennPed00083.png  
  inflating: PennFudanPed/PNGImages/PennPed00084.png  
  inflating: PennFudanPed/PNGImages/PennPed00085.png  
  inflating: PennFudanPed/PNGImages/PennPed00086.png  
  inflating: PennFudanPed/PNGImages/PennPed00087.png  
  inflating: PennFudanPed/PNGImages/PennPed00088.png  
  inflating: PennFudanPed/PNGImages/PennPed00089.png  
  inflating: PennFudanPed/PNGImages/PennPed00090.png  
  inflating: PennFudanPed/PNGImages/PennPed00091.png  
  inflating: PennFudanPed/PNGImages/PennPed00092.png  
  inflating: PennFudanPed/PNGImages/PennPed00093.png  
  inflating: PennFudanPed/PNGImages/PennPed00094.png  
  inflating: PennFudanPed/PNGImages/PennPed00095.png  
  inflating: PennFudanPed/PNGImages/PennPed00096.png  
  inflating: PennFudanPed/readme.txt  
</code></pre><p>Letâ€™s have a look at the dataset and how it is layed down.</p>
<p>The data is structured as follows<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PennFudanPed&#x2F;</span><br><span class="line">  PedMasks&#x2F;</span><br><span class="line">    FudanPed00001_mask.png</span><br><span class="line">    FudanPed00002_mask.png</span><br><span class="line">    FudanPed00003_mask.png</span><br><span class="line">    FudanPed00004_mask.png</span><br><span class="line">    ...</span><br><span class="line">  PNGImages&#x2F;</span><br><span class="line">    FudanPed00001.png</span><br><span class="line">    FudanPed00002.png</span><br><span class="line">    FudanPed00003.png</span><br><span class="line">    FudanPed00004.png</span><br></pre></td></tr></table></figure></p>
<p>Here is one example of an image in the dataset, with its corresponding instance segmentation mask</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">Image.open(<span class="string">'PennFudanPed/PNGImages/FudanPed00001.png'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_6_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask = Image.open(<span class="string">'PennFudanPed/PedMasks/FudanPed00001_mask.png'</span>)</span><br><span class="line"><span class="comment"># each mask instance has a different color, from zero to N, where</span></span><br><span class="line"><span class="comment"># N is the number of instances. In order to make visualization easier,</span></span><br><span class="line"><span class="comment"># let's adda color palette to the mask.</span></span><br><span class="line">mask.putpalette([</span><br><span class="line">    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># black background</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># index 1 is red</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>, <span class="comment"># index 2 is yellow</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">153</span>, <span class="number">0</span>, <span class="comment"># index 3 is orange</span></span><br><span class="line">])</span><br><span class="line">mask</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_7_0.png" alt="png"></p>
<p>So each image has a corresponding segmentation mask, where each color correspond to a different instance. Letâ€™s write a <code>torch.utils.data.Dataset</code> class for this dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PennFudanDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms=None)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># load all image files, sorting them to</span></span><br><span class="line">        <span class="comment"># ensure that they are aligned</span></span><br><span class="line">        self.imgs = list(sorted(os.listdir(os.path.join(root, <span class="string">"PNGImages"</span>))))</span><br><span class="line">        self.masks = list(sorted(os.listdir(os.path.join(root, <span class="string">"PedMasks"</span>))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="comment"># load images ad masks</span></span><br><span class="line">        img_path = os.path.join(self.root, <span class="string">"PNGImages"</span>, self.imgs[idx])</span><br><span class="line">        mask_path = os.path.join(self.root, <span class="string">"PedMasks"</span>, self.masks[idx])</span><br><span class="line">        img = Image.open(img_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        <span class="comment"># note that we haven't converted the mask to RGB,</span></span><br><span class="line">        <span class="comment"># because each color corresponds to a different instance</span></span><br><span class="line">        <span class="comment"># with 0 being background</span></span><br><span class="line">        mask = Image.open(mask_path)</span><br><span class="line"></span><br><span class="line">        mask = np.array(mask)</span><br><span class="line">        <span class="comment"># instances are encoded as different colors</span></span><br><span class="line">        obj_ids = np.unique(mask)</span><br><span class="line">        <span class="comment"># first id is the background, so remove it</span></span><br><span class="line">        obj_ids = obj_ids[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split the color-encoded mask into a set</span></span><br><span class="line">        <span class="comment"># of binary masks</span></span><br><span class="line">        masks = mask == obj_ids[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get bounding box coordinates for each mask</span></span><br><span class="line">        num_objs = len(obj_ids)</span><br><span class="line">        boxes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_objs):</span><br><span class="line">            pos = np.where(masks[i])</span><br><span class="line">            xmin = np.min(pos[<span class="number">1</span>])</span><br><span class="line">            xmax = np.max(pos[<span class="number">1</span>])</span><br><span class="line">            ymin = np.min(pos[<span class="number">0</span>])</span><br><span class="line">            ymax = np.max(pos[<span class="number">0</span>])</span><br><span class="line">            boxes.append([xmin, ymin, xmax, ymax])</span><br><span class="line"></span><br><span class="line">        boxes = torch.as_tensor(boxes, dtype=torch.float32)</span><br><span class="line">        <span class="comment"># there is only one class</span></span><br><span class="line">        labels = torch.ones((num_objs,), dtype=torch.int64)</span><br><span class="line">        masks = torch.as_tensor(masks, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        image_id = torch.tensor([idx])</span><br><span class="line">        area = (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]) * (boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>])</span><br><span class="line">        <span class="comment"># suppose all instances are not crowd</span></span><br><span class="line">        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">        target = &#123;&#125;</span><br><span class="line">        target[<span class="string">"boxes"</span>] = boxes</span><br><span class="line">        target[<span class="string">"labels"</span>] = labels</span><br><span class="line">        target[<span class="string">"masks"</span>] = masks</span><br><span class="line">        target[<span class="string">"image_id"</span>] = image_id</span><br><span class="line">        target[<span class="string">"area"</span>] = area</span><br><span class="line">        target[<span class="string">"iscrowd"</span>] = iscrowd</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img, target = self.transforms(img, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure>
<p>Thatâ€™s all for the dataset. Letâ€™s see how the outputs are structured for this dataset</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed/'</span>)</span><br><span class="line">dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(&lt;PIL.Image.Image image mode=RGB size=559x536 at 0x7FEBFE8767F0&gt;,
 {&#39;area&#39;: tensor([35358., 36225.]), &#39;boxes&#39;: tensor([[159., 181., 301., 430.],
          [419., 170., 534., 485.]]), &#39;image_id&#39;: tensor([0]), &#39;iscrowd&#39;: tensor([0, 0]), &#39;labels&#39;: tensor([1, 1]), &#39;masks&#39;: tensor([[[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]],

          [[0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           ...,
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0],
           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})
</code></pre><p>So we can see that by default, the dataset returns a <code>PIL.Image</code> and a dictionary<br>containing several fields, including <code>boxes</code>, <code>labels</code> and <code>masks</code>.</p>
<h2 id="Defining-your-model"><a href="#Defining-your-model" class="headerlink" title="Defining your model"></a>Defining your model</h2><p>In this tutorial, we will be using <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>, which is based on top of <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image03.png" alt="Faster R-CNN"></p>
<p>Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.</p>
<p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image04.png" alt="Mask R-CNN"></p>
<p>There are two common situations where one might want to modify one of the available models in torchvision modelzoo.<br>The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).</p>
<p>Letâ€™s go see how we would do one or another in the following sections.</p>
<h3 id="1-Finetuning-from-a-pretrained-model"><a href="#1-Finetuning-from-a-pretrained-model" class="headerlink" title="1 - Finetuning from a pretrained model"></a>1 - Finetuning from a pretrained model</h3><p>Letâ€™s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection.faster_rcnn import FastRCNNPredictor</span><br><span class="line"></span><br><span class="line"># load a model pre-trained pre-trained on COCO</span><br><span class="line">model &#x3D; torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained&#x3D;True)</span><br><span class="line"></span><br><span class="line"># replace the classifier with a new one, that has</span><br><span class="line"># num_classes which is user-defined</span><br><span class="line">num_classes &#x3D; 2  # 1 class (person) + background</span><br><span class="line"># get number of input features for the classifier</span><br><span class="line">in_features &#x3D; model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line"># replace the pre-trained head with a new one</span><br><span class="line">model.roi_heads.box_predictor &#x3D; FastRCNNPredictor(in_features, num_classes)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-Modifying-the-model-to-add-a-different-backbone"><a href="#2-Modifying-the-model-to-add-a-different-backbone" class="headerlink" title="2 - Modifying the model to add a different backbone"></a>2 - Modifying the model to add a different backbone</h3><p>Another common situation arises when the user wants to replace the backbone of a detection<br>model with a different one. For example, the current default backbone (ResNet-50) might be too big for some applications, and smaller models might be necessary.</p>
<p>Here is how we would go into leveraging the functions provided by torchvision to modify a backbone.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection import FasterRCNN</span><br><span class="line">from torchvision.models.detection.rpn import AnchorGenerator</span><br><span class="line"></span><br><span class="line"># load a pre-trained model for classification and return</span><br><span class="line"># only the features</span><br><span class="line">backbone &#x3D; torchvision.models.mobilenet_v2(pretrained&#x3D;True).features</span><br><span class="line"># FasterRCNN needs to know the number of</span><br><span class="line"># output channels in a backbone. For mobilenet_v2, it&#39;s 1280</span><br><span class="line"># so we need to add it here</span><br><span class="line">backbone.out_channels &#x3D; 1280</span><br><span class="line"></span><br><span class="line"># let&#39;s make the RPN generate 5 x 3 anchors per spatial</span><br><span class="line"># location, with 5 different sizes and 3 different aspect</span><br><span class="line"># ratios. We have a Tuple[Tuple[int]] because each feature</span><br><span class="line"># map could potentially have different sizes and</span><br><span class="line"># aspect ratios </span><br><span class="line">anchor_generator &#x3D; AnchorGenerator(sizes&#x3D;((32, 64, 128, 256, 512),),</span><br><span class="line">                                   aspect_ratios&#x3D;((0.5, 1.0, 2.0),))</span><br><span class="line"></span><br><span class="line"># let&#39;s define what are the feature maps that we will</span><br><span class="line"># use to perform the region of interest cropping, as well as</span><br><span class="line"># the size of the crop after rescaling.</span><br><span class="line"># if your backbone returns a Tensor, featmap_names is expected to</span><br><span class="line"># be [0]. More generally, the backbone should return an</span><br><span class="line"># OrderedDict[Tensor], and in featmap_names you can choose which</span><br><span class="line"># feature maps to use.</span><br><span class="line">roi_pooler &#x3D; torchvision.ops.MultiScaleRoIAlign(featmap_names&#x3D;[0],</span><br><span class="line">                                                output_size&#x3D;7,</span><br><span class="line">                                                sampling_ratio&#x3D;2)</span><br><span class="line"></span><br><span class="line"># put the pieces together inside a FasterRCNN model</span><br><span class="line">model &#x3D; FasterRCNN(backbone,</span><br><span class="line">                   num_classes&#x3D;2,</span><br><span class="line">                   rpn_anchor_generator&#x3D;anchor_generator,</span><br><span class="line">                   box_roi_pool&#x3D;roi_pooler)</span><br></pre></td></tr></table></figure>
<h3 id="An-Instance-segmentation-model-for-PennFudan-Dataset"><a href="#An-Instance-segmentation-model-for-PennFudan-Dataset" class="headerlink" title="An Instance segmentation model for PennFudan Dataset"></a>An Instance segmentation model for PennFudan Dataset</h3><p>In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.</p>
<p>Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> FastRCNNPredictor</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.mask_rcnn <span class="keyword">import</span> MaskRCNNPredictor</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_instance_segmentation_model</span><span class="params">(num_classes)</span>:</span></span><br><span class="line">    <span class="comment"># load an instance segmentation model pre-trained on COCO</span></span><br><span class="line">    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the number of input features for the classifier</span></span><br><span class="line">    in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    <span class="comment"># replace the pre-trained head with a new one</span></span><br><span class="line">    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now get the number of input features for the mask classifier</span></span><br><span class="line">    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels</span><br><span class="line">    hidden_layer = <span class="number">256</span></span><br><span class="line">    <span class="comment"># and replace the mask predictor with a new one</span></span><br><span class="line">    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,</span><br><span class="line">                                                       hidden_layer,</span><br><span class="line">                                                       num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>Thatâ€™s it, this will make model be ready to be trained and evaluated on our custom dataset.</p>
<h2 id="Training-and-evaluation-functions"><a href="#Training-and-evaluation-functions" class="headerlink" title="Training and evaluation functions"></a>Training and evaluation functions</h2><p>In <code>references/detection/,</code> we have a number of helper functions to simplify training and evaluating detection models.<br>Here, we will use <code>references/detection/engine.py</code>, <code>references/detection/utils.py</code> and <code>references/detection/transforms.py</code>.</p>
<p>Letâ€™s copy those files (and their dependencies) in here so that they are available in the notebook</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download TorchVision repo to use some files from</span></span><br><span class="line"><span class="comment"># references/detection</span></span><br><span class="line">git clone https://github.com/pytorch/vision.git</span><br><span class="line">cd vision</span><br><span class="line">git checkout v0<span class="number">.3</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">cp references/detection/utils.py ../</span><br><span class="line">cp references/detection/transforms.py ../</span><br><span class="line">cp references/detection/coco_eval.py ../</span><br><span class="line">cp references/detection/engine.py ../</span><br><span class="line">cp references/detection/coco_utils.py ../</span><br></pre></td></tr></table></figure>
<pre><code>Cloning into &#39;vision&#39;...
remote: Enumerating objects: 20, done.[K
remote: Counting objects: 100% (20/20), done.[K
remote: Compressing objects: 100% (20/20), done.[K
remote: Total 9278 (delta 7), reused 3 (delta 0), pack-reused 9258[K
Receiving objects: 100% (9278/9278), 11.24 MiB | 9.51 MiB/s, done.
Resolving deltas: 100% (6426/6426), done.
Note: checking out &#39;v0.3.0&#39;.

You are in &#39;detached HEAD&#39; state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b &lt;new-branch-name&gt;

HEAD is now at be37608 version check against PyTorch&#39;s CUDA version
</code></pre><p>Letâ€™s write some helper functions for data augmentation / transformation, which leverages the functions in <code>refereces/detection</code> that we have just copied:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> engine <span class="keyword">import</span> train_one_epoch, evaluate</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(train)</span>:</span></span><br><span class="line">    transforms = []</span><br><span class="line">    <span class="comment"># converts the image, a PIL image, into a PyTorch Tensor</span></span><br><span class="line">    transforms.append(T.ToTensor())</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        <span class="comment"># during training, randomly flip the training images</span></span><br><span class="line">        <span class="comment"># and ground-truth for data augmentation</span></span><br><span class="line">        transforms.append(T.RandomHorizontalFlip(<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> T.Compose(transforms)</span><br></pre></td></tr></table></figure>
<h4 id="Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model"><a href="#Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model" class="headerlink" title="Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."></a>Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.</h4><h3 id="Putting-everything-together"><a href="#Putting-everything-together" class="headerlink" title="Putting everything together"></a>Putting everything together</h3><p>We now have the dataset class, the models and the data transforms. Letâ€™s instantiate them</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use our dataset and defined transformations</span></span><br><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">True</span>))</span><br><span class="line">dataset_test = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split the dataset in train and test set</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">indices = torch.randperm(len(dataset)).tolist()</span><br><span class="line">dataset = torch.utils.data.Subset(dataset, indices[:<span class="number">-50</span>])</span><br><span class="line">dataset_test = torch.utils.data.Subset(dataset_test, indices[<span class="number">-50</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># define training and validation data loaders</span></span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br><span class="line"></span><br><span class="line">data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_test, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br></pre></td></tr></table></figure>
<p>Now letâ€™s instantiate the model and the optimizer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># our dataset has two classes only - background and person</span></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get the model using our helper function</span></span><br><span class="line">model = get_instance_segmentation_model(num_classes)</span><br><span class="line"><span class="comment"># move model to the right device</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct an optimizer</span></span><br><span class="line">params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=<span class="number">0.005</span>,</span><br><span class="line">                            momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and a learning rate scheduler which decreases the learning rate by</span></span><br><span class="line"><span class="comment"># 10x every 3 epochs</span></span><br><span class="line">lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,</span><br><span class="line">                                               step_size=<span class="number">3</span>,</span><br><span class="line">                                               gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading: &quot;https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth&quot; to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth



HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value=&#39;&#39;)))
</code></pre><p>And now letâ€™s train the model for 10 epochs, evaluating at the end of every epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># let's train it for 10 epochs</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># train for one epoch, printing every 10 iterations</span></span><br><span class="line">    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># update the learning rate</span></span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    <span class="comment"># evaluate on the test dataset</span></span><br><span class="line">    evaluate(model, data_loader_test, device=device)</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:
    nonzero(Tensor input, *, Tensor out)
Consider using one of the following signatures instead:
    nonzero(Tensor input, *, bool as_tuple)


Epoch: [0]  [ 0/60]  eta: 0:02:18  lr: 0.000090  loss: 3.5827 (3.5827)  loss_classifier: 0.7385 (0.7385)  loss_box_reg: 0.1523 (0.1523)  loss_mask: 2.6620 (2.6620)  loss_objectness: 0.0224 (0.0224)  loss_rpn_box_reg: 0.0076 (0.0076)  time: 2.3152  data: 0.2933  max mem: 2303
Epoch: [0]  [10/60]  eta: 0:01:14  lr: 0.000936  loss: 1.5605 (2.1212)  loss_classifier: 0.4479 (0.4976)  loss_box_reg: 0.1826 (0.1906)  loss_mask: 0.9259 (1.4017)  loss_objectness: 0.0224 (0.0208)  loss_rpn_box_reg: 0.0090 (0.0105)  time: 1.4865  data: 0.0356  max mem: 2860
Epoch: [0]  [20/60]  eta: 0:00:57  lr: 0.001783  loss: 0.8700 (1.4312)  loss_classifier: 0.2338 (0.3409)  loss_box_reg: 0.1579 (0.1731)  loss_mask: 0.4010 (0.8836)  loss_objectness: 0.0191 (0.0216)  loss_rpn_box_reg: 0.0099 (0.0120)  time: 1.3888  data: 0.0096  max mem: 2861
Epoch: [0]  [30/60]  eta: 0:00:43  lr: 0.002629  loss: 0.5382 (1.1211)  loss_classifier: 0.0968 (0.2569)  loss_box_reg: 0.1155 (0.1598)  loss_mask: 0.2489 (0.6751)  loss_objectness: 0.0105 (0.0176)  loss_rpn_box_reg: 0.0099 (0.0117)  time: 1.4144  data: 0.0095  max mem: 3596
Epoch: [0]  [40/60]  eta: 0:00:28  lr: 0.003476  loss: 0.4041 (0.9495)  loss_classifier: 0.0690 (0.2099)  loss_box_reg: 0.1090 (0.1521)  loss_mask: 0.2121 (0.5609)  loss_objectness: 0.0038 (0.0142)  loss_rpn_box_reg: 0.0118 (0.0124)  time: 1.4593  data: 0.0098  max mem: 3596
Epoch: [0]  [50/60]  eta: 0:00:14  lr: 0.004323  loss: 0.3387 (0.8263)  loss_classifier: 0.0496 (0.1785)  loss_box_reg: 0.0833 (0.1393)  loss_mask: 0.1797 (0.4837)  loss_objectness: 0.0035 (0.0122)  loss_rpn_box_reg: 0.0118 (0.0128)  time: 1.4368  data: 0.0101  max mem: 3596
Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.2567 (0.7367)  loss_classifier: 0.0392 (0.1566)  loss_box_reg: 0.0545 (0.1240)  loss_mask: 0.1464 (0.4332)  loss_objectness: 0.0020 (0.0106)  loss_rpn_box_reg: 0.0109 (0.0122)  time: 1.4374  data: 0.0101  max mem: 3596
Epoch: [0] Total time: 0:01:26 (1.4425 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3444 (0.3444)  evaluator_time: 0.0059 (0.0059)  time: 0.4881  data: 0.1360  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)  time: 0.3262  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3309 s / it)
Averaged stats: model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.698
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.901
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.759
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.700
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.886
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.716
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.741
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.745
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.751
Epoch: [1]  [ 0/60]  eta: 0:01:39  lr: 0.005000  loss: 0.1716 (0.1716)  loss_classifier: 0.0231 (0.0231)  loss_box_reg: 0.0309 (0.0309)  loss_mask: 0.1041 (0.1041)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0124 (0.0124)  time: 1.6632  data: 0.3040  max mem: 3596
Epoch: [1]  [10/60]  eta: 0:01:14  lr: 0.005000  loss: 0.2137 (0.2460)  loss_classifier: 0.0314 (0.0385)  loss_box_reg: 0.0309 (0.0406)  loss_mask: 0.1438 (0.1540)  loss_objectness: 0.0011 (0.0017)  loss_rpn_box_reg: 0.0113 (0.0111)  time: 1.4996  data: 0.0356  max mem: 3596
Epoch: [1]  [20/60]  eta: 0:00:59  lr: 0.005000  loss: 0.2565 (0.2636)  loss_classifier: 0.0484 (0.0464)  loss_box_reg: 0.0338 (0.0442)  loss_mask: 0.1639 (0.1582)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0123 (0.0131)  time: 1.4682  data: 0.0102  max mem: 3596
Epoch: [1]  [30/60]  eta: 0:00:43  lr: 0.005000  loss: 0.2174 (0.2409)  loss_classifier: 0.0349 (0.0410)  loss_box_reg: 0.0266 (0.0365)  loss_mask: 0.1426 (0.1502)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0080 (0.0115)  time: 1.4462  data: 0.0105  max mem: 3596
Epoch: [1]  [40/60]  eta: 0:00:29  lr: 0.005000  loss: 0.1930 (0.2327)  loss_classifier: 0.0274 (0.0406)  loss_box_reg: 0.0189 (0.0334)  loss_mask: 0.1380 (0.1463)  loss_objectness: 0.0007 (0.0015)  loss_rpn_box_reg: 0.0075 (0.0109)  time: 1.4625  data: 0.0096  max mem: 3596
Epoch: [1]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.2011 (0.2291)  loss_classifier: 0.0344 (0.0409)  loss_box_reg: 0.0253 (0.0325)  loss_mask: 0.1287 (0.1427)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0079 (0.0115)  time: 1.5020  data: 0.0099  max mem: 3596
Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1680 (0.2220)  loss_classifier: 0.0294 (0.0398)  loss_box_reg: 0.0148 (0.0302)  loss_mask: 0.1265 (0.1394)  loss_objectness: 0.0011 (0.0016)  loss_rpn_box_reg: 0.0075 (0.0110)  time: 1.4470  data: 0.0098  max mem: 3596
Epoch: [1] Total time: 0:01:27 (1.4662 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3606 (0.3606)  evaluator_time: 0.0046 (0.0046)  time: 0.5041  data: 0.1374  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)  time: 0.3234  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3253 s / it)
Averaged stats: model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.772
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.747
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.891
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.789
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794
Epoch: [2]  [ 0/60]  eta: 0:01:38  lr: 0.005000  loss: 0.1239 (0.1239)  loss_classifier: 0.0123 (0.0123)  loss_box_reg: 0.0068 (0.0068)  loss_mask: 0.0971 (0.0971)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.6336  data: 0.2465  max mem: 3596
Epoch: [2]  [10/60]  eta: 0:01:12  lr: 0.005000  loss: 0.1866 (0.1751)  loss_classifier: 0.0283 (0.0308)  loss_box_reg: 0.0135 (0.0162)  loss_mask: 0.1129 (0.1189)  loss_objectness: 0.0007 (0.0011)  loss_rpn_box_reg: 0.0072 (0.0082)  time: 1.4478  data: 0.0310  max mem: 3596
Epoch: [2]  [20/60]  eta: 0:00:55  lr: 0.005000  loss: 0.1433 (0.1623)  loss_classifier: 0.0203 (0.0253)  loss_box_reg: 0.0094 (0.0129)  loss_mask: 0.1074 (0.1162)  loss_objectness: 0.0003 (0.0008)  loss_rpn_box_reg: 0.0046 (0.0071)  time: 1.3800  data: 0.0095  max mem: 3596
Epoch: [2]  [30/60]  eta: 0:00:42  lr: 0.005000  loss: 0.1621 (0.1821)  loss_classifier: 0.0218 (0.0294)  loss_box_reg: 0.0101 (0.0170)  loss_mask: 0.1160 (0.1257)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0077 (0.0088)  time: 1.4109  data: 0.0095  max mem: 3596
Epoch: [2]  [40/60]  eta: 0:00:28  lr: 0.005000  loss: 0.1841 (0.1834)  loss_classifier: 0.0286 (0.0291)  loss_box_reg: 0.0157 (0.0164)  loss_mask: 0.1288 (0.1278)  loss_objectness: 0.0005 (0.0012)  loss_rpn_box_reg: 0.0081 (0.0088)  time: 1.4780  data: 0.0099  max mem: 3596
Epoch: [2]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.1970 (0.1878)  loss_classifier: 0.0279 (0.0295)  loss_box_reg: 0.0173 (0.0175)  loss_mask: 0.1317 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0083 (0.0092)  time: 1.4749  data: 0.0099  max mem: 3596
Epoch: [2]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1872 (0.1894)  loss_classifier: 0.0279 (0.0307)  loss_box_reg: 0.0173 (0.0177)  loss_mask: 0.1296 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0094 (0.0095)  time: 1.5513  data: 0.0099  max mem: 3596
Epoch: [2] Total time: 0:01:28 (1.4738 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:21  model_time: 0.3020 (0.3020)  evaluator_time: 0.0047 (0.0047)  time: 0.4358  data: 0.1272  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)  time: 0.3183  data: 0.0053  max mem: 3596
Test: Total time: 0:00:16 (0.3203 s / it)
Averaged stats: model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.921
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798
Epoch: [3]  [ 0/60]  eta: 0:01:55  lr: 0.000500  loss: 0.1690 (0.1690)  loss_classifier: 0.0193 (0.0193)  loss_box_reg: 0.0098 (0.0098)  loss_mask: 0.1339 (0.1339)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0058 (0.0058)  time: 1.9331  data: 0.4201  max mem: 3596
Epoch: [3]  [10/60]  eta: 0:01:19  lr: 0.000500  loss: 0.1668 (0.1790)  loss_classifier: 0.0294 (0.0273)  loss_box_reg: 0.0102 (0.0148)  loss_mask: 0.1203 (0.1283)  loss_objectness: 0.0005 (0.0010)  loss_rpn_box_reg: 0.0058 (0.0076)  time: 1.5992  data: 0.0464  max mem: 3596
Epoch: [3]  [20/60]  eta: 0:01:01  lr: 0.000500  loss: 0.1635 (0.1723)  loss_classifier: 0.0225 (0.0257)  loss_box_reg: 0.0088 (0.0133)  loss_mask: 0.1203 (0.1243)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0061 (0.0079)  time: 1.5232  data: 0.0096  max mem: 3596
Epoch: [3]  [30/60]  eta: 0:00:44  lr: 0.000500  loss: 0.1603 (0.1683)  loss_classifier: 0.0212 (0.0251)  loss_box_reg: 0.0083 (0.0121)  loss_mask: 0.1198 (0.1228)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0060 (0.0073)  time: 1.4131  data: 0.0097  max mem: 3596
Epoch: [3]  [40/60]  eta: 0:00:29  lr: 0.000500  loss: 0.1603 (0.1725)  loss_classifier: 0.0266 (0.0268)  loss_box_reg: 0.0093 (0.0127)  loss_mask: 0.1150 (0.1239)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0069 (0.0082)  time: 1.4049  data: 0.0097  max mem: 3596
Epoch: [3]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1715 (0.1754)  loss_classifier: 0.0266 (0.0267)  loss_box_reg: 0.0109 (0.0134)  loss_mask: 0.1232 (0.1261)  loss_objectness: 0.0005 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0083)  time: 1.4872  data: 0.0099  max mem: 3596
Epoch: [3]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1509 (0.1709)  loss_classifier: 0.0256 (0.0263)  loss_box_reg: 0.0093 (0.0126)  loss_mask: 0.1055 (0.1231)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0081)  time: 1.4687  data: 0.0096  max mem: 3596
Epoch: [3] Total time: 0:01:28 (1.4791 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3690 (0.3690)  evaluator_time: 0.0046 (0.0046)  time: 0.5078  data: 0.1324  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)  time: 0.3199  data: 0.0051  max mem: 3596
Test: Total time: 0:00:16 (0.3224 s / it)
Averaged stats: model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.509
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.377
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.869
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.755
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.917
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.805
Epoch: [4]  [ 0/60]  eta: 0:01:27  lr: 0.000500  loss: 0.1045 (0.1045)  loss_classifier: 0.0070 (0.0070)  loss_box_reg: 0.0029 (0.0029)  loss_mask: 0.0902 (0.0902)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0043 (0.0043)  time: 1.4538  data: 0.2039  max mem: 3596
Epoch: [4]  [10/60]  eta: 0:01:12  lr: 0.000500  loss: 0.1510 (0.1583)  loss_classifier: 0.0209 (0.0197)  loss_box_reg: 0.0101 (0.0126)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0007)  loss_rpn_box_reg: 0.0071 (0.0075)  time: 1.4482  data: 0.0278  max mem: 3596
Epoch: [4]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1510 (0.1582)  loss_classifier: 0.0209 (0.0215)  loss_box_reg: 0.0073 (0.0110)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0060 (0.0066)  time: 1.3827  data: 0.0100  max mem: 3596
Epoch: [4]  [30/60]  eta: 0:00:41  lr: 0.000500  loss: 0.1427 (0.1624)  loss_classifier: 0.0227 (0.0243)  loss_box_reg: 0.0077 (0.0117)  loss_mask: 0.1005 (0.1176)  loss_objectness: 0.0005 (0.0014)  loss_rpn_box_reg: 0.0058 (0.0074)  time: 1.3627  data: 0.0097  max mem: 3596
Epoch: [4]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1472 (0.1611)  loss_classifier: 0.0255 (0.0253)  loss_box_reg: 0.0085 (0.0114)  loss_mask: 0.1079 (0.1161)  loss_objectness: 0.0004 (0.0013)  loss_rpn_box_reg: 0.0063 (0.0071)  time: 1.4537  data: 0.0094  max mem: 3596
Epoch: [4]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1548 (0.1612)  loss_classifier: 0.0250 (0.0249)  loss_box_reg: 0.0079 (0.0113)  loss_mask: 0.1106 (0.1161)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0068 (0.0077)  time: 1.4913  data: 0.0094  max mem: 3596
Epoch: [4]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1548 (0.1647)  loss_classifier: 0.0250 (0.0256)  loss_box_reg: 0.0079 (0.0121)  loss_mask: 0.1106 (0.1179)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0079 (0.0080)  time: 1.4724  data: 0.0097  max mem: 3596
Epoch: [4] Total time: 0:01:26 (1.4357 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3692 (0.3692)  evaluator_time: 0.0045 (0.0045)  time: 0.5054  data: 0.1301  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)  time: 0.3202  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3220 s / it)
Averaged stats: model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.813
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.944
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.520
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.825
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.868
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.920
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815
Epoch: [5]  [ 0/60]  eta: 0:02:13  lr: 0.000500  loss: 0.1545 (0.1545)  loss_classifier: 0.0223 (0.0223)  loss_box_reg: 0.0059 (0.0059)  loss_mask: 0.1200 (0.1200)  loss_objectness: 0.0004 (0.0004)  loss_rpn_box_reg: 0.0059 (0.0059)  time: 2.2323  data: 0.5519  max mem: 3596
Epoch: [5]  [10/60]  eta: 0:01:10  lr: 0.000500  loss: 0.1409 (0.1489)  loss_classifier: 0.0178 (0.0211)  loss_box_reg: 0.0076 (0.0094)  loss_mask: 0.1140 (0.1118)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0057 (0.0061)  time: 1.4098  data: 0.0540  max mem: 3596
Epoch: [5]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1379 (0.1454)  loss_classifier: 0.0189 (0.0208)  loss_box_reg: 0.0076 (0.0088)  loss_mask: 0.1032 (0.1091)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0054 (0.0061)  time: 1.3437  data: 0.0070  max mem: 3596
Epoch: [5]  [30/60]  eta: 0:00:42  lr: 0.000500  loss: 0.1430 (0.1597)  loss_classifier: 0.0240 (0.0246)  loss_box_reg: 0.0092 (0.0114)  loss_mask: 0.1032 (0.1154)  loss_objectness: 0.0005 (0.0008)  loss_rpn_box_reg: 0.0069 (0.0075)  time: 1.4147  data: 0.0100  max mem: 3596
Epoch: [5]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1503 (0.1609)  loss_classifier: 0.0242 (0.0243)  loss_box_reg: 0.0102 (0.0117)  loss_mask: 0.1148 (0.1163)  loss_objectness: 0.0004 (0.0008)  loss_rpn_box_reg: 0.0083 (0.0078)  time: 1.4826  data: 0.0101  max mem: 3596
Epoch: [5]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1397 (0.1571)  loss_classifier: 0.0198 (0.0237)  loss_box_reg: 0.0075 (0.0107)  loss_mask: 0.1017 (0.1144)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0066 (0.0075)  time: 1.4890  data: 0.0096  max mem: 3596
Epoch: [5]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1422 (0.1581)  loss_classifier: 0.0197 (0.0241)  loss_box_reg: 0.0066 (0.0107)  loss_mask: 0.1042 (0.1149)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0064 (0.0076)  time: 1.5030  data: 0.0094  max mem: 3596
Epoch: [5] Total time: 0:01:27 (1.4584 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3761 (0.3761)  evaluator_time: 0.0041 (0.0041)  time: 0.5475  data: 0.1655  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)  time: 0.3195  data: 0.0049  max mem: 3596
Test: Total time: 0:00:16 (0.3235 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [6]  [ 0/60]  eta: 0:01:53  lr: 0.000050  loss: 0.1645 (0.1645)  loss_classifier: 0.0255 (0.0255)  loss_box_reg: 0.0121 (0.0121)  loss_mask: 0.1195 (0.1195)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.8885  data: 0.2840  max mem: 3596
Epoch: [6]  [10/60]  eta: 0:01:17  lr: 0.000050  loss: 0.1414 (0.1454)  loss_classifier: 0.0206 (0.0221)  loss_box_reg: 0.0056 (0.0076)  loss_mask: 0.1039 (0.1099)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0058 (0.0053)  time: 1.5460  data: 0.0336  max mem: 3596
Epoch: [6]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1414 (0.1516)  loss_classifier: 0.0206 (0.0241)  loss_box_reg: 0.0065 (0.0101)  loss_mask: 0.1030 (0.1104)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0059 (0.0066)  time: 1.5057  data: 0.0092  max mem: 3596
Epoch: [6]  [30/60]  eta: 0:00:45  lr: 0.000050  loss: 0.1479 (0.1531)  loss_classifier: 0.0255 (0.0261)  loss_box_reg: 0.0087 (0.0099)  loss_mask: 0.1030 (0.1098)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0072 (0.0068)  time: 1.4797  data: 0.0104  max mem: 3596
Epoch: [6]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1493 (0.1593)  loss_classifier: 0.0255 (0.0267)  loss_box_reg: 0.0087 (0.0111)  loss_mask: 0.1043 (0.1137)  loss_objectness: 0.0005 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0072)  time: 1.4498  data: 0.0104  max mem: 3596
Epoch: [6]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1440 (0.1584)  loss_classifier: 0.0234 (0.0261)  loss_box_reg: 0.0088 (0.0110)  loss_mask: 0.1129 (0.1135)  loss_objectness: 0.0004 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0073)  time: 1.4308  data: 0.0097  max mem: 3596
Epoch: [6]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1440 (0.1588)  loss_classifier: 0.0216 (0.0260)  loss_box_reg: 0.0080 (0.0110)  loss_mask: 0.1118 (0.1140)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0070 (0.0073)  time: 1.4508  data: 0.0095  max mem: 3596
Epoch: [6] Total time: 0:01:28 (1.4739 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3615 (0.3615)  evaluator_time: 0.0038 (0.0038)  time: 0.5174  data: 0.1505  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)  time: 0.3199  data: 0.0057  max mem: 3596
Test: Total time: 0:00:16 (0.3225 s / it)
Averaged stats: model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.351
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [7]  [ 0/60]  eta: 0:01:47  lr: 0.000050  loss: 0.1122 (0.1122)  loss_classifier: 0.0151 (0.0151)  loss_box_reg: 0.0039 (0.0039)  loss_mask: 0.0920 (0.0920)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.7859  data: 0.4771  max mem: 3596
Epoch: [7]  [10/60]  eta: 0:01:11  lr: 0.000050  loss: 0.1316 (0.1467)  loss_classifier: 0.0157 (0.0221)  loss_box_reg: 0.0052 (0.0086)  loss_mask: 0.1004 (0.1097)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0037 (0.0053)  time: 1.4340  data: 0.0504  max mem: 3596
Epoch: [7]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1570 (0.1557)  loss_classifier: 0.0288 (0.0274)  loss_box_reg: 0.0085 (0.0100)  loss_mask: 0.1075 (0.1104)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0066 (0.0068)  time: 1.4943  data: 0.0092  max mem: 3596
Epoch: [7]  [30/60]  eta: 0:00:44  lr: 0.000050  loss: 0.1447 (0.1519)  loss_classifier: 0.0257 (0.0255)  loss_box_reg: 0.0076 (0.0093)  loss_mask: 0.1062 (0.1092)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0071 (0.0068)  time: 1.4985  data: 0.0104  max mem: 3596
Epoch: [7]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1418 (0.1546)  loss_classifier: 0.0222 (0.0251)  loss_box_reg: 0.0068 (0.0098)  loss_mask: 0.1095 (0.1120)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0054 (0.0069)  time: 1.4120  data: 0.0099  max mem: 3596
Epoch: [7]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1616 (0.1590)  loss_classifier: 0.0232 (0.0253)  loss_box_reg: 0.0082 (0.0107)  loss_mask: 0.1132 (0.1150)  loss_objectness: 0.0002 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0072)  time: 1.4197  data: 0.0096  max mem: 3596
Epoch: [7]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1474 (0.1592)  loss_classifier: 0.0230 (0.0256)  loss_box_reg: 0.0060 (0.0106)  loss_mask: 0.1101 (0.1150)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0057 (0.0072)  time: 1.4280  data: 0.0095  max mem: 3596
Epoch: [7] Total time: 0:01:27 (1.4505 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3761 (0.3761)  evaluator_time: 0.0044 (0.0044)  time: 0.5139  data: 0.1315  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)  time: 0.3181  data: 0.0050  max mem: 3596
Test: Total time: 0:00:16 (0.3223 s / it)
Averaged stats: model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.946
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.923
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.382
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.352
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [8]  [ 0/60]  eta: 0:01:59  lr: 0.000050  loss: 0.1533 (0.1533)  loss_classifier: 0.0187 (0.0187)  loss_box_reg: 0.0076 (0.0076)  loss_mask: 0.1242 (0.1242)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0027 (0.0027)  time: 1.9847  data: 0.7161  max mem: 3596
Epoch: [8]  [10/60]  eta: 0:01:14  lr: 0.000050  loss: 0.1533 (0.1537)  loss_classifier: 0.0227 (0.0245)  loss_box_reg: 0.0076 (0.0104)  loss_mask: 0.1094 (0.1121)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0047 (0.0061)  time: 1.4974  data: 0.0700  max mem: 3596
Epoch: [8]  [20/60]  eta: 0:00:58  lr: 0.000050  loss: 0.1512 (0.1592)  loss_classifier: 0.0217 (0.0230)  loss_box_reg: 0.0084 (0.0112)  loss_mask: 0.1071 (0.1168)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0070 (0.0072)  time: 1.4405  data: 0.0081  max mem: 3596
Epoch: [8]  [30/60]  eta: 0:00:43  lr: 0.000050  loss: 0.1390 (0.1557)  loss_classifier: 0.0217 (0.0237)  loss_box_reg: 0.0084 (0.0111)  loss_mask: 0.1021 (0.1130)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0071)  time: 1.4221  data: 0.0102  max mem: 3596
Epoch: [8]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1438 (0.1632)  loss_classifier: 0.0257 (0.0257)  loss_box_reg: 0.0086 (0.0117)  loss_mask: 0.1112 (0.1172)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0076 (0.0075)  time: 1.4616  data: 0.0096  max mem: 3596
Epoch: [8]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1613 (0.1628)  loss_classifier: 0.0265 (0.0265)  loss_box_reg: 0.0104 (0.0116)  loss_mask: 0.1130 (0.1157)  loss_objectness: 0.0005 (0.0011)  loss_rpn_box_reg: 0.0076 (0.0080)  time: 1.5084  data: 0.0096  max mem: 3596
Epoch: [8]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1426 (0.1593)  loss_classifier: 0.0209 (0.0254)  loss_box_reg: 0.0060 (0.0107)  loss_mask: 0.1046 (0.1148)  loss_objectness: 0.0003 (0.0011)  loss_rpn_box_reg: 0.0056 (0.0073)  time: 1.4303  data: 0.0097  max mem: 3596
Epoch: [8] Total time: 0:01:27 (1.4531 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3871 (0.3871)  evaluator_time: 0.0041 (0.0041)  time: 0.5413  data: 0.1481  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)  time: 0.3191  data: 0.0052  max mem: 3596
Test: Total time: 0:00:16 (0.3230 s / it)
Averaged stats: model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
Epoch: [9]  [ 0/60]  eta: 0:01:35  lr: 0.000005  loss: 0.1384 (0.1384)  loss_classifier: 0.0122 (0.0122)  loss_box_reg: 0.0035 (0.0035)  loss_mask: 0.1193 (0.1193)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0031 (0.0031)  time: 1.5976  data: 0.1940  max mem: 3596
Epoch: [9]  [10/60]  eta: 0:01:12  lr: 0.000005  loss: 0.1391 (0.1678)  loss_classifier: 0.0229 (0.0239)  loss_box_reg: 0.0087 (0.0123)  loss_mask: 0.1188 (0.1247)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0055 (0.0064)  time: 1.4416  data: 0.0261  max mem: 3596
Epoch: [9]  [20/60]  eta: 0:00:57  lr: 0.000005  loss: 0.1595 (0.1658)  loss_classifier: 0.0253 (0.0262)  loss_box_reg: 0.0106 (0.0121)  loss_mask: 0.1154 (0.1203)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0056 (0.0066)  time: 1.4367  data: 0.0094  max mem: 3596
Epoch: [9]  [30/60]  eta: 0:00:43  lr: 0.000005  loss: 0.1595 (0.1680)  loss_classifier: 0.0256 (0.0275)  loss_box_reg: 0.0088 (0.0125)  loss_mask: 0.1150 (0.1199)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0060 (0.0074)  time: 1.4554  data: 0.0095  max mem: 3596
Epoch: [9]  [40/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1449 (0.1605)  loss_classifier: 0.0212 (0.0258)  loss_box_reg: 0.0070 (0.0111)  loss_mask: 0.1049 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0064 (0.0069)  time: 1.4446  data: 0.0094  max mem: 3596
Epoch: [9]  [50/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1504 (0.1591)  loss_classifier: 0.0195 (0.0256)  loss_box_reg: 0.0083 (0.0109)  loss_mask: 0.1037 (0.1149)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0071)  time: 1.4887  data: 0.0095  max mem: 3596
Epoch: [9]  [59/60]  eta: 0:00:01  lr: 0.000005  loss: 0.1527 (0.1602)  loss_classifier: 0.0224 (0.0256)  loss_box_reg: 0.0083 (0.0108)  loss_mask: 0.1102 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0072)  time: 1.4984  data: 0.0097  max mem: 3596
Epoch: [9] Total time: 0:01:27 (1.4592 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3668 (0.3668)  evaluator_time: 0.0042 (0.0042)  time: 0.5024  data: 0.1296  max mem: 3596
Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)  time: 0.3215  data: 0.0061  max mem: 3596
Test: Total time: 0:00:16 (0.3233 s / it)
Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.822
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.832
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.874
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.774
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814
</code></pre><p>Now that training has finished, letâ€™s have a look at what it actually predicts in a test image</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pick one image from the test set</span></span><br><span class="line">img, _ = dataset_test[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># put the model in evaluation mode</span></span><br><span class="line">model.eval()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    prediction = model([img.to(device)])</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;
</code></pre><p>Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.<br>The dictionary contains the predictions for the image we passed. In this case, we can see that it contains <code>boxes</code>, <code>labels</code>, <code>masks</code> and <code>scores</code> as fields.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prediction</span><br></pre></td></tr></table></figure>
<pre><code>[{&#39;boxes&#39;: tensor([[ 59.6432,  41.9334, 195.6993, 327.8640],
          [276.4631,  22.6867, 290.8581,  73.6079]], device=&#39;cuda:0&#39;),
  &#39;labels&#39;: tensor([1, 1], device=&#39;cuda:0&#39;),
  &#39;masks&#39;: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]],


          [[[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]]], device=&#39;cuda:0&#39;),
  &#39;scores&#39;: tensor([0.9991, 0.8170], device=&#39;cuda:0&#39;)}]
</code></pre><p>Letâ€™s inspect the image and the predicted segmentation masks.</p>
<p>For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in <code>[C, H, W]</code> format.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Image.fromarray(img.mul(<span class="number">255</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).byte().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_31_0.png" alt="png"></p>
<p>And letâ€™s now visualize the top predicted segmentation mask. The masks are predicted as <code>[N, 1, H, W]</code>, where <code>N</code> is the number of predictions, and are probability maps between 0-1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Image.fromarray(prediction[<span class="number">0</span>][<span class="string">'masks'</span>][<span class="number">0</span>, <span class="number">0</span>].mul(<span class="number">255</span>).byte().cpu().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_33_0.png" alt="png"></p>
<p>Looks pretty good!</p>
<h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset.<br>For that, you wrote a <code>torch.utils.data.Dataset</code> class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.</p>
<p>For a more complete example, which includes multi-machine / multi-gpu training, check <code>references/detection/train.py</code>, which is present in the <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision GitHub repo</a>. </p>
<p>#<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">coordinates:åæ ‡, åº§æ ‡,åè°ƒ, é…åˆ, æ¥åº”</span><br><span class="line">segmentation:åˆ†å‰²</span><br><span class="line">backbone:ä¸»å¹²</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Image</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Audio-torchaudio</title>
    <url>/2020/07/24/Pytorch-Audio-torchaudio/</url>
    <content><![CDATA[<p>Pytorch-Audio-torchaudio:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%shell</span><br><span class="line">pip install torchaudio</span><br></pre></td></tr></table></figure>
<pre><code>Collecting torchaudio
[?25l  Downloading https://files.pythonhosted.org/packages/e9/0a/40e53c686c2af65b2a4e818d11d9b76fa79178440caf99f3ceb2a32c3b04/torchaudio-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 2.8MB/s 
[?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)
Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (0.16.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (1.18.5)
Installing collected packages: torchaudio
Successfully installed torchaudio-0.5.1
</code></pre><h1 id="torchaudio-Tutorial"><a href="#torchaudio-Tutorial" class="headerlink" title="torchaudio Tutorial"></a>torchaudio Tutorial</h1><p>PyTorch is an open source deep learning platform that provides a<br>seamless path from research prototyping to production deployment with<br>GPU support.</p>
<p>Significant effort in solving machine learning problems goes into data<br>preparation. <code>torchaudio</code> leverages PyTorchâ€™s GPU support, and provides<br>many tools to make data loading easy and more readable. In this<br>tutorial, we will see how to load and preprocess data from a simple<br>dataset.</p>
<p>For this tutorial, please make sure the <code>matplotlib</code> package is<br>installed for easier visualization.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchaudio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="Opening-a-file"><a href="#Opening-a-file" class="headerlink" title="Opening a file"></a>Opening a file</h2><p><code>torchaudio</code> also supports loading sound files in the wav and mp3 format. We<br>call waveform the resulting raw audio signal.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename = <span class="string">"test.mp3"</span></span><br><span class="line">waveform, sample_rate = torchaudio.load(filename)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of waveform: &#123;&#125;"</span>.format(waveform.size()))</span><br><span class="line">print(<span class="string">"Sample rate of waveform: &#123;&#125;"</span>.format(sample_rate))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of waveform: torch.Size([2, 10857600])
Sample rate of waveform: 44100





[&lt;matplotlib.lines.Line2D at 0x7f91dd44a4a8&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dd44a5c0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_5_2.png" alt="png"></p>
<p>When you load a file in <code>torchaudio</code>, you can optionally specify the backend to use either<br><code>SoX &lt;https://pypi.org/project/sox/&gt;</code>_ or <code>SoundFile &lt;https://pypi.org/project/SoundFile/&gt;</code>_<br>via <code>torchaudio.set_audio_backend</code>. These backends are loaded lazily when needed.</p>
<p><code>torchaudio</code> also makes JIT compilation optional for functions, and uses <code>nn.Module</code> where possible.</p>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p><code>torchaudio</code> supports a growing list of<br><code>transformations &lt;https://pytorch.org/audio/transforms.html&gt;</code>_.</p>
<ul>
<li><strong>Resample</strong>: Resample waveform to a different sample rate.</li>
<li><strong>Spectrogram</strong>: Create a spectrogram from a waveform.</li>
<li><strong>GriffinLim</strong>: Compute waveform from a linear scale magnitude spectrogram using<br>the Griffin-Lim transformation.</li>
<li><strong>ComputeDeltas</strong>: Compute delta coefficients of a tensor, usually a spectrogram.</li>
<li><strong>ComplexNorm</strong>: Compute the norm of a complex tensor.</li>
<li><strong>MelScale</strong>: This turns a normal STFT into a Mel-frequency STFT,<br>using a conversion matrix.</li>
<li><strong>AmplitudeToDB</strong>: This turns a spectrogram from the<br>power/amplitude scale to the decibel scale.</li>
<li><strong>MFCC</strong>: Create the Mel-frequency cepstrum coefficients from a<br>waveform.</li>
<li><strong>MelSpectrogram</strong>: Create MEL Spectrograms from a waveform using the<br>STFT function in PyTorch.</li>
<li><strong>MuLawEncoding</strong>: Encode waveform based on mu-law companding.</li>
<li><strong>MuLawDecoding</strong>: Decode mu-law encoded waveform.</li>
<li><strong>TimeStretch</strong>: Stretch a spectrogram in time without modifying pitch for a given rate.</li>
<li><strong>FrequencyMasking</strong>: Apply masking to a spectrogram in the frequency domain.</li>
<li><strong>TimeMasking</strong>: Apply masking to a spectrogram in the time domain.</li>
</ul>
<p>Each transform supports batching: you can perform a transform on a single raw<br>audio signal or spectrogram, or many of the same shape.</p>
<p>Since all transforms are <code>nn.Modules</code> or <code>jit.ScriptModules</code>, they can be<br>used as part of a neural network at any point.</p>
<p>To start, we can look at the log of the spectrogram on a log scale.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">specgram = torchaudio.transforms.Spectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([2, 201, 54289])





&lt;matplotlib.image.AxesImage at 0x7f91dcf1ec50&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_9_2.png" alt="png"></p>
<p>Or we can look at the Mel Spectrogram on a log scale.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">specgram = torchaudio.transforms.MelSpectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">p = plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([2, 128, 54289])
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_11_1.png" alt="png"></p>
<p>We can resample the waveform, one channel at a time.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_sample_rate = sample_rate/<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Since Resample applies to a single channel, we resample first channel here</span></span><br><span class="line">channel = <span class="number">0</span></span><br><span class="line">transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([1, 1085760])





[&lt;matplotlib.lines.Line2D at 0x7f91dce23240&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_13_2.png" alt="png"></p>
<p>As another example of transformations, we can encode the signal based on<br>Mu-Law enconding. But to do so, we need the signal to be between -1 and</p>
<ol>
<li>Since the tensor is just a regular PyTorch tensor, we can apply<br>standard operators on it.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Let's check if the tensor is in the interval [-1,1]</span></span><br><span class="line">print(<span class="string">"Min of waveform: &#123;&#125;\nMax of waveform: &#123;&#125;\nMean of waveform: &#123;&#125;"</span>.format(waveform.min(), waveform.max(), waveform.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Min of waveform: -1.0
Max of waveform: 1.0
Mean of waveform: -4.018312756670639e-05
</code></pre><p>Since the waveform is already between -1 and 1, we do not need to<br>normalize it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    <span class="comment"># Subtract the mean, and scale to the interval [-1,1]</span></span><br><span class="line">    tensor_minusmean = tensor - tensor.mean()</span><br><span class="line">    <span class="keyword">return</span> tensor_minusmean/tensor_minusmean.abs().max()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's normalize to the full interval [-1,1]</span></span><br><span class="line"><span class="comment"># waveform = normalize(waveform)</span></span><br></pre></td></tr></table></figure>
<p>Letâ€™s apply encode the waveform.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transformed = torchaudio.transforms.MuLawEncoding()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dce02b38&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_19_2.png" alt="png"></p>
<p>And now decode.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reconstructed = torchaudio.transforms.MuLawDecoding()(transformed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of recovered waveform: &#123;&#125;"</span>.format(reconstructed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(reconstructed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of recovered waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dcd62ef0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_21_2.png" alt="png"></p>
<p>We can finally compare the original waveform with its reconstructed<br>version.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Compute median relative difference</span></span><br><span class="line">err = ((waveform-reconstructed).abs() / waveform.abs()).median()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Median relative difference between original and MuLaw reconstucted signals: &#123;:.2%&#125;"</span>.format(err))</span><br></pre></td></tr></table></figure>
<pre><code>Median relative difference between original and MuLaw reconstucted signals: 1.20%
</code></pre><h2 id="Functional"><a href="#Functional" class="headerlink" title="Functional"></a>Functional</h2><p>The transformations seen above rely on lower level stateless functions for their computations.<br>These functions are available under <code>torchaudio.functional</code>. The complete list is available<br><code>here &lt;https://pytorch.org/audio/functional.html&gt;</code>_ and includes:</p>
<ul>
<li><strong>istft</strong>: Inverse short time Fourier Transform.</li>
<li><strong>gain</strong>: Applies amplification or attenuation to the whole waveform.</li>
<li><strong>dither</strong>: Increases the perceived dynamic range of audio stored at a<br>particular bit-depth.</li>
<li><strong>compute_deltas</strong>: Compute delta coefficients of a tensor.</li>
<li><strong>equalizer_biquad</strong>: Design biquad peaking equalizer filter and perform filtering.</li>
<li><strong>lowpass_biquad</strong>: Design biquad lowpass filter and perform filtering.</li>
<li><strong>highpass_biquad</strong>:Design biquad highpass filter and perform filtering.</li>
</ul>
<p>For example, letâ€™s try the <code>mu_law_encoding</code> functional:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mu_law_encoding_waveform = torchaudio.functional.mu_law_encoding(waveform, quantization_channels=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(mu_law_encoding_waveform.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(mu_law_encoding_waveform[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Shape of transformed waveform: torch.Size([2, 10857600])





[&lt;matplotlib.lines.Line2D at 0x7f91dcd545f8&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_25_2.png" alt="png"></p>
<p>You can see how the output fron <code>torchaudio.functional.mu_law_encoding</code> is the same as<br>the output from <code>torchaudio.transforms.MuLawEncoding</code>.</p>
<p>Now letâ€™s experiment with a few of the other functionals and visualize their output. Taking our<br>spectogram, we can compute itâ€™s deltas:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">computed = torchaudio.functional.compute_deltas(specgram.contiguous(), win_length=<span class="number">3</span>)</span><br><span class="line">print(<span class="string">"Shape of computed deltas: &#123;&#125;"</span>.format(computed.shape))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(computed.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of computed deltas: torch.Size([2, 128, 54289])





&lt;matplotlib.image.AxesImage at 0x7f91dccb0b70&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_27_2.png" alt="png"></p>
<p>We can take the original waveform and apply different effects to it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gain_waveform = torchaudio.functional.gain(waveform, gain_db=<span class="number">5.0</span>)</span><br><span class="line">print(<span class="string">"Min of gain_waveform: &#123;&#125;\nMax of gain_waveform: &#123;&#125;\nMean of gain_waveform: &#123;&#125;"</span>.format(gain_waveform.min(), gain_waveform.max(), gain_waveform.mean()))</span><br><span class="line"></span><br><span class="line">dither_waveform = torchaudio.functional.dither(waveform)</span><br><span class="line">print(<span class="string">"Min of dither_waveform: &#123;&#125;\nMax of dither_waveform: &#123;&#125;\nMean of dither_waveform: &#123;&#125;"</span>.format(dither_waveform.min(), dither_waveform.max(), dither_waveform.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Min of gain_waveform: -1.778279423713684
Max of gain_waveform: 1.778279423713684
Mean of gain_waveform: -7.145693234633654e-05
Min of dither_waveform: -0.99993896484375
Max of dither_waveform: 0.999969482421875
Mean of dither_waveform: -2.492486237315461e-05
</code></pre><p>Another example of the capabilities in <code>torchaudio.functional</code> are applying filters to our<br>waveform. Applying the lowpass biquad filter to our waveform will output a new waveform with<br>the signal of the frequency modified.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowpass_waveform = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of lowpass_waveform: &#123;&#125;\nMax of lowpass_waveform: &#123;&#125;\nMean of lowpass_waveform: &#123;&#125;"</span>.format(lowpass_waveform.min(), lowpass_waveform.max(), lowpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(lowpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Min of lowpass_waveform: -1.0
Max of lowpass_waveform: 1.0
Mean of lowpass_waveform: -4.02079094783403e-05





[&lt;matplotlib.lines.Line2D at 0x7f91dcb8e278&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dcb8e390&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_31_2.png" alt="png"></p>
<p>We can also visualize a waveform with the highpass biquad filter.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">highpass_waveform = torchaudio.functional.highpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of highpass_waveform: &#123;&#125;\nMax of highpass_waveform: &#123;&#125;\nMean of highpass_waveform: &#123;&#125;"</span>.format(highpass_waveform.min(), highpass_waveform.max(), highpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(highpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Min of highpass_waveform: -0.8367071151733398
Max of highpass_waveform: 0.7935813069343567
Mean of highpass_waveform: -9.841002679422672e-09





[&lt;matplotlib.lines.Line2D at 0x7f91dcaf3e48&gt;,
 &lt;matplotlib.lines.Line2D at 0x7f91dcaf3f60&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_33_2.png" alt="png"></p>
<h2 id="Migrating-to-torchaudio-from-Kaldi"><a href="#Migrating-to-torchaudio-from-Kaldi" class="headerlink" title="Migrating to torchaudio from Kaldi"></a>Migrating to torchaudio from Kaldi</h2><p>Users may be familiar with<br><code>Kaldi &lt;http://github.com/kaldi-asr/kaldi&gt;</code>_, a toolkit for speech<br>recognition. <code>torchaudio</code> offers compatibility with it in<br><code>torchaudio.kaldi_io</code>. It can indeed read from kaldi scp, or ark file<br>or streams with:</p>
<ul>
<li>read_vec_int_ark</li>
<li>read_vec_flt_scp</li>
<li>read_vec_flt_arkfile/stream</li>
<li>read_mat_scp</li>
<li>read_mat_ark</li>
</ul>
<p><code>torchaudio</code> provides Kaldi-compatible transforms for <code>spectrogram</code>,<br><code>fbank</code>, <code>mfcc</code>, and <code>`resample_waveform with the benefit of GPU support, see</code>here <compliance.kaldi.html>`__ for more information.</compliance.kaldi.html></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_fft = <span class="number">400.0</span></span><br><span class="line">frame_length = n_fft / sample_rate * <span class="number">1000.0</span></span><br><span class="line">frame_shift = frame_length / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">"channel"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"dither"</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">"window_type"</span>: <span class="string">"hanning"</span>,</span><br><span class="line">    <span class="string">"frame_length"</span>: frame_length,</span><br><span class="line">    <span class="string">"frame_shift"</span>: frame_shift,</span><br><span class="line">    <span class="string">"remove_dc_offset"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"round_to_power_of_two"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"sample_frequency"</span>: sample_rate,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of spectrogram: torch.Size([54287, 201])





&lt;matplotlib.image.AxesImage at 0x7f91dca6d240&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_35_2.png" alt="png"></p>
<p>We also support computing the filterbank features from waveforms,<br>matching Kaldiâ€™s implementation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fbank = torchaudio.compliance.kaldi.fbank(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of fbank: &#123;&#125;"</span>.format(fbank.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(fbank.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of fbank: torch.Size([54287, 23])





&lt;matplotlib.image.AxesImage at 0x7f91dca440f0&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_37_2.png" alt="png"></p>
<p>You can create mel frequency cepstral coefficients from a raw audio signal<br>This matches the input/output of Kaldiâ€™s compute-mfcc-feats.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mfcc = torchaudio.compliance.kaldi.mfcc(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of mfcc: &#123;&#125;"</span>.format(mfcc.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(mfcc.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of mfcc: torch.Size([54287, 13])





&lt;matplotlib.image.AxesImage at 0x7f91dca16828&gt;
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_39_2.png" alt="png"></p>
<h2 id="Available-Datasets"><a href="#Available-Datasets" class="headerlink" title="Available Datasets"></a>Available Datasets</h2><p>If you do not want to create your own dataset to train your model, <code>torchaudio</code> offers a<br>unified dataset interface. This interface supports lazy-loading of files to memory, download<br>and extract functions, and datasets to build models.</p>
<p>The datasets <code>torchaudio</code> currently supports are:</p>
<ul>
<li><strong>VCTK</strong>: Speech data uttered by 109 native speakers of English with various accents<br>(<code>Read more here &lt;https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html&gt;</code>_).</li>
<li><strong>Yesno</strong>: Sixty recordings of one individual saying yes or no in Hebrew; each<br>recording is eight words long (<code>Read more here &lt;https://www.openslr.org/1/&gt;</code>_).</li>
<li><strong>Common Voice</strong>: An open source, multi-language dataset of voices that anyone can use<br>to train speech-enabled applications (<code>Read more here &lt;https://voice.mozilla.org/en/datasets&gt;</code>_).</li>
<li><strong>LibriSpeech</strong>: Large-scale (1000 hours) corpus of read English speech (<code>Read more here &lt;http://www.openslr.org/12&gt;</code>_).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yesno_data = torchaudio.datasets.YESNO(<span class="string">'./'</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A data point in Yesno is a tuple (waveform, sample_rate, labels) where labels is a list of integers with 1 for yes and 0 for no.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pick data point number 3 to see an example of the the yesno_data:</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">waveform, sample_rate, labels = yesno_data[n]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Waveform: &#123;&#125;\nSample rate: &#123;&#125;\nLabels: &#123;&#125;"</span>.format(waveform, sample_rate, labels))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=4703754.0), HTML(value=&#39;&#39;)))



Waveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -8.5449e-04,
         -1.0986e-03, -8.8501e-04]])
Sample rate: 8000
Labels: [1, 0, 0, 0, 1, 0, 0, 1]





[&lt;matplotlib.lines.Line2D at 0x7f91dbcfb1d0&gt;]
</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_41_3.png" alt="png"></p>
<p>Now, whenever you ask for a sound file from the dataset, it is loaded in memory only when you ask for it.<br>Meaning, the dataset only loads and keeps in memory the items that you want and use, saving on memory.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We used an example raw audio signal, or waveform, to illustrate how to<br>open an audio file using <code>torchaudio</code>, and how to pre-process,<br>transform, and apply functions to such waveform. We also demonstrated how<br>to use familiar Kaldi functions, as well as utilize built-in datasets to<br>construct our models. Given that <code>torchaudio</code> is built on PyTorch,<br>these techniques can be used as building blocks for more advanced audio<br>applications, such as speech recognition, while leveraging GPUs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Audio</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Reinforcement-Learning</title>
    <url>/2020/07/25/Pytorch-Reinforcement-Learning/</url>
    <content><![CDATA[<p>Pytorch-Reinforcement-Learning:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Reinforcement-Learning-DQN-Tutorial"><a href="#Reinforcement-Learning-DQN-Tutorial" class="headerlink" title="Reinforcement Learning (DQN) Tutorial"></a>Reinforcement Learning (DQN) Tutorial</h1><p><strong>Author</strong>: <code>Adam Paszke &lt;https://github.com/apaszke&gt;</code>_</p>
<p>This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent<br>on the CartPole-v0 task from the <code>OpenAI Gym &lt;https://gym.openai.com/&gt;</code>__.</p>
<p><strong>Task</strong></p>
<p>The agent has to decide between two actions - moving the cart left or<br>right - so that the pole attached to it stays upright. You can find an<br>official leaderboard with various algorithms and visualizations at the<br><code>Gym website &lt;https://gym.openai.com/envs/CartPole-v0&gt;</code>__.</p>
<p><img src="https://pytorch.org/tutorials/_images/cartpole.gif" alt></p>
<p>   cartpole</p>
<p>As the agent observes the current state of the environment and chooses<br>an action, the environment <em>transitions</em> to a new state, and also<br>returns a reward that indicates the consequences of the action. In this<br>task, rewards are +1 for every incremental timestep and the environment<br>terminates if the pole falls over too far or the cart moves more then 2.4<br>units away from center. This means better performing scenarios will run<br>for longer duration, accumulating larger return.</p>
<p>The CartPole task is designed so that the inputs to the agent are 4 real<br>values representing the environment state (position, velocity, etc.).<br>However, neural networks can solve the task purely by looking at the<br>scene, so weâ€™ll use a patch of the screen centered on the cart as an<br>input. Because of this, our results arenâ€™t directly comparable to the<br>ones from the official leaderboard - our task is much harder.<br>Unfortunately this does slow down the training, because we have to<br>render all the frames.</p>
<p>Strictly speaking, we will present the state as the difference between<br>the current screen patch and the previous one. This will allow the agent<br>to take the velocity of the pole into account from one image.</p>
<p><strong>Packages</strong></p>
<p>First, letâ€™s import needed packages. Firstly, we need<br><code>gym &lt;https://gym.openai.com/docs&gt;</code>__ for the environment<br>(Install using <code>pip install gym</code>).<br>Weâ€™ll also use the following from PyTorch:</p>
<ul>
<li>neural networks (<code>torch.nn</code>)</li>
<li>optimization (<code>torch.optim</code>)</li>
<li>automatic differentiation (<code>torch.autograd</code>)</li>
<li>utilities for vision tasks (<code>torchvision</code> - <code>a separate
package &lt;https://github.com/pytorch/vision&gt;</code>__).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line"><span class="comment"># set up matplotlib</span></span><br><span class="line">is_ipython = <span class="string">'inline'</span> <span class="keyword">in</span> matplotlib.get_backend()</span><br><span class="line"><span class="keyword">if</span> is_ipython:</span><br><span class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="comment"># if gpu is to be used</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><p>Weâ€™ll be using experience replay memory for training our DQN. It stores<br>the transitions that the agent observes, allowing us to reuse this data<br>later. By sampling from it randomly, the transitions that build up a<br>batch are decorrelated. It has been shown that this greatly stabilizes<br>and improves the DQN training procedure.</p>
<p>For this, weâ€™re going to need two classses:</p>
<ul>
<li><code>Transition</code> - a named tuple representing a single transition in<br>our environment. It essentially maps (state, action) pairs<br>to their (next_state, reward) result, with the state being the<br>screen difference image as described later on.</li>
<li><code>ReplayMemory</code> - a cyclic buffer of bounded size that holds the<br>transitions observed recently. It also implements a <code>.sample()</code><br>method for selecting a random batch of transitions for training.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>,</span><br><span class="line">                        (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.memory = []</span><br><span class="line">        self.position = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Saves a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.memory) &lt; self.capacity:</span><br><span class="line">            self.memory.append(<span class="literal">None</span>)</span><br><span class="line">        self.memory[self.position] = Transition(*args)</span><br><span class="line">        self.position = (self.position + <span class="number">1</span>) % self.capacity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.memory, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.memory)</span><br></pre></td></tr></table></figure>
<p>Now, letâ€™s define our model. But first, let quickly recap what a DQN is.</p>
<h2 id="DQN-algorithm"><a href="#DQN-algorithm" class="headerlink" title="DQN algorithm"></a>DQN algorithm</h2><p>Our environment is deterministic, so all equations presented here are<br>also formulated deterministically for the sake of simplicity. In the<br>reinforcement learning literature, they would also contain expectations<br>over stochastic transitions in the environment.</p>
<p>Our aim will be to train a policy that tries to maximize the discounted,<br>cumulative reward<br>$R_{t_0} = \sum_{t=t_0}^{\infty} \gamma^{t - t_0} r_t$, where<br>$R_{t_0}$ is also known as the <em>return</em>. The discount,<br>$\gamma$, should be a constant between $0$ and $1$<br>that ensures the sum converges. It makes rewards from the uncertain far<br>future less important for our agent than the ones in the near future<br>that it can be fairly confident about.</p>
<p>The main idea behind Q-learning is that if we had a function<br>$Q^*: State \times Action \rightarrow \mathbb{R}$, that could tell<br>us what our return would be, if we were to take an action in a given<br>state, then we could easily construct a policy that maximizes our<br>rewards:</p>
<p>\begin{align}\pi^<em>(s) = \arg!\max_a \ Q^</em>(s, a)\end{align}</p>
<p>However, we donâ€™t know everything about the world, so we donâ€™t have<br>access to $Q^<em>$. But, since neural networks are universal function<br>approximators, we can simply create one and train it to resemble<br>$Q^</em>$.</p>
<p>For our training update rule, weâ€™ll use a fact that every $Q$<br>function for some policy obeys the Bellman equation:</p>
<p>\begin{align}Q^{\pi}(s, a) = r + \gamma Q^{\pi}(sâ€™, \pi(sâ€™))\end{align}</p>
<p>The difference between the two sides of the equality is known as the<br>temporal difference error, $\delta$:</p>
<p>\begin{align}\delta = Q(s, a) - (r + \gamma \max_a Q(sâ€™, a))\end{align}</p>
<p>To minimise this error, we will use the <code>Huber
loss &lt;https://en.wikipedia.org/wiki/Huber_loss&gt;</code>__. The Huber loss acts<br>like the mean squared error when the error is small, but like the mean<br>absolute error when the error is large - this makes it more robust to<br>outliers when the estimates of $Q$ are very noisy. We calculate<br>this over a batch of transitions, $B$, sampled from the replay<br>memory:</p>
<p>\begin{align}\mathcal{L} = \frac{1}{|B|}\sum_{(s, a, sâ€™, r) \ \in \ B} \mathcal{L}(\delta)\end{align}</p>
<p>\begin{align}\text{where} \quad \mathcal{L}(\delta) = \begin{cases}<br>     \frac{1}{2}{\delta^2}  &amp; \text{for } |\delta| \le 1, \\<br>     |\delta| - \frac{1}{2} &amp; \text{otherwise.}<br>   \end{cases}\end{align}</p>
<p>Q-network</p>
<p>Our model will be a convolutional neural network that takes in the<br>difference between the current and previous screen patches. It has two<br>outputs, representing $Q(s, \mathrm{left})$ and<br>$Q(s, \mathrm{right})$ (where $s$ is the input to the<br>network). In effect, the network is trying to predict the <em>expected return</em> of<br>taking each action given the current input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, w, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Number of Linear input connections depends on output of conv2d layers</span></span><br><span class="line">        <span class="comment"># and therefore the input image size, so compute it.</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv2d_size_out</span><span class="params">(size, kernel_size = <span class="number">5</span>, stride = <span class="number">2</span>)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> (size - (kernel_size - <span class="number">1</span>) - <span class="number">1</span>) // stride  + <span class="number">1</span></span><br><span class="line">        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))</span><br><span class="line">        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))</span><br><span class="line">        linear_input_size = convw * convh * <span class="number">32</span></span><br><span class="line">        self.head = nn.Linear(linear_input_size, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Called with either one element to determine next action, or a batch</span></span><br><span class="line">    <span class="comment"># during optimization. Returns tensor([[left0exp,right0exp]...]).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        <span class="keyword">return</span> self.head(x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<p>Input extraction</p>
<p>The code below are utilities for extracting and processing rendered<br>images from the environment. It uses the <code>torchvision</code> package, which<br>makes it easy to compose image transforms. Once you run the cell it will<br>display an example patch that it extracted.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">resize = T.Compose([T.ToPILImage(),</span><br><span class="line">                    T.Resize(<span class="number">40</span>, interpolation=Image.CUBIC),</span><br><span class="line">                    T.ToTensor()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cart_location</span><span class="params">(screen_width)</span>:</span></span><br><span class="line">    world_width = env.x_threshold * <span class="number">2</span></span><br><span class="line">    scale = screen_width / world_width</span><br><span class="line">    <span class="keyword">return</span> int(env.state[<span class="number">0</span>] * scale + screen_width / <span class="number">2.0</span>)  <span class="comment"># MIDDLE OF CART</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_screen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Returned screen requested by gym is 400x600x3, but is sometimes larger</span></span><br><span class="line">    <span class="comment"># such as 800x1200x3. Transpose it into torch order (CHW).</span></span><br><span class="line">    screen = env.render(mode=<span class="string">'rgb_array'</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Cart is in the lower half, so strip off the top and bottom of the screen</span></span><br><span class="line">    _, screen_height, screen_width = screen.shape</span><br><span class="line">    screen = screen[:, int(screen_height*<span class="number">0.4</span>):int(screen_height * <span class="number">0.8</span>)]</span><br><span class="line">    view_width = int(screen_width * <span class="number">0.6</span>)</span><br><span class="line">    cart_location = get_cart_location(screen_width)</span><br><span class="line">    <span class="keyword">if</span> cart_location &lt; view_width // <span class="number">2</span>:</span><br><span class="line">        slice_range = slice(view_width)</span><br><span class="line">    <span class="keyword">elif</span> cart_location &gt; (screen_width - view_width // <span class="number">2</span>):</span><br><span class="line">        slice_range = slice(-view_width, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        slice_range = slice(cart_location - view_width // <span class="number">2</span>,</span><br><span class="line">                            cart_location + view_width // <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Strip off the edges, so that we have a square image centered on a cart</span></span><br><span class="line">    screen = screen[:, :, slice_range]</span><br><span class="line">    <span class="comment"># Convert to float, rescale, convert to torch tensor</span></span><br><span class="line">    <span class="comment"># (this doesn't require a copy)</span></span><br><span class="line">    screen = np.ascontiguousarray(screen, dtype=np.float32) / <span class="number">255</span></span><br><span class="line">    screen = torch.from_numpy(screen)</span><br><span class="line">    <span class="comment"># Resize, and add a batch dimension (BCHW)</span></span><br><span class="line">    <span class="keyword">return</span> resize(screen).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env.reset()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(get_screen().cpu().squeeze(<span class="number">0</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy(),</span><br><span class="line">           interpolation=<span class="string">'none'</span>)</span><br><span class="line">plt.title(<span class="string">'Example extracted screen'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/25/Pytorch-Reinforcement-Learning/output_8_0.png" alt="png"></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Hyperparameters and utilities</p>
<p>This cell instantiates our model and its optimizer, and defines some<br>utilities:</p>
<ul>
<li><code>select_action</code> - will select an action accordingly to an epsilon<br>greedy policy. Simply put, weâ€™ll sometimes use our model for choosing<br>the action, and sometimes weâ€™ll just sample one uniformly. The<br>probability of choosing a random action will start at <code>EPS_START</code><br>and will decay exponentially towards <code>EPS_END</code>. <code>EPS_DECAY</code><br>controls the rate of the decay.</li>
<li><code>plot_durations</code> - a helper for plotting the durations of episodes,<br>along with an average over the last 100 episodes (the measure used in<br>the official evaluations). The plot will be underneath the cell<br>containing the main training loop, and will update after every<br>episode.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.999</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get screen size so that we can initialize layers correctly based on shape</span></span><br><span class="line"><span class="comment"># returned from AI gym. Typical dimensions at this point are close to 3x40x90</span></span><br><span class="line"><span class="comment"># which is the result of a clamped and down-scaled render buffer in get_screen()</span></span><br><span class="line">init_screen = get_screen()</span><br><span class="line">_, _, screen_height, screen_width = init_screen.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get number of actions from gym action space</span></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line"></span><br><span class="line">policy_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.RMSprop(policy_net.parameters())</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * \</span><br><span class="line">        math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(state).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">episode_durations = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_durations</span><span class="params">()</span>:</span></span><br><span class="line">    plt.figure(<span class="number">2</span>)</span><br><span class="line">    plt.clf()</span><br><span class="line">    durations_t = torch.tensor(episode_durations, dtype=torch.float)</span><br><span class="line">    plt.title(<span class="string">'Training...'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Episode'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Duration'</span>)</span><br><span class="line">    plt.plot(durations_t.numpy())</span><br><span class="line">    <span class="comment"># Take 100 episode averages and plot them too</span></span><br><span class="line">    <span class="keyword">if</span> len(durations_t) &gt;= <span class="number">100</span>:</span><br><span class="line">        means = durations_t.unfold(<span class="number">0</span>, <span class="number">100</span>, <span class="number">1</span>).mean(<span class="number">1</span>).view(<span class="number">-1</span>)</span><br><span class="line">        means = torch.cat((torch.zeros(<span class="number">99</span>), means))</span><br><span class="line">        plt.plot(means.numpy())</span><br><span class="line"></span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line">    <span class="keyword">if</span> is_ipython:</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">        display.display(plt.gcf())</span><br></pre></td></tr></table></figure>
<p>Training loop</p>
<p>Finally, the code for training our model.</p>
<p>Here, you can find an <code>optimize_model</code> function that performs a<br>single step of the optimization. It first samples a batch, concatenates<br>all the tensors into a single one, computes $Q(s_t, a_t)$ and<br>$V(s_{t+1}) = \max_a Q(s_{t+1}, a)$, and combines them into our<br>loss. By defition we set $V(s) = 0$ if $s$ is a terminal<br>state. We also use a target network to compute $V(s_{t+1})$ for<br>added stability. The target network has its weights kept frozen most of<br>the time, but is updated with the policy networkâ€™s weights every so often.<br>This is usually a set number of steps but we shall use episodes for<br>simplicity.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    <span class="comment"># Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for</span></span><br><span class="line">    <span class="comment"># detailed explanation). This converts batch-array of Transitions</span></span><br><span class="line">    <span class="comment"># to Transition of batch-arrays.</span></span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute a mask of non-final states and concatenate the batch elements</span></span><br><span class="line">    <span class="comment"># (a final state would've been the one after which simulation ended)</span></span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span> s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">                                          batch.next_state)), device=device, dtype=torch.bool)</span><br><span class="line">    non_final_next_states = torch.cat([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state</span><br><span class="line">                                                <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>])</span><br><span class="line">    state_batch = torch.cat(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.cat(batch.reward)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q(s_t, a) - the model computes Q(s_t), then we select the</span></span><br><span class="line">    <span class="comment"># columns of actions taken. These are the actions which would've been taken</span></span><br><span class="line">    <span class="comment"># for each batch state according to policy_net</span></span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute V(s_&#123;t+1&#125;) for all next states.</span></span><br><span class="line">    <span class="comment"># Expected values of actions for non_final_next_states are computed based</span></span><br><span class="line">    <span class="comment"># on the "older" target_net; selecting their best reward with max(1)[0].</span></span><br><span class="line">    <span class="comment"># This is merged based on the mask, such that we'll have either the expected</span></span><br><span class="line">    <span class="comment"># state value or 0 in case the state was final.</span></span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE, device=device)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    <span class="comment"># Compute the expected Q values</span></span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Huber loss</span></span><br><span class="line">    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimize the model</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>Below, you can find the main training loop. At the beginning we reset<br>the environment and initialize the <code>state</code> Tensor. Then, we sample<br>an action, execute it, observe the next screen and the reward (always<br>1), and optimize our model once. When the episode ends (our model<br>fails), we restart the loop.</p>
<p>Below, <code>num_episodes</code> is set small. You should download<br>the notebook and run lot more epsiodes, such as 300+ for meaningful<br>duration improvements.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_episodes = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    <span class="comment"># Initialize the environment and state</span></span><br><span class="line">    env.reset()</span><br><span class="line">    last_screen = get_screen()</span><br><span class="line">    current_screen = get_screen()</span><br><span class="line">    state = current_screen - last_screen</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        <span class="comment"># Select and perform an action</span></span><br><span class="line">        action = select_action(state)</span><br><span class="line">        _, reward, done, _ = env.step(action.item())</span><br><span class="line">        reward = torch.tensor([reward], device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Observe new state</span></span><br><span class="line">        last_screen = current_screen</span><br><span class="line">        current_screen = get_screen()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> done:</span><br><span class="line">            next_state = current_screen - last_screen</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Store the transition in memory</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Move to the next state</span></span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform one step of the optimization (on the target network)</span></span><br><span class="line">        optimize_model()</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            episode_durations.append(t + <span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># Update the target network, copying all weights and biases in DQN</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Complete'</span>)</span><br><span class="line">env.render()</span><br><span class="line">env.close()</span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;


Complete



&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre><p>Here is the diagram that illustrates the overall resulting data flow.</p>
<p><img src="https://pytorch.org/tutorials/_images/reinforcement_learning_diagram.jpg" alt></p>
<p>Actions are chosen either randomly or based on a policy, getting the next<br>step sample from the gym environment. We record the results in the<br>replay memory and also run optimization step on every iteration.<br>Optimization picks a random batch from the replay memory to do training of the<br>new policy. â€œOlderâ€ target_net is also used in optimization to compute the<br>expected Q values; it is updated occasionally to keep it current.</p>
<h2 id="æˆ‘ä¸è®¤è¯†çš„å•è¯"><a href="#æˆ‘ä¸è®¤è¯†çš„å•è¯" class="headerlink" title="æˆ‘ä¸è®¤è¯†çš„å•è¯"></a>æˆ‘ä¸è®¤è¯†çš„å•è¯</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample:é‡‡æ ·</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-ReinforcementLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-ä½¿ç”¨Sequence2Sequenceç½‘ç»œå’Œæ³¨æ„åŠ›è¿›è¡Œç¿»è¯‘ä½¿ç”¨Sequence2Sequenceç½‘ç»œå’Œæ³¨æ„åŠ›è¿›è¡Œç¿»è¯‘</title>
    <url>/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<p>Pytorch-Text-ä½¿ç”¨Sequence2Sequenceç½‘ç»œå’Œæ³¨æ„åŠ›è¿›è¡Œç¿»è¯‘ä½¿ç”¨Sequence2Sequenceç½‘ç»œå’Œæ³¨æ„åŠ›è¿›è¡Œç¿»è¯‘:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>This is the third and final tutorial on doing â€œNLP From Scratchâ€, where we<br>write our own classes and functions to preprocess the data to do our NLP<br>modeling tasks. We hope after you complete this tutorial that youâ€™ll proceed to<br>learn how <code>torchtext</code> can handle much of this preprocessing for you in the<br>three tutorials immediately following this one.</p>
<p>In this project we will be teaching a neural network to translate from<br>French to English.</p>
<p>::</p>
<pre><code>[KEY: &gt; input, = target, &lt; output]

&gt; il est en train de peindre un tableau .
= he is painting a picture .
&lt; he is painting a picture .

&gt; pourquoi ne pas essayer ce vin delicieux ?
= why not try that delicious wine ?
&lt; why not try that delicious wine ?

&gt; elle n est pas poete mais romanciere .
= she is not a poet but a novelist .
&lt; she not not a poet but a novelist .

&gt; vous etes trop maigre .
= you re too skinny .
&lt; you re all alone .
</code></pre><p>â€¦ to varying degrees of success.</p>
<p>This is made possible by the simple but powerful idea of the <code>sequence
to sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code>__, in which two<br>recurrent neural networks work together to transform one sequence to<br>another. An encoder network condenses an input sequence into a vector,<br>and a decoder network unfolds that vector into a new sequence.</p>
<p>.. figure:: /_static/img/seq-seq-images/seq2seq.png<br>   :alt:</p>
<p>To improve upon this model weâ€™ll use an <code>attention
mechanism &lt;https://arxiv.org/abs/1409.0473&gt;</code>__, which lets the decoder<br>learn to focus over a specific range of the input sequence.</p>
<p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about Sequence to Sequence networks and<br>how they work:</p>
<ul>
<li><code>Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li>
<li><code>Sequence to Sequence Learning with Neural
Networks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li>
<li><code>Neural Machine Translation by Jointly Learning to Align and
Translate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li>
<li><code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li>
</ul>
<p>You will also find the previous tutorials on<br>:doc:<code>/intermediate/char_rnn_classification_tutorial</code><br>and :doc:<code>/intermediate/char_rnn_generation_tutorial</code><br>helpful as those concepts are very similar to the Encoder and Decoder<br>models, respectively.</p>
<p>And for more, read the papers that introduced these topics:</p>
<ul>
<li><code>Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li>
<li><code>Sequence to Sequence Learning with Neural
Networks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li>
<li><code>Neural Machine Translation by Jointly Learning to Align and
Translate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li>
<li><code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li>
</ul>
<p><strong>Requirements</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Loading-data-files"><a href="#Loading-data-files" class="headerlink" title="Loading data files"></a>Loading data files</h1><p>The data for this project is a set of many thousands of English to<br>French translation pairs.</p>
<p><code>This question on Open Data Stack
Exchange &lt;https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages&gt;</code>__<br>pointed me to the open translation site <a href="https://tatoeba.org/" target="_blank" rel="noopener">https://tatoeba.org/</a> which has<br>downloads available at <a href="https://tatoeba.org/eng/downloads" target="_blank" rel="noopener">https://tatoeba.org/eng/downloads</a> - and better<br>yet, someone did the extra work of splitting language pairs into<br>individual text files here: <a href="https://www.manythings.org/anki/" target="_blank" rel="noopener">https://www.manythings.org/anki/</a></p>
<p>The English to French pairs are too big to include in the repo, so<br>download to <code>data/eng-fra.txt</code> before continuing. The file is a tab<br>separated list of translation pairs:</p>
<p>::</p>
<pre><code>I am cold.    J&#39;ai froid.
</code></pre><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>Similar to the character encoding used in the character-level RNN<br>tutorials, we will be representing each word in a language as a one-hot<br>vector, or giant vector of zeros except for a single one (at the index<br>of the word). Compared to the dozens of characters that might exist in a<br>language, there are many many more words, so the encoding vector is much<br>larger. We will however cheat a bit and trim the data to only use a few<br>thousand words per language.</p>
<p>.. figure:: /_static/img/seq-seq-images/word-encoding.png<br>   :alt:</p>
<p>Weâ€™ll need a unique index per word to use as the inputs and targets of<br>the networks later. To keep track of all this we will use a helper class<br>called <code>Lang</code> which has word â†’ index (<code>word2index</code>) and index â†’ word<br>(<code>index2word</code>) dictionaries, as well as a count of each word<br><code>word2count</code> to use to later replace rare words.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SOS_token = <span class="number">0</span></span><br><span class="line">EOS_token = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lang</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        self.word2count = &#123;&#125;</span><br><span class="line">        self.index2word = &#123;<span class="number">0</span>: <span class="string">"SOS"</span>, <span class="number">1</span>: <span class="string">"EOS"</span>&#125;</span><br><span class="line">        self.n_words = <span class="number">2</span>  <span class="comment"># Count SOS and EOS</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addSentence</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</span><br><span class="line">            self.addWord(word)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2index:</span><br><span class="line">            self.word2index[word] = self.n_words</span><br><span class="line">            self.word2count[word] = <span class="number">1</span></span><br><span class="line">            self.index2word[self.n_words] = word</span><br><span class="line">            self.n_words += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.word2count[word] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>The files are all in Unicode, to simplify we will turn Unicode<br>characters to ASCII, make everything lowercase, and trim most<br>punctuation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to</span></span><br><span class="line"><span class="comment"># https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># åœ¨Unicodeä¸­ï¼ŒæŸäº›å­—ç¬¦èƒ½å¤Ÿç”¨å¤šä¸ªåˆæ³•çš„ç¼–ç è¡¨ç¤ºï¼Œåœ¨éœ€è¦æ¯”è¾ƒå­—ç¬¦ä¸²çš„ç¨‹åºä¸­ä½¿ç”¨å­—ç¬¦çš„å¤šç§è¡¨ç¤ºä¼šäº§ç”Ÿé—®é¢˜ã€‚ </span></span><br><span class="line"><span class="comment"># ä¸ºäº†ä¿®æ­£è¿™ä¸ªé—®é¢˜ï¼Œä½ å¯ä»¥ä½¿ç”¨unicodedataæ¨¡å—å…ˆå°†æ–‡æœ¬æ ‡å‡†åŒ–</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Lowercase, trim, and remove non-letter characters</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeString</span><span class="params">(s)</span>:</span></span><br><span class="line">    s = unicodeToAscii(s.lower().strip())</span><br><span class="line">    s = re.sub(<span class="string">r"([.!?])"</span>, <span class="string">r" \1"</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">r"[^a-zA-Z.!?]+"</span>, <span class="string">r" "</span>, s)</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p>To read the data file we will split the file into lines, and then split<br>lines into pairs. The files are all English â†’ Other Language, so if we<br>want to translate from Other Language â†’ English I added the <code>reverse</code><br>flag to reverse the pairs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLangs</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    print(<span class="string">"Reading lines..."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Read the file and split into lines</span></span><br><span class="line">    lines = open(<span class="string">'data/%s-%s.txt'</span> % (lang1, lang2), encoding=<span class="string">'utf-8'</span>).\</span><br><span class="line">        read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split every line into pairs and normalize</span></span><br><span class="line">    pairs = [[normalizeString(s) <span class="keyword">for</span> s <span class="keyword">in</span> l.split(<span class="string">'\t'</span>)] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reverse pairs, make Lang instances</span></span><br><span class="line">    <span class="keyword">if</span> reverse:</span><br><span class="line">        pairs = [list(reversed(p)) <span class="keyword">for</span> p <span class="keyword">in</span> pairs]</span><br><span class="line">        input_lang = Lang(lang2)</span><br><span class="line">        output_lang = Lang(lang1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_lang = Lang(lang1)</span><br><span class="line">        output_lang = Lang(lang2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br></pre></td></tr></table></figure>
<p>Since there are a <em>lot</em> of example sentences and we want to train<br>something quickly, weâ€™ll trim the data set to only relatively short and<br>simple sentences. Here the maximum length is 10 words (that includes<br>ending punctuation) and weâ€™re filtering to sentences that translate to<br>the form â€œI amâ€ or â€œHe isâ€ etc. (accounting for apostrophes replaced<br>earlier).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_LENGTH = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">eng_prefixes = (</span><br><span class="line">    <span class="string">"i am "</span>, <span class="string">"i m "</span>,</span><br><span class="line">    <span class="string">"he is"</span>, <span class="string">"he s "</span>,</span><br><span class="line">    <span class="string">"she is"</span>, <span class="string">"she s "</span>,</span><br><span class="line">    <span class="string">"you are"</span>, <span class="string">"you re "</span>,</span><br><span class="line">    <span class="string">"we are"</span>, <span class="string">"we re "</span>,</span><br><span class="line">    <span class="string">"they are"</span>, <span class="string">"they re "</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPair</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(p[<span class="number">0</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        len(p[<span class="number">1</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        p[<span class="number">1</span>].startswith(eng_prefixes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPairs</span><span class="params">(pairs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [pair <span class="keyword">for</span> pair <span class="keyword">in</span> pairs <span class="keyword">if</span> filterPair(pair)]</span><br></pre></td></tr></table></figure>
<p>The full process for preparing the data is:</p>
<ul>
<li>Read text file and split into lines, split lines into pairs</li>
<li>Normalize text, filter by length and content</li>
<li>Make word lists from sentences in pairs</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareData</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)</span><br><span class="line">    print(<span class="string">"Read %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    pairs = filterPairs(pairs)</span><br><span class="line">    print(<span class="string">"Trimmed to %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    print(<span class="string">"Counting words..."</span>)</span><br><span class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</span><br><span class="line">        input_lang.addSentence(pair[<span class="number">0</span>])</span><br><span class="line">        output_lang.addSentence(pair[<span class="number">1</span>])</span><br><span class="line">    print(<span class="string">"Counted words:"</span>)</span><br><span class="line">    print(input_lang.name, input_lang.n_words)</span><br><span class="line">    print(output_lang.name, output_lang.n_words)</span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_lang, output_lang, pairs = prepareData(<span class="string">'eng'</span>, <span class="string">'fra'</span>, <span class="literal">True</span>)</span><br><span class="line">print(random.choice(pairs))</span><br></pre></td></tr></table></figure>
<pre><code>Reading lines...
Read 135842 sentence pairs
Trimmed to 10599 sentence pairs
Counting words...
Counted words:
fra 4345
eng 2803
[&#39;nous sommes tous sur le meme bateau .&#39;, &#39;we re all in the same boat .&#39;]
</code></pre><h1 id="The-Seq2Seq-Model"><a href="#The-Seq2Seq-Model" class="headerlink" title="The Seq2Seq Model"></a>The Seq2Seq Model</h1><p>A Recurrent Neural Network, or RNN, is a network that operates on a<br>sequence and uses its own output as input for subsequent steps.</p>
<p>A <code>Sequence to Sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code><strong>, or<br>seq2seq network, or <code>Encoder Decoder
network &lt;https://arxiv.org/pdf/1406.1078v3.pdf&gt;</code></strong>, is a model<br>consisting of two RNNs called the encoder and decoder. The encoder reads<br>an input sequence and outputs a single vector, and the decoder reads<br>that vector to produce an output sequence.</p>
<p><img src="https://pytorch.org/tutorials/_images/seq2seq.png" alt></p>
<p>Unlike sequence prediction with a single RNN, where every input<br>corresponds to an output, the seq2seq model frees us from sequence<br>length and order, which makes it ideal for translation between two<br>languages.</p>
<p>Consider the sentence â€œJe ne suis pas le chat noirâ€ â†’ â€œI am not the<br>black catâ€. Most of the words in the input sentence have a direct<br>translation in the output sentence, but are in slightly different<br>orders, e.g. â€œchat noirâ€ and â€œblack catâ€. Because of the â€œne/pasâ€<br>construction there is also one more word in the input sentence. It would<br>be difficult to produce a correct translation directly from the sequence<br>of input words.</p>
<p>With a seq2seq model the encoder creates a single vector which, in the<br>ideal case, encodes the â€œmeaningâ€ of the input sequence into a single<br>vector â€” a single point in some N dimensional space of sentences.</p>
<h2 id="The-Encoder"><a href="#The-Encoder" class="headerlink" title="The Encoder"></a>The Encoder</h2><p>The encoder of a seq2seq network is a RNN that outputs some value for<br>every word from the input sentence. For every input word the encoder<br>outputs a vector and a hidden state, and uses the hidden state for the<br>next input word.</p>
<p><img src="https://pytorch.org/tutorials/_images/decoder-network.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size)</span>:</span></span><br><span class="line">        super(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = embedded</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>
<h2 id="The-Decoder"><a href="#The-Decoder" class="headerlink" title="The Decoder"></a>The Decoder</h2><p>The decoder is another RNN that takes the encoder output vector(s) and<br>outputs a sequence of words to create the translation.</p>
<p>Simple Decoder</p>
<p>In the simplest seq2seq decoder we use only last output of the encoder.<br>This last output is sometimes called the <em>context vector</em> as it encodes<br>context from the entire sequence. This context vector is used as the<br>initial hidden state of the decoder.</p>
<p>At every step of decoding, the decoder is given an input token and<br>hidden state. The initial input token is the start-of-string <code>&lt;SOS&gt;</code><br>token, and the first hidden state is the context vector (the encoderâ€™s<br>last hidden state).</p>
<p><img src="https://pytorch.org/tutorials/_images/attention-decoder-network.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(DecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line">        self.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        output = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        output = self.softmax(self.out(output[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>
<p>I encourage you to train and observe the results of this model, but to<br>save space weâ€™ll be going straight for the gold and introducing the<br>Attention Mechanism.</p>
<p>Attention Decoder</p>
<p>If only the context vector is passed betweeen the encoder and decoder,<br>that single vector carries the burden of encoding the entire sentence.</p>
<p>Attention allows the decoder network to â€œfocusâ€ on a different part of<br>the encoderâ€™s outputs for every step of the decoderâ€™s own outputs. First<br>we calculate a set of <em>attention weights</em>. These will be multiplied by<br>the encoder output vectors to create a weighted combination. The result<br>(called <code>attn_applied</code> in the code) should contain information about<br>that specific part of the input sequence, and thus help the decoder<br>choose the right output words.</p>
<p>.. figure:: <a href="https://i.imgur.com/1152PYf.png" target="_blank" rel="noopener">https://i.imgur.com/1152PYf.png</a><br>   :alt:</p>
<p>Calculating the attention weights is done with another feed-forward<br>layer <code>attn</code>, using the decoderâ€™s input and hidden state as inputs.<br>Because there are sentences of all sizes in the training data, to<br>actually create and train this layer we have to choose a maximum<br>sentence length (input length, for encoder outputs) that it can apply<br>to. Sentences of the maximum length will use all the attention weights,<br>while shorter sentences will only use the first few.</p>
<p>.. figure:: /_static/img/seq-seq-images/attention-decoder-network.png<br>   :alt:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnDecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">        super(AttnDecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.dropout_p = dropout_p</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</span><br><span class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(self.dropout_p)</span><br><span class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</span><br><span class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden, encoder_outputs)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line"></span><br><span class="line">        attn_weights = F.softmax(</span><br><span class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)), dim=<span class="number">1</span>)</span><br><span class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>)</span><br><span class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure>
<div class="alert alert-info"><h4>Note</h4><p>There are other forms of attention that work around the length
  limitation by using a relative position approach. Read about "local
  attention" in `Effective Approaches to Attention-based Neural Machine
  Translation <https: arxiv.org abs 1508.04025>`__.</https:></p></div>

<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-Training-Data"><a href="#Preparing-Training-Data" class="headerlink" title="Preparing Training Data"></a>Preparing Training Data</h2><p>To train, for each pair we will need an input tensor (indexes of the<br>words in the input sentence) and target tensor (indexes of the words in<br>the target sentence). While creating these vectors we will append the<br>EOS token to both sequences.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">indexesFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [lang.word2index[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    indexes = indexesFromSentence(lang, sentence)</span><br><span class="line">    indexes.append(EOS_token)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(indexes, dtype=torch.long, device=device).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorsFromPair</span><span class="params">(pair)</span>:</span></span><br><span class="line">    input_tensor = tensorFromSentence(input_lang, pair[<span class="number">0</span>])</span><br><span class="line">    target_tensor = tensorFromSentence(output_lang, pair[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> (input_tensor, target_tensor)</span><br></pre></td></tr></table></figure>
<h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>To train we run the input sentence through the encoder, and keep track<br>of every output and the latest hidden state. Then the decoder is given<br>the <code>&lt;SOS&gt;</code> token as its first input, and the last hidden state of the<br>encoder as its first hidden state.</p>
<p>â€œTeacher forcingâ€ is the concept of using the real target outputs as<br>each next input, instead of using the decoderâ€™s guess as the next input.<br>Using teacher forcing causes it to converge faster but <code>when the trained
network is exploited, it may exhibit
instability &lt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&amp;rep=rep1&amp;type=pdf&gt;</code>__.</p>
<p>You can observe outputs of teacher-forced networks that read with<br>coherent grammar but wander far from the correct translation -<br>intuitively it has learned to represent the output grammar and can â€œpick<br>upâ€ the meaning once the teacher tells it the first few words, but it<br>has not properly learned how to create the sentence from the translation<br>in the first place.</p>
<p>Because of the freedom PyTorchâ€™s autograd gives us, we can randomly<br>choose to use teacher forcing or not with a simple if statement. Turn<br><code>teacher_forcing_ratio</code> up to use more of it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">teacher_forcing_ratio = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.zero_grad()</span><br><span class="line">    decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    input_length = input_tensor.size(<span class="number">0</span>)</span><br><span class="line">    target_length = target_tensor.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">        encoder_output, encoder_hidden = encoder(</span><br><span class="line">            input_tensor[ei], encoder_hidden)</span><br><span class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoder_input = torch.tensor([[SOS_token]], device=device)</span><br><span class="line"></span><br><span class="line">    decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">    use_teacher_forcing = <span class="literal">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_teacher_forcing:</span><br><span class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            decoder_input = target_tensor[di]  <span class="comment"># Teacher forcing</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            topv, topi = decoder_output.topk(<span class="number">1</span>)</span><br><span class="line">            decoder_input = topi.squeeze().detach()  <span class="comment"># detach from history as input</span></span><br><span class="line"></span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            <span class="keyword">if</span> decoder_input.item() == EOS_token:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.step()</span><br><span class="line">    decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item() / target_length</span><br></pre></td></tr></table></figure>
<p>This is a helper function to print time elapsed and estimated time<br>remaining given the current time and progress %.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asMinutes</span><span class="params">(s)</span>:</span></span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since, percent)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    es = s / (percent)</span><br><span class="line">    rs = es - s</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%s (- %s)'</span> % (asMinutes(s), asMinutes(rs))</span><br></pre></td></tr></table></figure>
<p>The whole training process looks like this:</p>
<ul>
<li>Start a timer</li>
<li>Initialize optimizers and criterion</li>
<li>Create set of training pairs</li>
<li>Start empty losses array for plotting</li>
</ul>
<p>Then we call <code>train</code> many times and occasionally print the progress (%<br>of examples, time so far, estimated time) and average loss.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainIters</span><span class="params">(encoder, decoder, n_iters, print_every=<span class="number">1000</span>, plot_every=<span class="number">100</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    plot_losses = []</span><br><span class="line">    print_loss_total = <span class="number">0</span>  <span class="comment"># Reset every print_every</span></span><br><span class="line">    plot_loss_total = <span class="number">0</span>  <span class="comment"># Reset every plot_every</span></span><br><span class="line"></span><br><span class="line">    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)</span><br><span class="line">    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)</span><br><span class="line">    training_pairs = [tensorsFromPair(random.choice(pairs))</span><br><span class="line">                      <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iters)]</span><br><span class="line">    criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">        training_pair = training_pairs[iter - <span class="number">1</span>]</span><br><span class="line">        input_tensor = training_pair[<span class="number">0</span>]</span><br><span class="line">        target_tensor = training_pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        loss = train(input_tensor, target_tensor, encoder,</span><br><span class="line">                     decoder, encoder_optimizer, decoder_optimizer, criterion)</span><br><span class="line">        print_loss_total += loss</span><br><span class="line">        plot_loss_total += loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">            print_loss_avg = print_loss_total / print_every</span><br><span class="line">            print_loss_total = <span class="number">0</span></span><br><span class="line">            print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start, iter / n_iters),</span><br><span class="line">                                         iter, iter / n_iters * <span class="number">100</span>, print_loss_avg))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">            plot_loss_avg = plot_loss_total / plot_every</span><br><span class="line">            plot_losses.append(plot_loss_avg)</span><br><span class="line">            plot_loss_total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    showPlot(plot_losses)</span><br></pre></td></tr></table></figure>
<h2 id="Plotting-results"><a href="#Plotting-results" class="headerlink" title="Plotting results"></a>Plotting results</h2><p>Plotting is done with matplotlib, using the array of loss values<br><code>plot_losses</code> saved while training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.switch_backend(<span class="string">'agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPlot</span><span class="params">(points)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># this locator puts ticks at regular intervals</span></span><br><span class="line">    loc = ticker.MultipleLocator(base=<span class="number">0.2</span>)</span><br><span class="line">    ax.yaxis.set_major_locator(loc)</span><br><span class="line">    plt.plot(points)</span><br></pre></td></tr></table></figure>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Evaluation is mostly the same as training, but there are no targets so<br>we simply feed the decoderâ€™s predictions back to itself for each step.<br>Every time it predicts a word we add it to the output string, and if it<br>predicts the EOS token we stop there. We also store the decoderâ€™s<br>attention outputs for display later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(encoder, decoder, sentence, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_tensor = tensorFromSentence(input_lang, sentence)</span><br><span class="line">        input_length = input_tensor.size()[<span class="number">0</span>]</span><br><span class="line">        encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">            encoder_output, encoder_hidden = encoder(input_tensor[ei],</span><br><span class="line">                                                     encoder_hidden)</span><br><span class="line">            encoder_outputs[ei] += encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        decoder_input = torch.tensor([[SOS_token]], device=device)  <span class="comment"># SOS</span></span><br><span class="line"></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">        decoded_words = []</span><br><span class="line">        decoder_attentions = torch.zeros(max_length, max_length)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(max_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            decoder_attentions[di] = decoder_attention.data</span><br><span class="line">            topv, topi = decoder_output.data.topk(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> topi.item() == EOS_token:</span><br><span class="line">                decoded_words.append(<span class="string">'&lt;EOS&gt;'</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                decoded_words.append(output_lang.index2word[topi.item()])</span><br><span class="line"></span><br><span class="line">            decoder_input = topi.squeeze().detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoded_words, decoder_attentions[:di + <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>We can evaluate random sentences from the training set and print out the<br>input, target, and output to make some subjective quality judgements:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateRandomly</span><span class="params">(encoder, decoder, n=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        pair = random.choice(pairs)</span><br><span class="line">        print(<span class="string">'&gt;'</span>, pair[<span class="number">0</span>])</span><br><span class="line">        print(<span class="string">'='</span>, pair[<span class="number">1</span>])</span><br><span class="line">        output_words, attentions = evaluate(encoder, decoder, pair[<span class="number">0</span>])</span><br><span class="line">        output_sentence = <span class="string">' '</span>.join(output_words)</span><br><span class="line">        print(<span class="string">'&lt;'</span>, output_sentence)</span><br><span class="line">        print(<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Training-and-Evaluating"><a href="#Training-and-Evaluating" class="headerlink" title="Training and Evaluating"></a>Training and Evaluating</h1><p>With all these helper functions in place (it looks like extra work, but<br>it makes it easier to run multiple experiments) we can actually<br>initialize a network and start training.</p>
<p>Remember that the input sentences were heavily filtered. For this small<br>dataset we can use relatively small networks of 256 hidden nodes and a<br>single GRU layer. After about 40 minutes on a MacBook CPU weâ€™ll get some<br>reasonable results.</p>
<p>.. Note::<br>   If you run this notebook you can train, interrupt the kernel,<br>   evaluate, and continue training later. Comment out the lines where the<br>   encoder and decoder are initialized and run <code>trainIters</code> again.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line">encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)</span><br><span class="line">attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">trainIters(encoder1, attn_decoder1, <span class="number">75000</span>, print_every=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>15m 15s (- 213m 42s) (5000 6%) 2.8335
36m 35s (- 237m 48s) (10000 13%) 2.3080
60m 22s (- 241m 29s) (15000 20%) 1.9768
86m 20s (- 237m 26s) (20000 26%) 1.7509
103m 50s (- 207m 41s) (25000 33%) 1.5515
123m 36s (- 185m 24s) (30000 40%) 1.3811
141m 30s (- 161m 43s) (35000 46%) 1.2262
161m 12s (- 141m 3s) (40000 53%) 1.1208
180m 57s (- 120m 38s) (45000 60%) 1.0367
195m 38s (- 97m 49s) (50000 66%) 0.9097
206m 41s (- 75m 9s) (55000 73%) 0.8348
217m 46s (- 54m 26s) (60000 80%) 0.7563
228m 59s (- 35m 13s) (65000 86%) 0.7075
246m 41s (- 17m 37s) (70000 93%) 0.6615
263m 49s (- 0m 0s) (75000 100%) 0.6048
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluateRandomly(encoder1, attn_decoder1)</span><br></pre></td></tr></table></figure>
<pre><code>&gt; tu m ennuies .
= you re annoying .
&lt; you re embarrassing . &lt;EOS&gt;

&gt; il est maintenant etudiant a la fac .
= he s now a college student .
&lt; he s now a student now . &lt;EOS&gt;

&gt; je ne suis pas votre ami .
= i m not your friend .
&lt; i m not your friend . &lt;EOS&gt;

&gt; je suis tres fatiguee par le dur labeur .
= i am very tired from the hard work .
&lt; i am very interested in the next . . &lt;EOS&gt;

&gt; ce sont des illets .
= they re carnations .
&lt; they re carnations . &lt;EOS&gt;

&gt; il est toujours en train de se plaindre .
= he is constantly complaining .
&lt; he is always complaining . &lt;EOS&gt;

&gt; je suis submerge de travail .
= i am swamped with work .
&lt; i am swamped with work . &lt;EOS&gt;

&gt; tu es mon meilleur ami .
= you re my best friend .
&lt; you are my best friend . &lt;EOS&gt;

&gt; je vous suis reconnaissant pour votre aide .
= i am grateful to you for your help .
&lt; i am grateful for your help . &lt;EOS&gt;

&gt; je vais te conter un secret .
= i m going to tell you a secret .
&lt; i m going to tell you a secret . &lt;EOS&gt;
</code></pre><h2 id="Visualizing-Attention"><a href="#Visualizing-Attention" class="headerlink" title="Visualizing Attention"></a>Visualizing Attention</h2><p>A useful property of the attention mechanism is its highly interpretable<br>outputs. Because it is used to weight specific encoder outputs of the<br>input sequence, we can imagine looking where the network is focused most<br>at each time step.</p>
<p>You could simply run <code>plt.matshow(attentions)</code> to see attention output<br>displayed as a matrix, with the columns being input steps and rows being<br>output steps:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output_words, attentions = evaluate(</span><br><span class="line">    encoder1, attn_decoder1, <span class="string">"je suis trop froid ."</span>)</span><br><span class="line">plt.matshow(attentions.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x2424cc9a438&gt;
</code></pre><p>For a better viewing experience we will do the extra work of adding axes<br>and labels:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showAttention</span><span class="params">(input_sentence, output_words, attentions)</span>:</span></span><br><span class="line">    <span class="comment"># Set up figure with colorbar</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    cax = ax.matshow(attentions.numpy(), cmap=<span class="string">'bone'</span>)</span><br><span class="line">    fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up axes</span></span><br><span class="line">    ax.set_xticklabels([<span class="string">''</span>] + input_sentence.split(<span class="string">' '</span>) +</span><br><span class="line">                       [<span class="string">'&lt;EOS&gt;'</span>], rotation=<span class="number">90</span>)</span><br><span class="line">    ax.set_yticklabels([<span class="string">''</span>] + output_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show label at every tick</span></span><br><span class="line">    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateAndShowAttention</span><span class="params">(input_sentence)</span>:</span></span><br><span class="line">    output_words, attentions = evaluate(</span><br><span class="line">        encoder1, attn_decoder1, input_sentence)</span><br><span class="line">    print(<span class="string">'input ='</span>, input_sentence)</span><br><span class="line">    print(<span class="string">'output ='</span>, <span class="string">' '</span>.join(output_words))</span><br><span class="line">    showAttention(input_sentence, output_words, attentions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle a cinq ans de moins que moi ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle est trop petit ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"je ne crains pas de mourir ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"c est un jeune directeur plein de talent ."</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input = elle a cinq ans de moins que moi .
output = she is five years younger than me . &lt;EOS&gt;
input = elle est trop petit .
output = she is too short . &lt;EOS&gt;
input = je ne crains pas de mourir .
output = i m not scared to die . &lt;EOS&gt;


C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator
  # Remove the CWD from sys.path while we load stuff.
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator
  # This is added back by InteractiveShellApp.init_path()
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.


input = c est un jeune directeur plein de talent .
output = he s a talented and . &lt;EOS&gt;
</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset</p>
<ul>
<li>Another language pair</li>
<li>Human â†’ Machine (e.g. IOT commands)</li>
<li>Chat â†’ Response</li>
<li>Question â†’ Answer</li>
</ul>
</li>
<li><p>Replace the embeddings with pre-trained word embeddings such as word2vec or<br>GloVe</p>
</li>
<li>Try with more layers, more hidden units, and more sentences. Compare<br>the training time and results.</li>
<li><p>If you use a translation file where pairs have two of the same phrase<br>(<code>I am test \t I am test</code>), you can use this as an autoencoder. Try<br>this:</p>
<ul>
<li>Train as an autoencoder</li>
<li>Save only the Encoder network</li>
<li>Train a new Decoder for translation from there</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-TORCHTEXTçš„æ–‡æœ¬åˆ†ç±»</title>
    <url>/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>Pytorch-Text-TORCHTEXTçš„æ–‡æœ¬åˆ†ç±»:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Text-Classification-with-TorchText"><a href="#Text-Classification-with-TorchText" class="headerlink" title="Text Classification with TorchText"></a>Text Classification with TorchText</h1><p>This tutorial shows how to use the text classification datasets<br>in <code>torchtext</code>, including</p>
<p>::</p>
<ul>
<li>AG_NEWS,</li>
<li>SogouNews,</li>
<li>DBpedia,</li>
<li>YelpReviewPolarity,</li>
<li>YelpReviewFull,</li>
<li>YahooAnswers,</li>
<li>AmazonReviewPolarity,</li>
<li>AmazonReviewFull</li>
</ul>
<p>This example shows how to train a supervised learning algorithm for<br>classification using one of these <code>TextClassification</code> datasets.</p>
<h2 id="Load-data-with-ngrams"><a href="#Load-data-with-ngrams" class="headerlink" title="Load data with ngrams"></a>Load data with ngrams</h2><p>A bag of ngrams feature is applied to capture some partial information<br>about the local word order. In practice, bi-gram or tri-gram are applied<br>to provide more benefits as word groups than only one word. An example:</p>
<p>::</p>
<p>   â€œload data with ngramsâ€<br>   Bi-grams results: â€œload dataâ€, â€œdata withâ€, â€œwith ngramsâ€<br>   Tri-grams results: â€œload data withâ€, â€œdata with ngramsâ€</p>
<p><code>TextClassification</code> Dataset supports the ngrams method. By setting<br>ngrams to 2, the example text in the dataset will be a list of single<br>words plus bi-grams string.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> text_classification</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> torchtext.utils <span class="keyword">import</span> extract_archive, unicode_csv_reader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> _csv_iterator,_create_data_from_iterator</span><br><span class="line">NGRAMS = <span class="number">2</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./.data'</span>):</span><br><span class="line">	os.mkdir(<span class="string">'./.data'</span>)</span><br><span class="line"><span class="comment"># train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](</span></span><br><span class="line"><span class="comment">#     root='./.data', ngrams=NGRAMS, vocab=None)</span></span><br><span class="line"> <span class="comment">#å®šä¹‰åˆ›å»ºæ•°æ®é›†å‡½æ•°ï¼ŒåŸå‡½æ•°åœ¨torchtext.datasets.text_classificationæ–‡ä»¶ä¸­ï¼Œæœ¬æ•™ç¨‹æ‰€éœ€å‚æ•°ç›´æ¥è®¾æˆäº†é»˜è®¤å€¼</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_setup_datasets</span><span class="params">(dataset_tar=<span class="string">'./.data/ag_news_csv.tar.gz'</span>,dataset_name=<span class="string">"AG_NEWS"</span>, root=<span class="string">'./.data'</span>, ngrams=NGRAMS, vocab=None, include_unk=False)</span>:</span></span><br><span class="line">    <span class="comment"># æ³¨é‡Šæ‰ä¸‹è½½æ•°æ®çš„ä»£ç </span></span><br><span class="line">    <span class="comment">#     dataset_tar = download_from_url(URLS[dataset_name], root=root)</span></span><br><span class="line">    extracted_files = extract_archive(dataset_tar)  <span class="comment">#è§£å‹æ•°æ®æ–‡ä»¶</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> extracted_files:</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'train.csv'</span>):</span><br><span class="line">            train_csv_path = fname</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'test.csv'</span>):</span><br><span class="line">            test_csv_path = fname</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        logging.info(<span class="string">'Building Vocab based on &#123;&#125;'</span>.format(train_csv_path))</span><br><span class="line">        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams)) <span class="comment">#åˆ›å»ºè¯å…¸</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(vocab, Vocab):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"Passed vocabulary is not of type Vocab"</span>)</span><br><span class="line">    logging.info(<span class="string">'Vocab has &#123;&#125; entries'</span>.format(len(vocab)))</span><br><span class="line">    logging.info(<span class="string">'Creating training data'</span>)</span><br><span class="line">    train_data, train_labels = _create_data_from_iterator(   <span class="comment">#åˆ›å»ºè®­ç»ƒæ•°æ®</span></span><br><span class="line">        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk) </span><br><span class="line">    logging.info(<span class="string">'Creating testing data'</span>)</span><br><span class="line">    test_data, test_labels = _create_data_from_iterator(   <span class="comment">#åˆ›å»ºæµ‹è¯•æ•°æ®</span></span><br><span class="line">        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk)</span><br><span class="line">    <span class="keyword">if</span> len(train_labels ^ test_labels) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Training and test labels don't match"</span>)</span><br><span class="line">    <span class="keyword">return</span> (TextClassificationDataset(vocab, train_data, train_labels),  <span class="comment">#è¿”å›æ•°æ®é›†å®ä¾‹</span></span><br><span class="line">            TextClassificationDataset(vocab, test_data, test_labels))</span><br><span class="line">train_dataset, test_dataset = _setup_datasets()</span><br><span class="line">BATCH_SIZE = <span class="number">16</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>120000lines [00:21, 5495.55lines/s]
120000lines [00:54, 2186.84lines/s]
7600lines [00:03, 1978.44lines/s]
</code></pre><h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>The model is composed of the<br><code>EmbeddingBag &lt;https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag&gt;</code>__<br>layer and the linear layer (see the figure below). <code>nn.EmbeddingBag</code><br>computes the mean value of a â€œbagâ€ of embeddings. The text entries here<br>have different lengths. <code>nn.EmbeddingBag</code> requires no padding here<br>since the text lengths are saved in offsets.</p>
<p>Additionally, since <code>nn.EmbeddingBag</code> accumulates the average across<br>the embeddings on the fly, <code>nn.EmbeddingBag</code> can enhance the<br>performance and memory efficiency to process a sequence of tensors.</p>
<p><img src="https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextSentiment</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_dim, num_class)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(embed_dim, num_class)</span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.5</span></span><br><span class="line">        self.embedding.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, offsets)</span>:</span></span><br><span class="line">        embedded = self.embedding(text, offsets)</span><br><span class="line">        <span class="keyword">return</span> self.fc(embedded)</span><br></pre></td></tr></table></figure>
<h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The AG_NEWS dataset has four labels and therefore the number of classes<br>is four.</p>
<p>::</p>
<p>   1 : World<br>   2 : Sports<br>   3 : Business<br>   4 : Sci/Tec</p>
<p>The vocab size is equal to the length of vocab (including single word<br>and ngrams). The number of classes is equal to the number of labels,<br>which is four in AG_NEWS case.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">VOCAB_SIZE = len(train_dataset.get_vocab())</span><br><span class="line">EMBED_DIM = <span class="number">32</span></span><br><span class="line">NUN_CLASS = len(train_dataset.get_labels())</span><br><span class="line">model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Functions-used-to-generate-batch"><a href="#Functions-used-to-generate-batch" class="headerlink" title="Functions used to generate batch"></a>Functions used to generate batch</h2><p>Since the text entries have different lengths, a custom function<br>generate_batch() is used to generate data batches and offsets. The<br>function is passed to <code>collate_fn</code> in <code>torch.utils.data.DataLoader</code>.<br>The input to <code>collate_fn</code> is a list of tensors with the size of<br>batch_size, and the <code>collate_fn</code> function packs them into a<br>mini-batch. Pay attention here and make sure that <code>collate_fn</code> is<br>declared as a top level def. This ensures that the function is available<br>in each worker.</p>
<p>The text entries in the original data batch input are packed into a list<br>and concatenated as a single tensor as the input of <code>nn.EmbeddingBag</code>.<br>The offsets is a tensor of delimiters to represent the beginning index<br>of the individual sequence in the text tensor. Label is a tensor saving<br>the labels of individual text entries.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch)</span>:</span></span><br><span class="line">    label = torch.tensor([entry[<span class="number">0</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch])</span><br><span class="line">    text = [entry[<span class="number">1</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch]</span><br><span class="line">    offsets = [<span class="number">0</span>] + [len(entry) <span class="keyword">for</span> entry <span class="keyword">in</span> text]</span><br><span class="line">    <span class="comment"># torch.Tensor.cumsum returns the cumulative sum</span></span><br><span class="line">    <span class="comment"># of elements in the dimension dim.</span></span><br><span class="line">    <span class="comment"># torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)</span></span><br><span class="line"></span><br><span class="line">    offsets = torch.tensor(offsets[:<span class="number">-1</span>]).cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    text = torch.cat(text)</span><br><span class="line">    <span class="keyword">return</span> text, offsets, label</span><br></pre></td></tr></table></figure>
<h2 id="Define-functions-to-train-the-model-and-evaluate-results"><a href="#Define-functions-to-train-the-model-and-evaluate-results" class="headerlink" title="Define functions to train the model and evaluate results."></a>Define functions to train the model and evaluate results.</h2><p><code>torch.utils.data.DataLoader &lt;https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader&gt;</code><strong><br>is recommended for PyTorch users, and it makes data loading in parallel<br>easily (a tutorial is<br><code>here &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code></strong>).<br>We use <code>DataLoader</code> here to load AG_NEWS datasets and send it to the<br>model for training/validation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_func</span><span class="params">(sub_train_)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the model</span></span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    train_acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>,</span><br><span class="line">                      collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> i, (text, offsets, cls) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        output = model(text, offsets)</span><br><span class="line">        loss = criterion(output, cls)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adjust the learning rate</span></span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_loss / len(sub_train_), train_acc / len(sub_train_)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(data_)</span>:</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> text, offsets, cls <span class="keyword">in</span> data:</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            output = model(text, offsets)</span><br><span class="line">            loss = criterion(output, cls)</span><br><span class="line">            loss += loss.item()</span><br><span class="line">            acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss / len(data_), acc / len(data_)</span><br></pre></td></tr></table></figure>
<h2 id="Split-the-dataset-and-run-the-model"><a href="#Split-the-dataset-and-run-the-model" class="headerlink" title="Split the dataset and run the model"></a>Split the dataset and run the model</h2><p>Since the original AG_NEWS has no valid dataset, we split the training<br>dataset into train/valid sets with a split ratio of 0.95 (train) and<br>0.05 (valid). Here we use<br><code>torch.utils.data.dataset.random_split &lt;https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split&gt;</code>__<br>function in PyTorch core library.</p>
<p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.<br>It is useful when training a classification problem with C classes.<br><code>SGD &lt;https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html&gt;</code></strong><br>implements stochastic gradient descent method as optimizer. The initial<br>learning rate is set to 4.0.<br><code>StepLR &lt;https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR&gt;</code>__<br>is used here to adjust the learning rate through epochs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> random_split</span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">min_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().to(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">4.0</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1</span>, gamma=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">train_len = int(len(train_dataset) * <span class="number">0.95</span>)</span><br><span class="line">sub_train_, sub_valid_ = \</span><br><span class="line">    random_split(train_dataset, [train_len, len(train_dataset) - train_len])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train_func(sub_train_)</span><br><span class="line">    valid_loss, valid_acc = test(sub_valid_)</span><br><span class="line"></span><br><span class="line">    secs = int(time.time() - start_time)</span><br><span class="line">    mins = secs / <span class="number">60</span></span><br><span class="line">    secs = secs % <span class="number">60</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch: %d'</span> %(epoch + <span class="number">1</span>), <span class="string">" | time in %d minutes, %d seconds"</span> %(mins, secs))</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>(train)\t|\tAcc: <span class="subst">&#123;train_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(train)'</span>)</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;valid_loss:<span class="number">.4</span>f&#125;</span>(valid)\t|\tAcc: <span class="subst">&#123;valid_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(valid)'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 1  | time in 1 minutes, 36 seconds
    Loss: 0.0261(train)    |    Acc: 84.8%(train)
    Loss: 0.0001(valid)    |    Acc: 90.5%(valid)
Epoch: 2  | time in 1 minutes, 57 seconds
    Loss: 0.0118(train)    |    Acc: 93.8%(train)
    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)
Epoch: 3  | time in 1 minutes, 35 seconds
    Loss: 0.0069(train)    |    Acc: 96.4%(train)
    Loss: 0.0001(valid)    |    Acc: 89.9%(valid)
Epoch: 4  | time in 1 minutes, 36 seconds
    Loss: 0.0038(train)    |    Acc: 98.1%(train)
    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)
Epoch: 5  | time in 1 minutes, 37 seconds
    Loss: 0.0023(train)    |    Acc: 99.0%(train)
    Loss: 0.0001(valid)    |    Acc: 91.5%(valid)
</code></pre><p>Running the model on GPU with the following information:</p>
<p>Epoch: 1 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0263(train)     |       Acc: 84.5%(train)
   Loss: 0.0001(valid)     |       Acc: 89.0%(valid)
</code></pre><p>Epoch: 2 | time in 0 minutes, 10 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0119(train)     |       Acc: 93.6%(train)
   Loss: 0.0000(valid)     |       Acc: 89.6%(valid)
</code></pre><p>Epoch: 3 | time in 0 minutes, 9 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0069(train)     |       Acc: 96.4%(train)
   Loss: 0.0000(valid)     |       Acc: 90.5%(valid)
</code></pre><p>Epoch: 4 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0038(train)     |       Acc: 98.2%(train)
   Loss: 0.0000(valid)     |       Acc: 90.4%(valid)
</code></pre><p>Epoch: 5 | time in 0 minutes, 11 seconds</p>
<p>::</p>
<pre><code>   Loss: 0.0022(train)     |       Acc: 99.0%(train)
   Loss: 0.0000(valid)     |       Acc: 91.0%(valid)
</code></pre><h2 id="Evaluate-the-model-with-test-dataset"><a href="#Evaluate-the-model-with-test-dataset" class="headerlink" title="Evaluate the model with test dataset"></a>Evaluate the model with test dataset</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Checking the results of test dataset...'</span>)</span><br><span class="line">test_loss, test_acc = test(test_dataset)</span><br><span class="line">print(<span class="string">f'\tLoss: <span class="subst">&#123;test_loss:<span class="number">.4</span>f&#125;</span>(test)\t|\tAcc: <span class="subst">&#123;test_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(test)'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Checking the results of test dataset...
    Loss: 0.0003(test)    |    Acc: 90.5%(test)
</code></pre><p>Checking the results of test datasetâ€¦</p>
<p>::</p>
<pre><code>   Loss: 0.0237(test)      |       Acc: 90.5%(test)
</code></pre><h2 id="Test-on-a-random-news"><a href="#Test-on-a-random-news" class="headerlink" title="Test on a random news"></a>Test on a random news</h2><p>Use the best model so far and test a golf news. The label information is<br>available<br><code>here &lt;https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> ngrams_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line">ag_news_label = &#123;<span class="number">1</span> : <span class="string">"World"</span>,</span><br><span class="line">                 <span class="number">2</span> : <span class="string">"Sports"</span>,</span><br><span class="line">                 <span class="number">3</span> : <span class="string">"Business"</span>,</span><br><span class="line">                 <span class="number">4</span> : <span class="string">"Sci/Tec"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(text, model, vocab, ngrams)</span>:</span></span><br><span class="line">    tokenizer = get_tokenizer(<span class="string">"basic_english"</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        text = torch.tensor([vocab[token]</span><br><span class="line">                            <span class="keyword">for</span> token <span class="keyword">in</span> ngrams_iterator(tokenizer(text), ngrams)])</span><br><span class="line">        output = model(text, torch.tensor([<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output.argmax(<span class="number">1</span>).item() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">ex_text_str = <span class="string">"MEMPHIS, Tenn. â€“ Four days ago, Jon Rahm was \</span></span><br><span class="line"><span class="string">    enduring the seasonâ€™s worst weather conditions on Sunday at The \</span></span><br><span class="line"><span class="string">    Open on his way to a closing 75 at Royal Portrush, which \</span></span><br><span class="line"><span class="string">    considering the wind and the rain was a respectable showing. \</span></span><br><span class="line"><span class="string">    Thursdayâ€™s first round at the WGC-FedEx St. Jude Invitational \</span></span><br><span class="line"><span class="string">    was another story. With temperatures in the mid-80s and hardly any \</span></span><br><span class="line"><span class="string">    wind, the Spaniard was 13 strokes better in a flawless round. \</span></span><br><span class="line"><span class="string">    Thanks to his best putting performance on the PGA Tour, Rahm \</span></span><br><span class="line"><span class="string">    finished with an 8-under 62 for a three-stroke lead, which \</span></span><br><span class="line"><span class="string">    was even more impressive considering heâ€™d never played the \</span></span><br><span class="line"><span class="string">    front nine at TPC Southwind."</span></span><br><span class="line"></span><br><span class="line">vocab = train_dataset.get_vocab()</span><br><span class="line">model = model.to(<span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"This is a %s news"</span> %ag_news_label[predict(ex_text_str, model, vocab, <span class="number">2</span>)])</span><br></pre></td></tr></table></figure>
<pre><code>This is a Sports news
</code></pre><p>This is a Sports news</p>
<p>You can find the code examples displayed in this note<br><code>here &lt;https://github.com/pytorch/text/tree/master/examples/text_classification&gt;</code>__.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-ä½¿ç”¨å­—ç¬¦çº§RNNå¯¹åç§°è¿›è¡Œåˆ†ç±»</title>
    <url>/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>Pytorch-Text-ä½¿ç”¨å­—ç¬¦çº§RNNå¯¹åç§°è¿›è¡Œåˆ†ç±»:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Classifying Names with a Character-Level RNN</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>We will be building and training a basic character-level RNN to classify<br>words. This tutorial, along with the following two, show how to do<br>preprocess data for NLP modeling â€œfrom scratchâ€, in particular not using<br>many of the convenience functions of <code>torchtext</code>, so you can see how<br>preprocessing for NLP modeling works at a low level.</p>
<p>A character-level RNN reads words as a series of characters -<br>outputting a prediction and â€œhidden stateâ€ at each step, feeding its<br>previous hidden state into each next step. We take the final prediction<br>to be the output, i.e. which class the word belongs to.</p>
<p>Specifically, weâ€™ll train on a few thousand surnames from 18 languages<br>of origin, and predict which language a name is from based on the<br>spelling:</p>
<p>::</p>
<pre><code>$ python predict.py Hinton
(-0.47) Scottish
(-1.52) English
(-3.57) Irish

$ python predict.py Schmidhuber
(-0.19) German
(-2.48) Czech
(-2.68) Dutch
</code></pre><p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about RNNs and how they work:</p>
<ul>
<li><code>The Unreasonable Effectiveness of Recurrent Neural
Networks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li>
<li><code>Understanding LSTM
Networks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li>
</ul>
<h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>Included in the <code>data/names</code> directory are 18 text files named as<br>â€œ[Language].txtâ€. Each file contains a bunch of names, one name per<br>line, mostly romanized (but we still need to convert from Unicode to<br>ASCII).</p>
<p>Weâ€™ll end up with a dictionary of lists of names per language,<br><code>{language: [names ...]}</code>. The generic variables â€œcategoryâ€ and â€œlineâ€<br>(for language and name in our case) are used for later extensibility.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># glob.globè¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line">print(findFiles(<span class="string">'data/names/*.txt'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></span><br><span class="line">n_letters = len(all_letters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># åœ¨Unicodeä¸­ï¼ŒæŸäº›å­—ç¬¦èƒ½å¤Ÿç”¨å¤šä¸ªåˆæ³•çš„ç¼–ç è¡¨ç¤ºï¼Œåœ¨éœ€è¦æ¯”è¾ƒå­—ç¬¦ä¸²çš„ç¨‹åºä¸­ä½¿ç”¨å­—ç¬¦çš„å¤šç§è¡¨ç¤ºä¼šäº§ç”Ÿé—®é¢˜ã€‚ </span></span><br><span class="line"><span class="comment"># ä¸ºäº†ä¿®æ­£è¿™ä¸ªé—®é¢˜ï¼Œä½ å¯ä»¥ä½¿ç”¨unicodedataæ¨¡å—å…ˆå°†æ–‡æœ¬æ ‡å‡†åŒ–</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">print(unicodeToAscii(<span class="string">'ÅšlusÃ rski'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of names per language</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;data/names\\Arabic.txt&#39;, &#39;data/names\\Chinese.txt&#39;, &#39;data/names\\Czech.txt&#39;, &#39;data/names\\Dutch.txt&#39;, &#39;data/names\\English.txt&#39;, &#39;data/names\\French.txt&#39;, &#39;data/names\\German.txt&#39;, &#39;data/names\\Greek.txt&#39;, &#39;data/names\\Irish.txt&#39;, &#39;data/names\\Italian.txt&#39;, &#39;data/names\\Japanese.txt&#39;, &#39;data/names\\Korean.txt&#39;, &#39;data/names\\Polish.txt&#39;, &#39;data/names\\Portuguese.txt&#39;, &#39;data/names\\Russian.txt&#39;, &#39;data/names\\Scottish.txt&#39;, &#39;data/names\\Spanish.txt&#39;, &#39;data/names\\Vietnamese.txt&#39;]
Slusarski
</code></pre><p>Now we have <code>category_lines</code>, a dictionary mapping each category<br>(language) to a list of lines (names). We also kept track of<br><code>all_categories</code> (just a list of languages) and <code>n_categories</code> for<br>later reference.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(category_lines[<span class="string">'Italian'</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;Abandonato&#39;, &#39;Abatangelo&#39;, &#39;Abatantuono&#39;, &#39;Abate&#39;, &#39;Abategiovanni&#39;]
</code></pre><h2 id="Turning-Names-into-Tensors"><a href="#Turning-Names-into-Tensors" class="headerlink" title="Turning Names into Tensors"></a>Turning Names into Tensors</h2><p>Now that we have all the names organized, we need to turn them into<br>Tensors to make any use of them.</p>
<p>To represent a single letter, we use a â€œone-hot vectorâ€ of size<br><code>&lt;1 x n_letters&gt;</code>. A one-hot vector is filled with 0s except for a 1<br>at index of the current letter, e.g. <code>&quot;b&quot; = &lt;0 1 0 0 0 ...&gt;</code>.</p>
<p>To make a word we join a bunch of those into a 2D matrix<br><code>&lt;line_length x 1 x n_letters&gt;</code>.</p>
<p>That extra 1 dimension is because PyTorch assumes everything is in<br>batches - weâ€™re just using a batch size of 1 here.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find letter index from all_letters, e.g. "a" = 0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToIndex</span><span class="params">(letter)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> all_letters.find(letter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToTensor</span><span class="params">(letter)</span>:</span></span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_letters)</span><br><span class="line">    tensor[<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a line into a &lt;line_length x 1 x n_letters&gt;,</span></span><br><span class="line"><span class="comment"># or an array of one-hot letter vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lineToTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li, letter <span class="keyword">in</span> enumerate(line):</span><br><span class="line">        tensor[li][<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line">print(letterToTensor(<span class="string">'J'</span>))</span><br><span class="line"></span><br><span class="line">print(lineToTensor(<span class="string">'Jones'</span>).size())</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0.]])
torch.Size([5, 1, 57])
</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>Before autograd, creating a recurrent neural network in Torch involved<br>cloning the parameters of a layer over several timesteps. The layers<br>held hidden state and gradients which are now entirely handled by the<br>graph itself. This means you can implement a RNN in a very â€œpureâ€ way,<br>as regular feed-forward layers.</p>
<p>This RNN module (mostly copied from <code>the PyTorch for Torch users
tutorial &lt;https://pytorch.org/tutorials/beginner/former_torchies/
nn_tutorial.html#example-2-recurrent-net&gt;</code>__)<br>is just 2 linear layers which operate on an input and hidden state, with<br>a LogSoftmax layer after the output.<br><img src="https://i.imgur.com/Z2xbySO.png" alt></p>
<h1 id="Torch-cat"><a href="#Torch-cat" class="headerlink" title="Torch.cat()"></a>Torch.cat()</h1><p>catæ˜¯concatnateçš„æ„æ€ï¼šæ‹¼æ¥ï¼Œè”ç³»åœ¨ä¸€èµ·ã€‚</p>
<p>å…ˆè¯´cat( )çš„æ™®é€šç”¨æ³•</p>
<p>å¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªtensoræ˜¯Aå’ŒBï¼Œæƒ³æŠŠä»–ä»¬æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œéœ€è¦å¦‚ä¸‹æ“ä½œï¼š<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C &#x3D; torch.cat( (A,B),0 )  #æŒ‰ç»´æ•°0æ‹¼æ¥ï¼ˆç«–ç€æ‹¼ï¼‰</span><br><span class="line">C &#x3D; torch.cat( (A,B),1 )  #æŒ‰ç»´æ•°1æ‹¼æ¥ï¼ˆæ¨ªç€æ‹¼ï¼‰</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; A&#x3D;torch.ones(2,3)    #2x3çš„å¼ é‡ï¼ˆçŸ©é˜µï¼‰                                     </span><br><span class="line">&gt;&gt;&gt; A</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.]])</span><br><span class="line">&gt;&gt;&gt; B&#x3D;2*torch.ones(4,3)  #4x3çš„å¼ é‡ï¼ˆçŸ©é˜µï¼‰                                    </span><br><span class="line">&gt;&gt;&gt; B</span><br><span class="line">tensor([[ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,B),0)  #æŒ‰ç»´æ•°0ï¼ˆè¡Œï¼‰æ‹¼æ¥</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">         [ 1.,  1.,  1.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([6, 3])</span><br><span class="line">&gt;&gt;&gt; D&#x3D;2*torch.ones(2,4) #2x4çš„å¼ é‡ï¼ˆçŸ©é˜µï¼‰</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,D),1)#æŒ‰ç»´æ•°1ï¼ˆåˆ—ï¼‰æ‹¼æ¥</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],</span><br><span class="line">        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([2, 7])</span><br></pre></td></tr></table></figure><br>å…¶æ¬¡ï¼Œcatè¿˜å¯ä»¥æŠŠlistä¸­çš„tensoræ‹¼æ¥èµ·æ¥ã€‚<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line"></span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line">len(x1)</span><br><span class="line">&gt;&gt;3</span><br><span class="line"></span><br><span class="line">x1</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">[tensor([[2.],</span><br><span class="line">         [4.],</span><br><span class="line">         [6]]),tensor([[2.],[4.],[6.]]),tensor([[2.],[4.],[6.]])]</span><br><span class="line"></span><br><span class="line">x2 &#x3D; &#x3D; torch.cat((x1),1)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">&gt;&gt;tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br><span class="line"></span><br><span class="line">type(x1)</span><br><span class="line">&gt;&gt; list</span><br></pre></td></tr></table></figure><br>ä¸Šé¢çš„ä»£ç å¯ä»¥åˆæˆä¸€è¡Œæ¥å†™ï¼š<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x2 &#x3D; torch.cat([x*2 for i in range(1,4)])</span><br><span class="line">x2</span><br><span class="line">&gt;&gt;</span><br><span class="line">tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(input_size + hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        combined = torch.cat((input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(combined)</span><br><span class="line">        output = self.i2o(combined)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br><span class="line"></span><br><span class="line">n_hidden = <span class="number">128</span></span><br><span class="line">rnn = RNN(n_letters, n_hidden, n_categories)</span><br></pre></td></tr></table></figure>
<p>To run a step of this network we need to pass an input (in our case, the<br>Tensor for the current letter) and a previous hidden state (which we<br>initialize as zeros at first). Weâ€™ll get back the output (probability of<br>each language) and a next hidden state (which we keep for the next<br>step).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = letterToTensor(<span class="string">'A'</span>)</span><br><span class="line">hidden =torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input, hidden)</span><br></pre></td></tr></table></figure>
<p>For the sake of efficiency we donâ€™t want to be creating a new Tensor for<br>every step, so we will use <code>lineToTensor</code> instead of<br><code>letterToTensor</code> and use slices. This could be further optimized by<br>pre-computing batches of Tensors.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input = lineToTensor(<span class="string">'Albert'</span>)</span><br><span class="line">hidden = torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input[<span class="number">0</span>], hidden)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[-2.8013, -3.0086, -2.8838, -2.8652, -2.8300, -2.7883, -2.8614, -2.9069,
         -2.9787, -2.8336, -2.9085, -2.9029, -2.9565, -2.8683, -2.9269, -2.9332,
         -2.9334, -2.8689]], grad_fn=&lt;LogSoftmaxBackward&gt;)
</code></pre><p>As you can see the output is a <code>&lt;1 x n_categories&gt;</code> Tensor, where<br>every item is the likelihood of that category (higher is more likely).</p>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>Before going into training we should make a few helper functions. The<br>first is to interpret the output of the network, which we know to be a<br>likelihood of each category. We can use <code>Tensor.topk</code> to get the index<br>of the greatest value:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryFromOutput</span><span class="params">(output)</span>:</span></span><br><span class="line">    top_n, top_i = output.topk(<span class="number">1</span>)</span><br><span class="line">    category_i = top_i[<span class="number">0</span>].item()</span><br><span class="line">    <span class="keyword">return</span> all_categories[category_i], category_i</span><br><span class="line"></span><br><span class="line">print(categoryFromOutput(output))</span><br></pre></td></tr></table></figure>
<pre><code>(&#39;French&#39;, 5)
</code></pre><p>We will also want a quick way to get a training example (a name and its<br>language):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)</span><br><span class="line">    line_tensor = lineToTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category, line, category_tensor, line_tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    print(<span class="string">'category ='</span>, category, <span class="string">'/ line ='</span>, line)</span><br></pre></td></tr></table></figure>
<pre><code>category = German / line = Reiter
category = Spanish / line = Roldan
category = Vietnamese / line = Lieu
category = Japanese / line = Maita
category = Polish / line = Wojda
category = Greek / line = Forakis
category = Italian / line = Voltolini
category = Scottish / line = Hill
category = Portuguese / line = Nunes
category = Scottish / line = Wilson
</code></pre><h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>Now all it takes to train this network is show it a bunch of examples,<br>have it make guesses, and tell it if itâ€™s wrong.</p>
<p>For the loss function <code>nn.NLLLoss</code> is appropriate, since the last<br>layer of the RNN is <code>nn.LogSoftmax</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br></pre></td></tr></table></figure>
<p>Each loop of training will:</p>
<ul>
<li>Create input and target tensors</li>
<li>Create a zeroed initial hidden state</li>
<li><p>Read each letter in and</p>
<ul>
<li>Keep hidden state for next letter</li>
</ul>
</li>
<li><p>Compare final output to target</p>
</li>
<li>Back-propagate</li>
<li>Return the output and loss</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.005</span> <span class="comment"># If you set this too high, it might explode. If too low, it might not learn</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    loss = criterion(output, category_tensor)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add parameters' gradients to their values, multiplied by learning rate</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item()</span><br></pre></td></tr></table></figure>
<p>Now we just have to run that with a bunch of examples. Since the<br><code>train</code> function returns both the output and loss we can print its<br>guesses and also keep track of loss for plotting. Since there are 1000s<br>of examples we print only every <code>print_every</code> examples, and take an<br>average of the loss.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep track of losses for plotting</span></span><br><span class="line">current_loss = <span class="number">0</span></span><br><span class="line">all_losses = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output, loss = train(category_tensor, line_tensor)</span><br><span class="line">    current_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print iter number, loss, name and guess</span></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        guess, guess_i = categoryFromOutput(output)</span><br><span class="line">        correct = <span class="string">'âœ“'</span> <span class="keyword">if</span> guess == category <span class="keyword">else</span> <span class="string">'âœ— (%s)'</span> % category</span><br><span class="line">        print(<span class="string">'%d %d%% (%s) %.4f %s / %s %s'</span> % (iter, iter / n_iters * <span class="number">100</span>, timeSince(start), loss, line, guess, correct))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add current loss avg to list of losses</span></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(current_loss / plot_every)</span><br><span class="line">        current_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>5000 5% (0m 26s) 2.2285 Tsen / Chinese âœ“
10000 10% (0m 45s) 1.7232 Ursler / Dutch âœ— (German)
15000 15% (1m 2s) 3.3629 Power / German âœ— (Irish)
20000 20% (1m 19s) 1.1055 Ferreiro / Portuguese âœ“
25000 25% (1m 37s) 1.1813 Do / Vietnamese âœ“
30000 30% (1m 53s) 1.9952 Pak / Chinese âœ— (Korean)
35000 35% (2m 11s) 1.0685 Zientek / Czech âœ— (Polish)
40000 40% (2m 34s) 0.3656 Arnoni / Italian âœ“
45000 45% (2m 56s) 2.5408 Schuchardt / Czech âœ— (German)
50000 50% (3m 19s) 0.9137 Ellwood / English âœ“
55000 55% (3m 43s) 2.6915 Griffiths / Greek âœ— (English)
60000 60% (4m 5s) 0.0363 Quach / Vietnamese âœ“
65000 65% (4m 27s) 0.1474 Rijnders / Dutch âœ“
70000 70% (4m 49s) 1.8646 Clements / Portuguese âœ— (English)
75000 75% (5m 13s) 0.3696 Bobienski / Polish âœ“
80000 80% (5m 37s) 1.0411 Klerx / Dutch âœ“
85000 85% (5m 58s) 2.3457 Maria / Spanish âœ— (Portuguese)
90000 90% (6m 24s) 0.5750 Echevarria / Spanish âœ“
95000 95% (6m 47s) 0.0762 Ohmiya / Japanese âœ“
100000 100% (7m 9s) 2.5785 Kock / Czech âœ— (German)
</code></pre><h2 id="Plotting-the-Results"><a href="#Plotting-the-Results" class="headerlink" title="Plotting the Results"></a>Plotting the Results</h2><p>Plotting the historical loss from <code>all_losses</code> shows the network<br>learning:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x18c353c04a8&gt;]
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_25_1.png" alt="png"></p>
<h1 id="Evaluating-the-Results"><a href="#Evaluating-the-Results" class="headerlink" title="Evaluating the Results"></a>Evaluating the Results</h1><p>To see how well the network performs on different categories, we will<br>create a confusion matrix, indicating for every actual language (rows)<br>which language the network guesses (columns). To calculate the confusion<br>matrix a bunch of samples are run through the network with<br><code>evaluate()</code>, which is the same as <code>train()</code> minus the backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Keep track of correct guesses in a confusion matrix</span></span><br><span class="line">confusion = torch.zeros(n_categories, n_categories)</span><br><span class="line">n_confusion = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Just return an output given a line</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># Go through a bunch of examples and record which are correctly guessed</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_confusion):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output = evaluate(line_tensor)</span><br><span class="line">    guess, guess_i = categoryFromOutput(output)</span><br><span class="line">    category_i = all_categories.index(category)</span><br><span class="line">    confusion[category_i][guess_i] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize by dividing every row by its sum</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_categories):</span><br><span class="line">    confusion[i] = confusion[i] / confusion[i].sum()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up plot</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(confusion.numpy())</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up axes</span></span><br><span class="line">ax.set_xticklabels([<span class="string">''</span>] + all_categories, rotation=<span class="number">90</span>)</span><br><span class="line">ax.set_yticklabels([<span class="string">''</span>] + all_categories)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Force label at every tick</span></span><br><span class="line">ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sphinx_gallery_thumbnail_number = 2</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator
C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:34: UserWarning: FixedFormatter should only be used together with FixedLocator
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_27_1.png" alt="png"></p>
<p>You can pick out bright spots off the main axis that show which<br>languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish<br>for Italian. It seems to do very well with Greek, and very poorly with<br>English (perhaps because of overlap with other languages).</p>
<h2 id="Running-on-User-Input"><a href="#Running-on-User-Input" class="headerlink" title="Running on User Input"></a>Running on User Input</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(input_line, n_predictions=<span class="number">3</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'\n&gt; %s'</span> % input_line)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = evaluate(lineToTensor(input_line))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get top N categories</span></span><br><span class="line">        topv, topi = output.topk(n_predictions, <span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">        predictions = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_predictions):</span><br><span class="line">            value = topv[<span class="number">0</span>][i].item()</span><br><span class="line">            category_index = topi[<span class="number">0</span>][i].item()</span><br><span class="line">            print(<span class="string">'(%.2f) %s'</span> % (value, all_categories[category_index]))</span><br><span class="line">            predictions.append([value, all_categories[category_index]])</span><br><span class="line"></span><br><span class="line">predict(<span class="string">'Dovesky'</span>)</span><br><span class="line">predict(<span class="string">'Jackson'</span>)</span><br><span class="line">predict(<span class="string">'Satoshi'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&gt; Dovesky
(-0.80) Russian
(-1.53) Czech
(-1.98) English

&gt; Jackson
(-0.33) Scottish
(-1.99) English
(-3.31) Russian

&gt; Satoshi
(-0.90) Italian
(-1.65) Japanese
(-2.18) Arabic
</code></pre><p>The final versions of the scripts <code>in the Practical PyTorch
repo &lt;https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification&gt;</code>__<br>split the above code into a few files:</p>
<ul>
<li><code>data.py</code> (loads files)</li>
<li><code>model.py</code> (defines the RNN)</li>
<li><code>train.py</code> (runs training)</li>
<li><code>predict.py</code> (runs <code>predict()</code> with command line arguments)</li>
<li><code>server.py</code> (serve prediction as a JSON API with bottle.py)</li>
</ul>
<p>Run <code>train.py</code> to train and save the network.</p>
<p>Run <code>predict.py</code> with a name to view predictions:</p>
<p>::</p>
<pre><code>$ python predict.py Hazaki
(-0.42) Japanese
(-1.39) Polish
(-3.51) Czech
</code></pre><p>Run <code>server.py</code> and visit <a href="http://localhost:5533/Yourname" target="_blank" rel="noopener">http://localhost:5533/Yourname</a> to get JSON<br>output of predictions.</p>
<h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset of line -&gt; category, for example:</p>
<ul>
<li>Any word -&gt; language</li>
<li>First name -&gt; gender</li>
<li>Character name -&gt; writer</li>
<li>Page title -&gt; blog or subreddit</li>
</ul>
</li>
<li><p>Get better results with a bigger and/or better shaped network</p>
<ul>
<li>Add more linear layers</li>
<li>Try the <code>nn.LSTM</code> and <code>nn.GRU</code> layers</li>
<li>Combine multiple of these RNNs as a higher level network</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-TORCHTEXTçš„è¯­è¨€ç¿»è¯‘</title>
    <url>/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<p>Pytorch-Text-TORCHTEXTçš„è¯­è¨€ç¿»è¯‘:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Language-Translation-with-TorchText"><a href="#Language-Translation-with-TorchText" class="headerlink" title="Language Translation with TorchText"></a>Language Translation with TorchText</h1><p>This tutorial shows how to use several convenience classes of <code>torchtext</code> to preprocess<br>data from a well-known dataset containing sentences in both English and German and use it to<br>train a sequence-to-sequence model with attention that can translate German sentences<br>into English.</p>
<p>It is based off of<br><code>this tutorial &lt;https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code><strong><br>from PyTorch community member <code>Ben Trevett &lt;https://github.com/bentrevett&gt;</code></strong><br>and was created by <code>Seth Weidman &lt;https://github.com/SethHWeidman/&gt;</code>__ with Benâ€™s permission.</p>
<p>By the end of this tutorial, you will be able to:</p>
<ul>
<li>Preprocess sentences into a commonly-used format for NLP modeling using the following <code>torchtext</code> convenience classes:<ul>
<li><code>TranslationDataset &lt;https://torchtext.readthedocs.io/en/latest/datasets.html#torchtext.datasets.TranslationDataset&gt;</code>__</li>
<li><code>Field &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field&gt;</code>__</li>
<li><code>BucketIterator &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator&gt;</code>__</li>
</ul>
</li>
</ul>
<h2 id="Field-and-TranslationDataset"><a href="#Field-and-TranslationDataset" class="headerlink" title="Field and TranslationDataset"></a><code>Field</code> and <code>TranslationDataset</code></h2><p><code>torchtext</code> has utilities for creating datasets that can be easily<br>iterated through for the purposes of creating a language translation<br>model. One key class is a<br><code>Field &lt;https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L64&gt;</code><strong>,<br>which specifies the way each sentence should be preprocessed, and another is the<br><code>TranslationDataset</code> ; <code>torchtext</code><br>has several such datasets; in this tutorial weâ€™ll use the<br><code>Multi30k dataset &lt;https://github.com/multi30k/dataset&gt;</code></strong>, which contains about<br>30,000 sentences (averaging about 13 words in length) in both English and German.</p>
<p>Note: the tokenization in this tutorial requires <code>Spacy &lt;https://spacy.io&gt;</code><strong><br>We use Spacy because it provides strong support for tokenization in languages<br>other than English. <code>torchtext</code> provides a <code>basic_english</code> tokenizer<br>and supports other tokenizers for English (e.g.<br><code>Moses &lt;https://bitbucket.org/luismsgomes/mosestokenizer/src/default/&gt;</code></strong>)<br>but for language translation - where multiple languages are required -<br>Spacy is your best bet.</p>
<p>To run this tutorial, first install <code>spacy</code> using <code>pip</code> or <code>conda</code>.<br>Next, download the raw data for the English and German Spacy tokenizers:</p>
<p>::</p>
<p>   python -m spacy download en<br>   python -m spacy download de</p>
<p>With Spacy installed, the following code will tokenize each of the sentences<br>in the <code>TranslationDataset</code> based on the tokenizer defined in the <code>Field</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> Multi30k</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, BucketIterator</span><br><span class="line"></span><br><span class="line">SRC = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"de"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">TRG = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"en"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_data, valid_data, test_data = Multi30k.splits(exts = (<span class="string">'.de'</span>, <span class="string">'.en'</span>),</span><br><span class="line">                                                    fields = (SRC, TRG))</span><br></pre></td></tr></table></figure>
<pre><code>downloading training.tar.gz


.data\multi30k\training.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:35&lt;00:00, 33.8kB/s]


downloading validation.tar.gz


.data\multi30k\validation.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.3k/46.3k [00:01&lt;00:00, 35.0kB/s]


downloading mmt_task1_test2016.tar.gz


.data\multi30k\mmt_task1_test2016.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.2k/66.2k [00:02&lt;00:00, 26.5kB/s]
</code></pre><p>Now that weâ€™ve defined <code>train_data</code>, we can see an extremely useful<br>feature of <code>torchtext</code>â€˜s <code>Field</code>: the <code>build_vocab</code> method<br>now allows us to create the vocabulary associated with each language</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SRC.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br><span class="line">TRG.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>Once these lines of code have been run, <code>SRC.vocab.stoi</code> will  be a<br>dictionary with the tokens in the vocabulary as keys and their<br>corresponding indices as values; <code>SRC.vocab.itos</code> will be the same<br>dictionary with the keys and values swapped. We wonâ€™t make extensive<br>use of this fact in this tutorial, but this will likely be useful in<br>other NLP tasks youâ€™ll encounter.</p>
<h2 id="BucketIterator"><a href="#BucketIterator" class="headerlink" title="BucketIterator"></a><code>BucketIterator</code></h2><p>The last <code>torchtext</code> specific feature weâ€™ll use is the <code>BucketIterator</code>,<br>which is easy to use since it takes a <code>TranslationDataset</code> as its<br>first argument. Specifically, as the docs say:<br>Defines an iterator that batches examples of similar lengths together.<br>Minimizes amount of padding needed while producing freshly shuffled<br>batches for each new epoch. See pool for the bucketing procedure used.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_iterator, valid_iterator, test_iterator = BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data),</span><br><span class="line">    batch_size = BATCH_SIZE,</span><br><span class="line">    device = device)</span><br></pre></td></tr></table></figure>
<p>These iterators can be called just like <code>DataLoader</code>s; below, in<br>the <code>train</code> and <code>evaluate</code> functions, they are called simply with:</p>
<p>::</p>
<p>   for i, batch in enumerate(iterator):</p>
<p>Each <code>batch</code> then has <code>src</code> and <code>trg</code> attributes:</p>
<p>::</p>
<p>   src = batch.src<br>   trg = batch.trg</p>
<h2 id="Defining-our-nn-Module-and-Optimizer"><a href="#Defining-our-nn-Module-and-Optimizer" class="headerlink" title="Defining our nn.Module and Optimizer"></a>Defining our <code>nn.Module</code> and <code>Optimizer</code></h2><p>Thatâ€™s mostly it from a <code>torchtext</code> perspecive: with the dataset built<br>and the iterator defined, the rest of this tutorial simply defines our<br>model as an <code>nn.Module</code>, along with an <code>Optimizer</code>, and then trains it.</p>
<p>Our model specifically, follows the architecture described<br><code>here &lt;https://arxiv.org/abs/1409.0473&gt;</code><strong> (you can find a<br>significantly more commented version<br><code>here &lt;https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code></strong>).</p>
<p>Note: this model is just an example model that can be used for language<br>translation; we choose it because it is a standard model for the task,<br>not because it is the recommended model to use for translation. As youâ€™re<br>likely aware, state-of-the-art models are currently based on Transformers;<br>you can see PyTorchâ€™s capabilities for implementing Transformer layers<br><code>here &lt;https://pytorch.org/docs/stable/nn.html#transformer-layers&gt;</code>__; and<br>in particular, the â€œattentionâ€ used in the model below is different from<br>the multi-headed self-attention present in a transformer model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Tuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 input_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: float)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(enc_hid_dim * <span class="number">2</span>, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(src))</span><br><span class="line"></span><br><span class="line">        outputs, hidden = self.rnn(embedded)</span><br><span class="line"></span><br><span class="line">        hidden = torch.tanh(self.fc(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attn_dim: int)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn_in = (enc_hid_dim * <span class="number">2</span>) + dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn = nn.Linear(self.attn_in, attn_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        src_len = encoder_outputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        repeated_decoder_hidden = decoder_hidden.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, src_len, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        energy = torch.tanh(self.attn(torch.cat((</span><br><span class="line">            repeated_decoder_hidden,</span><br><span class="line">            encoder_outputs),</span><br><span class="line">            dim = <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">        attention = torch.sum(energy, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> F.softmax(attention, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 output_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attention: nn.Module)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.attention = attention</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU((enc_hid_dim * <span class="number">2</span>) + emb_dim, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weighted_encoder_rep</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                              decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        a = self.attention(decoder_hidden, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        a = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = torch.bmm(a, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weighted_encoder_rep</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                input: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        input = input.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(input))</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,</span><br><span class="line">                                                          encoder_outputs)</span><br><span class="line"></span><br><span class="line">        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        embedded = embedded.squeeze(<span class="number">0</span>)</span><br><span class="line">        output = output.squeeze(<span class="number">0</span>)</span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = self.out(torch.cat((output,</span><br><span class="line">                                     weighted_encoder_rep,</span><br><span class="line">                                     embedded), dim = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, decoder_hidden.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 decoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device: torch.device)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                trg: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                teacher_forcing_ratio: float = <span class="number">0.5</span>)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        batch_size = src.shape[<span class="number">1</span>]</span><br><span class="line">        max_len = trg.shape[<span class="number">0</span>]</span><br><span class="line">        trg_vocab_size = self.decoder.output_dim</span><br><span class="line"></span><br><span class="line">        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)</span><br><span class="line"></span><br><span class="line">        encoder_outputs, hidden = self.encoder(src)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># first input to the decoder is the &lt;sos&gt; token</span></span><br><span class="line">        output = trg[<span class="number">0</span>,:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, max_len):</span><br><span class="line">            output, hidden = self.decoder(output, hidden, encoder_outputs)</span><br><span class="line">            outputs[t] = output</span><br><span class="line">            teacher_force = random.random() &lt; teacher_forcing_ratio</span><br><span class="line">            top1 = output.max(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            output = (trg[t] <span class="keyword">if</span> teacher_force <span class="keyword">else</span> top1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = len(SRC.vocab)</span><br><span class="line">OUTPUT_DIM = len(TRG.vocab)</span><br><span class="line"><span class="comment"># ENC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># DEC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># ENC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># DEC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># ATTN_DIM = 64</span></span><br><span class="line"><span class="comment"># ENC_DROPOUT = 0.5</span></span><br><span class="line"><span class="comment"># DEC_DROPOUT = 0.5</span></span><br><span class="line"></span><br><span class="line">ENC_EMB_DIM = <span class="number">32</span></span><br><span class="line">DEC_EMB_DIM = <span class="number">32</span></span><br><span class="line">ENC_HID_DIM = <span class="number">64</span></span><br><span class="line">DEC_HID_DIM = <span class="number">64</span></span><br><span class="line">ATTN_DIM = <span class="number">8</span></span><br><span class="line">ENC_DROPOUT = <span class="number">0.5</span></span><br><span class="line">DEC_DROPOUT = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)</span><br><span class="line"></span><br><span class="line">attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)</span><br><span class="line"></span><br><span class="line">dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)</span><br><span class="line"></span><br><span class="line">model = Seq2Seq(enc, dec, device).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">            nn.init.normal_(param.data, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nn.init.constant_(param.data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.apply(init_weights)</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The model has 1,856,653 trainable parameters
</code></pre><p>Note: when scoring the performance of a language translation model in<br>particular, we have to tell the <code>nn.CrossEntropyLoss</code> function to<br>ignore the indices where the target is simply padding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PAD_IDX = TRG.vocab.stoi[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)</span><br></pre></td></tr></table></figure>
<p>Finally, we can train and evaluate this model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">          optimizer: optim.Optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">          criterion: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          clip: float)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">        src = batch.src</span><br><span class="line">        trg = batch.trg</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(src, trg)</span><br><span class="line"></span><br><span class="line">        output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">        trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">             iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">             criterion: nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">            src = batch.src</span><br><span class="line">            trg = batch.trg</span><br><span class="line"></span><br><span class="line">            output = model(src, trg, <span class="number">0</span>) <span class="comment">#turn off teacher forcing</span></span><br><span class="line"></span><br><span class="line">            output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">            trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time: int,</span></span></span><br><span class="line"><span class="function"><span class="params">               end_time: int)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line">CLIP = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)</span><br><span class="line">    valid_loss = evaluate(model, valid_iterator, criterion)</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train PPL: <span class="subst">&#123;math.exp(train_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. PPL: <span class="subst">&#123;math.exp(valid_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">test_loss = evaluate(model, test_iterator, criterion)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'| Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test PPL: <span class="subst">&#123;math.exp(test_loss):<span class="number">7.3</span>f&#125;</span> |'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 01 | Time: 15m 34s
    Train Loss: 5.681 | Train PPL: 293.100
     Val. Loss: 5.244 |  Val. PPL: 189.491
Epoch: 02 | Time: 18m 13s
    Train Loss: 5.039 | Train PPL: 154.341
     Val. Loss: 5.152 |  Val. PPL: 172.773
Epoch: 03 | Time: 15m 47s
    Train Loss: 4.788 | Train PPL: 120.088
     Val. Loss: 5.044 |  Val. PPL: 155.033
Epoch: 04 | Time: 15m 27s
    Train Loss: 4.619 | Train PPL: 101.417
     Val. Loss: 5.146 |  Val. PPL: 171.670
Epoch: 05 | Time: 16m 16s
    Train Loss: 4.491 | Train PPL:  89.179
     Val. Loss: 5.014 |  Val. PPL: 150.444
Epoch: 06 | Time: 17m 34s
    Train Loss: 4.394 | Train PPL:  80.928
     Val. Loss: 5.014 |  Val. PPL: 150.472
Epoch: 07 | Time: 18m 31s
    Train Loss: 4.306 | Train PPL:  74.153
     Val. Loss: 4.899 |  Val. PPL: 134.150
Epoch: 08 | Time: 18m 48s
    Train Loss: 4.255 | Train PPL:  70.459
     Val. Loss: 4.872 |  Val. PPL: 130.520
Epoch: 09 | Time: 18m 21s
    Train Loss: 4.200 | Train PPL:  66.700
     Val. Loss: 4.807 |  Val. PPL: 122.399
Epoch: 10 | Time: 18m 59s
    Train Loss: 4.142 | Train PPL:  62.920
     Val. Loss: 4.644 |  Val. PPL: 103.988
| Test Loss: 4.650 | Test PPL: 104.534 |
</code></pre><h2 id="Next-steps"><a href="#Next-steps" class="headerlink" title="Next steps"></a>Next steps</h2><ul>
<li>Check out the rest of Ben Trevettâ€™s tutorials using <code>torchtext</code><br><code>here &lt;https://github.com/bentrevett/&gt;</code>__</li>
<li>Stay tuned for a tutorial using other <code>torchtext</code> features along<br>with <code>nn.Transformer</code> for language modeling via next word prediction!</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-ç”¨NN.TRANFORMERå’ŒTORCHTEXTè¿›è¡Œåºåˆ—åˆ°åºåˆ—å»ºæ¨¡</title>
    <url>/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>Pytorch-Text-ç”¨NN.TRANFORMERå’ŒTORCHTEXTè¿›è¡Œåºåˆ—åˆ°åºåˆ—å»ºæ¨¡:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText"><a href="#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText" class="headerlink" title="Sequence-to-Sequence Modeling with nn.Transformer and TorchText"></a>Sequence-to-Sequence Modeling with nn.Transformer and TorchText</h1><p>This is a tutorial on how to train a sequence-to-sequence model<br>that uses the<br><code>nn.Transformer &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&gt;</code>__ module.</p>
<p>PyTorch 1.2 release includes a standard transformer module based on the<br>paper <code>Attention is All You
Need &lt;https://arxiv.org/pdf/1706.03762.pdf&gt;</code><strong>. The transformer model<br>has been proved to be superior in quality for many sequence-to-sequence<br>problems while being more parallelizable. The <code>nn.Transformer</code> module<br>relies entirely on an attention mechanism (another module recently<br>implemented as <code>nn.MultiheadAttention &lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&gt;</code></strong>) to draw global dependencies<br>between input and output. The <code>nn.Transformer</code> module is now highly<br>modularized such that a single component (like <code>nn.TransformerEncoder &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&gt;</code>__<br>in this tutorial) can be easily adapted/composed.</p>
<p><img src="https://pytorch.org/tutorials/_images/transformer_architecture.jpg" alt></p>
<h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>In this tutorial, we train <code>nn.TransformerEncoder</code> model on a<br>language modeling task. The language modeling task is to assign a<br>probability for the likelihood of a given word (or a sequence of words)<br>to follow a sequence of words. A sequence of tokens are passed to the embedding<br>layer first, followed by a positional encoding layer to account for the order<br>of the word (see the next paragraph for more details). The<br><code>nn.TransformerEncoder</code> consists of multiple layers of<br><code>nn.TransformerEncoderLayer &lt;https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer&gt;</code>__. Along with the input sequence, a square<br>attention mask is required because the self-attention layers in<br><code>nn.TransformerEncoder</code> are only allowed to attend the earlier positions in<br>the sequence. For the language modeling task, any tokens on the future<br>positions should be masked. To have the actual words, the output<br>of <code>nn.TransformerEncoder</code> model is sent to the final Linear<br>layer, which is followed by a log-Softmax function.</p>
<hr>
<h2 id="torch-triu-input-diagonal-0-out-None-â†’-Tensor"><a href="#torch-triu-input-diagonal-0-out-None-â†’-Tensor" class="headerlink" title="torch.triu(input, diagonal=0, out=None) â†’ Tensor"></a>torch.triu(input, diagonal=0, out=None) â†’ Tensor</h2><p>è¿”å›çŸ©é˜µä¸Šä¸‰è§’éƒ¨åˆ†ï¼Œå…¶ä½™éƒ¨åˆ†å®šä¹‰ä¸º0ã€‚</p>
<p>Parameters:</p>
<ul>
<li>input (Tensor) â€“ the input tensor</li>
<li>diagonal (int, optional) â€“ the diagonal to consider</li>
<li>out (Tensor, optional) â€“ the output tensor</li>
</ul>
<hr>
<ul>
<li>å¦‚æœdiagonalä¸ºç©ºï¼Œè¾“å…¥çŸ©é˜µä¿ç•™ä¸»å¯¹è§’çº¿ä¸ä¸»å¯¹è§’çº¿ä»¥ä¸Šçš„å…ƒç´ ï¼›</li>
<li>å¦‚æœdiagonalä¸ºæ­£æ•°nï¼Œè¾“å…¥çŸ©é˜µä¿ç•™ä¸»å¯¹è§’çº¿ä¸ä¸»å¯¹è§’çº¿ä»¥ä¸Šé™¤å»nè¡Œçš„å…ƒç´ ï¼›</li>
<li>å¦‚æœdiagonalä¸ºè´Ÿæ•°-nï¼Œè¾“å…¥çŸ©é˜µä¿ç•™ä¸»å¯¹è§’çº¿ä¸ä¸»å¯¹è§’çº¿ä»¥ä¸Šä¸ä¸»å¯¹è§’çº¿ä¸‹æ–¹hè¡Œå¯¹è§’çº¿çš„å…ƒç´ ï¼›<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a &#x3D; torch.randn(3, 3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.3480, -0.5211, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;1)</span><br><span class="line">tensor([[ 0.0000,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000,  0.0000,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;-1)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000, -0.5211, -0.4573]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="pytorch-mask-filledç”¨æ³•"><a href="#pytorch-mask-filledç”¨æ³•" class="headerlink" title="pytorch mask_filledç”¨æ³•"></a>pytorch mask_filledç”¨æ³•</h2><p>å°† maskå¿…é¡»æ˜¯ä¸€ä¸ª ByteTensor è€Œä¸”shapeå¿…é¡»å’Œ aä¸€æ · å¹¶ä¸”å…ƒç´ åªèƒ½æ˜¯ 0æˆ–è€…1 ï¼Œæ˜¯å°† maskä¸­ä¸º1çš„ å…ƒç´ æ‰€åœ¨çš„ç´¢å¼•ï¼Œåœ¨aä¸­ç›¸åŒçš„çš„ç´¢å¼•å¤„æ›¿æ¢ä¸º value  ,mask valueå¿…é¡»åŒä¸ºtensor<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.tensor([1,0,2,3])</span><br><span class="line">a.masked_fill(mask &#x3D; torch.ByteTensor([1,1,0,0]), value&#x3D;torch.tensor(-1e9))</span><br><span class="line"></span><br><span class="line">tensor([-1.0000e+09, -1.0000e+09, Â 2.0000e+00, Â 3.0000e+00])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ntoken, ninp, nhead, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(TransformerModel, self).__init__()</span><br><span class="line">        <span class="keyword">from</span> torch.nn <span class="keyword">import</span> TransformerEncoder, TransformerEncoderLayer</span><br><span class="line">        self.model_type = <span class="string">'Transformer'</span></span><br><span class="line">        self.src_mask = <span class="literal">None</span></span><br><span class="line">        self.pos_encoder = PositionalEncoding(ninp, dropout)</span><br><span class="line">        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)</span><br><span class="line">        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        self.ninp = ninp</span><br><span class="line">        self.decoder = nn.Linear(ninp, ntoken)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_square_subsequent_mask</span><span class="params">(self, sz)</span>:</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.src_mask <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self.src_mask.size(<span class="number">0</span>) != len(src):</span><br><span class="line">            device = src.device</span><br><span class="line">            mask = self._generate_square_subsequent_mask(len(src)).to(device)</span><br><span class="line">            self.src_mask = mask</span><br><span class="line"></span><br><span class="line">        src = self.encoder(src) * math.sqrt(self.ninp)</span><br><span class="line">        src = self.pos_encoder(src)</span><br><span class="line">        output = self.transformer_encoder(src, self.src_mask)</span><br><span class="line">        output = self.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p><code>PositionalEncoding</code> module injects some information about the<br>relative or absolute position of the tokens in the sequence. The<br>positional encodings have the same dimension as the embeddings so that<br>the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of<br>different frequencies.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="Load-and-batch-data"><a href="#Load-and-batch-data" class="headerlink" title="Load and batch data"></a>Load and batch data</h2><p>The training process uses Wikitext-2 dataset from <code>torchtext</code>. The<br>vocab object is built based on the train dataset and is used to numericalize<br>tokens into tensors. Starting from sequential data, the <code>batchify()</code><br>function arranges the dataset into columns, trimming off any tokens remaining<br>after the data has been divided into batches of size <code>batch_size</code>.<br>For instance, with the alphabet as the sequence (total length of 26)<br>and a batch size of 4, we would divide the alphabet into 4 sequences of<br>length 6:</p>
<p>\begin{align}\begin{bmatrix}<br>  \text{A} &amp; \text{B} &amp; \text{C} &amp; \ldots &amp; \text{X} &amp; \text{Y} &amp; \text{Z}<br>  \end{bmatrix}<br>  \Rightarrow<br>  \begin{bmatrix}<br>  \begin{bmatrix}\text{A} \\ \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{G} \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{M} \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{S} \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix}<br>  \end{bmatrix}\end{align}</p>
<p>These columns are treated as independent by the model, which means that<br>the dependence of <code>G</code> and <code>F</code> can not be learned, but allows more<br>efficient batch processing.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line">TEXT = torchtext.data.Field(tokenize=get_tokenizer(<span class="string">"basic_english"</span>),</span><br><span class="line">                            init_token=<span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">                            eos_token=<span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">                            lower=<span class="literal">True</span>)</span><br><span class="line">train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)</span><br><span class="line">TEXT.build_vocab(train_txt)</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    data = TEXT.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line">    <span class="comment"># Divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure>
<p>Functions to generate input and target sequence<br><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del>~~~~</p>
<p><code>get_batch()</code> function generates the input and target sequence for<br>the transformer model. It subdivides the source data into chunks of<br>length <code>bptt</code>. For the language modeling task, the model needs the<br>following words as <code>Target</code>. For example, with a <code>bptt</code> value of 2,<br>weâ€™d get the following two Variables for <code>i</code> = 0:</p>
<p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/_static/img/transformer_input_target.png" alt></p>
<p>It should be noted that the chunks are along dimension 0, consistent<br>with the <code>S</code> dimension in the Transformer model. The batch dimension<br><code>N</code> is along dimension 1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bptt = <span class="number">35</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(source, i)</span>:</span></span><br><span class="line">    seq_len = min(bptt, len(source) - <span class="number">1</span> - i)</span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure>
<h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The model is set up with the hyperparameter below. The vocab size is<br>equal to the length of the vocab object.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ntokens = len(TEXT.vocab.stoi) <span class="comment"># the size of vocabulary</span></span><br><span class="line">emsize = <span class="number">200</span> <span class="comment"># embedding dimension</span></span><br><span class="line">nhid = <span class="number">200</span> <span class="comment"># the dimension of the feedforward network model in nn.TransformerEncoder</span></span><br><span class="line">nlayers = <span class="number">2</span> <span class="comment"># the number of nn.TransformerEncoderLayer in nn.TransformerEncoder</span></span><br><span class="line">nhead = <span class="number">2</span> <span class="comment"># the number of heads in the multiheadattention models</span></span><br><span class="line">dropout = <span class="number">0.2</span> <span class="comment"># the dropout value</span></span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Run-the-model"><a href="#Run-the-model" class="headerlink" title="Run the model"></a>Run the model</h2><p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>is applied to track the loss and<br><code>SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD&gt;</code></strong><br>implements stochastic gradient descent method as the optimizer. The initial<br>learning rate is set to 5.0. <code>StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR&gt;</code><strong> is<br>applied to adjust the learn rate through epochs. During the<br>training, we use<br><code>nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_&gt;</code></strong><br>function to scale all the gradient together to prevent exploding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">lr = <span class="number">5.0</span> <span class="comment"># learning rate</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train() <span class="comment"># Turn on the train mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> enumerate(range(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output.view(<span class="number">-1</span>, ntokens), targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        log_interval = <span class="number">200</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            print(<span class="string">'| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | '</span></span><br><span class="line">                  <span class="string">'lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | '</span></span><br><span class="line">                  <span class="string">'loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, math.exp(cur_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(eval_model, data_source)</span>:</span></span><br><span class="line">    eval_model.eval() <span class="comment"># Turn on the evaluation mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, data_source.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            data, targets = get_batch(data_source, i)</span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            output_flat = output.view(<span class="number">-1</span>, ntokens)</span><br><span class="line">            total_loss += len(data) * criterion(output_flat, targets).item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / (len(data_source) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Loop over epochs. Save the model if the validation loss is the best<br>weâ€™ve seen so far. Adjust the learning rate after each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_val_loss = float(<span class="string">"inf"</span>)</span><br><span class="line">epochs = <span class="number">3</span> <span class="comment"># The number of epochs</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train()</span><br><span class="line">    val_loss = evaluate(model, val_data)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line">    print(<span class="string">'| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | '</span></span><br><span class="line">          <span class="string">'valid ppl &#123;:8.2f&#125;'</span>.format(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                     val_loss, math.exp(val_loss)))</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">        best_val_loss = val_loss</span><br><span class="line">        best_model = model</span><br><span class="line"></span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\torch\optim\lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  &quot;please use `get_last_lr()`.&quot;, UserWarning)


| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 2144.51 | loss  8.03 | ppl  3063.62
| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 1757.64 | loss  6.78 | ppl   876.40
| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 1952.50 | loss  6.37 | ppl   584.33
| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 1758.54 | loss  6.22 | ppl   501.41
| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 1863.17 | loss  6.11 | ppl   450.46
| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 1836.79 | loss  6.10 | ppl   443.68
| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 1795.15 | loss  6.05 | ppl   422.43
| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 1902.40 | loss  6.05 | ppl   425.49
| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 1827.80 | loss  5.96 | ppl   386.04
| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 1814.35 | loss  5.96 | ppl   388.23
| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 1845.57 | loss  5.85 | ppl   346.70
| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 1798.20 | loss  5.90 | ppl   364.75
| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 1925.91 | loss  5.90 | ppl   365.12
| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 1709.37 | loss  5.80 | ppl   331.20
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 5766.46s | valid loss  5.76 | valid ppl   317.98
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 1861.38 | loss  5.80 | ppl   330.78
| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 1823.89 | loss  5.77 | ppl   320.61
| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 1858.68 | loss  5.60 | ppl   270.75
| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 1831.67 | loss  5.63 | ppl   278.20
| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 1785.64 | loss  5.59 | ppl   266.78
| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 1808.74 | loss  5.61 | ppl   273.60
| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 1926.79 | loss  5.63 | ppl   278.59
| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 1821.76 | loss  5.67 | ppl   289.91
| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 1797.76 | loss  5.59 | ppl   267.51
| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 1764.14 | loss  5.62 | ppl   274.93
| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 1895.50 | loss  5.51 | ppl   246.68
| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 1793.70 | loss  5.58 | ppl   265.49
| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 1824.77 | loss  5.58 | ppl   266.10
| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 1891.16 | loss  5.51 | ppl   247.06
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 5701.96s | valid loss  5.59 | valid ppl   268.83
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 1818.12 | loss  5.55 | ppl   256.50
| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 1945.54 | loss  5.55 | ppl   257.43
| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 1616.58 | loss  5.36 | ppl   213.27
| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 1689.64 | loss  5.42 | ppl   224.94
| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 1608.36 | loss  5.38 | ppl   216.13
| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 1630.67 | loss  5.41 | ppl   222.92
| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 1686.69 | loss  5.44 | ppl   229.34
| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 1647.30 | loss  5.48 | ppl   239.58
| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 1633.68 | loss  5.40 | ppl   221.38
| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 1613.04 | loss  5.43 | ppl   228.14
| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 1608.29 | loss  5.32 | ppl   204.06
| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 1624.74 | loss  5.40 | ppl   220.31
| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 1609.86 | loss  5.41 | ppl   224.71
| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 1559.15 | loss  5.33 | ppl   207.22
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 6002.66s | valid loss  5.51 | valid ppl   247.75
-----------------------------------------------------------------------------------------
</code></pre><h2 id="Evaluate-the-model-with-the-test-dataset"><a href="#Evaluate-the-model-with-the-test-dataset" class="headerlink" title="Evaluate the model with the test dataset"></a>Evaluate the model with the test dataset</h2><p>Apply the best model to check the result with the test dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br><span class="line">print(<span class="string">'| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure>
<pre><code>=========================================================================================
| End of training | test loss  5.43 | test ppl   227.23
=========================================================================================
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parallelizable:å¯å¹¶è¡ŒåŒ–</span><br><span class="line">superior:ä¼˜è¶Š</span><br><span class="line">mechanism:æœºåˆ¶</span><br><span class="line">modularized:æ¨¡å—åŒ–</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Text-ç”¨å­—ç¬¦çº§RNNç”Ÿæˆåç§°</title>
    <url>/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/</url>
    <content><![CDATA[<p>Pytorch-Text-ç”¨å­—ç¬¦çº§RNNç”Ÿæˆåç§°:<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>NLP From Scratch: Generating Names with a Character-Level RNN</p>
<hr>
<p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p>
<p>This is our second of three tutorials on â€œNLP From Scratchâ€.<br>In the <code>first tutorial &lt;/intermediate/char_rnn_classification_tutorial&gt;</code><br>we used a RNN to classify names into their language of origin. This time<br>weâ€™ll turn around and generate names from languages.</p>
<p>::</p>
<pre><code>&gt; python sample.py Russian RUS
Rovakov
Uantov
Shavakov

&gt; python sample.py German GER
Gerren
Ereng
Rosher

&gt; python sample.py Spanish SPA
Salla
Parer
Allan

&gt; python sample.py Chinese CHI
Chan
Hang
Iun
</code></pre><p>We are still hand-crafting a small RNN with a few linear layers. The big<br>difference is instead of predicting a category after reading in all the<br>letters of a name, we input a category and output one letter at a time.<br>Recurrently predicting characters to form language (this could also be<br>done with words or other higher order constructs) is often referred to<br>as a â€œlanguage modelâ€.</p>
<p><strong>Recommended Reading:</strong></p>
<p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p>
<ul>
<li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li>
<li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li>
<li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li>
<li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li>
</ul>
<p>It would also be useful to know about RNNs and how they work:</p>
<ul>
<li><code>The Unreasonable Effectiveness of Recurrent Neural
Networks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li>
<li><code>Understanding LSTM
Networks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li>
</ul>
<p>I also suggest the previous tutorial, :doc:<code>/intermediate/char_rnn_classification_tutorial</code></p>
<h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p>
<p>See the last tutorial for more detail of this process. In short, there<br>are a bunch of plain text files <code>data/names/[Language].txt</code> with a<br>name per line. We split lines into an array, convert Unicode to ASCII,<br>and end up with a dictionary <code>{language: [names ...]}</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'-"</span></span><br><span class="line">n_letters = len(all_letters) + <span class="number">1</span> <span class="comment"># Plus EOS marker</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of lines per category</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> n_categories == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">'Data not found. Make sure that you downloaded data '</span></span><br><span class="line">        <span class="string">'from https://download.pytorch.org/tutorial/data.zip and extract it to '</span></span><br><span class="line">        <span class="string">'the current directory.'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'# categories:'</span>, n_categories, all_categories)</span><br><span class="line">print(unicodeToAscii(<span class="string">"O'NÃ©Ã l"</span>))</span><br></pre></td></tr></table></figure>
<pre><code># categories: 18 [&#39;Arabic&#39;, &#39;Chinese&#39;, &#39;Czech&#39;, &#39;Dutch&#39;, &#39;English&#39;, &#39;French&#39;, &#39;German&#39;, &#39;Greek&#39;, &#39;Irish&#39;, &#39;Italian&#39;, &#39;Japanese&#39;, &#39;Korean&#39;, &#39;Polish&#39;, &#39;Portuguese&#39;, &#39;Russian&#39;, &#39;Scottish&#39;, &#39;Spanish&#39;, &#39;Vietnamese&#39;]
O&#39;Neal
</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>This network extends <code>the last tutorial&#39;s RNN &lt;#Creating-the-Network&gt;</code>__<br>with an extra argument for the category tensor, which is concatenated<br>along with the others. The category tensor is a one-hot vector just like<br>the letter input.</p>
<p>We will interpret the output as the probability of the next letter. When<br>sampling, the most likely output letter is used as the next input<br>letter.</p>
<p>I added a second linear layer <code>o2o</code> (after combining hidden and<br>output) to give it more muscle to work with. Thereâ€™s also a dropout<br>layer, which <code>randomly zeros parts of its
input &lt;https://arxiv.org/abs/1207.0580&gt;</code>__ with a given probability<br>(here 0.1) and is usually used to fuzz inputs to prevent overfitting.<br>Here weâ€™re using it towards the end of the network to purposely add some<br>chaos and increase sampling variety.</p>
<p>.. figure:: <a href="https://i.imgur.com/jzVrf7f.png" target="_blank" rel="noopener">https://i.imgur.com/jzVrf7f.png</a><br>   :alt:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)</span><br><span class="line">        self.o2o = nn.Linear(hidden_size + output_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, category, input, hidden)</span>:</span></span><br><span class="line">        input_combined = torch.cat((category, input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(input_combined)</span><br><span class="line">        output = self.i2o(input_combined)</span><br><span class="line">        output_combined = torch.cat((hidden, output), <span class="number">1</span>)</span><br><span class="line">        output = self.o2o(output_combined)</span><br><span class="line">        output = self.dropout(output)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>First of all, helper functions to get random pairs of (category, line):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Random item from a list</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a random category and random line from that category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    <span class="keyword">return</span> category, line</span><br></pre></td></tr></table></figure>
<p>For each timestep (that is, for each letter in a training word) the<br>inputs of the network will be<br><code>(category, current letter, hidden state)</code> and the outputs will be<br><code>(next letter, next hidden state)</code>. So for each training set, weâ€™ll<br>need the category, a set of input letters, and a set of output/target<br>letters.</p>
<p>Since we are predicting the next letter from the current letter for each<br>timestep, the letter pairs are groups of consecutive letters from the<br>line - e.g. for <code>&quot;ABCD&lt;EOS&gt;&quot;</code> we would create (â€œAâ€, â€œBâ€), (â€œBâ€, â€œCâ€),<br>(â€œCâ€, â€œDâ€), (â€œDâ€, â€œEOSâ€).</p>
<p>.. figure:: <a href="https://i.imgur.com/JH58tXY.png" target="_blank" rel="noopener">https://i.imgur.com/JH58tXY.png</a><br>   :alt:</p>
<p>The category tensor is a <code>one-hot
tensor &lt;https://en.wikipedia.org/wiki/One-hot&gt;</code>__ of size<br><code>&lt;1 x n_categories&gt;</code>. When training we feed it to the network at every<br>timestep - this is a design choice, it could have been included as part<br>of initial hidden state or some other strategy.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># One-hot vector for category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryTensor</span><span class="params">(category)</span>:</span></span><br><span class="line">    li = all_categories.index(category)</span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_categories)</span><br><span class="line">    tensor[<span class="number">0</span>][li] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot matrix of first to last letters (not including EOS) for input</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> range(len(line)):</span><br><span class="line">        letter = line[li]</span><br><span class="line">        tensor[li][<span class="number">0</span>][all_letters.find(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># LongTensor of second letter to end (EOS) for target</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    letter_indexes = [all_letters.find(line[li]) <span class="keyword">for</span> li <span class="keyword">in</span> range(<span class="number">1</span>, len(line))]</span><br><span class="line">    letter_indexes.append(n_letters - <span class="number">1</span>) <span class="comment"># EOS</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(letter_indexes)</span><br></pre></td></tr></table></figure>
<p>For convenience during training weâ€™ll make a <code>randomTrainingExample</code><br>function that fetches a random (category, line) pair and turns them into<br>the required (category, input, target) tensors.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Make category, input, and target tensors from a random category, line pair</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category, line = randomTrainingPair()</span><br><span class="line">    category_tensor = categoryTensor(category)</span><br><span class="line">    input_line_tensor = inputTensor(line)</span><br><span class="line">    target_line_tensor = targetTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category_tensor, input_line_tensor, target_line_tensor</span><br></pre></td></tr></table></figure>
<h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>In contrast to classification, where only the last output is used, we<br>are making a prediction at every step, so we are calculating loss at<br>every step.</p>
<p>The magic of autograd allows you to simply sum these losses at each step<br>and call backward at the end.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, input_line_tensor, target_line_tensor)</span>:</span></span><br><span class="line">    target_line_tensor.unsqueeze_(<span class="number">-1</span>)</span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(input_line_tensor.size(<span class="number">0</span>)):</span><br><span class="line">        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)</span><br><span class="line">        l = criterion(output, target_line_tensor[i])</span><br><span class="line">        loss += l</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item() / input_line_tensor.size(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>To keep track of how long training takes I am adding a<br><code>timeSince(timestamp)</code> function which returns a human readable string:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br></pre></td></tr></table></figure>
<p>Training is business as usual - call train a bunch of times and wait a<br>few minutes, printing the current time and loss every <code>print_every</code><br>examples, and keeping store of an average loss per <code>plot_every</code> examples<br>in <code>all_losses</code> for plotting later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rnn = RNN(n_letters, <span class="number">128</span>, n_letters)</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">500</span></span><br><span class="line">all_losses = []</span><br><span class="line">total_loss = <span class="number">0</span> <span class="comment"># Reset every plot_every iters</span></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    output, loss = train(*randomTrainingExample())</span><br><span class="line">    total_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start), iter, iter / n_iters * <span class="number">100</span>, loss))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(total_loss / plot_every)</span><br><span class="line">        total_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>0m 40s (5000 5%) 2.6821
1m 17s (10000 10%) 3.1606
1m 50s (15000 15%) 2.3541
2m 23s (20000 20%) 2.4859
2m 57s (25000 25%) 2.1573
3m 30s (30000 30%) 2.2910
4m 3s (35000 35%) 2.6906
4m 37s (40000 40%) 2.1542
20m 27s (45000 45%) 2.1909
21m 10s (50000 50%) 1.8939
21m 51s (55000 55%) 2.9425
22m 34s (60000 60%) 2.8395
23m 15s (65000 65%) 3.0346
23m 55s (70000 70%) 2.5686
24m 34s (75000 75%) 2.6037
25m 13s (80000 80%) 2.5966
25m 56s (85000 85%) 2.6650
26m 39s (90000 90%) 2.7412
27m 18s (95000 95%) 2.6140
27m 58s (100000 100%) 1.9323
</code></pre><h2 id="Plotting-the-Losses"><a href="#Plotting-the-Losses" class="headerlink" title="Plotting the Losses"></a>Plotting the Losses</h2><p>Plotting the historical loss from all_losses shows the network<br>learning:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x12e2623bba8&gt;]
</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/output_18_1.png" alt="png"></p>
<h1 id="Sampling-the-Network"><a href="#Sampling-the-Network" class="headerlink" title="Sampling the Network"></a>Sampling the Network</h1><p>To sample we give the network a letter and ask what the next one is,<br>feed that in as the next letter, and repeat until the EOS token.</p>
<ul>
<li>Create tensors for input category, starting letter, and empty hidden<br>state</li>
<li>Create a string <code>output_name</code> with the starting letter</li>
<li><p>Up to a maximum output length,</p>
<ul>
<li>Feed the current letter to the network</li>
<li>Get the next letter from highest output, and next hidden state</li>
<li>If the letter is EOS, stop here</li>
<li>If a regular letter, add to <code>output_name</code> and continue</li>
</ul>
</li>
<li><p>Return the final name</p>
</li>
</ul>
<p>.. Note::<br>   Rather than having to give it a starting letter, another<br>   strategy would have been to include a â€œstart of stringâ€ token in<br>   training and have the network choose its own starting letter.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_length = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from a category and starting letter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(category, start_letter=<span class="string">'A'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># no need to track history in sampling</span></span><br><span class="line">        category_tensor = categoryTensor(category)</span><br><span class="line">        input = inputTensor(start_letter)</span><br><span class="line">        hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">        output_name = start_letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hidden = rnn(category_tensor, input[<span class="number">0</span>], hidden)</span><br><span class="line">            topv, topi = output.topk(<span class="number">1</span>)</span><br><span class="line">            topi = topi[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> topi == n_letters - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                letter = all_letters[topi]</span><br><span class="line">                output_name += letter</span><br><span class="line">            input = inputTensor(letter)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get multiple samples from one category and multiple starting letters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">samples</span><span class="params">(category, start_letters=<span class="string">'ABC'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> start_letter <span class="keyword">in</span> start_letters:</span><br><span class="line">        print(sample(category, start_letter))</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Russian'</span>, <span class="string">'RUS'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'German'</span>, <span class="string">'GER'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Spanish'</span>, <span class="string">'SPA'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Chinese'</span>, <span class="string">'CHI'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Romankovovovoshollosh
Uantovovovokovosskoss
Shaverovovovovovoshol
Gerter
Eeller
Ronger
Sara
Pare
Aran
Chan
Han
Iou
</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul>
<li><p>Try with a different dataset of category -&gt; line, for example:</p>
<ul>
<li>Fictional series -&gt; Character name</li>
<li>Part of speech -&gt; Word</li>
<li>Country -&gt; City</li>
</ul>
</li>
<li><p>Use a â€œstart of sentenceâ€ token so that sampling can be done without<br>choosing a start letter</p>
</li>
<li><p>Get better results with a bigger and/or better shaped network</p>
<ul>
<li>Try the nn.LSTM and nn.GRU layers</li>
<li>Combine multiple of these RNNs as a higher level network</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>pytorch1.5.1å®˜ç½‘æ•™ç¨‹</tag>
        <tag>Pytorch1.5.1å®˜ç½‘æ•™ç¨‹-Text</tag>
      </tags>
  </entry>
</search>
