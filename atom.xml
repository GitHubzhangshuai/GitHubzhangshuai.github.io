<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张帅的Blog</title>
  
  <subtitle>用hexo搭建的简易博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-25T01:30:43.992Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhangshuai</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pytorch-Reinforcement-Learning</title>
    <link href="http://yoursite.com/2020/07/25/Pytorch-Reinforcement-Learning/"/>
    <id>http://yoursite.com/2020/07/25/Pytorch-Reinforcement-Learning/</id>
    <published>2020-07-25T01:29:26.000Z</published>
    <updated>2020-07-25T01:30:43.992Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Reinforcement-Learning:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Reinforcement-Learning-DQN-Tutorial"><a href="#Reinforcement-Learning-DQN-Tutorial" class="headerlink" title="Reinforcement Learning (DQN) Tutorial"></a>Reinforcement Learning (DQN) Tutorial</h1><p><strong>Author</strong>: <code>Adam Paszke &lt;https://github.com/apaszke&gt;</code>_</p><p>This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent<br>on the CartPole-v0 task from the <code>OpenAI Gym &lt;https://gym.openai.com/&gt;</code>__.</p><p><strong>Task</strong></p><p>The agent has to decide between two actions - moving the cart left or<br>right - so that the pole attached to it stays upright. You can find an<br>official leaderboard with various algorithms and visualizations at the<br><code>Gym website &lt;https://gym.openai.com/envs/CartPole-v0&gt;</code>__.</p><p><img src="https://pytorch.org/tutorials/_images/cartpole.gif" alt></p><p>   cartpole</p><p>As the agent observes the current state of the environment and chooses<br>an action, the environment <em>transitions</em> to a new state, and also<br>returns a reward that indicates the consequences of the action. In this<br>task, rewards are +1 for every incremental timestep and the environment<br>terminates if the pole falls over too far or the cart moves more then 2.4<br>units away from center. This means better performing scenarios will run<br>for longer duration, accumulating larger return.</p><p>The CartPole task is designed so that the inputs to the agent are 4 real<br>values representing the environment state (position, velocity, etc.).<br>However, neural networks can solve the task purely by looking at the<br>scene, so we’ll use a patch of the screen centered on the cart as an<br>input. Because of this, our results aren’t directly comparable to the<br>ones from the official leaderboard - our task is much harder.<br>Unfortunately this does slow down the training, because we have to<br>render all the frames.</p><p>Strictly speaking, we will present the state as the difference between<br>the current screen patch and the previous one. This will allow the agent<br>to take the velocity of the pole into account from one image.</p><p><strong>Packages</strong></p><p>First, let’s import needed packages. Firstly, we need<br><code>gym &lt;https://gym.openai.com/docs&gt;</code>__ for the environment<br>(Install using <code>pip install gym</code>).<br>We’ll also use the following from PyTorch:</p><ul><li>neural networks (<code>torch.nn</code>)</li><li>optimization (<code>torch.optim</code>)</li><li>automatic differentiation (<code>torch.autograd</code>)</li><li>utilities for vision tasks (<code>torchvision</code> - <code>a separatepackage &lt;https://github.com/pytorch/vision&gt;</code>__).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line"><span class="comment"># set up matplotlib</span></span><br><span class="line">is_ipython = <span class="string">'inline'</span> <span class="keyword">in</span> matplotlib.get_backend()</span><br><span class="line"><span class="keyword">if</span> is_ipython:</span><br><span class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="comment"># if gpu is to be used</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><p>We’ll be using experience replay memory for training our DQN. It stores<br>the transitions that the agent observes, allowing us to reuse this data<br>later. By sampling from it randomly, the transitions that build up a<br>batch are decorrelated. It has been shown that this greatly stabilizes<br>and improves the DQN training procedure.</p><p>For this, we’re going to need two classses:</p><ul><li><code>Transition</code> - a named tuple representing a single transition in<br>our environment. It essentially maps (state, action) pairs<br>to their (next_state, reward) result, with the state being the<br>screen difference image as described later on.</li><li><code>ReplayMemory</code> - a cyclic buffer of bounded size that holds the<br>transitions observed recently. It also implements a <code>.sample()</code><br>method for selecting a random batch of transitions for training.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>,</span><br><span class="line">                        (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.memory = []</span><br><span class="line">        self.position = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Saves a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.memory) &lt; self.capacity:</span><br><span class="line">            self.memory.append(<span class="literal">None</span>)</span><br><span class="line">        self.memory[self.position] = Transition(*args)</span><br><span class="line">        self.position = (self.position + <span class="number">1</span>) % self.capacity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.memory, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.memory)</span><br></pre></td></tr></table></figure><p>Now, let’s define our model. But first, let quickly recap what a DQN is.</p><h2 id="DQN-algorithm"><a href="#DQN-algorithm" class="headerlink" title="DQN algorithm"></a>DQN algorithm</h2><p>Our environment is deterministic, so all equations presented here are<br>also formulated deterministically for the sake of simplicity. In the<br>reinforcement learning literature, they would also contain expectations<br>over stochastic transitions in the environment.</p><p>Our aim will be to train a policy that tries to maximize the discounted,<br>cumulative reward<br>$R_{t_0} = \sum_{t=t_0}^{\infty} \gamma^{t - t_0} r_t$, where<br>$R_{t_0}$ is also known as the <em>return</em>. The discount,<br>$\gamma$, should be a constant between $0$ and $1$<br>that ensures the sum converges. It makes rewards from the uncertain far<br>future less important for our agent than the ones in the near future<br>that it can be fairly confident about.</p><p>The main idea behind Q-learning is that if we had a function<br>$Q^*: State \times Action \rightarrow \mathbb{R}$, that could tell<br>us what our return would be, if we were to take an action in a given<br>state, then we could easily construct a policy that maximizes our<br>rewards:</p><p>\begin{align}\pi^<em>(s) = \arg!\max_a \ Q^</em>(s, a)\end{align}</p><p>However, we don’t know everything about the world, so we don’t have<br>access to $Q^<em>$. But, since neural networks are universal function<br>approximators, we can simply create one and train it to resemble<br>$Q^</em>$.</p><p>For our training update rule, we’ll use a fact that every $Q$<br>function for some policy obeys the Bellman equation:</p><p>\begin{align}Q^{\pi}(s, a) = r + \gamma Q^{\pi}(s’, \pi(s’))\end{align}</p><p>The difference between the two sides of the equality is known as the<br>temporal difference error, $\delta$:</p><p>\begin{align}\delta = Q(s, a) - (r + \gamma \max_a Q(s’, a))\end{align}</p><p>To minimise this error, we will use the <code>Huberloss &lt;https://en.wikipedia.org/wiki/Huber_loss&gt;</code>__. The Huber loss acts<br>like the mean squared error when the error is small, but like the mean<br>absolute error when the error is large - this makes it more robust to<br>outliers when the estimates of $Q$ are very noisy. We calculate<br>this over a batch of transitions, $B$, sampled from the replay<br>memory:</p><p>\begin{align}\mathcal{L} = \frac{1}{|B|}\sum_{(s, a, s’, r) \ \in \ B} \mathcal{L}(\delta)\end{align}</p><p>\begin{align}\text{where} \quad \mathcal{L}(\delta) = \begin{cases}<br>     \frac{1}{2}{\delta^2}  &amp; \text{for } |\delta| \le 1, \\<br>     |\delta| - \frac{1}{2} &amp; \text{otherwise.}<br>   \end{cases}\end{align}</p><p>Q-network</p><p>Our model will be a convolutional neural network that takes in the<br>difference between the current and previous screen patches. It has two<br>outputs, representing $Q(s, \mathrm{left})$ and<br>$Q(s, \mathrm{right})$ (where $s$ is the input to the<br>network). In effect, the network is trying to predict the <em>expected return</em> of<br>taking each action given the current input.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, w, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Number of Linear input connections depends on output of conv2d layers</span></span><br><span class="line">        <span class="comment"># and therefore the input image size, so compute it.</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv2d_size_out</span><span class="params">(size, kernel_size = <span class="number">5</span>, stride = <span class="number">2</span>)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> (size - (kernel_size - <span class="number">1</span>) - <span class="number">1</span>) // stride  + <span class="number">1</span></span><br><span class="line">        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))</span><br><span class="line">        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))</span><br><span class="line">        linear_input_size = convw * convh * <span class="number">32</span></span><br><span class="line">        self.head = nn.Linear(linear_input_size, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Called with either one element to determine next action, or a batch</span></span><br><span class="line">    <span class="comment"># during optimization. Returns tensor([[left0exp,right0exp]...]).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        <span class="keyword">return</span> self.head(x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br></pre></td></tr></table></figure><p>Input extraction</p><p>The code below are utilities for extracting and processing rendered<br>images from the environment. It uses the <code>torchvision</code> package, which<br>makes it easy to compose image transforms. Once you run the cell it will<br>display an example patch that it extracted.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">resize = T.Compose([T.ToPILImage(),</span><br><span class="line">                    T.Resize(<span class="number">40</span>, interpolation=Image.CUBIC),</span><br><span class="line">                    T.ToTensor()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cart_location</span><span class="params">(screen_width)</span>:</span></span><br><span class="line">    world_width = env.x_threshold * <span class="number">2</span></span><br><span class="line">    scale = screen_width / world_width</span><br><span class="line">    <span class="keyword">return</span> int(env.state[<span class="number">0</span>] * scale + screen_width / <span class="number">2.0</span>)  <span class="comment"># MIDDLE OF CART</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_screen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Returned screen requested by gym is 400x600x3, but is sometimes larger</span></span><br><span class="line">    <span class="comment"># such as 800x1200x3. Transpose it into torch order (CHW).</span></span><br><span class="line">    screen = env.render(mode=<span class="string">'rgb_array'</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Cart is in the lower half, so strip off the top and bottom of the screen</span></span><br><span class="line">    _, screen_height, screen_width = screen.shape</span><br><span class="line">    screen = screen[:, int(screen_height*<span class="number">0.4</span>):int(screen_height * <span class="number">0.8</span>)]</span><br><span class="line">    view_width = int(screen_width * <span class="number">0.6</span>)</span><br><span class="line">    cart_location = get_cart_location(screen_width)</span><br><span class="line">    <span class="keyword">if</span> cart_location &lt; view_width // <span class="number">2</span>:</span><br><span class="line">        slice_range = slice(view_width)</span><br><span class="line">    <span class="keyword">elif</span> cart_location &gt; (screen_width - view_width // <span class="number">2</span>):</span><br><span class="line">        slice_range = slice(-view_width, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        slice_range = slice(cart_location - view_width // <span class="number">2</span>,</span><br><span class="line">                            cart_location + view_width // <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Strip off the edges, so that we have a square image centered on a cart</span></span><br><span class="line">    screen = screen[:, :, slice_range]</span><br><span class="line">    <span class="comment"># Convert to float, rescale, convert to torch tensor</span></span><br><span class="line">    <span class="comment"># (this doesn't require a copy)</span></span><br><span class="line">    screen = np.ascontiguousarray(screen, dtype=np.float32) / <span class="number">255</span></span><br><span class="line">    screen = torch.from_numpy(screen)</span><br><span class="line">    <span class="comment"># Resize, and add a batch dimension (BCHW)</span></span><br><span class="line">    <span class="keyword">return</span> resize(screen).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env.reset()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(get_screen().cpu().squeeze(<span class="number">0</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy(),</span><br><span class="line">           interpolation=<span class="string">'none'</span>)</span><br><span class="line">plt.title(<span class="string">'Example extracted screen'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/07/25/Pytorch-Reinforcement-Learning/output_8_0.png" alt="png"></p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Hyperparameters and utilities</p><p>This cell instantiates our model and its optimizer, and defines some<br>utilities:</p><ul><li><code>select_action</code> - will select an action accordingly to an epsilon<br>greedy policy. Simply put, we’ll sometimes use our model for choosing<br>the action, and sometimes we’ll just sample one uniformly. The<br>probability of choosing a random action will start at <code>EPS_START</code><br>and will decay exponentially towards <code>EPS_END</code>. <code>EPS_DECAY</code><br>controls the rate of the decay.</li><li><code>plot_durations</code> - a helper for plotting the durations of episodes,<br>along with an average over the last 100 episodes (the measure used in<br>the official evaluations). The plot will be underneath the cell<br>containing the main training loop, and will update after every<br>episode.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.999</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get screen size so that we can initialize layers correctly based on shape</span></span><br><span class="line"><span class="comment"># returned from AI gym. Typical dimensions at this point are close to 3x40x90</span></span><br><span class="line"><span class="comment"># which is the result of a clamped and down-scaled render buffer in get_screen()</span></span><br><span class="line">init_screen = get_screen()</span><br><span class="line">_, _, screen_height, screen_width = init_screen.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get number of actions from gym action space</span></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line"></span><br><span class="line">policy_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net = DQN(screen_height, screen_width, n_actions).to(device)</span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.RMSprop(policy_net.parameters())</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * \</span><br><span class="line">        math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(state).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">episode_durations = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_durations</span><span class="params">()</span>:</span></span><br><span class="line">    plt.figure(<span class="number">2</span>)</span><br><span class="line">    plt.clf()</span><br><span class="line">    durations_t = torch.tensor(episode_durations, dtype=torch.float)</span><br><span class="line">    plt.title(<span class="string">'Training...'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Episode'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Duration'</span>)</span><br><span class="line">    plt.plot(durations_t.numpy())</span><br><span class="line">    <span class="comment"># Take 100 episode averages and plot them too</span></span><br><span class="line">    <span class="keyword">if</span> len(durations_t) &gt;= <span class="number">100</span>:</span><br><span class="line">        means = durations_t.unfold(<span class="number">0</span>, <span class="number">100</span>, <span class="number">1</span>).mean(<span class="number">1</span>).view(<span class="number">-1</span>)</span><br><span class="line">        means = torch.cat((torch.zeros(<span class="number">99</span>), means))</span><br><span class="line">        plt.plot(means.numpy())</span><br><span class="line"></span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line">    <span class="keyword">if</span> is_ipython:</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">        display.display(plt.gcf())</span><br></pre></td></tr></table></figure><p>Training loop</p><p>Finally, the code for training our model.</p><p>Here, you can find an <code>optimize_model</code> function that performs a<br>single step of the optimization. It first samples a batch, concatenates<br>all the tensors into a single one, computes $Q(s_t, a_t)$ and<br>$V(s_{t+1}) = \max_a Q(s_{t+1}, a)$, and combines them into our<br>loss. By defition we set $V(s) = 0$ if $s$ is a terminal<br>state. We also use a target network to compute $V(s_{t+1})$ for<br>added stability. The target network has its weights kept frozen most of<br>the time, but is updated with the policy network’s weights every so often.<br>This is usually a set number of steps but we shall use episodes for<br>simplicity.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    <span class="comment"># Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for</span></span><br><span class="line">    <span class="comment"># detailed explanation). This converts batch-array of Transitions</span></span><br><span class="line">    <span class="comment"># to Transition of batch-arrays.</span></span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute a mask of non-final states and concatenate the batch elements</span></span><br><span class="line">    <span class="comment"># (a final state would've been the one after which simulation ended)</span></span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span> s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">                                          batch.next_state)), device=device, dtype=torch.bool)</span><br><span class="line">    non_final_next_states = torch.cat([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state</span><br><span class="line">                                                <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>])</span><br><span class="line">    state_batch = torch.cat(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.cat(batch.reward)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q(s_t, a) - the model computes Q(s_t), then we select the</span></span><br><span class="line">    <span class="comment"># columns of actions taken. These are the actions which would've been taken</span></span><br><span class="line">    <span class="comment"># for each batch state according to policy_net</span></span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute V(s_&#123;t+1&#125;) for all next states.</span></span><br><span class="line">    <span class="comment"># Expected values of actions for non_final_next_states are computed based</span></span><br><span class="line">    <span class="comment"># on the "older" target_net; selecting their best reward with max(1)[0].</span></span><br><span class="line">    <span class="comment"># This is merged based on the mask, such that we'll have either the expected</span></span><br><span class="line">    <span class="comment"># state value or 0 in case the state was final.</span></span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE, device=device)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    <span class="comment"># Compute the expected Q values</span></span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Huber loss</span></span><br><span class="line">    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimize the model</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p>Below, you can find the main training loop. At the beginning we reset<br>the environment and initialize the <code>state</code> Tensor. Then, we sample<br>an action, execute it, observe the next screen and the reward (always<br>1), and optimize our model once. When the episode ends (our model<br>fails), we restart the loop.</p><p>Below, <code>num_episodes</code> is set small. You should download<br>the notebook and run lot more epsiodes, such as 300+ for meaningful<br>duration improvements.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    <span class="comment"># Initialize the environment and state</span></span><br><span class="line">    env.reset()</span><br><span class="line">    last_screen = get_screen()</span><br><span class="line">    current_screen = get_screen()</span><br><span class="line">    state = current_screen - last_screen</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        <span class="comment"># Select and perform an action</span></span><br><span class="line">        action = select_action(state)</span><br><span class="line">        _, reward, done, _ = env.step(action.item())</span><br><span class="line">        reward = torch.tensor([reward], device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Observe new state</span></span><br><span class="line">        last_screen = current_screen</span><br><span class="line">        current_screen = get_screen()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> done:</span><br><span class="line">            next_state = current_screen - last_screen</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Store the transition in memory</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Move to the next state</span></span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform one step of the optimization (on the target network)</span></span><br><span class="line">        optimize_model()</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            episode_durations.append(t + <span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># Update the target network, copying all weights and biases in DQN</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Complete'</span>)</span><br><span class="line">env.render()</span><br><span class="line">env.close()</span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>&lt;Figure size 432x288 with 0 Axes&gt;Complete&lt;Figure size 432x288 with 0 Axes&gt;</code></pre><p>Here is the diagram that illustrates the overall resulting data flow.</p><p><img src="https://pytorch.org/tutorials/_images/reinforcement_learning_diagram.jpg" alt></p><p>Actions are chosen either randomly or based on a policy, getting the next<br>step sample from the gym environment. We record the results in the<br>replay memory and also run optimization step on every iteration.<br>Optimization picks a random batch from the replay memory to do training of the<br>new policy. “Older” target_net is also used in optimization to compute the<br>expected Q values; it is updated occasionally to keep it current.</p><h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample:采样</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Reinforcement-Learning:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Text" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Text-TORCHTEXT的语言翻译</title>
    <link href="http://yoursite.com/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91/</id>
    <published>2020-07-25T01:20:51.000Z</published>
    <updated>2020-07-25T01:23:00.725Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Text-TORCHTEXT的语言翻译:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Language-Translation-with-TorchText"><a href="#Language-Translation-with-TorchText" class="headerlink" title="Language Translation with TorchText"></a>Language Translation with TorchText</h1><p>This tutorial shows how to use several convenience classes of <code>torchtext</code> to preprocess<br>data from a well-known dataset containing sentences in both English and German and use it to<br>train a sequence-to-sequence model with attention that can translate German sentences<br>into English.</p><p>It is based off of<br><code>this tutorial &lt;https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code><strong><br>from PyTorch community member <code>Ben Trevett &lt;https://github.com/bentrevett&gt;</code></strong><br>and was created by <code>Seth Weidman &lt;https://github.com/SethHWeidman/&gt;</code>__ with Ben’s permission.</p><p>By the end of this tutorial, you will be able to:</p><ul><li>Preprocess sentences into a commonly-used format for NLP modeling using the following <code>torchtext</code> convenience classes:<ul><li><code>TranslationDataset &lt;https://torchtext.readthedocs.io/en/latest/datasets.html#torchtext.datasets.TranslationDataset&gt;</code>__</li><li><code>Field &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field&gt;</code>__</li><li><code>BucketIterator &lt;https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator&gt;</code>__</li></ul></li></ul><h2 id="Field-and-TranslationDataset"><a href="#Field-and-TranslationDataset" class="headerlink" title="Field and TranslationDataset"></a><code>Field</code> and <code>TranslationDataset</code></h2><p><code>torchtext</code> has utilities for creating datasets that can be easily<br>iterated through for the purposes of creating a language translation<br>model. One key class is a<br><code>Field &lt;https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L64&gt;</code><strong>,<br>which specifies the way each sentence should be preprocessed, and another is the<br><code>TranslationDataset</code> ; <code>torchtext</code><br>has several such datasets; in this tutorial we’ll use the<br><code>Multi30k dataset &lt;https://github.com/multi30k/dataset&gt;</code></strong>, which contains about<br>30,000 sentences (averaging about 13 words in length) in both English and German.</p><p>Note: the tokenization in this tutorial requires <code>Spacy &lt;https://spacy.io&gt;</code><strong><br>We use Spacy because it provides strong support for tokenization in languages<br>other than English. <code>torchtext</code> provides a <code>basic_english</code> tokenizer<br>and supports other tokenizers for English (e.g.<br><code>Moses &lt;https://bitbucket.org/luismsgomes/mosestokenizer/src/default/&gt;</code></strong>)<br>but for language translation - where multiple languages are required -<br>Spacy is your best bet.</p><p>To run this tutorial, first install <code>spacy</code> using <code>pip</code> or <code>conda</code>.<br>Next, download the raw data for the English and German Spacy tokenizers:</p><p>::</p><p>   python -m spacy download en<br>   python -m spacy download de</p><p>With Spacy installed, the following code will tokenize each of the sentences<br>in the <code>TranslationDataset</code> based on the tokenizer defined in the <code>Field</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> Multi30k</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, BucketIterator</span><br><span class="line"></span><br><span class="line">SRC = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"de"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">TRG = Field(tokenize = <span class="string">"spacy"</span>,</span><br><span class="line">            tokenizer_language=<span class="string">"en"</span>,</span><br><span class="line">            init_token = <span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">            eos_token = <span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">            lower = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_data, valid_data, test_data = Multi30k.splits(exts = (<span class="string">'.de'</span>, <span class="string">'.en'</span>),</span><br><span class="line">                                                    fields = (SRC, TRG))</span><br></pre></td></tr></table></figure><pre><code>downloading training.tar.gz.data\multi30k\training.tar.gz: 100%|█████████████████████████████████████████████| 1.21M/1.21M [00:35&lt;00:00, 33.8kB/s]downloading validation.tar.gz.data\multi30k\validation.tar.gz: 100%|███████████████████████████████████████████| 46.3k/46.3k [00:01&lt;00:00, 35.0kB/s]downloading mmt_task1_test2016.tar.gz.data\multi30k\mmt_task1_test2016.tar.gz: 100%|███████████████████████████████████| 66.2k/66.2k [00:02&lt;00:00, 26.5kB/s]</code></pre><p>Now that we’ve defined <code>train_data</code>, we can see an extremely useful<br>feature of <code>torchtext</code>‘s <code>Field</code>: the <code>build_vocab</code> method<br>now allows us to create the vocabulary associated with each language</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SRC.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br><span class="line">TRG.build_vocab(train_data, min_freq = <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>Once these lines of code have been run, <code>SRC.vocab.stoi</code> will  be a<br>dictionary with the tokens in the vocabulary as keys and their<br>corresponding indices as values; <code>SRC.vocab.itos</code> will be the same<br>dictionary with the keys and values swapped. We won’t make extensive<br>use of this fact in this tutorial, but this will likely be useful in<br>other NLP tasks you’ll encounter.</p><h2 id="BucketIterator"><a href="#BucketIterator" class="headerlink" title="BucketIterator"></a><code>BucketIterator</code></h2><p>The last <code>torchtext</code> specific feature we’ll use is the <code>BucketIterator</code>,<br>which is easy to use since it takes a <code>TranslationDataset</code> as its<br>first argument. Specifically, as the docs say:<br>Defines an iterator that batches examples of similar lengths together.<br>Minimizes amount of padding needed while producing freshly shuffled<br>batches for each new epoch. See pool for the bucketing procedure used.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_iterator, valid_iterator, test_iterator = BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data),</span><br><span class="line">    batch_size = BATCH_SIZE,</span><br><span class="line">    device = device)</span><br></pre></td></tr></table></figure><p>These iterators can be called just like <code>DataLoader</code>s; below, in<br>the <code>train</code> and <code>evaluate</code> functions, they are called simply with:</p><p>::</p><p>   for i, batch in enumerate(iterator):</p><p>Each <code>batch</code> then has <code>src</code> and <code>trg</code> attributes:</p><p>::</p><p>   src = batch.src<br>   trg = batch.trg</p><h2 id="Defining-our-nn-Module-and-Optimizer"><a href="#Defining-our-nn-Module-and-Optimizer" class="headerlink" title="Defining our nn.Module and Optimizer"></a>Defining our <code>nn.Module</code> and <code>Optimizer</code></h2><p>That’s mostly it from a <code>torchtext</code> perspecive: with the dataset built<br>and the iterator defined, the rest of this tutorial simply defines our<br>model as an <code>nn.Module</code>, along with an <code>Optimizer</code>, and then trains it.</p><p>Our model specifically, follows the architecture described<br><code>here &lt;https://arxiv.org/abs/1409.0473&gt;</code><strong> (you can find a<br>significantly more commented version<br><code>here &lt;https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb&gt;</code></strong>).</p><p>Note: this model is just an example model that can be used for language<br>translation; we choose it because it is a standard model for the task,<br>not because it is the recommended model to use for translation. As you’re<br>likely aware, state-of-the-art models are currently based on Transformers;<br>you can see PyTorch’s capabilities for implementing Transformer layers<br><code>here &lt;https://pytorch.org/docs/stable/nn.html#transformer-layers&gt;</code>__; and<br>in particular, the “attention” used in the model below is different from<br>the multi-headed self-attention present in a transformer model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Tuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 input_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: float)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(enc_hid_dim * <span class="number">2</span>, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(src))</span><br><span class="line"></span><br><span class="line">        outputs, hidden = self.rnn(embedded)</span><br><span class="line"></span><br><span class="line">        hidden = torch.tanh(self.fc(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attn_dim: int)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn_in = (enc_hid_dim * <span class="number">2</span>) + dec_hid_dim</span><br><span class="line"></span><br><span class="line">        self.attn = nn.Linear(self.attn_in, attn_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        src_len = encoder_outputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        repeated_decoder_hidden = decoder_hidden.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, src_len, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        energy = torch.tanh(self.attn(torch.cat((</span><br><span class="line">            repeated_decoder_hidden,</span><br><span class="line">            encoder_outputs),</span><br><span class="line">            dim = <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">        attention = torch.sum(energy, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> F.softmax(attention, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 output_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 emb_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 enc_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dec_hid_dim: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attention: nn.Module)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.enc_hid_dim = enc_hid_dim</span><br><span class="line">        self.dec_hid_dim = dec_hid_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.attention = attention</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_dim, emb_dim)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.GRU((enc_hid_dim * <span class="number">2</span>) + emb_dim, dec_hid_dim)</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weighted_encoder_rep</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                              decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              encoder_outputs: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        a = self.attention(decoder_hidden, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        a = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = torch.bmm(a, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weighted_encoder_rep</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                input: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                decoder_hidden: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder_outputs: Tensor)</span> -&gt; Tuple[Tensor]:</span></span><br><span class="line"></span><br><span class="line">        input = input.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(input))</span><br><span class="line"></span><br><span class="line">        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,</span><br><span class="line">                                                          encoder_outputs)</span><br><span class="line"></span><br><span class="line">        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        embedded = embedded.squeeze(<span class="number">0</span>)</span><br><span class="line">        output = output.squeeze(<span class="number">0</span>)</span><br><span class="line">        weighted_encoder_rep = weighted_encoder_rep.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = self.out(torch.cat((output,</span><br><span class="line">                                     weighted_encoder_rep,</span><br><span class="line">                                     embedded), dim = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, decoder_hidden.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 decoder: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device: torch.device)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                src: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                trg: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                teacher_forcing_ratio: float = <span class="number">0.5</span>)</span> -&gt; Tensor:</span></span><br><span class="line"></span><br><span class="line">        batch_size = src.shape[<span class="number">1</span>]</span><br><span class="line">        max_len = trg.shape[<span class="number">0</span>]</span><br><span class="line">        trg_vocab_size = self.decoder.output_dim</span><br><span class="line"></span><br><span class="line">        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)</span><br><span class="line"></span><br><span class="line">        encoder_outputs, hidden = self.encoder(src)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># first input to the decoder is the &lt;sos&gt; token</span></span><br><span class="line">        output = trg[<span class="number">0</span>,:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, max_len):</span><br><span class="line">            output, hidden = self.decoder(output, hidden, encoder_outputs)</span><br><span class="line">            outputs[t] = output</span><br><span class="line">            teacher_force = random.random() &lt; teacher_forcing_ratio</span><br><span class="line">            top1 = output.max(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            output = (trg[t] <span class="keyword">if</span> teacher_force <span class="keyword">else</span> top1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = len(SRC.vocab)</span><br><span class="line">OUTPUT_DIM = len(TRG.vocab)</span><br><span class="line"><span class="comment"># ENC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># DEC_EMB_DIM = 256</span></span><br><span class="line"><span class="comment"># ENC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># DEC_HID_DIM = 512</span></span><br><span class="line"><span class="comment"># ATTN_DIM = 64</span></span><br><span class="line"><span class="comment"># ENC_DROPOUT = 0.5</span></span><br><span class="line"><span class="comment"># DEC_DROPOUT = 0.5</span></span><br><span class="line"></span><br><span class="line">ENC_EMB_DIM = <span class="number">32</span></span><br><span class="line">DEC_EMB_DIM = <span class="number">32</span></span><br><span class="line">ENC_HID_DIM = <span class="number">64</span></span><br><span class="line">DEC_HID_DIM = <span class="number">64</span></span><br><span class="line">ATTN_DIM = <span class="number">8</span></span><br><span class="line">ENC_DROPOUT = <span class="number">0.5</span></span><br><span class="line">DEC_DROPOUT = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)</span><br><span class="line"></span><br><span class="line">attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)</span><br><span class="line"></span><br><span class="line">dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)</span><br><span class="line"></span><br><span class="line">model = Seq2Seq(enc, dec, device).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">            nn.init.normal_(param.data, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nn.init.constant_(param.data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.apply(init_weights)</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model: nn.Module)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure><pre><code>The model has 1,856,653 trainable parameters</code></pre><p>Note: when scoring the performance of a language translation model in<br>particular, we have to tell the <code>nn.CrossEntropyLoss</code> function to<br>ignore the indices where the target is simply padding.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PAD_IDX = TRG.vocab.stoi[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)</span><br></pre></td></tr></table></figure><p>Finally, we can train and evaluate this model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">          optimizer: optim.Optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">          criterion: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">          clip: float)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">        src = batch.src</span><br><span class="line">        trg = batch.trg</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(src, trg)</span><br><span class="line"></span><br><span class="line">        output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">        trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">             iterator: BucketIterator,</span></span></span><br><span class="line"><span class="function"><span class="params">             criterion: nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line"></span><br><span class="line">            src = batch.src</span><br><span class="line">            trg = batch.trg</span><br><span class="line"></span><br><span class="line">            output = model(src, trg, <span class="number">0</span>) <span class="comment">#turn off teacher forcing</span></span><br><span class="line"></span><br><span class="line">            output = output[<span class="number">1</span>:].view(<span class="number">-1</span>, output.shape[<span class="number">-1</span>])</span><br><span class="line">            trg = trg[<span class="number">1</span>:].view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            loss = criterion(output, trg)</span><br><span class="line"></span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time: int,</span></span></span><br><span class="line"><span class="function"><span class="params">               end_time: int)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line">CLIP = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)</span><br><span class="line">    valid_loss = evaluate(model, valid_iterator, criterion)</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train PPL: <span class="subst">&#123;math.exp(train_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. PPL: <span class="subst">&#123;math.exp(valid_loss):<span class="number">7.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">test_loss = evaluate(model, test_iterator, criterion)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'| Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test PPL: <span class="subst">&#123;math.exp(test_loss):<span class="number">7.3</span>f&#125;</span> |'</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch: 01 | Time: 15m 34s    Train Loss: 5.681 | Train PPL: 293.100     Val. Loss: 5.244 |  Val. PPL: 189.491Epoch: 02 | Time: 18m 13s    Train Loss: 5.039 | Train PPL: 154.341     Val. Loss: 5.152 |  Val. PPL: 172.773Epoch: 03 | Time: 15m 47s    Train Loss: 4.788 | Train PPL: 120.088     Val. Loss: 5.044 |  Val. PPL: 155.033Epoch: 04 | Time: 15m 27s    Train Loss: 4.619 | Train PPL: 101.417     Val. Loss: 5.146 |  Val. PPL: 171.670Epoch: 05 | Time: 16m 16s    Train Loss: 4.491 | Train PPL:  89.179     Val. Loss: 5.014 |  Val. PPL: 150.444Epoch: 06 | Time: 17m 34s    Train Loss: 4.394 | Train PPL:  80.928     Val. Loss: 5.014 |  Val. PPL: 150.472Epoch: 07 | Time: 18m 31s    Train Loss: 4.306 | Train PPL:  74.153     Val. Loss: 4.899 |  Val. PPL: 134.150Epoch: 08 | Time: 18m 48s    Train Loss: 4.255 | Train PPL:  70.459     Val. Loss: 4.872 |  Val. PPL: 130.520Epoch: 09 | Time: 18m 21s    Train Loss: 4.200 | Train PPL:  66.700     Val. Loss: 4.807 |  Val. PPL: 122.399Epoch: 10 | Time: 18m 59s    Train Loss: 4.142 | Train PPL:  62.920     Val. Loss: 4.644 |  Val. PPL: 103.988| Test Loss: 4.650 | Test PPL: 104.534 |</code></pre><h2 id="Next-steps"><a href="#Next-steps" class="headerlink" title="Next steps"></a>Next steps</h2><ul><li>Check out the rest of Ben Trevett’s tutorials using <code>torchtext</code><br><code>here &lt;https://github.com/bentrevett/&gt;</code>__</li><li>Stay tuned for a tutorial using other <code>torchtext</code> features along<br>with <code>nn.Transformer</code> for language modeling via next word prediction!</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Text-TORCHTEXT的语言翻译:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Text" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Text-TORCHTEXT的文本分类</title>
    <link href="http://yoursite.com/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2020/07/25/Pytorch-Text-TORCHTEXT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</id>
    <published>2020-07-25T01:15:46.000Z</published>
    <updated>2020-07-25T01:19:42.009Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Text-TORCHTEXT的文本分类:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Text-Classification-with-TorchText"><a href="#Text-Classification-with-TorchText" class="headerlink" title="Text Classification with TorchText"></a>Text Classification with TorchText</h1><p>This tutorial shows how to use the text classification datasets<br>in <code>torchtext</code>, including</p><p>::</p><ul><li>AG_NEWS,</li><li>SogouNews,</li><li>DBpedia,</li><li>YelpReviewPolarity,</li><li>YelpReviewFull,</li><li>YahooAnswers,</li><li>AmazonReviewPolarity,</li><li>AmazonReviewFull</li></ul><p>This example shows how to train a supervised learning algorithm for<br>classification using one of these <code>TextClassification</code> datasets.</p><h2 id="Load-data-with-ngrams"><a href="#Load-data-with-ngrams" class="headerlink" title="Load data with ngrams"></a>Load data with ngrams</h2><p>A bag of ngrams feature is applied to capture some partial information<br>about the local word order. In practice, bi-gram or tri-gram are applied<br>to provide more benefits as word groups than only one word. An example:</p><p>::</p><p>   “load data with ngrams”<br>   Bi-grams results: “load data”, “data with”, “with ngrams”<br>   Tri-grams results: “load data with”, “data with ngrams”</p><p><code>TextClassification</code> Dataset supports the ngrams method. By setting<br>ngrams to 2, the example text in the dataset will be a list of single<br>words plus bi-grams string.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> text_classification</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> torchtext.utils <span class="keyword">import</span> extract_archive, unicode_csv_reader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets.text_classification <span class="keyword">import</span> _csv_iterator,_create_data_from_iterator</span><br><span class="line">NGRAMS = <span class="number">2</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./.data'</span>):</span><br><span class="line">os.mkdir(<span class="string">'./.data'</span>)</span><br><span class="line"><span class="comment"># train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](</span></span><br><span class="line"><span class="comment">#     root='./.data', ngrams=NGRAMS, vocab=None)</span></span><br><span class="line"> <span class="comment">#定义创建数据集函数，原函数在torchtext.datasets.text_classification文件中，本教程所需参数直接设成了默认值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_setup_datasets</span><span class="params">(dataset_tar=<span class="string">'./.data/ag_news_csv.tar.gz'</span>,dataset_name=<span class="string">"AG_NEWS"</span>, root=<span class="string">'./.data'</span>, ngrams=NGRAMS, vocab=None, include_unk=False)</span>:</span></span><br><span class="line">    <span class="comment"># 注释掉下载数据的代码</span></span><br><span class="line">    <span class="comment">#     dataset_tar = download_from_url(URLS[dataset_name], root=root)</span></span><br><span class="line">    extracted_files = extract_archive(dataset_tar)  <span class="comment">#解压数据文件</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> extracted_files:</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'train.csv'</span>):</span><br><span class="line">            train_csv_path = fname</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(<span class="string">'test.csv'</span>):</span><br><span class="line">            test_csv_path = fname</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        logging.info(<span class="string">'Building Vocab based on &#123;&#125;'</span>.format(train_csv_path))</span><br><span class="line">        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams)) <span class="comment">#创建词典</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(vocab, Vocab):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"Passed vocabulary is not of type Vocab"</span>)</span><br><span class="line">    logging.info(<span class="string">'Vocab has &#123;&#125; entries'</span>.format(len(vocab)))</span><br><span class="line">    logging.info(<span class="string">'Creating training data'</span>)</span><br><span class="line">    train_data, train_labels = _create_data_from_iterator(   <span class="comment">#创建训练数据</span></span><br><span class="line">        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk) </span><br><span class="line">    logging.info(<span class="string">'Creating testing data'</span>)</span><br><span class="line">    test_data, test_labels = _create_data_from_iterator(   <span class="comment">#创建测试数据</span></span><br><span class="line">        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=<span class="literal">True</span>), include_unk)</span><br><span class="line">    <span class="keyword">if</span> len(train_labels ^ test_labels) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Training and test labels don't match"</span>)</span><br><span class="line">    <span class="keyword">return</span> (TextClassificationDataset(vocab, train_data, train_labels),  <span class="comment">#返回数据集实例</span></span><br><span class="line">            TextClassificationDataset(vocab, test_data, test_labels))</span><br><span class="line">train_dataset, test_dataset = _setup_datasets()</span><br><span class="line">BATCH_SIZE = <span class="number">16</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><pre><code>120000lines [00:21, 5495.55lines/s]120000lines [00:54, 2186.84lines/s]7600lines [00:03, 1978.44lines/s]</code></pre><h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>The model is composed of the<br><code>EmbeddingBag &lt;https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag&gt;</code>__<br>layer and the linear layer (see the figure below). <code>nn.EmbeddingBag</code><br>computes the mean value of a “bag” of embeddings. The text entries here<br>have different lengths. <code>nn.EmbeddingBag</code> requires no padding here<br>since the text lengths are saved in offsets.</p><p>Additionally, since <code>nn.EmbeddingBag</code> accumulates the average across<br>the embeddings on the fly, <code>nn.EmbeddingBag</code> can enhance the<br>performance and memory efficiency to process a sequence of tensors.</p><p><img src="https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextSentiment</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_dim, num_class)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(embed_dim, num_class)</span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.5</span></span><br><span class="line">        self.embedding.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.fc.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, offsets)</span>:</span></span><br><span class="line">        embedded = self.embedding(text, offsets)</span><br><span class="line">        <span class="keyword">return</span> self.fc(embedded)</span><br></pre></td></tr></table></figure><h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The AG_NEWS dataset has four labels and therefore the number of classes<br>is four.</p><p>::</p><p>   1 : World<br>   2 : Sports<br>   3 : Business<br>   4 : Sci/Tec</p><p>The vocab size is equal to the length of vocab (including single word<br>and ngrams). The number of classes is equal to the number of labels,<br>which is four in AG_NEWS case.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = len(train_dataset.get_vocab())</span><br><span class="line">EMBED_DIM = <span class="number">32</span></span><br><span class="line">NUN_CLASS = len(train_dataset.get_labels())</span><br><span class="line">model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)</span><br></pre></td></tr></table></figure><h2 id="Functions-used-to-generate-batch"><a href="#Functions-used-to-generate-batch" class="headerlink" title="Functions used to generate batch"></a>Functions used to generate batch</h2><p>Since the text entries have different lengths, a custom function<br>generate_batch() is used to generate data batches and offsets. The<br>function is passed to <code>collate_fn</code> in <code>torch.utils.data.DataLoader</code>.<br>The input to <code>collate_fn</code> is a list of tensors with the size of<br>batch_size, and the <code>collate_fn</code> function packs them into a<br>mini-batch. Pay attention here and make sure that <code>collate_fn</code> is<br>declared as a top level def. This ensures that the function is available<br>in each worker.</p><p>The text entries in the original data batch input are packed into a list<br>and concatenated as a single tensor as the input of <code>nn.EmbeddingBag</code>.<br>The offsets is a tensor of delimiters to represent the beginning index<br>of the individual sequence in the text tensor. Label is a tensor saving<br>the labels of individual text entries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch)</span>:</span></span><br><span class="line">    label = torch.tensor([entry[<span class="number">0</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch])</span><br><span class="line">    text = [entry[<span class="number">1</span>] <span class="keyword">for</span> entry <span class="keyword">in</span> batch]</span><br><span class="line">    offsets = [<span class="number">0</span>] + [len(entry) <span class="keyword">for</span> entry <span class="keyword">in</span> text]</span><br><span class="line">    <span class="comment"># torch.Tensor.cumsum returns the cumulative sum</span></span><br><span class="line">    <span class="comment"># of elements in the dimension dim.</span></span><br><span class="line">    <span class="comment"># torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)</span></span><br><span class="line"></span><br><span class="line">    offsets = torch.tensor(offsets[:<span class="number">-1</span>]).cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    text = torch.cat(text)</span><br><span class="line">    <span class="keyword">return</span> text, offsets, label</span><br></pre></td></tr></table></figure><h2 id="Define-functions-to-train-the-model-and-evaluate-results"><a href="#Define-functions-to-train-the-model-and-evaluate-results" class="headerlink" title="Define functions to train the model and evaluate results."></a>Define functions to train the model and evaluate results.</h2><p><code>torch.utils.data.DataLoader &lt;https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader&gt;</code><strong><br>is recommended for PyTorch users, and it makes data loading in parallel<br>easily (a tutorial is<br><code>here &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code></strong>).<br>We use <code>DataLoader</code> here to load AG_NEWS datasets and send it to the<br>model for training/validation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_func</span><span class="params">(sub_train_)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the model</span></span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    train_acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>,</span><br><span class="line">                      collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> i, (text, offsets, cls) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        output = model(text, offsets)</span><br><span class="line">        loss = criterion(output, cls)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adjust the learning rate</span></span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_loss / len(sub_train_), train_acc / len(sub_train_)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(data_)</span>:</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    acc = <span class="number">0</span></span><br><span class="line">    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)</span><br><span class="line">    <span class="keyword">for</span> text, offsets, cls <span class="keyword">in</span> data:</span><br><span class="line">        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            output = model(text, offsets)</span><br><span class="line">            loss = criterion(output, cls)</span><br><span class="line">            loss += loss.item()</span><br><span class="line">            acc += (output.argmax(<span class="number">1</span>) == cls).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss / len(data_), acc / len(data_)</span><br></pre></td></tr></table></figure><h2 id="Split-the-dataset-and-run-the-model"><a href="#Split-the-dataset-and-run-the-model" class="headerlink" title="Split the dataset and run the model"></a>Split the dataset and run the model</h2><p>Since the original AG_NEWS has no valid dataset, we split the training<br>dataset into train/valid sets with a split ratio of 0.95 (train) and<br>0.05 (valid). Here we use<br><code>torch.utils.data.dataset.random_split &lt;https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split&gt;</code>__<br>function in PyTorch core library.</p><p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.<br>It is useful when training a classification problem with C classes.<br><code>SGD &lt;https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html&gt;</code></strong><br>implements stochastic gradient descent method as optimizer. The initial<br>learning rate is set to 4.0.<br><code>StepLR &lt;https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR&gt;</code>__<br>is used here to adjust the learning rate through epochs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> random_split</span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">min_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().to(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">4.0</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1</span>, gamma=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">train_len = int(len(train_dataset) * <span class="number">0.95</span>)</span><br><span class="line">sub_train_, sub_valid_ = \</span><br><span class="line">    random_split(train_dataset, [train_len, len(train_dataset) - train_len])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train_func(sub_train_)</span><br><span class="line">    valid_loss, valid_acc = test(sub_valid_)</span><br><span class="line"></span><br><span class="line">    secs = int(time.time() - start_time)</span><br><span class="line">    mins = secs / <span class="number">60</span></span><br><span class="line">    secs = secs % <span class="number">60</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch: %d'</span> %(epoch + <span class="number">1</span>), <span class="string">" | time in %d minutes, %d seconds"</span> %(mins, secs))</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>(train)\t|\tAcc: <span class="subst">&#123;train_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(train)'</span>)</span><br><span class="line">    print(<span class="string">f'\tLoss: <span class="subst">&#123;valid_loss:<span class="number">.4</span>f&#125;</span>(valid)\t|\tAcc: <span class="subst">&#123;valid_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(valid)'</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch: 1  | time in 1 minutes, 36 seconds    Loss: 0.0261(train)    |    Acc: 84.8%(train)    Loss: 0.0001(valid)    |    Acc: 90.5%(valid)Epoch: 2  | time in 1 minutes, 57 seconds    Loss: 0.0118(train)    |    Acc: 93.8%(train)    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)Epoch: 3  | time in 1 minutes, 35 seconds    Loss: 0.0069(train)    |    Acc: 96.4%(train)    Loss: 0.0001(valid)    |    Acc: 89.9%(valid)Epoch: 4  | time in 1 minutes, 36 seconds    Loss: 0.0038(train)    |    Acc: 98.1%(train)    Loss: 0.0001(valid)    |    Acc: 91.1%(valid)Epoch: 5  | time in 1 minutes, 37 seconds    Loss: 0.0023(train)    |    Acc: 99.0%(train)    Loss: 0.0001(valid)    |    Acc: 91.5%(valid)</code></pre><p>Running the model on GPU with the following information:</p><p>Epoch: 1 | time in 0 minutes, 11 seconds</p><p>::</p><pre><code>   Loss: 0.0263(train)     |       Acc: 84.5%(train)   Loss: 0.0001(valid)     |       Acc: 89.0%(valid)</code></pre><p>Epoch: 2 | time in 0 minutes, 10 seconds</p><p>::</p><pre><code>   Loss: 0.0119(train)     |       Acc: 93.6%(train)   Loss: 0.0000(valid)     |       Acc: 89.6%(valid)</code></pre><p>Epoch: 3 | time in 0 minutes, 9 seconds</p><p>::</p><pre><code>   Loss: 0.0069(train)     |       Acc: 96.4%(train)   Loss: 0.0000(valid)     |       Acc: 90.5%(valid)</code></pre><p>Epoch: 4 | time in 0 minutes, 11 seconds</p><p>::</p><pre><code>   Loss: 0.0038(train)     |       Acc: 98.2%(train)   Loss: 0.0000(valid)     |       Acc: 90.4%(valid)</code></pre><p>Epoch: 5 | time in 0 minutes, 11 seconds</p><p>::</p><pre><code>   Loss: 0.0022(train)     |       Acc: 99.0%(train)   Loss: 0.0000(valid)     |       Acc: 91.0%(valid)</code></pre><h2 id="Evaluate-the-model-with-test-dataset"><a href="#Evaluate-the-model-with-test-dataset" class="headerlink" title="Evaluate the model with test dataset"></a>Evaluate the model with test dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Checking the results of test dataset...'</span>)</span><br><span class="line">test_loss, test_acc = test(test_dataset)</span><br><span class="line">print(<span class="string">f'\tLoss: <span class="subst">&#123;test_loss:<span class="number">.4</span>f&#125;</span>(test)\t|\tAcc: <span class="subst">&#123;test_acc * <span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%(test)'</span>)</span><br></pre></td></tr></table></figure><pre><code>Checking the results of test dataset...    Loss: 0.0003(test)    |    Acc: 90.5%(test)</code></pre><p>Checking the results of test dataset…</p><p>::</p><pre><code>   Loss: 0.0237(test)      |       Acc: 90.5%(test)</code></pre><h2 id="Test-on-a-random-news"><a href="#Test-on-a-random-news" class="headerlink" title="Test on a random news"></a>Test on a random news</h2><p>Use the best model so far and test a golf news. The label information is<br>available<br><code>here &lt;https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS&gt;</code>__.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> ngrams_iterator</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line">ag_news_label = &#123;<span class="number">1</span> : <span class="string">"World"</span>,</span><br><span class="line">                 <span class="number">2</span> : <span class="string">"Sports"</span>,</span><br><span class="line">                 <span class="number">3</span> : <span class="string">"Business"</span>,</span><br><span class="line">                 <span class="number">4</span> : <span class="string">"Sci/Tec"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(text, model, vocab, ngrams)</span>:</span></span><br><span class="line">    tokenizer = get_tokenizer(<span class="string">"basic_english"</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        text = torch.tensor([vocab[token]</span><br><span class="line">                            <span class="keyword">for</span> token <span class="keyword">in</span> ngrams_iterator(tokenizer(text), ngrams)])</span><br><span class="line">        output = model(text, torch.tensor([<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output.argmax(<span class="number">1</span>).item() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">ex_text_str = <span class="string">"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \</span></span><br><span class="line"><span class="string">    enduring the season’s worst weather conditions on Sunday at The \</span></span><br><span class="line"><span class="string">    Open on his way to a closing 75 at Royal Portrush, which \</span></span><br><span class="line"><span class="string">    considering the wind and the rain was a respectable showing. \</span></span><br><span class="line"><span class="string">    Thursday’s first round at the WGC-FedEx St. Jude Invitational \</span></span><br><span class="line"><span class="string">    was another story. With temperatures in the mid-80s and hardly any \</span></span><br><span class="line"><span class="string">    wind, the Spaniard was 13 strokes better in a flawless round. \</span></span><br><span class="line"><span class="string">    Thanks to his best putting performance on the PGA Tour, Rahm \</span></span><br><span class="line"><span class="string">    finished with an 8-under 62 for a three-stroke lead, which \</span></span><br><span class="line"><span class="string">    was even more impressive considering he’d never played the \</span></span><br><span class="line"><span class="string">    front nine at TPC Southwind."</span></span><br><span class="line"></span><br><span class="line">vocab = train_dataset.get_vocab()</span><br><span class="line">model = model.to(<span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"This is a %s news"</span> %ag_news_label[predict(ex_text_str, model, vocab, <span class="number">2</span>)])</span><br></pre></td></tr></table></figure><pre><code>This is a Sports news</code></pre><p>This is a Sports news</p><p>You can find the code examples displayed in this note<br><code>here &lt;https://github.com/pytorch/text/tree/master/examples/text_classification&gt;</code>__.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Text-TORCHTEXT的文本分类:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Text" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Text-使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译</title>
    <link href="http://yoursite.com/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%E4%BD%BF%E7%94%A8Sequence2Sequence%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91/</id>
    <published>2020-07-25T01:08:23.000Z</published>
    <updated>2020-07-25T06:06:26.676Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Text-使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p>NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</p><hr><p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p><p>This is the third and final tutorial on doing “NLP From Scratch”, where we<br>write our own classes and functions to preprocess the data to do our NLP<br>modeling tasks. We hope after you complete this tutorial that you’ll proceed to<br>learn how <code>torchtext</code> can handle much of this preprocessing for you in the<br>three tutorials immediately following this one.</p><p>In this project we will be teaching a neural network to translate from<br>French to English.</p><p>::</p><pre><code>[KEY: &gt; input, = target, &lt; output]&gt; il est en train de peindre un tableau .= he is painting a picture .&lt; he is painting a picture .&gt; pourquoi ne pas essayer ce vin delicieux ?= why not try that delicious wine ?&lt; why not try that delicious wine ?&gt; elle n est pas poete mais romanciere .= she is not a poet but a novelist .&lt; she not not a poet but a novelist .&gt; vous etes trop maigre .= you re too skinny .&lt; you re all alone .</code></pre><p>… to varying degrees of success.</p><p>This is made possible by the simple but powerful idea of the <code>sequenceto sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code>__, in which two<br>recurrent neural networks work together to transform one sequence to<br>another. An encoder network condenses an input sequence into a vector,<br>and a decoder network unfolds that vector into a new sequence.</p><p>.. figure:: /_static/img/seq-seq-images/seq2seq.png<br>   :alt:</p><p>To improve upon this model we’ll use an <code>attentionmechanism &lt;https://arxiv.org/abs/1409.0473&gt;</code>__, which lets the decoder<br>learn to focus over a specific range of the input sequence.</p><p><strong>Recommended Reading:</strong></p><p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li><li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li><li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li><li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li></ul><p>It would also be useful to know about Sequence to Sequence networks and<br>how they work:</p><ul><li><code>Learning Phrase Representations using RNN Encoder-Decoder forStatistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li><li><code>Sequence to Sequence Learning with NeuralNetworks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li><li><code>Neural Machine Translation by Jointly Learning to Align andTranslate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li><li><code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li></ul><p>You will also find the previous tutorials on<br>:doc:<code>/intermediate/char_rnn_classification_tutorial</code><br>and :doc:<code>/intermediate/char_rnn_generation_tutorial</code><br>helpful as those concepts are very similar to the Encoder and Decoder<br>models, respectively.</p><p>And for more, read the papers that introduced these topics:</p><ul><li><code>Learning Phrase Representations using RNN Encoder-Decoder forStatistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li><li><code>Sequence to Sequence Learning with NeuralNetworks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li><li><code>Neural Machine Translation by Jointly Learning to Align andTranslate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li><li><code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li></ul><p><strong>Requirements</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><h1 id="Loading-data-files"><a href="#Loading-data-files" class="headerlink" title="Loading data files"></a>Loading data files</h1><p>The data for this project is a set of many thousands of English to<br>French translation pairs.</p><p><code>This question on Open Data StackExchange &lt;https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages&gt;</code>__<br>pointed me to the open translation site <a href="https://tatoeba.org/" target="_blank" rel="noopener">https://tatoeba.org/</a> which has<br>downloads available at <a href="https://tatoeba.org/eng/downloads" target="_blank" rel="noopener">https://tatoeba.org/eng/downloads</a> - and better<br>yet, someone did the extra work of splitting language pairs into<br>individual text files here: <a href="https://www.manythings.org/anki/" target="_blank" rel="noopener">https://www.manythings.org/anki/</a></p><p>The English to French pairs are too big to include in the repo, so<br>download to <code>data/eng-fra.txt</code> before continuing. The file is a tab<br>separated list of translation pairs:</p><p>::</p><pre><code>I am cold.    J&#39;ai froid.</code></pre><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p><p>Similar to the character encoding used in the character-level RNN<br>tutorials, we will be representing each word in a language as a one-hot<br>vector, or giant vector of zeros except for a single one (at the index<br>of the word). Compared to the dozens of characters that might exist in a<br>language, there are many many more words, so the encoding vector is much<br>larger. We will however cheat a bit and trim the data to only use a few<br>thousand words per language.</p><p>.. figure:: /_static/img/seq-seq-images/word-encoding.png<br>   :alt:</p><p>We’ll need a unique index per word to use as the inputs and targets of<br>the networks later. To keep track of all this we will use a helper class<br>called <code>Lang</code> which has word → index (<code>word2index</code>) and index → word<br>(<code>index2word</code>) dictionaries, as well as a count of each word<br><code>word2count</code> to use to later replace rare words.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">SOS_token = <span class="number">0</span></span><br><span class="line">EOS_token = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lang</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        self.word2count = &#123;&#125;</span><br><span class="line">        self.index2word = &#123;<span class="number">0</span>: <span class="string">"SOS"</span>, <span class="number">1</span>: <span class="string">"EOS"</span>&#125;</span><br><span class="line">        self.n_words = <span class="number">2</span>  <span class="comment"># Count SOS and EOS</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addSentence</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</span><br><span class="line">            self.addWord(word)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2index:</span><br><span class="line">            self.word2index[word] = self.n_words</span><br><span class="line">            self.word2count[word] = <span class="number">1</span></span><br><span class="line">            self.index2word[self.n_words] = word</span><br><span class="line">            self.n_words += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.word2count[word] += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>The files are all in Unicode, to simplify we will turn Unicode<br>characters to ASCII, make everything lowercase, and trim most<br>punctuation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to</span></span><br><span class="line"><span class="comment"># https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># 在Unicode中，某些字符能够用多个合法的编码表示，在需要比较字符串的程序中使用字符的多种表示会产生问题。 </span></span><br><span class="line"><span class="comment"># 为了修正这个问题，你可以使用unicodedata模块先将文本标准化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Lowercase, trim, and remove non-letter characters</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeString</span><span class="params">(s)</span>:</span></span><br><span class="line">    s = unicodeToAscii(s.lower().strip())</span><br><span class="line">    s = re.sub(<span class="string">r"([.!?])"</span>, <span class="string">r" \1"</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">r"[^a-zA-Z.!?]+"</span>, <span class="string">r" "</span>, s)</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><p>To read the data file we will split the file into lines, and then split<br>lines into pairs. The files are all English → Other Language, so if we<br>want to translate from Other Language → English I added the <code>reverse</code><br>flag to reverse the pairs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLangs</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    print(<span class="string">"Reading lines..."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Read the file and split into lines</span></span><br><span class="line">    lines = open(<span class="string">'data/%s-%s.txt'</span> % (lang1, lang2), encoding=<span class="string">'utf-8'</span>).\</span><br><span class="line">        read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split every line into pairs and normalize</span></span><br><span class="line">    pairs = [[normalizeString(s) <span class="keyword">for</span> s <span class="keyword">in</span> l.split(<span class="string">'\t'</span>)] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reverse pairs, make Lang instances</span></span><br><span class="line">    <span class="keyword">if</span> reverse:</span><br><span class="line">        pairs = [list(reversed(p)) <span class="keyword">for</span> p <span class="keyword">in</span> pairs]</span><br><span class="line">        input_lang = Lang(lang2)</span><br><span class="line">        output_lang = Lang(lang1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_lang = Lang(lang1)</span><br><span class="line">        output_lang = Lang(lang2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br></pre></td></tr></table></figure><p>Since there are a <em>lot</em> of example sentences and we want to train<br>something quickly, we’ll trim the data set to only relatively short and<br>simple sentences. Here the maximum length is 10 words (that includes<br>ending punctuation) and we’re filtering to sentences that translate to<br>the form “I am” or “He is” etc. (accounting for apostrophes replaced<br>earlier).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">MAX_LENGTH = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">eng_prefixes = (</span><br><span class="line">    <span class="string">"i am "</span>, <span class="string">"i m "</span>,</span><br><span class="line">    <span class="string">"he is"</span>, <span class="string">"he s "</span>,</span><br><span class="line">    <span class="string">"she is"</span>, <span class="string">"she s "</span>,</span><br><span class="line">    <span class="string">"you are"</span>, <span class="string">"you re "</span>,</span><br><span class="line">    <span class="string">"we are"</span>, <span class="string">"we re "</span>,</span><br><span class="line">    <span class="string">"they are"</span>, <span class="string">"they re "</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPair</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(p[<span class="number">0</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        len(p[<span class="number">1</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</span><br><span class="line">        p[<span class="number">1</span>].startswith(eng_prefixes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPairs</span><span class="params">(pairs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [pair <span class="keyword">for</span> pair <span class="keyword">in</span> pairs <span class="keyword">if</span> filterPair(pair)]</span><br></pre></td></tr></table></figure><p>The full process for preparing the data is:</p><ul><li>Read text file and split into lines, split lines into pairs</li><li>Normalize text, filter by length and content</li><li>Make word lists from sentences in pairs</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareData</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></span><br><span class="line">    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)</span><br><span class="line">    print(<span class="string">"Read %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    pairs = filterPairs(pairs)</span><br><span class="line">    print(<span class="string">"Trimmed to %s sentence pairs"</span> % len(pairs))</span><br><span class="line">    print(<span class="string">"Counting words..."</span>)</span><br><span class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</span><br><span class="line">        input_lang.addSentence(pair[<span class="number">0</span>])</span><br><span class="line">        output_lang.addSentence(pair[<span class="number">1</span>])</span><br><span class="line">    print(<span class="string">"Counted words:"</span>)</span><br><span class="line">    print(input_lang.name, input_lang.n_words)</span><br><span class="line">    print(output_lang.name, output_lang.n_words)</span><br><span class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_lang, output_lang, pairs = prepareData(<span class="string">'eng'</span>, <span class="string">'fra'</span>, <span class="literal">True</span>)</span><br><span class="line">print(random.choice(pairs))</span><br></pre></td></tr></table></figure><pre><code>Reading lines...Read 135842 sentence pairsTrimmed to 10599 sentence pairsCounting words...Counted words:fra 4345eng 2803[&#39;nous sommes tous sur le meme bateau .&#39;, &#39;we re all in the same boat .&#39;]</code></pre><h1 id="The-Seq2Seq-Model"><a href="#The-Seq2Seq-Model" class="headerlink" title="The Seq2Seq Model"></a>The Seq2Seq Model</h1><p>A Recurrent Neural Network, or RNN, is a network that operates on a<br>sequence and uses its own output as input for subsequent steps.</p><p>A <code>Sequence to Sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code><strong>, or<br>seq2seq network, or <code>Encoder Decodernetwork &lt;https://arxiv.org/pdf/1406.1078v3.pdf&gt;</code></strong>, is a model<br>consisting of two RNNs called the encoder and decoder. The encoder reads<br>an input sequence and outputs a single vector, and the decoder reads<br>that vector to produce an output sequence.</p><p><img src="https://pytorch.org/tutorials/_images/seq2seq.png" alt></p><p>Unlike sequence prediction with a single RNN, where every input<br>corresponds to an output, the seq2seq model frees us from sequence<br>length and order, which makes it ideal for translation between two<br>languages.</p><p>Consider the sentence “Je ne suis pas le chat noir” → “I am not the<br>black cat”. Most of the words in the input sentence have a direct<br>translation in the output sentence, but are in slightly different<br>orders, e.g. “chat noir” and “black cat”. Because of the “ne/pas”<br>construction there is also one more word in the input sentence. It would<br>be difficult to produce a correct translation directly from the sequence<br>of input words.</p><p>With a seq2seq model the encoder creates a single vector which, in the<br>ideal case, encodes the “meaning” of the input sequence into a single<br>vector — a single point in some N dimensional space of sentences.</p><h2 id="The-Encoder"><a href="#The-Encoder" class="headerlink" title="The Encoder"></a>The Encoder</h2><p>The encoder of a seq2seq network is a RNN that outputs some value for<br>every word from the input sentence. For every input word the encoder<br>outputs a vector and a hidden state, and uses the hidden state for the<br>next input word.</p><p><img src="https://pytorch.org/tutorials/_images/decoder-network.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size)</span>:</span></span><br><span class="line">        super(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = embedded</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure><h2 id="The-Decoder"><a href="#The-Decoder" class="headerlink" title="The Decoder"></a>The Decoder</h2><p>The decoder is another RNN that takes the encoder output vector(s) and<br>outputs a sequence of words to create the translation.</p><p>Simple Decoder</p><p>In the simplest seq2seq decoder we use only last output of the encoder.<br>This last output is sometimes called the <em>context vector</em> as it encodes<br>context from the entire sequence. This context vector is used as the<br>initial hidden state of the decoder.</p><p>At every step of decoding, the decoder is given an input token and<br>hidden state. The initial input token is the start-of-string <code>&lt;SOS&gt;</code><br>token, and the first hidden state is the context vector (the encoder’s<br>last hidden state).</p><p><img src="https://pytorch.org/tutorials/_images/attention-decoder-network.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(DecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(output_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line">        self.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        output = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        output = self.softmax(self.out(output[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure><p>I encourage you to train and observe the results of this model, but to<br>save space we’ll be going straight for the gold and introducing the<br>Attention Mechanism.</p><p>Attention Decoder</p><p>If only the context vector is passed betweeen the encoder and decoder,<br>that single vector carries the burden of encoding the entire sentence.</p><p>Attention allows the decoder network to “focus” on a different part of<br>the encoder’s outputs for every step of the decoder’s own outputs. First<br>we calculate a set of <em>attention weights</em>. These will be multiplied by<br>the encoder output vectors to create a weighted combination. The result<br>(called <code>attn_applied</code> in the code) should contain information about<br>that specific part of the input sequence, and thus help the decoder<br>choose the right output words.</p><p>.. figure:: <a href="https://i.imgur.com/1152PYf.png" target="_blank" rel="noopener">https://i.imgur.com/1152PYf.png</a><br>   :alt:</p><p>Calculating the attention weights is done with another feed-forward<br>layer <code>attn</code>, using the decoder’s input and hidden state as inputs.<br>Because there are sentences of all sizes in the training data, to<br>actually create and train this layer we have to choose a maximum<br>sentence length (input length, for encoder outputs) that it can apply<br>to. Sentences of the maximum length will use all the attention weights,<br>while shorter sentences will only use the first few.</p><p>.. figure:: /_static/img/seq-seq-images/attention-decoder-network.png<br>   :alt:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnDecoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">        super(AttnDecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.dropout_p = dropout_p</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</span><br><span class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(self.dropout_p)</span><br><span class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</span><br><span class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden, encoder_outputs)</span>:</span></span><br><span class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line"></span><br><span class="line">        attn_weights = F.softmax(</span><br><span class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)), dim=<span class="number">1</span>)</span><br><span class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>)</span><br><span class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br></pre></td></tr></table></figure><div class="alert alert-info"><h4>Note</h4><p>There are other forms of attention that work around the length  limitation by using a relative position approach. Read about "local  attention" in `Effective Approaches to Attention-based Neural Machine  Translation <https: arxiv.org abs 1508.04025>`__.</https:></p></div><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-Training-Data"><a href="#Preparing-Training-Data" class="headerlink" title="Preparing Training Data"></a>Preparing Training Data</h2><p>To train, for each pair we will need an input tensor (indexes of the<br>words in the input sentence) and target tensor (indexes of the words in<br>the target sentence). While creating these vectors we will append the<br>EOS token to both sequences.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">indexesFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [lang.word2index[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorFromSentence</span><span class="params">(lang, sentence)</span>:</span></span><br><span class="line">    indexes = indexesFromSentence(lang, sentence)</span><br><span class="line">    indexes.append(EOS_token)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(indexes, dtype=torch.long, device=device).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorsFromPair</span><span class="params">(pair)</span>:</span></span><br><span class="line">    input_tensor = tensorFromSentence(input_lang, pair[<span class="number">0</span>])</span><br><span class="line">    target_tensor = tensorFromSentence(output_lang, pair[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> (input_tensor, target_tensor)</span><br></pre></td></tr></table></figure><h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>To train we run the input sentence through the encoder, and keep track<br>of every output and the latest hidden state. Then the decoder is given<br>the <code>&lt;SOS&gt;</code> token as its first input, and the last hidden state of the<br>encoder as its first hidden state.</p><p>“Teacher forcing” is the concept of using the real target outputs as<br>each next input, instead of using the decoder’s guess as the next input.<br>Using teacher forcing causes it to converge faster but <code>when the trainednetwork is exploited, it may exhibitinstability &lt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&amp;rep=rep1&amp;type=pdf&gt;</code>__.</p><p>You can observe outputs of teacher-forced networks that read with<br>coherent grammar but wander far from the correct translation -<br>intuitively it has learned to represent the output grammar and can “pick<br>up” the meaning once the teacher tells it the first few words, but it<br>has not properly learned how to create the sentence from the translation<br>in the first place.</p><p>Because of the freedom PyTorch’s autograd gives us, we can randomly<br>choose to use teacher forcing or not with a simple if statement. Turn<br><code>teacher_forcing_ratio</code> up to use more of it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">teacher_forcing_ratio = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.zero_grad()</span><br><span class="line">    decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    input_length = input_tensor.size(<span class="number">0</span>)</span><br><span class="line">    target_length = target_tensor.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">        encoder_output, encoder_hidden = encoder(</span><br><span class="line">            input_tensor[ei], encoder_hidden)</span><br><span class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoder_input = torch.tensor([[SOS_token]], device=device)</span><br><span class="line"></span><br><span class="line">    decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">    use_teacher_forcing = <span class="literal">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_teacher_forcing:</span><br><span class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            decoder_input = target_tensor[di]  <span class="comment"># Teacher forcing</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            topv, topi = decoder_output.topk(<span class="number">1</span>)</span><br><span class="line">            decoder_input = topi.squeeze().detach()  <span class="comment"># detach from history as input</span></span><br><span class="line"></span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            <span class="keyword">if</span> decoder_input.item() == EOS_token:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.step()</span><br><span class="line">    decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item() / target_length</span><br></pre></td></tr></table></figure><p>This is a helper function to print time elapsed and estimated time<br>remaining given the current time and progress %.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asMinutes</span><span class="params">(s)</span>:</span></span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since, percent)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    es = s / (percent)</span><br><span class="line">    rs = es - s</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%s (- %s)'</span> % (asMinutes(s), asMinutes(rs))</span><br></pre></td></tr></table></figure><p>The whole training process looks like this:</p><ul><li>Start a timer</li><li>Initialize optimizers and criterion</li><li>Create set of training pairs</li><li>Start empty losses array for plotting</li></ul><p>Then we call <code>train</code> many times and occasionally print the progress (%<br>of examples, time so far, estimated time) and average loss.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainIters</span><span class="params">(encoder, decoder, n_iters, print_every=<span class="number">1000</span>, plot_every=<span class="number">100</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    plot_losses = []</span><br><span class="line">    print_loss_total = <span class="number">0</span>  <span class="comment"># Reset every print_every</span></span><br><span class="line">    plot_loss_total = <span class="number">0</span>  <span class="comment"># Reset every plot_every</span></span><br><span class="line"></span><br><span class="line">    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)</span><br><span class="line">    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)</span><br><span class="line">    training_pairs = [tensorsFromPair(random.choice(pairs))</span><br><span class="line">                      <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iters)]</span><br><span class="line">    criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">        training_pair = training_pairs[iter - <span class="number">1</span>]</span><br><span class="line">        input_tensor = training_pair[<span class="number">0</span>]</span><br><span class="line">        target_tensor = training_pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        loss = train(input_tensor, target_tensor, encoder,</span><br><span class="line">                     decoder, encoder_optimizer, decoder_optimizer, criterion)</span><br><span class="line">        print_loss_total += loss</span><br><span class="line">        plot_loss_total += loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">            print_loss_avg = print_loss_total / print_every</span><br><span class="line">            print_loss_total = <span class="number">0</span></span><br><span class="line">            print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start, iter / n_iters),</span><br><span class="line">                                         iter, iter / n_iters * <span class="number">100</span>, print_loss_avg))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">            plot_loss_avg = plot_loss_total / plot_every</span><br><span class="line">            plot_losses.append(plot_loss_avg)</span><br><span class="line">            plot_loss_total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    showPlot(plot_losses)</span><br></pre></td></tr></table></figure><h2 id="Plotting-results"><a href="#Plotting-results" class="headerlink" title="Plotting results"></a>Plotting results</h2><p>Plotting is done with matplotlib, using the array of loss values<br><code>plot_losses</code> saved while training.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.switch_backend(<span class="string">'agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPlot</span><span class="params">(points)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># this locator puts ticks at regular intervals</span></span><br><span class="line">    loc = ticker.MultipleLocator(base=<span class="number">0.2</span>)</span><br><span class="line">    ax.yaxis.set_major_locator(loc)</span><br><span class="line">    plt.plot(points)</span><br></pre></td></tr></table></figure><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Evaluation is mostly the same as training, but there are no targets so<br>we simply feed the decoder’s predictions back to itself for each step.<br>Every time it predicts a word we add it to the output string, and if it<br>predicts the EOS token we stop there. We also store the decoder’s<br>attention outputs for display later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(encoder, decoder, sentence, max_length=MAX_LENGTH)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_tensor = tensorFromSentence(input_lang, sentence)</span><br><span class="line">        input_length = input_tensor.size()[<span class="number">0</span>]</span><br><span class="line">        encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</span><br><span class="line">            encoder_output, encoder_hidden = encoder(input_tensor[ei],</span><br><span class="line">                                                     encoder_hidden)</span><br><span class="line">            encoder_outputs[ei] += encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        decoder_input = torch.tensor([[SOS_token]], device=device)  <span class="comment"># SOS</span></span><br><span class="line"></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">        decoded_words = []</span><br><span class="line">        decoder_attentions = torch.zeros(max_length, max_length)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(max_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            decoder_attentions[di] = decoder_attention.data</span><br><span class="line">            topv, topi = decoder_output.data.topk(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> topi.item() == EOS_token:</span><br><span class="line">                decoded_words.append(<span class="string">'&lt;EOS&gt;'</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                decoded_words.append(output_lang.index2word[topi.item()])</span><br><span class="line"></span><br><span class="line">            decoder_input = topi.squeeze().detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoded_words, decoder_attentions[:di + <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>We can evaluate random sentences from the training set and print out the<br>input, target, and output to make some subjective quality judgements:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateRandomly</span><span class="params">(encoder, decoder, n=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        pair = random.choice(pairs)</span><br><span class="line">        print(<span class="string">'&gt;'</span>, pair[<span class="number">0</span>])</span><br><span class="line">        print(<span class="string">'='</span>, pair[<span class="number">1</span>])</span><br><span class="line">        output_words, attentions = evaluate(encoder, decoder, pair[<span class="number">0</span>])</span><br><span class="line">        output_sentence = <span class="string">' '</span>.join(output_words)</span><br><span class="line">        print(<span class="string">'&lt;'</span>, output_sentence)</span><br><span class="line">        print(<span class="string">''</span>)</span><br></pre></td></tr></table></figure><h1 id="Training-and-Evaluating"><a href="#Training-and-Evaluating" class="headerlink" title="Training and Evaluating"></a>Training and Evaluating</h1><p>With all these helper functions in place (it looks like extra work, but<br>it makes it easier to run multiple experiments) we can actually<br>initialize a network and start training.</p><p>Remember that the input sentences were heavily filtered. For this small<br>dataset we can use relatively small networks of 256 hidden nodes and a<br>single GRU layer. After about 40 minutes on a MacBook CPU we’ll get some<br>reasonable results.</p><p>.. Note::<br>   If you run this notebook you can train, interrupt the kernel,<br>   evaluate, and continue training later. Comment out the lines where the<br>   encoder and decoder are initialized and run <code>trainIters</code> again.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line">encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)</span><br><span class="line">attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=<span class="number">0.1</span>).to(device)</span><br><span class="line"></span><br><span class="line">trainIters(encoder1, attn_decoder1, <span class="number">75000</span>, print_every=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure><pre><code>15m 15s (- 213m 42s) (5000 6%) 2.833536m 35s (- 237m 48s) (10000 13%) 2.308060m 22s (- 241m 29s) (15000 20%) 1.976886m 20s (- 237m 26s) (20000 26%) 1.7509103m 50s (- 207m 41s) (25000 33%) 1.5515123m 36s (- 185m 24s) (30000 40%) 1.3811141m 30s (- 161m 43s) (35000 46%) 1.2262161m 12s (- 141m 3s) (40000 53%) 1.1208180m 57s (- 120m 38s) (45000 60%) 1.0367195m 38s (- 97m 49s) (50000 66%) 0.9097206m 41s (- 75m 9s) (55000 73%) 0.8348217m 46s (- 54m 26s) (60000 80%) 0.7563228m 59s (- 35m 13s) (65000 86%) 0.7075246m 41s (- 17m 37s) (70000 93%) 0.6615263m 49s (- 0m 0s) (75000 100%) 0.6048</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evaluateRandomly(encoder1, attn_decoder1)</span><br></pre></td></tr></table></figure><pre><code>&gt; tu m ennuies .= you re annoying .&lt; you re embarrassing . &lt;EOS&gt;&gt; il est maintenant etudiant a la fac .= he s now a college student .&lt; he s now a student now . &lt;EOS&gt;&gt; je ne suis pas votre ami .= i m not your friend .&lt; i m not your friend . &lt;EOS&gt;&gt; je suis tres fatiguee par le dur labeur .= i am very tired from the hard work .&lt; i am very interested in the next . . &lt;EOS&gt;&gt; ce sont des illets .= they re carnations .&lt; they re carnations . &lt;EOS&gt;&gt; il est toujours en train de se plaindre .= he is constantly complaining .&lt; he is always complaining . &lt;EOS&gt;&gt; je suis submerge de travail .= i am swamped with work .&lt; i am swamped with work . &lt;EOS&gt;&gt; tu es mon meilleur ami .= you re my best friend .&lt; you are my best friend . &lt;EOS&gt;&gt; je vous suis reconnaissant pour votre aide .= i am grateful to you for your help .&lt; i am grateful for your help . &lt;EOS&gt;&gt; je vais te conter un secret .= i m going to tell you a secret .&lt; i m going to tell you a secret . &lt;EOS&gt;</code></pre><h2 id="Visualizing-Attention"><a href="#Visualizing-Attention" class="headerlink" title="Visualizing Attention"></a>Visualizing Attention</h2><p>A useful property of the attention mechanism is its highly interpretable<br>outputs. Because it is used to weight specific encoder outputs of the<br>input sequence, we can imagine looking where the network is focused most<br>at each time step.</p><p>You could simply run <code>plt.matshow(attentions)</code> to see attention output<br>displayed as a matrix, with the columns being input steps and rows being<br>output steps:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output_words, attentions = evaluate(</span><br><span class="line">    encoder1, attn_decoder1, <span class="string">"je suis trop froid ."</span>)</span><br><span class="line">plt.matshow(attentions.numpy())</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x2424cc9a438&gt;</code></pre><p>For a better viewing experience we will do the extra work of adding axes<br>and labels:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showAttention</span><span class="params">(input_sentence, output_words, attentions)</span>:</span></span><br><span class="line">    <span class="comment"># Set up figure with colorbar</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    cax = ax.matshow(attentions.numpy(), cmap=<span class="string">'bone'</span>)</span><br><span class="line">    fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up axes</span></span><br><span class="line">    ax.set_xticklabels([<span class="string">''</span>] + input_sentence.split(<span class="string">' '</span>) +</span><br><span class="line">                       [<span class="string">'&lt;EOS&gt;'</span>], rotation=<span class="number">90</span>)</span><br><span class="line">    ax.set_yticklabels([<span class="string">''</span>] + output_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show label at every tick</span></span><br><span class="line">    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateAndShowAttention</span><span class="params">(input_sentence)</span>:</span></span><br><span class="line">    output_words, attentions = evaluate(</span><br><span class="line">        encoder1, attn_decoder1, input_sentence)</span><br><span class="line">    print(<span class="string">'input ='</span>, input_sentence)</span><br><span class="line">    print(<span class="string">'output ='</span>, <span class="string">' '</span>.join(output_words))</span><br><span class="line">    showAttention(input_sentence, output_words, attentions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle a cinq ans de moins que moi ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"elle est trop petit ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"je ne crains pas de mourir ."</span>)</span><br><span class="line"></span><br><span class="line">evaluateAndShowAttention(<span class="string">"c est un jeune directeur plein de talent ."</span>)</span><br></pre></td></tr></table></figure><pre><code>input = elle a cinq ans de moins que moi .output = she is five years younger than me . &lt;EOS&gt;input = elle est trop petit .output = she is too short . &lt;EOS&gt;input = je ne crains pas de mourir .output = i m not scared to die . &lt;EOS&gt;C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator  # Remove the CWD from sys.path while we load stuff.C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator  # This is added back by InteractiveShellApp.init_path()C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.input = c est un jeune directeur plein de talent .output = he s a talented and . &lt;EOS&gt;</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul><li><p>Try with a different dataset</p><ul><li>Another language pair</li><li>Human → Machine (e.g. IOT commands)</li><li>Chat → Response</li><li>Question → Answer</li></ul></li><li><p>Replace the embeddings with pre-trained word embeddings such as word2vec or<br>GloVe</p></li><li>Try with more layers, more hidden units, and more sentences. Compare<br>the training time and results.</li><li><p>If you use a translation file where pairs have two of the same phrase<br>(<code>I am test \t I am test</code>), you can use this as an autoencoder. Try<br>this:</p><ul><li>Train as an autoencoder</li><li>Save only the Encoder network</li><li>Train a new Decoder for translation from there</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Text-使用Sequence2Sequence网络和注意力进行翻译使用Sequence2Sequence网络和注意力进行翻译:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Text" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Text-用字符级RNN生成名称</title>
    <link href="http://yoursite.com/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/"/>
    <id>http://yoursite.com/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/</id>
    <published>2020-07-25T01:03:07.000Z</published>
    <updated>2020-07-25T01:05:50.388Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Text-用字符级RNN生成名称:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p>NLP From Scratch: Generating Names with a Character-Level RNN</p><hr><p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p><p>This is our second of three tutorials on “NLP From Scratch”.<br>In the <code>first tutorial &lt;/intermediate/char_rnn_classification_tutorial&gt;</code><br>we used a RNN to classify names into their language of origin. This time<br>we’ll turn around and generate names from languages.</p><p>::</p><pre><code>&gt; python sample.py Russian RUSRovakovUantovShavakov&gt; python sample.py German GERGerrenErengRosher&gt; python sample.py Spanish SPASallaParerAllan&gt; python sample.py Chinese CHIChanHangIun</code></pre><p>We are still hand-crafting a small RNN with a few linear layers. The big<br>difference is instead of predicting a category after reading in all the<br>letters of a name, we input a category and output one letter at a time.<br>Recurrently predicting characters to form language (this could also be<br>done with words or other higher order constructs) is often referred to<br>as a “language model”.</p><p><strong>Recommended Reading:</strong></p><p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li><li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li><li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li><li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li></ul><p>It would also be useful to know about RNNs and how they work:</p><ul><li><code>The Unreasonable Effectiveness of Recurrent NeuralNetworks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li><li><code>Understanding LSTMNetworks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li></ul><p>I also suggest the previous tutorial, :doc:<code>/intermediate/char_rnn_classification_tutorial</code></p><h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p><p>See the last tutorial for more detail of this process. In short, there<br>are a bunch of plain text files <code>data/names/[Language].txt</code> with a<br>name per line. We split lines into an array, convert Unicode to ASCII,<br>and end up with a dictionary <code>{language: [names ...]}</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'-"</span></span><br><span class="line">n_letters = len(all_letters) + <span class="number">1</span> <span class="comment"># Plus EOS marker</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of lines per category</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> n_categories == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">'Data not found. Make sure that you downloaded data '</span></span><br><span class="line">        <span class="string">'from https://download.pytorch.org/tutorial/data.zip and extract it to '</span></span><br><span class="line">        <span class="string">'the current directory.'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'# categories:'</span>, n_categories, all_categories)</span><br><span class="line">print(unicodeToAscii(<span class="string">"O'Néàl"</span>))</span><br></pre></td></tr></table></figure><pre><code># categories: 18 [&#39;Arabic&#39;, &#39;Chinese&#39;, &#39;Czech&#39;, &#39;Dutch&#39;, &#39;English&#39;, &#39;French&#39;, &#39;German&#39;, &#39;Greek&#39;, &#39;Irish&#39;, &#39;Italian&#39;, &#39;Japanese&#39;, &#39;Korean&#39;, &#39;Polish&#39;, &#39;Portuguese&#39;, &#39;Russian&#39;, &#39;Scottish&#39;, &#39;Spanish&#39;, &#39;Vietnamese&#39;]O&#39;Neal</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>This network extends <code>the last tutorial&#39;s RNN &lt;#Creating-the-Network&gt;</code>__<br>with an extra argument for the category tensor, which is concatenated<br>along with the others. The category tensor is a one-hot vector just like<br>the letter input.</p><p>We will interpret the output as the probability of the next letter. When<br>sampling, the most likely output letter is used as the next input<br>letter.</p><p>I added a second linear layer <code>o2o</code> (after combining hidden and<br>output) to give it more muscle to work with. There’s also a dropout<br>layer, which <code>randomly zeros parts of itsinput &lt;https://arxiv.org/abs/1207.0580&gt;</code>__ with a given probability<br>(here 0.1) and is usually used to fuzz inputs to prevent overfitting.<br>Here we’re using it towards the end of the network to purposely add some<br>chaos and increase sampling variety.</p><p>.. figure:: <a href="https://i.imgur.com/jzVrf7f.png" target="_blank" rel="noopener">https://i.imgur.com/jzVrf7f.png</a><br>   :alt:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)</span><br><span class="line">        self.o2o = nn.Linear(hidden_size + output_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, category, input, hidden)</span>:</span></span><br><span class="line">        input_combined = torch.cat((category, input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(input_combined)</span><br><span class="line">        output = self.i2o(input_combined)</span><br><span class="line">        output_combined = torch.cat((hidden, output), <span class="number">1</span>)</span><br><span class="line">        output = self.o2o(output_combined)</span><br><span class="line">        output = self.dropout(output)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>First of all, helper functions to get random pairs of (category, line):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Random item from a list</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a random category and random line from that category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    <span class="keyword">return</span> category, line</span><br></pre></td></tr></table></figure><p>For each timestep (that is, for each letter in a training word) the<br>inputs of the network will be<br><code>(category, current letter, hidden state)</code> and the outputs will be<br><code>(next letter, next hidden state)</code>. So for each training set, we’ll<br>need the category, a set of input letters, and a set of output/target<br>letters.</p><p>Since we are predicting the next letter from the current letter for each<br>timestep, the letter pairs are groups of consecutive letters from the<br>line - e.g. for <code>&quot;ABCD&lt;EOS&gt;&quot;</code> we would create (“A”, “B”), (“B”, “C”),<br>(“C”, “D”), (“D”, “EOS”).</p><p>.. figure:: <a href="https://i.imgur.com/JH58tXY.png" target="_blank" rel="noopener">https://i.imgur.com/JH58tXY.png</a><br>   :alt:</p><p>The category tensor is a <code>one-hottensor &lt;https://en.wikipedia.org/wiki/One-hot&gt;</code>__ of size<br><code>&lt;1 x n_categories&gt;</code>. When training we feed it to the network at every<br>timestep - this is a design choice, it could have been included as part<br>of initial hidden state or some other strategy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># One-hot vector for category</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryTensor</span><span class="params">(category)</span>:</span></span><br><span class="line">    li = all_categories.index(category)</span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_categories)</span><br><span class="line">    tensor[<span class="number">0</span>][li] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot matrix of first to last letters (not including EOS) for input</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> range(len(line)):</span><br><span class="line">        letter = line[li]</span><br><span class="line">        tensor[li][<span class="number">0</span>][all_letters.find(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># LongTensor of second letter to end (EOS) for target</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    letter_indexes = [all_letters.find(line[li]) <span class="keyword">for</span> li <span class="keyword">in</span> range(<span class="number">1</span>, len(line))]</span><br><span class="line">    letter_indexes.append(n_letters - <span class="number">1</span>) <span class="comment"># EOS</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(letter_indexes)</span><br></pre></td></tr></table></figure><p>For convenience during training we’ll make a <code>randomTrainingExample</code><br>function that fetches a random (category, line) pair and turns them into<br>the required (category, input, target) tensors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make category, input, and target tensors from a random category, line pair</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category, line = randomTrainingPair()</span><br><span class="line">    category_tensor = categoryTensor(category)</span><br><span class="line">    input_line_tensor = inputTensor(line)</span><br><span class="line">    target_line_tensor = targetTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category_tensor, input_line_tensor, target_line_tensor</span><br></pre></td></tr></table></figure><h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>In contrast to classification, where only the last output is used, we<br>are making a prediction at every step, so we are calculating loss at<br>every step.</p><p>The magic of autograd allows you to simply sum these losses at each step<br>and call backward at the end.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, input_line_tensor, target_line_tensor)</span>:</span></span><br><span class="line">    target_line_tensor.unsqueeze_(<span class="number">-1</span>)</span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(input_line_tensor.size(<span class="number">0</span>)):</span><br><span class="line">        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)</span><br><span class="line">        l = criterion(output, target_line_tensor[i])</span><br><span class="line">        loss += l</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item() / input_line_tensor.size(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>To keep track of how long training takes I am adding a<br><code>timeSince(timestamp)</code> function which returns a human readable string:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br></pre></td></tr></table></figure><p>Training is business as usual - call train a bunch of times and wait a<br>few minutes, printing the current time and loss every <code>print_every</code><br>examples, and keeping store of an average loss per <code>plot_every</code> examples<br>in <code>all_losses</code> for plotting later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">rnn = RNN(n_letters, <span class="number">128</span>, n_letters)</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">500</span></span><br><span class="line">all_losses = []</span><br><span class="line">total_loss = <span class="number">0</span> <span class="comment"># Reset every plot_every iters</span></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    output, loss = train(*randomTrainingExample())</span><br><span class="line">    total_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start), iter, iter / n_iters * <span class="number">100</span>, loss))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(total_loss / plot_every)</span><br><span class="line">        total_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure><pre><code>0m 40s (5000 5%) 2.68211m 17s (10000 10%) 3.16061m 50s (15000 15%) 2.35412m 23s (20000 20%) 2.48592m 57s (25000 25%) 2.15733m 30s (30000 30%) 2.29104m 3s (35000 35%) 2.69064m 37s (40000 40%) 2.154220m 27s (45000 45%) 2.190921m 10s (50000 50%) 1.893921m 51s (55000 55%) 2.942522m 34s (60000 60%) 2.839523m 15s (65000 65%) 3.034623m 55s (70000 70%) 2.568624m 34s (75000 75%) 2.603725m 13s (80000 80%) 2.596625m 56s (85000 85%) 2.665026m 39s (90000 90%) 2.741227m 18s (95000 95%) 2.614027m 58s (100000 100%) 1.9323</code></pre><h2 id="Plotting-the-Losses"><a href="#Plotting-the-Losses" class="headerlink" title="Plotting the Losses"></a>Plotting the Losses</h2><p>Plotting the historical loss from all_losses shows the network<br>learning:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x12e2623bba8&gt;]</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E7%94%9F%E6%88%90%E5%90%8D%E7%A7%B0/output_18_1.png" alt="png"></p><h1 id="Sampling-the-Network"><a href="#Sampling-the-Network" class="headerlink" title="Sampling the Network"></a>Sampling the Network</h1><p>To sample we give the network a letter and ask what the next one is,<br>feed that in as the next letter, and repeat until the EOS token.</p><ul><li>Create tensors for input category, starting letter, and empty hidden<br>state</li><li>Create a string <code>output_name</code> with the starting letter</li><li><p>Up to a maximum output length,</p><ul><li>Feed the current letter to the network</li><li>Get the next letter from highest output, and next hidden state</li><li>If the letter is EOS, stop here</li><li>If a regular letter, add to <code>output_name</code> and continue</li></ul></li><li><p>Return the final name</p></li></ul><p>.. Note::<br>   Rather than having to give it a starting letter, another<br>   strategy would have been to include a “start of string” token in<br>   training and have the network choose its own starting letter.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">max_length = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample from a category and starting letter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(category, start_letter=<span class="string">'A'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># no need to track history in sampling</span></span><br><span class="line">        category_tensor = categoryTensor(category)</span><br><span class="line">        input = inputTensor(start_letter)</span><br><span class="line">        hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">        output_name = start_letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hidden = rnn(category_tensor, input[<span class="number">0</span>], hidden)</span><br><span class="line">            topv, topi = output.topk(<span class="number">1</span>)</span><br><span class="line">            topi = topi[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> topi == n_letters - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                letter = all_letters[topi]</span><br><span class="line">                output_name += letter</span><br><span class="line">            input = inputTensor(letter)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get multiple samples from one category and multiple starting letters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">samples</span><span class="params">(category, start_letters=<span class="string">'ABC'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> start_letter <span class="keyword">in</span> start_letters:</span><br><span class="line">        print(sample(category, start_letter))</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Russian'</span>, <span class="string">'RUS'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'German'</span>, <span class="string">'GER'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Spanish'</span>, <span class="string">'SPA'</span>)</span><br><span class="line"></span><br><span class="line">samples(<span class="string">'Chinese'</span>, <span class="string">'CHI'</span>)</span><br></pre></td></tr></table></figure><pre><code>RomankovovovosholloshUantovovovokovosskossShaverovovovovovosholGerterEellerRongerSaraPareAranChanHanIou</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul><li><p>Try with a different dataset of category -&gt; line, for example:</p><ul><li>Fictional series -&gt; Character name</li><li>Part of speech -&gt; Word</li><li>Country -&gt; City</li></ul></li><li><p>Use a “start of sentence” token so that sampling can be done without<br>choosing a start letter</p></li><li><p>Get better results with a bigger and/or better shaped network</p><ul><li>Try the nn.LSTM and nn.GRU layers</li><li>Combine multiple of these RNNs as a higher level network</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Text-用字符级RNN生成名称:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Text" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Text-使用字符级RNN对名称进行分类</title>
    <link href="http://yoursite.com/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/</id>
    <published>2020-07-25T01:01:47.000Z</published>
    <updated>2020-07-25T01:02:28.938Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Text-使用字符级RNN对名称进行分类:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p>NLP From Scratch: Classifying Names with a Character-Level RNN</p><hr><p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p><p>We will be building and training a basic character-level RNN to classify<br>words. This tutorial, along with the following two, show how to do<br>preprocess data for NLP modeling “from scratch”, in particular not using<br>many of the convenience functions of <code>torchtext</code>, so you can see how<br>preprocessing for NLP modeling works at a low level.</p><p>A character-level RNN reads words as a series of characters -<br>outputting a prediction and “hidden state” at each step, feeding its<br>previous hidden state into each next step. We take the final prediction<br>to be the output, i.e. which class the word belongs to.</p><p>Specifically, we’ll train on a few thousand surnames from 18 languages<br>of origin, and predict which language a name is from based on the<br>spelling:</p><p>::</p><pre><code>$ python predict.py Hinton(-0.47) Scottish(-1.52) English(-3.57) Irish$ python predict.py Schmidhuber(-0.19) German(-2.48) Czech(-2.68) Dutch</code></pre><p><strong>Recommended Reading:</strong></p><p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a> For installation instructions</li><li>:doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li><li>:doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li><li>:doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li></ul><p>It would also be useful to know about RNNs and how they work:</p><ul><li><code>The Unreasonable Effectiveness of Recurrent NeuralNetworks &lt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&gt;</code>__<br>shows a bunch of real life examples</li><li><code>Understanding LSTMNetworks &lt;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;</code>__<br>is about LSTMs specifically but also informative about RNNs in<br>general</li></ul><h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p><p>Included in the <code>data/names</code> directory are 18 text files named as<br>“[Language].txt”. Each file contains a bunch of names, one name per<br>line, mostly romanized (but we still need to convert from Unicode to<br>ASCII).</p><p>We’ll end up with a dictionary of lists of names per language,<br><code>{language: [names ...]}</code>. The generic variables “category” and “line”<br>(for language and name in our case) are used for later extensibility.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># glob.glob返回所有匹配的文件路径列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</span><br><span class="line"></span><br><span class="line">print(findFiles(<span class="string">'data/names/*.txt'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></span><br><span class="line">n_letters = len(all_letters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span></span><br><span class="line"><span class="comment"># 在Unicode中，某些字符能够用多个合法的编码表示，在需要比较字符串的程序中使用字符的多种表示会产生问题。 </span></span><br><span class="line"><span class="comment"># 为了修正这个问题，你可以使用unicodedata模块先将文本标准化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</span><br><span class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</span><br><span class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></span><br><span class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">print(unicodeToAscii(<span class="string">'Ślusàrski'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the category_lines dictionary, a list of names per language</span></span><br><span class="line">category_lines = &#123;&#125;</span><br><span class="line">all_categories = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a file and split into lines</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></span><br><span class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</span><br><span class="line">    category = os.path.splitext(os.path.basename(filename))[<span class="number">0</span>]</span><br><span class="line">    all_categories.append(category)</span><br><span class="line">    lines = readLines(filename)</span><br><span class="line">    category_lines[category] = lines</span><br><span class="line"></span><br><span class="line">n_categories = len(all_categories)</span><br></pre></td></tr></table></figure><pre><code>[&#39;data/names\\Arabic.txt&#39;, &#39;data/names\\Chinese.txt&#39;, &#39;data/names\\Czech.txt&#39;, &#39;data/names\\Dutch.txt&#39;, &#39;data/names\\English.txt&#39;, &#39;data/names\\French.txt&#39;, &#39;data/names\\German.txt&#39;, &#39;data/names\\Greek.txt&#39;, &#39;data/names\\Irish.txt&#39;, &#39;data/names\\Italian.txt&#39;, &#39;data/names\\Japanese.txt&#39;, &#39;data/names\\Korean.txt&#39;, &#39;data/names\\Polish.txt&#39;, &#39;data/names\\Portuguese.txt&#39;, &#39;data/names\\Russian.txt&#39;, &#39;data/names\\Scottish.txt&#39;, &#39;data/names\\Spanish.txt&#39;, &#39;data/names\\Vietnamese.txt&#39;]Slusarski</code></pre><p>Now we have <code>category_lines</code>, a dictionary mapping each category<br>(language) to a list of lines (names). We also kept track of<br><code>all_categories</code> (just a list of languages) and <code>n_categories</code> for<br>later reference.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(category_lines[<span class="string">'Italian'</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>[&#39;Abandonato&#39;, &#39;Abatangelo&#39;, &#39;Abatantuono&#39;, &#39;Abate&#39;, &#39;Abategiovanni&#39;]</code></pre><h2 id="Turning-Names-into-Tensors"><a href="#Turning-Names-into-Tensors" class="headerlink" title="Turning Names into Tensors"></a>Turning Names into Tensors</h2><p>Now that we have all the names organized, we need to turn them into<br>Tensors to make any use of them.</p><p>To represent a single letter, we use a “one-hot vector” of size<br><code>&lt;1 x n_letters&gt;</code>. A one-hot vector is filled with 0s except for a 1<br>at index of the current letter, e.g. <code>&quot;b&quot; = &lt;0 1 0 0 0 ...&gt;</code>.</p><p>To make a word we join a bunch of those into a 2D matrix<br><code>&lt;line_length x 1 x n_letters&gt;</code>.</p><p>That extra 1 dimension is because PyTorch assumes everything is in<br>batches - we’re just using a batch size of 1 here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find letter index from all_letters, e.g. "a" = 0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToIndex</span><span class="params">(letter)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> all_letters.find(letter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToTensor</span><span class="params">(letter)</span>:</span></span><br><span class="line">    tensor = torch.zeros(<span class="number">1</span>, n_letters)</span><br><span class="line">    tensor[<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn a line into a &lt;line_length x 1 x n_letters&gt;,</span></span><br><span class="line"><span class="comment"># or an array of one-hot letter vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lineToTensor</span><span class="params">(line)</span>:</span></span><br><span class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</span><br><span class="line">    <span class="keyword">for</span> li, letter <span class="keyword">in</span> enumerate(line):</span><br><span class="line">        tensor[li][<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line">print(letterToTensor(<span class="string">'J'</span>))</span><br><span class="line"></span><br><span class="line">print(lineToTensor(<span class="string">'Jones'</span>).size())</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,         0., 0., 0.]])torch.Size([5, 1, 57])</code></pre><h1 id="Creating-the-Network"><a href="#Creating-the-Network" class="headerlink" title="Creating the Network"></a>Creating the Network</h1><p>Before autograd, creating a recurrent neural network in Torch involved<br>cloning the parameters of a layer over several timesteps. The layers<br>held hidden state and gradients which are now entirely handled by the<br>graph itself. This means you can implement a RNN in a very “pure” way,<br>as regular feed-forward layers.</p><p>This RNN module (mostly copied from <code>the PyTorch for Torch userstutorial &lt;https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net&gt;</code>__)<br>is just 2 linear layers which operate on an input and hidden state, with<br>a LogSoftmax layer after the output.<br><img src="https://i.imgur.com/Z2xbySO.png" alt></p><h1 id="Torch-cat"><a href="#Torch-cat" class="headerlink" title="Torch.cat()"></a>Torch.cat()</h1><p>cat是concatnate的意思：拼接，联系在一起。</p><p>先说cat( )的普通用法</p><p>如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C &#x3D; torch.cat( (A,B),0 )  #按维数0拼接（竖着拼）</span><br><span class="line">C &#x3D; torch.cat( (A,B),1 )  #按维数1拼接（横着拼）</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; A&#x3D;torch.ones(2,3)    #2x3的张量（矩阵）                                     </span><br><span class="line">&gt;&gt;&gt; A</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.]])</span><br><span class="line">&gt;&gt;&gt; B&#x3D;2*torch.ones(4,3)  #4x3的张量（矩阵）                                    </span><br><span class="line">&gt;&gt;&gt; B</span><br><span class="line">tensor([[ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.],</span><br><span class="line">        [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,B),0)  #按维数0（行）拼接</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.],</span><br><span class="line">         [ 1.,  1.,  1.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.],</span><br><span class="line">         [ 2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([6, 3])</span><br><span class="line">&gt;&gt;&gt; D&#x3D;2*torch.ones(2,4) #2x4的张量（矩阵）</span><br><span class="line">&gt;&gt;&gt; C&#x3D;torch.cat((A,D),1)#按维数1（列）拼接</span><br><span class="line">&gt;&gt;&gt; C</span><br><span class="line">tensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],</span><br><span class="line">        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])</span><br><span class="line">&gt;&gt;&gt; C.size()</span><br><span class="line">torch.Size([2, 7])</span><br></pre></td></tr></table></figure><br>其次，cat还可以把list中的tensor拼接起来。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line"></span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x &#x3D; torch.Tensor([[1],[2],[3]])</span><br><span class="line">x.shape</span><br><span class="line">torch.Size([3,1])</span><br><span class="line"></span><br><span class="line">x1 &#x3D; [x*2 for i in range(1,4)]</span><br><span class="line">len(x1)</span><br><span class="line">&gt;&gt;3</span><br><span class="line"></span><br><span class="line">x1</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">[tensor([[2.],</span><br><span class="line">         [4.],</span><br><span class="line">         [6]]),tensor([[2.],[4.],[6.]]),tensor([[2.],[4.],[6.]])]</span><br><span class="line"></span><br><span class="line">x2 &#x3D; &#x3D; torch.cat((x1),1)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">&gt;&gt;tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br><span class="line"></span><br><span class="line">type(x1)</span><br><span class="line">&gt;&gt; list</span><br></pre></td></tr></table></figure><br>上面的代码可以合成一行来写：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x2 &#x3D; torch.cat([x*2 for i in range(1,4)])</span><br><span class="line">x2</span><br><span class="line">&gt;&gt;</span><br><span class="line">tensor ([[2.,2.,2.],[4.,4.,4.],[6.,6.,6]])</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        self.i2o = nn.Linear(input_size + hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        combined = torch.cat((input, hidden), <span class="number">1</span>)</span><br><span class="line">        hidden = self.i2h(combined)</span><br><span class="line">        output = self.i2o(combined)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.hidden_size)</span><br><span class="line"></span><br><span class="line">n_hidden = <span class="number">128</span></span><br><span class="line">rnn = RNN(n_letters, n_hidden, n_categories)</span><br></pre></td></tr></table></figure><p>To run a step of this network we need to pass an input (in our case, the<br>Tensor for the current letter) and a previous hidden state (which we<br>initialize as zeros at first). We’ll get back the output (probability of<br>each language) and a next hidden state (which we keep for the next<br>step).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input = letterToTensor(<span class="string">'A'</span>)</span><br><span class="line">hidden =torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input, hidden)</span><br></pre></td></tr></table></figure><p>For the sake of efficiency we don’t want to be creating a new Tensor for<br>every step, so we will use <code>lineToTensor</code> instead of<br><code>letterToTensor</code> and use slices. This could be further optimized by<br>pre-computing batches of Tensors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input = lineToTensor(<span class="string">'Albert'</span>)</span><br><span class="line">hidden = torch.zeros(<span class="number">1</span>, n_hidden)</span><br><span class="line"></span><br><span class="line">output, next_hidden = rnn(input[<span class="number">0</span>], hidden)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><pre><code>tensor([[-2.8013, -3.0086, -2.8838, -2.8652, -2.8300, -2.7883, -2.8614, -2.9069,         -2.9787, -2.8336, -2.9085, -2.9029, -2.9565, -2.8683, -2.9269, -2.9332,         -2.9334, -2.8689]], grad_fn=&lt;LogSoftmaxBackward&gt;)</code></pre><p>As you can see the output is a <code>&lt;1 x n_categories&gt;</code> Tensor, where<br>every item is the likelihood of that category (higher is more likely).</p><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-for-Training"><a href="#Preparing-for-Training" class="headerlink" title="Preparing for Training"></a>Preparing for Training</h2><p>Before going into training we should make a few helper functions. The<br>first is to interpret the output of the network, which we know to be a<br>likelihood of each category. We can use <code>Tensor.topk</code> to get the index<br>of the greatest value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryFromOutput</span><span class="params">(output)</span>:</span></span><br><span class="line">    top_n, top_i = output.topk(<span class="number">1</span>)</span><br><span class="line">    category_i = top_i[<span class="number">0</span>].item()</span><br><span class="line">    <span class="keyword">return</span> all_categories[category_i], category_i</span><br><span class="line"></span><br><span class="line">print(categoryFromOutput(output))</span><br></pre></td></tr></table></figure><pre><code>(&#39;French&#39;, 5)</code></pre><p>We will also want a quick way to get a training example (a name and its<br>language):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingExample</span><span class="params">()</span>:</span></span><br><span class="line">    category = randomChoice(all_categories)</span><br><span class="line">    line = randomChoice(category_lines[category])</span><br><span class="line">    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)</span><br><span class="line">    line_tensor = lineToTensor(line)</span><br><span class="line">    <span class="keyword">return</span> category, line, category_tensor, line_tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    print(<span class="string">'category ='</span>, category, <span class="string">'/ line ='</span>, line)</span><br></pre></td></tr></table></figure><pre><code>category = German / line = Reitercategory = Spanish / line = Roldancategory = Vietnamese / line = Lieucategory = Japanese / line = Maitacategory = Polish / line = Wojdacategory = Greek / line = Forakiscategory = Italian / line = Voltolinicategory = Scottish / line = Hillcategory = Portuguese / line = Nunescategory = Scottish / line = Wilson</code></pre><h2 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h2><p>Now all it takes to train this network is show it a bunch of examples,<br>have it make guesses, and tell it if it’s wrong.</p><p>For the loss function <code>nn.NLLLoss</code> is appropriate, since the last<br>layer of the RNN is <code>nn.LogSoftmax</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br></pre></td></tr></table></figure><p>Each loop of training will:</p><ul><li>Create input and target tensors</li><li>Create a zeroed initial hidden state</li><li><p>Read each letter in and</p><ul><li>Keep hidden state for next letter</li></ul></li><li><p>Compare final output to target</p></li><li>Back-propagate</li><li>Return the output and loss</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.005</span> <span class="comment"># If you set this too high, it might explode. If too low, it might not learn</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    rnn.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    loss = criterion(output, category_tensor)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add parameters' gradients to their values, multiplied by learning rate</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</span><br><span class="line">        p.data.add_(p.grad.data, alpha=-learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, loss.item()</span><br></pre></td></tr></table></figure><p>Now we just have to run that with a bunch of examples. Since the<br><code>train</code> function returns both the output and loss we can print its<br>guesses and also keep track of loss for plotting. Since there are 1000s<br>of examples we print only every <code>print_every</code> examples, and take an<br>average of the loss.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">n_iters = <span class="number">100000</span></span><br><span class="line">print_every = <span class="number">5000</span></span><br><span class="line">plot_every = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep track of losses for plotting</span></span><br><span class="line">current_loss = <span class="number">0</span></span><br><span class="line">all_losses = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></span><br><span class="line">    now = time.time()</span><br><span class="line">    s = now - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(<span class="number">1</span>, n_iters + <span class="number">1</span>):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output, loss = train(category_tensor, line_tensor)</span><br><span class="line">    current_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print iter number, loss, name and guess</span></span><br><span class="line">    <span class="keyword">if</span> iter % print_every == <span class="number">0</span>:</span><br><span class="line">        guess, guess_i = categoryFromOutput(output)</span><br><span class="line">        correct = <span class="string">'✓'</span> <span class="keyword">if</span> guess == category <span class="keyword">else</span> <span class="string">'✗ (%s)'</span> % category</span><br><span class="line">        print(<span class="string">'%d %d%% (%s) %.4f %s / %s %s'</span> % (iter, iter / n_iters * <span class="number">100</span>, timeSince(start), loss, line, guess, correct))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add current loss avg to list of losses</span></span><br><span class="line">    <span class="keyword">if</span> iter % plot_every == <span class="number">0</span>:</span><br><span class="line">        all_losses.append(current_loss / plot_every)</span><br><span class="line">        current_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure><pre><code>5000 5% (0m 26s) 2.2285 Tsen / Chinese ✓10000 10% (0m 45s) 1.7232 Ursler / Dutch ✗ (German)15000 15% (1m 2s) 3.3629 Power / German ✗ (Irish)20000 20% (1m 19s) 1.1055 Ferreiro / Portuguese ✓25000 25% (1m 37s) 1.1813 Do / Vietnamese ✓30000 30% (1m 53s) 1.9952 Pak / Chinese ✗ (Korean)35000 35% (2m 11s) 1.0685 Zientek / Czech ✗ (Polish)40000 40% (2m 34s) 0.3656 Arnoni / Italian ✓45000 45% (2m 56s) 2.5408 Schuchardt / Czech ✗ (German)50000 50% (3m 19s) 0.9137 Ellwood / English ✓55000 55% (3m 43s) 2.6915 Griffiths / Greek ✗ (English)60000 60% (4m 5s) 0.0363 Quach / Vietnamese ✓65000 65% (4m 27s) 0.1474 Rijnders / Dutch ✓70000 70% (4m 49s) 1.8646 Clements / Portuguese ✗ (English)75000 75% (5m 13s) 0.3696 Bobienski / Polish ✓80000 80% (5m 37s) 1.0411 Klerx / Dutch ✓85000 85% (5m 58s) 2.3457 Maria / Spanish ✗ (Portuguese)90000 90% (6m 24s) 0.5750 Echevarria / Spanish ✓95000 95% (6m 47s) 0.0762 Ohmiya / Japanese ✓100000 100% (7m 9s) 2.5785 Kock / Czech ✗ (German)</code></pre><h2 id="Plotting-the-Results"><a href="#Plotting-the-Results" class="headerlink" title="Plotting the Results"></a>Plotting the Results</h2><p>Plotting the historical loss from <code>all_losses</code> shows the network<br>learning:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(all_losses)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x18c353c04a8&gt;]</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_25_1.png" alt="png"></p><h1 id="Evaluating-the-Results"><a href="#Evaluating-the-Results" class="headerlink" title="Evaluating the Results"></a>Evaluating the Results</h1><p>To see how well the network performs on different categories, we will<br>create a confusion matrix, indicating for every actual language (rows)<br>which language the network guesses (columns). To calculate the confusion<br>matrix a bunch of samples are run through the network with<br><code>evaluate()</code>, which is the same as <code>train()</code> minus the backprop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keep track of correct guesses in a confusion matrix</span></span><br><span class="line">confusion = torch.zeros(n_categories, n_categories)</span><br><span class="line">n_confusion = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Just return an output given a line</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(line_tensor)</span>:</span></span><br><span class="line">    hidden = rnn.initHidden()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</span><br><span class="line">        output, hidden = rnn(line_tensor[i], hidden)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># Go through a bunch of examples and record which are correctly guessed</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_confusion):</span><br><span class="line">    category, line, category_tensor, line_tensor = randomTrainingExample()</span><br><span class="line">    output = evaluate(line_tensor)</span><br><span class="line">    guess, guess_i = categoryFromOutput(output)</span><br><span class="line">    category_i = all_categories.index(category)</span><br><span class="line">    confusion[category_i][guess_i] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize by dividing every row by its sum</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_categories):</span><br><span class="line">    confusion[i] = confusion[i] / confusion[i].sum()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up plot</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(confusion.numpy())</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up axes</span></span><br><span class="line">ax.set_xticklabels([<span class="string">''</span>] + all_categories, rotation=<span class="number">90</span>)</span><br><span class="line">ax.set_yticklabels([<span class="string">''</span>] + all_categories)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Force label at every tick</span></span><br><span class="line">ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sphinx_gallery_thumbnail_number = 2</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:33: UserWarning: FixedFormatter should only be used together with FixedLocatorC:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\ipykernel_launcher.py:34: UserWarning: FixedFormatter should only be used together with FixedLocator</code></pre><p><img src="/2020/07/25/Pytorch-Text-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7RNN%E5%AF%B9%E5%90%8D%E7%A7%B0%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/output_27_1.png" alt="png"></p><p>You can pick out bright spots off the main axis that show which<br>languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish<br>for Italian. It seems to do very well with Greek, and very poorly with<br>English (perhaps because of overlap with other languages).</p><h2 id="Running-on-User-Input"><a href="#Running-on-User-Input" class="headerlink" title="Running on User Input"></a>Running on User Input</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(input_line, n_predictions=<span class="number">3</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'\n&gt; %s'</span> % input_line)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = evaluate(lineToTensor(input_line))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get top N categories</span></span><br><span class="line">        topv, topi = output.topk(n_predictions, <span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">        predictions = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_predictions):</span><br><span class="line">            value = topv[<span class="number">0</span>][i].item()</span><br><span class="line">            category_index = topi[<span class="number">0</span>][i].item()</span><br><span class="line">            print(<span class="string">'(%.2f) %s'</span> % (value, all_categories[category_index]))</span><br><span class="line">            predictions.append([value, all_categories[category_index]])</span><br><span class="line"></span><br><span class="line">predict(<span class="string">'Dovesky'</span>)</span><br><span class="line">predict(<span class="string">'Jackson'</span>)</span><br><span class="line">predict(<span class="string">'Satoshi'</span>)</span><br></pre></td></tr></table></figure><pre><code>&gt; Dovesky(-0.80) Russian(-1.53) Czech(-1.98) English&gt; Jackson(-0.33) Scottish(-1.99) English(-3.31) Russian&gt; Satoshi(-0.90) Italian(-1.65) Japanese(-2.18) Arabic</code></pre><p>The final versions of the scripts <code>in the Practical PyTorchrepo &lt;https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification&gt;</code>__<br>split the above code into a few files:</p><ul><li><code>data.py</code> (loads files)</li><li><code>model.py</code> (defines the RNN)</li><li><code>train.py</code> (runs training)</li><li><code>predict.py</code> (runs <code>predict()</code> with command line arguments)</li><li><code>server.py</code> (serve prediction as a JSON API with bottle.py)</li></ul><p>Run <code>train.py</code> to train and save the network.</p><p>Run <code>predict.py</code> with a name to view predictions:</p><p>::</p><pre><code>$ python predict.py Hazaki(-0.42) Japanese(-1.39) Polish(-3.51) Czech</code></pre><p>Run <code>server.py</code> and visit <a href="http://localhost:5533/Yourname" target="_blank" rel="noopener">http://localhost:5533/Yourname</a> to get JSON<br>output of predictions.</p><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul><li><p>Try with a different dataset of line -&gt; category, for example:</p><ul><li>Any word -&gt; language</li><li>First name -&gt; gender</li><li>Character name -&gt; writer</li><li>Page title -&gt; blog or subreddit</li></ul></li><li><p>Get better results with a bigger and/or better shaped network</p><ul><li>Add more linear layers</li><li>Try the <code>nn.LSTM</code> and <code>nn.GRU</code> layers</li><li>Combine multiple of these RNNs as a higher level network</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Text-使用字符级RNN对名称进行分类:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Text" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Text-用NN.TRANFORMER和TORCHTEXT进行序列到序列建模</title>
    <link href="http://yoursite.com/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/"/>
    <id>http://yoursite.com/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/</id>
    <published>2020-07-25T00:52:40.000Z</published>
    <updated>2020-07-25T00:54:41.101Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Text-用NN.TRANFORMER和TORCHTEXT进行序列到序列建模:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText"><a href="#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText" class="headerlink" title="Sequence-to-Sequence Modeling with nn.Transformer and TorchText"></a>Sequence-to-Sequence Modeling with nn.Transformer and TorchText</h1><p>This is a tutorial on how to train a sequence-to-sequence model<br>that uses the<br><code>nn.Transformer &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&gt;</code>__ module.</p><p>PyTorch 1.2 release includes a standard transformer module based on the<br>paper <code>Attention is All YouNeed &lt;https://arxiv.org/pdf/1706.03762.pdf&gt;</code><strong>. The transformer model<br>has been proved to be superior in quality for many sequence-to-sequence<br>problems while being more parallelizable. The <code>nn.Transformer</code> module<br>relies entirely on an attention mechanism (another module recently<br>implemented as <code>nn.MultiheadAttention &lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&gt;</code></strong>) to draw global dependencies<br>between input and output. The <code>nn.Transformer</code> module is now highly<br>modularized such that a single component (like <code>nn.TransformerEncoder &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&gt;</code>__<br>in this tutorial) can be easily adapted/composed.</p><p><img src="https://pytorch.org/tutorials/_images/transformer_architecture.jpg" alt></p><h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>In this tutorial, we train <code>nn.TransformerEncoder</code> model on a<br>language modeling task. The language modeling task is to assign a<br>probability for the likelihood of a given word (or a sequence of words)<br>to follow a sequence of words. A sequence of tokens are passed to the embedding<br>layer first, followed by a positional encoding layer to account for the order<br>of the word (see the next paragraph for more details). The<br><code>nn.TransformerEncoder</code> consists of multiple layers of<br><code>nn.TransformerEncoderLayer &lt;https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer&gt;</code>__. Along with the input sequence, a square<br>attention mask is required because the self-attention layers in<br><code>nn.TransformerEncoder</code> are only allowed to attend the earlier positions in<br>the sequence. For the language modeling task, any tokens on the future<br>positions should be masked. To have the actual words, the output<br>of <code>nn.TransformerEncoder</code> model is sent to the final Linear<br>layer, which is followed by a log-Softmax function.</p><hr><h2 id="torch-triu-input-diagonal-0-out-None-→-Tensor"><a href="#torch-triu-input-diagonal-0-out-None-→-Tensor" class="headerlink" title="torch.triu(input, diagonal=0, out=None) → Tensor"></a>torch.triu(input, diagonal=0, out=None) → Tensor</h2><p>返回矩阵上三角部分，其余部分定义为0。</p><p>Parameters:</p><ul><li>input (Tensor) – the input tensor</li><li>diagonal (int, optional) – the diagonal to consider</li><li>out (Tensor, optional) – the output tensor</li></ul><hr><ul><li>如果diagonal为空，输入矩阵保留主对角线与主对角线以上的元素；</li><li>如果diagonal为正数n，输入矩阵保留主对角线与主对角线以上除去n行的元素；</li><li>如果diagonal为负数-n，输入矩阵保留主对角线与主对角线以上与主对角线下方h行对角线的元素；<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a &#x3D; torch.randn(3, 3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.3480, -0.5211, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000, -0.4573]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;1)</span><br><span class="line">tensor([[ 0.0000,  0.5207,  2.0049],</span><br><span class="line">        [ 0.0000,  0.0000,  0.6602],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000]])</span><br><span class="line">&gt;&gt;&gt; torch.triu(a, diagonal&#x3D;-1)</span><br><span class="line">tensor([[ 0.2309,  0.5207,  2.0049],</span><br><span class="line">        [ 0.2072, -1.0680,  0.6602],</span><br><span class="line">        [ 0.0000, -0.5211, -0.4573]])</span><br></pre></td></tr></table></figure></li></ul><h2 id="pytorch-mask-filled用法"><a href="#pytorch-mask-filled用法" class="headerlink" title="pytorch mask_filled用法"></a>pytorch mask_filled用法</h2><p>将 mask必须是一个 ByteTensor 而且shape必须和 a一样 并且元素只能是 0或者1 ，是将 mask中为1的 元素所在的索引，在a中相同的的索引处替换为 value  ,mask value必须同为tensor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.tensor([1,0,2,3])</span><br><span class="line">a.masked_fill(mask &#x3D; torch.ByteTensor([1,1,0,0]), value&#x3D;torch.tensor(-1e9))</span><br><span class="line"></span><br><span class="line">tensor([-1.0000e+09, -1.0000e+09,  2.0000e+00,  3.0000e+00])</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ntoken, ninp, nhead, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(TransformerModel, self).__init__()</span><br><span class="line">        <span class="keyword">from</span> torch.nn <span class="keyword">import</span> TransformerEncoder, TransformerEncoderLayer</span><br><span class="line">        self.model_type = <span class="string">'Transformer'</span></span><br><span class="line">        self.src_mask = <span class="literal">None</span></span><br><span class="line">        self.pos_encoder = PositionalEncoding(ninp, dropout)</span><br><span class="line">        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)</span><br><span class="line">        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        self.ninp = ninp</span><br><span class="line">        self.decoder = nn.Linear(ninp, ntoken)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_square_subsequent_mask</span><span class="params">(self, sz)</span>:</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.src_mask <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self.src_mask.size(<span class="number">0</span>) != len(src):</span><br><span class="line">            device = src.device</span><br><span class="line">            mask = self._generate_square_subsequent_mask(len(src)).to(device)</span><br><span class="line">            self.src_mask = mask</span><br><span class="line"></span><br><span class="line">        src = self.encoder(src) * math.sqrt(self.ninp)</span><br><span class="line">        src = self.pos_encoder(src)</span><br><span class="line">        output = self.transformer_encoder(src, self.src_mask)</span><br><span class="line">        output = self.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><code>PositionalEncoding</code> module injects some information about the<br>relative or absolute position of the tokens in the sequence. The<br>positional encodings have the same dimension as the embeddings so that<br>the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of<br>different frequencies.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h2 id="Load-and-batch-data"><a href="#Load-and-batch-data" class="headerlink" title="Load and batch data"></a>Load and batch data</h2><p>The training process uses Wikitext-2 dataset from <code>torchtext</code>. The<br>vocab object is built based on the train dataset and is used to numericalize<br>tokens into tensors. Starting from sequential data, the <code>batchify()</code><br>function arranges the dataset into columns, trimming off any tokens remaining<br>after the data has been divided into batches of size <code>batch_size</code>.<br>For instance, with the alphabet as the sequence (total length of 26)<br>and a batch size of 4, we would divide the alphabet into 4 sequences of<br>length 6:</p><p>\begin{align}\begin{bmatrix}<br>  \text{A} &amp; \text{B} &amp; \text{C} &amp; \ldots &amp; \text{X} &amp; \text{Y} &amp; \text{Z}<br>  \end{bmatrix}<br>  \Rightarrow<br>  \begin{bmatrix}<br>  \begin{bmatrix}\text{A} \\ \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{G} \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{M} \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} &amp;<br>  \begin{bmatrix}\text{S} \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix}<br>  \end{bmatrix}\end{align}</p><p>These columns are treated as independent by the model, which means that<br>the dependence of <code>G</code> and <code>F</code> can not be learned, but allows more<br>efficient batch processing.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line">TEXT = torchtext.data.Field(tokenize=get_tokenizer(<span class="string">"basic_english"</span>),</span><br><span class="line">                            init_token=<span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">                            eos_token=<span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">                            lower=<span class="literal">True</span>)</span><br><span class="line">train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)</span><br><span class="line">TEXT.build_vocab(train_txt)</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    data = TEXT.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line">    <span class="comment"># Divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure><p>Functions to generate input and target sequence<br><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del>~~~~</p><p><code>get_batch()</code> function generates the input and target sequence for<br>the transformer model. It subdivides the source data into chunks of<br>length <code>bptt</code>. For the language modeling task, the model needs the<br>following words as <code>Target</code>. For example, with a <code>bptt</code> value of 2,<br>we’d get the following two Variables for <code>i</code> = 0:</p><p><img src="/2020/07/25/Pytorch-Text-%E7%94%A8NN-TRANFORMER%E5%92%8CTORCHTEXT%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/_static/img/transformer_input_target.png" alt></p><p>It should be noted that the chunks are along dimension 0, consistent<br>with the <code>S</code> dimension in the Transformer model. The batch dimension<br><code>N</code> is along dimension 1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bptt = <span class="number">35</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(source, i)</span>:</span></span><br><span class="line">    seq_len = min(bptt, len(source) - <span class="number">1</span> - i)</span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure><h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The model is set up with the hyperparameter below. The vocab size is<br>equal to the length of the vocab object.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ntokens = len(TEXT.vocab.stoi) <span class="comment"># the size of vocabulary</span></span><br><span class="line">emsize = <span class="number">200</span> <span class="comment"># embedding dimension</span></span><br><span class="line">nhid = <span class="number">200</span> <span class="comment"># the dimension of the feedforward network model in nn.TransformerEncoder</span></span><br><span class="line">nlayers = <span class="number">2</span> <span class="comment"># the number of nn.TransformerEncoderLayer in nn.TransformerEncoder</span></span><br><span class="line">nhead = <span class="number">2</span> <span class="comment"># the number of heads in the multiheadattention models</span></span><br><span class="line">dropout = <span class="number">0.2</span> <span class="comment"># the dropout value</span></span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br></pre></td></tr></table></figure><h2 id="Run-the-model"><a href="#Run-the-model" class="headerlink" title="Run the model"></a>Run the model</h2><p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><strong><br>is applied to track the loss and<br><code>SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD&gt;</code></strong><br>implements stochastic gradient descent method as the optimizer. The initial<br>learning rate is set to 5.0. <code>StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR&gt;</code><strong> is<br>applied to adjust the learn rate through epochs. During the<br>training, we use<br><code>nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_&gt;</code></strong><br>function to scale all the gradient together to prevent exploding.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">lr = <span class="number">5.0</span> <span class="comment"># learning rate</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train() <span class="comment"># Turn on the train mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> enumerate(range(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output.view(<span class="number">-1</span>, ntokens), targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        log_interval = <span class="number">200</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            print(<span class="string">'| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | '</span></span><br><span class="line">                  <span class="string">'lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | '</span></span><br><span class="line">                  <span class="string">'loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, math.exp(cur_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(eval_model, data_source)</span>:</span></span><br><span class="line">    eval_model.eval() <span class="comment"># Turn on the evaluation mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, data_source.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            data, targets = get_batch(data_source, i)</span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            output_flat = output.view(<span class="number">-1</span>, ntokens)</span><br><span class="line">            total_loss += len(data) * criterion(output_flat, targets).item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / (len(data_source) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>Loop over epochs. Save the model if the validation loss is the best<br>we’ve seen so far. Adjust the learning rate after each epoch.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">best_val_loss = float(<span class="string">"inf"</span>)</span><br><span class="line">epochs = <span class="number">3</span> <span class="comment"># The number of epochs</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train()</span><br><span class="line">    val_loss = evaluate(model, val_data)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line">    print(<span class="string">'| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | '</span></span><br><span class="line">          <span class="string">'valid ppl &#123;:8.2f&#125;'</span>.format(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                     val_loss, math.exp(val_loss)))</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">        best_val_loss = val_loss</span><br><span class="line">        best_model = model</span><br><span class="line"></span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><pre><code>C:\Users\18025\Anaconda3\envs\pytorch\lib\site-packages\torch\optim\lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.  &quot;please use `get_last_lr()`.&quot;, UserWarning)| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 2144.51 | loss  8.03 | ppl  3063.62| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 1757.64 | loss  6.78 | ppl   876.40| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 1952.50 | loss  6.37 | ppl   584.33| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 1758.54 | loss  6.22 | ppl   501.41| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 1863.17 | loss  6.11 | ppl   450.46| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 1836.79 | loss  6.10 | ppl   443.68| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 1795.15 | loss  6.05 | ppl   422.43| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 1902.40 | loss  6.05 | ppl   425.49| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 1827.80 | loss  5.96 | ppl   386.04| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 1814.35 | loss  5.96 | ppl   388.23| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 1845.57 | loss  5.85 | ppl   346.70| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 1798.20 | loss  5.90 | ppl   364.75| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 1925.91 | loss  5.90 | ppl   365.12| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 1709.37 | loss  5.80 | ppl   331.20-----------------------------------------------------------------------------------------| end of epoch   1 | time: 5766.46s | valid loss  5.76 | valid ppl   317.98-----------------------------------------------------------------------------------------| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 1861.38 | loss  5.80 | ppl   330.78| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 1823.89 | loss  5.77 | ppl   320.61| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 1858.68 | loss  5.60 | ppl   270.75| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 1831.67 | loss  5.63 | ppl   278.20| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 1785.64 | loss  5.59 | ppl   266.78| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 1808.74 | loss  5.61 | ppl   273.60| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 1926.79 | loss  5.63 | ppl   278.59| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 1821.76 | loss  5.67 | ppl   289.91| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 1797.76 | loss  5.59 | ppl   267.51| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 1764.14 | loss  5.62 | ppl   274.93| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 1895.50 | loss  5.51 | ppl   246.68| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 1793.70 | loss  5.58 | ppl   265.49| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 1824.77 | loss  5.58 | ppl   266.10| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 1891.16 | loss  5.51 | ppl   247.06-----------------------------------------------------------------------------------------| end of epoch   2 | time: 5701.96s | valid loss  5.59 | valid ppl   268.83-----------------------------------------------------------------------------------------| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 1818.12 | loss  5.55 | ppl   256.50| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 1945.54 | loss  5.55 | ppl   257.43| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 1616.58 | loss  5.36 | ppl   213.27| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 1689.64 | loss  5.42 | ppl   224.94| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 1608.36 | loss  5.38 | ppl   216.13| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 1630.67 | loss  5.41 | ppl   222.92| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 1686.69 | loss  5.44 | ppl   229.34| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 1647.30 | loss  5.48 | ppl   239.58| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 1633.68 | loss  5.40 | ppl   221.38| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 1613.04 | loss  5.43 | ppl   228.14| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 1608.29 | loss  5.32 | ppl   204.06| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 1624.74 | loss  5.40 | ppl   220.31| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 1609.86 | loss  5.41 | ppl   224.71| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 1559.15 | loss  5.33 | ppl   207.22-----------------------------------------------------------------------------------------| end of epoch   3 | time: 6002.66s | valid loss  5.51 | valid ppl   247.75-----------------------------------------------------------------------------------------</code></pre><h2 id="Evaluate-the-model-with-the-test-dataset"><a href="#Evaluate-the-model-with-the-test-dataset" class="headerlink" title="Evaluate the model with the test dataset"></a>Evaluate the model with the test dataset</h2><p>Apply the best model to check the result with the test dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br><span class="line">print(<span class="string">'| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure><pre><code>=========================================================================================| End of training | test loss  5.43 | test ppl   227.23=========================================================================================</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parallelizable:可并行化</span><br><span class="line">superior:优越</span><br><span class="line">mechanism:机制</span><br><span class="line">modularized:模块化</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Text-用NN.TRANFORMER和TORCHTEXT进行序列到序列建模:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Text" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Text/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Audio-torchaudio</title>
    <link href="http://yoursite.com/2020/07/24/Pytorch-Audio-torchaudio/"/>
    <id>http://yoursite.com/2020/07/24/Pytorch-Audio-torchaudio/</id>
    <published>2020-07-24T12:22:01.000Z</published>
    <updated>2020-07-24T12:23:15.331Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Audio-torchaudio:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%shell</span><br><span class="line">pip install torchaudio</span><br></pre></td></tr></table></figure><pre><code>Collecting torchaudio[?25l  Downloading https://files.pythonhosted.org/packages/e9/0a/40e53c686c2af65b2a4e818d11d9b76fa79178440caf99f3ceb2a32c3b04/torchaudio-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)[K     |████████████████████████████████| 3.2MB 2.8MB/s [?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (0.16.0)Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1-&gt;torchaudio) (1.18.5)Installing collected packages: torchaudioSuccessfully installed torchaudio-0.5.1</code></pre><h1 id="torchaudio-Tutorial"><a href="#torchaudio-Tutorial" class="headerlink" title="torchaudio Tutorial"></a>torchaudio Tutorial</h1><p>PyTorch is an open source deep learning platform that provides a<br>seamless path from research prototyping to production deployment with<br>GPU support.</p><p>Significant effort in solving machine learning problems goes into data<br>preparation. <code>torchaudio</code> leverages PyTorch’s GPU support, and provides<br>many tools to make data loading easy and more readable. In this<br>tutorial, we will see how to load and preprocess data from a simple<br>dataset.</p><p>For this tutorial, please make sure the <code>matplotlib</code> package is<br>installed for easier visualization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchaudio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="Opening-a-file"><a href="#Opening-a-file" class="headerlink" title="Opening a file"></a>Opening a file</h2><p><code>torchaudio</code> also supports loading sound files in the wav and mp3 format. We<br>call waveform the resulting raw audio signal.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">"test.mp3"</span></span><br><span class="line">waveform, sample_rate = torchaudio.load(filename)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of waveform: &#123;&#125;"</span>.format(waveform.size()))</span><br><span class="line">print(<span class="string">"Sample rate of waveform: &#123;&#125;"</span>.format(sample_rate))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure><pre><code>Shape of waveform: torch.Size([2, 10857600])Sample rate of waveform: 44100[&lt;matplotlib.lines.Line2D at 0x7f91dd44a4a8&gt;, &lt;matplotlib.lines.Line2D at 0x7f91dd44a5c0&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_5_2.png" alt="png"></p><p>When you load a file in <code>torchaudio</code>, you can optionally specify the backend to use either<br><code>SoX &lt;https://pypi.org/project/sox/&gt;</code>_ or <code>SoundFile &lt;https://pypi.org/project/SoundFile/&gt;</code>_<br>via <code>torchaudio.set_audio_backend</code>. These backends are loaded lazily when needed.</p><p><code>torchaudio</code> also makes JIT compilation optional for functions, and uses <code>nn.Module</code> where possible.</p><h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p><code>torchaudio</code> supports a growing list of<br><code>transformations &lt;https://pytorch.org/audio/transforms.html&gt;</code>_.</p><ul><li><strong>Resample</strong>: Resample waveform to a different sample rate.</li><li><strong>Spectrogram</strong>: Create a spectrogram from a waveform.</li><li><strong>GriffinLim</strong>: Compute waveform from a linear scale magnitude spectrogram using<br>the Griffin-Lim transformation.</li><li><strong>ComputeDeltas</strong>: Compute delta coefficients of a tensor, usually a spectrogram.</li><li><strong>ComplexNorm</strong>: Compute the norm of a complex tensor.</li><li><strong>MelScale</strong>: This turns a normal STFT into a Mel-frequency STFT,<br>using a conversion matrix.</li><li><strong>AmplitudeToDB</strong>: This turns a spectrogram from the<br>power/amplitude scale to the decibel scale.</li><li><strong>MFCC</strong>: Create the Mel-frequency cepstrum coefficients from a<br>waveform.</li><li><strong>MelSpectrogram</strong>: Create MEL Spectrograms from a waveform using the<br>STFT function in PyTorch.</li><li><strong>MuLawEncoding</strong>: Encode waveform based on mu-law companding.</li><li><strong>MuLawDecoding</strong>: Decode mu-law encoded waveform.</li><li><strong>TimeStretch</strong>: Stretch a spectrogram in time without modifying pitch for a given rate.</li><li><strong>FrequencyMasking</strong>: Apply masking to a spectrogram in the frequency domain.</li><li><strong>TimeMasking</strong>: Apply masking to a spectrogram in the time domain.</li></ul><p>Each transform supports batching: you can perform a transform on a single raw<br>audio signal or spectrogram, or many of the same shape.</p><p>Since all transforms are <code>nn.Modules</code> or <code>jit.ScriptModules</code>, they can be<br>used as part of a neural network at any point.</p><p>To start, we can look at the log of the spectrogram on a log scale.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">specgram = torchaudio.transforms.Spectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape of spectrogram: torch.Size([2, 201, 54289])&lt;matplotlib.image.AxesImage at 0x7f91dcf1ec50&gt;</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_9_2.png" alt="png"></p><p>Or we can look at the Mel Spectrogram on a log scale.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">specgram = torchaudio.transforms.MelSpectrogram()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">p = plt.imshow(specgram.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape of spectrogram: torch.Size([2, 128, 54289])</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_11_1.png" alt="png"></p><p>We can resample the waveform, one channel at a time.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">new_sample_rate = sample_rate/<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Since Resample applies to a single channel, we resample first channel here</span></span><br><span class="line">channel = <span class="number">0</span></span><br><span class="line">transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure><pre><code>Shape of transformed waveform: torch.Size([1, 1085760])[&lt;matplotlib.lines.Line2D at 0x7f91dce23240&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_13_2.png" alt="png"></p><p>As another example of transformations, we can encode the signal based on<br>Mu-Law enconding. But to do so, we need the signal to be between -1 and</p><ol><li>Since the tensor is just a regular PyTorch tensor, we can apply<br>standard operators on it.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's check if the tensor is in the interval [-1,1]</span></span><br><span class="line">print(<span class="string">"Min of waveform: &#123;&#125;\nMax of waveform: &#123;&#125;\nMean of waveform: &#123;&#125;"</span>.format(waveform.min(), waveform.max(), waveform.mean()))</span><br></pre></td></tr></table></figure><pre><code>Min of waveform: -1.0Max of waveform: 1.0Mean of waveform: -4.018312756670639e-05</code></pre><p>Since the waveform is already between -1 and 1, we do not need to<br>normalize it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    <span class="comment"># Subtract the mean, and scale to the interval [-1,1]</span></span><br><span class="line">    tensor_minusmean = tensor - tensor.mean()</span><br><span class="line">    <span class="keyword">return</span> tensor_minusmean/tensor_minusmean.abs().max()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's normalize to the full interval [-1,1]</span></span><br><span class="line"><span class="comment"># waveform = normalize(waveform)</span></span><br></pre></td></tr></table></figure><p>Let’s apply encode the waveform.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transformed = torchaudio.transforms.MuLawEncoding()(waveform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(transformed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(transformed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure><pre><code>Shape of transformed waveform: torch.Size([2, 10857600])[&lt;matplotlib.lines.Line2D at 0x7f91dce02b38&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_19_2.png" alt="png"></p><p>And now decode.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reconstructed = torchaudio.transforms.MuLawDecoding()(transformed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of recovered waveform: &#123;&#125;"</span>.format(reconstructed.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(reconstructed[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure><pre><code>Shape of recovered waveform: torch.Size([2, 10857600])[&lt;matplotlib.lines.Line2D at 0x7f91dcd62ef0&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_21_2.png" alt="png"></p><p>We can finally compare the original waveform with its reconstructed<br>version.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute median relative difference</span></span><br><span class="line">err = ((waveform-reconstructed).abs() / waveform.abs()).median()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Median relative difference between original and MuLaw reconstucted signals: &#123;:.2%&#125;"</span>.format(err))</span><br></pre></td></tr></table></figure><pre><code>Median relative difference between original and MuLaw reconstucted signals: 1.20%</code></pre><h2 id="Functional"><a href="#Functional" class="headerlink" title="Functional"></a>Functional</h2><p>The transformations seen above rely on lower level stateless functions for their computations.<br>These functions are available under <code>torchaudio.functional</code>. The complete list is available<br><code>here &lt;https://pytorch.org/audio/functional.html&gt;</code>_ and includes:</p><ul><li><strong>istft</strong>: Inverse short time Fourier Transform.</li><li><strong>gain</strong>: Applies amplification or attenuation to the whole waveform.</li><li><strong>dither</strong>: Increases the perceived dynamic range of audio stored at a<br>particular bit-depth.</li><li><strong>compute_deltas</strong>: Compute delta coefficients of a tensor.</li><li><strong>equalizer_biquad</strong>: Design biquad peaking equalizer filter and perform filtering.</li><li><strong>lowpass_biquad</strong>: Design biquad lowpass filter and perform filtering.</li><li><strong>highpass_biquad</strong>:Design biquad highpass filter and perform filtering.</li></ul><p>For example, let’s try the <code>mu_law_encoding</code> functional:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mu_law_encoding_waveform = torchaudio.functional.mu_law_encoding(waveform, quantization_channels=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of transformed waveform: &#123;&#125;"</span>.format(mu_law_encoding_waveform.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(mu_law_encoding_waveform[<span class="number">0</span>,:].numpy())</span><br></pre></td></tr></table></figure><pre><code>Shape of transformed waveform: torch.Size([2, 10857600])[&lt;matplotlib.lines.Line2D at 0x7f91dcd545f8&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_25_2.png" alt="png"></p><p>You can see how the output fron <code>torchaudio.functional.mu_law_encoding</code> is the same as<br>the output from <code>torchaudio.transforms.MuLawEncoding</code>.</p><p>Now let’s experiment with a few of the other functionals and visualize their output. Taking our<br>spectogram, we can compute it’s deltas:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">computed = torchaudio.functional.compute_deltas(specgram.contiguous(), win_length=<span class="number">3</span>)</span><br><span class="line">print(<span class="string">"Shape of computed deltas: &#123;&#125;"</span>.format(computed.shape))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(computed.log2()[<span class="number">0</span>,:,:].detach().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape of computed deltas: torch.Size([2, 128, 54289])&lt;matplotlib.image.AxesImage at 0x7f91dccb0b70&gt;</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_27_2.png" alt="png"></p><p>We can take the original waveform and apply different effects to it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gain_waveform = torchaudio.functional.gain(waveform, gain_db=<span class="number">5.0</span>)</span><br><span class="line">print(<span class="string">"Min of gain_waveform: &#123;&#125;\nMax of gain_waveform: &#123;&#125;\nMean of gain_waveform: &#123;&#125;"</span>.format(gain_waveform.min(), gain_waveform.max(), gain_waveform.mean()))</span><br><span class="line"></span><br><span class="line">dither_waveform = torchaudio.functional.dither(waveform)</span><br><span class="line">print(<span class="string">"Min of dither_waveform: &#123;&#125;\nMax of dither_waveform: &#123;&#125;\nMean of dither_waveform: &#123;&#125;"</span>.format(dither_waveform.min(), dither_waveform.max(), dither_waveform.mean()))</span><br></pre></td></tr></table></figure><pre><code>Min of gain_waveform: -1.778279423713684Max of gain_waveform: 1.778279423713684Mean of gain_waveform: -7.145693234633654e-05Min of dither_waveform: -0.99993896484375Max of dither_waveform: 0.999969482421875Mean of dither_waveform: -2.492486237315461e-05</code></pre><p>Another example of the capabilities in <code>torchaudio.functional</code> are applying filters to our<br>waveform. Applying the lowpass biquad filter to our waveform will output a new waveform with<br>the signal of the frequency modified.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lowpass_waveform = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of lowpass_waveform: &#123;&#125;\nMax of lowpass_waveform: &#123;&#125;\nMean of lowpass_waveform: &#123;&#125;"</span>.format(lowpass_waveform.min(), lowpass_waveform.max(), lowpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(lowpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure><pre><code>Min of lowpass_waveform: -1.0Max of lowpass_waveform: 1.0Mean of lowpass_waveform: -4.02079094783403e-05[&lt;matplotlib.lines.Line2D at 0x7f91dcb8e278&gt;, &lt;matplotlib.lines.Line2D at 0x7f91dcb8e390&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_31_2.png" alt="png"></p><p>We can also visualize a waveform with the highpass biquad filter.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">highpass_waveform = torchaudio.functional.highpass_biquad(waveform, sample_rate, cutoff_freq=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min of highpass_waveform: &#123;&#125;\nMax of highpass_waveform: &#123;&#125;\nMean of highpass_waveform: &#123;&#125;"</span>.format(highpass_waveform.min(), highpass_waveform.max(), highpass_waveform.mean()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(highpass_waveform.t().numpy())</span><br></pre></td></tr></table></figure><pre><code>Min of highpass_waveform: -0.8367071151733398Max of highpass_waveform: 0.7935813069343567Mean of highpass_waveform: -9.841002679422672e-09[&lt;matplotlib.lines.Line2D at 0x7f91dcaf3e48&gt;, &lt;matplotlib.lines.Line2D at 0x7f91dcaf3f60&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_33_2.png" alt="png"></p><h2 id="Migrating-to-torchaudio-from-Kaldi"><a href="#Migrating-to-torchaudio-from-Kaldi" class="headerlink" title="Migrating to torchaudio from Kaldi"></a>Migrating to torchaudio from Kaldi</h2><p>Users may be familiar with<br><code>Kaldi &lt;http://github.com/kaldi-asr/kaldi&gt;</code>_, a toolkit for speech<br>recognition. <code>torchaudio</code> offers compatibility with it in<br><code>torchaudio.kaldi_io</code>. It can indeed read from kaldi scp, or ark file<br>or streams with:</p><ul><li>read_vec_int_ark</li><li>read_vec_flt_scp</li><li>read_vec_flt_arkfile/stream</li><li>read_mat_scp</li><li>read_mat_ark</li></ul><p><code>torchaudio</code> provides Kaldi-compatible transforms for <code>spectrogram</code>,<br><code>fbank</code>, <code>mfcc</code>, and <code>`resample_waveform with the benefit of GPU support, see</code>here <compliance.kaldi.html>`__ for more information.</compliance.kaldi.html></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n_fft = <span class="number">400.0</span></span><br><span class="line">frame_length = n_fft / sample_rate * <span class="number">1000.0</span></span><br><span class="line">frame_shift = frame_length / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">"channel"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"dither"</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">"window_type"</span>: <span class="string">"hanning"</span>,</span><br><span class="line">    <span class="string">"frame_length"</span>: frame_length,</span><br><span class="line">    <span class="string">"frame_shift"</span>: frame_shift,</span><br><span class="line">    <span class="string">"remove_dc_offset"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"round_to_power_of_two"</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"sample_frequency"</span>: sample_rate,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of spectrogram: &#123;&#125;"</span>.format(specgram.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(specgram.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape of spectrogram: torch.Size([54287, 201])&lt;matplotlib.image.AxesImage at 0x7f91dca6d240&gt;</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_35_2.png" alt="png"></p><p>We also support computing the filterbank features from waveforms,<br>matching Kaldi’s implementation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fbank = torchaudio.compliance.kaldi.fbank(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of fbank: &#123;&#125;"</span>.format(fbank.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(fbank.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape of fbank: torch.Size([54287, 23])&lt;matplotlib.image.AxesImage at 0x7f91dca440f0&gt;</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_37_2.png" alt="png"></p><p>You can create mel frequency cepstral coefficients from a raw audio signal<br>This matches the input/output of Kaldi’s compute-mfcc-feats.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mfcc = torchaudio.compliance.kaldi.mfcc(waveform, **params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Shape of mfcc: &#123;&#125;"</span>.format(mfcc.size()))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(mfcc.t().numpy(), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape of mfcc: torch.Size([54287, 13])&lt;matplotlib.image.AxesImage at 0x7f91dca16828&gt;</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_39_2.png" alt="png"></p><h2 id="Available-Datasets"><a href="#Available-Datasets" class="headerlink" title="Available Datasets"></a>Available Datasets</h2><p>If you do not want to create your own dataset to train your model, <code>torchaudio</code> offers a<br>unified dataset interface. This interface supports lazy-loading of files to memory, download<br>and extract functions, and datasets to build models.</p><p>The datasets <code>torchaudio</code> currently supports are:</p><ul><li><strong>VCTK</strong>: Speech data uttered by 109 native speakers of English with various accents<br>(<code>Read more here &lt;https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html&gt;</code>_).</li><li><strong>Yesno</strong>: Sixty recordings of one individual saying yes or no in Hebrew; each<br>recording is eight words long (<code>Read more here &lt;https://www.openslr.org/1/&gt;</code>_).</li><li><strong>Common Voice</strong>: An open source, multi-language dataset of voices that anyone can use<br>to train speech-enabled applications (<code>Read more here &lt;https://voice.mozilla.org/en/datasets&gt;</code>_).</li><li><strong>LibriSpeech</strong>: Large-scale (1000 hours) corpus of read English speech (<code>Read more here &lt;http://www.openslr.org/12&gt;</code>_).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">yesno_data = torchaudio.datasets.YESNO(<span class="string">'./'</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A data point in Yesno is a tuple (waveform, sample_rate, labels) where labels is a list of integers with 1 for yes and 0 for no.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pick data point number 3 to see an example of the the yesno_data:</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">waveform, sample_rate, labels = yesno_data[n]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Waveform: &#123;&#125;\nSample rate: &#123;&#125;\nLabels: &#123;&#125;"</span>.format(waveform, sample_rate, labels))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(waveform.t().numpy())</span><br></pre></td></tr></table></figure><pre><code>HBox(children=(FloatProgress(value=0.0, max=4703754.0), HTML(value=&#39;&#39;)))Waveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -8.5449e-04,         -1.0986e-03, -8.8501e-04]])Sample rate: 8000Labels: [1, 0, 0, 0, 1, 0, 0, 1][&lt;matplotlib.lines.Line2D at 0x7f91dbcfb1d0&gt;]</code></pre><p><img src="/2020/07/24/Pytorch-Audio-torchaudio/output_41_3.png" alt="png"></p><p>Now, whenever you ask for a sound file from the dataset, it is loaded in memory only when you ask for it.<br>Meaning, the dataset only loads and keeps in memory the items that you want and use, saving on memory.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We used an example raw audio signal, or waveform, to illustrate how to<br>open an audio file using <code>torchaudio</code>, and how to pre-process,<br>transform, and apply functions to such waveform. We also demonstrated how<br>to use familiar Kaldi functions, as well as utilize built-in datasets to<br>construct our models. Given that <code>torchaudio</code> is built on PyTorch,<br>these techniques can be used as building blocks for more advanced audio<br>applications, such as speech recognition, while leveraging GPUs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Audio-torchaudio:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Audio" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Audio/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Image-DCGAN教程</title>
    <link href="http://yoursite.com/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/"/>
    <id>http://yoursite.com/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/</id>
    <published>2020-07-24T06:47:58.000Z</published>
    <updated>2020-07-24T08:43:59.117Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Image-DCGAN教程:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="DCGAN-Tutorial"><a href="#DCGAN-Tutorial" class="headerlink" title="DCGAN Tutorial"></a>DCGAN Tutorial</h1><p><strong>Author</strong>: <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This tutorial will give an introduction to DCGANs through an example. We<br>will train a generative adversarial network (GAN) to generate new<br>celebrities after showing it pictures of many real celebrities. Most of<br>the code here is from the dcgan implementation in<br><code>pytorch/examples &lt;https://github.com/pytorch/examples&gt;</code>__, and this<br>document will give a thorough explanation of the implementation and shed<br>light on how and why this model works. But don’t worry, no prior<br>knowledge of GANs is required, but it may require a first-timer to spend<br>some time reasoning about what is actually happening under the hood.<br>Also, for the sake of time it will help to have a GPU, or two. Lets<br>start from the beginning.</p><h2 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h2><p>What is a GAN?</p><p>GANs are a framework for teaching a DL model to capture the training<br>data’s distribution so we can generate new data from that same<br>distribution. GANs were invented by Ian Goodfellow in 2014 and first<br>described in the paper <code>Generative AdversarialNets &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__.<br>They are made of two distinct models, a <em>generator</em> and a<br><em>discriminator</em>. The job of the generator is to spawn ‘fake’ images that<br>look like the training images. The job of the discriminator is to look<br>at an image and output whether or not it is a real training image or a<br>fake image from the generator. During training, the generator is<br>constantly trying to outsmart the discriminator by generating better and<br>better fakes, while the discriminator is working to become a better<br>detective and correctly classify the real and fake images. The<br>equilibrium of this game is when the generator is generating perfect<br>fakes that look as if they came directly from the training data, and the<br>discriminator is left to always guess at 50% confidence that the<br>generator output is real or fake.</p><p>Now, lets define some notation to be used throughout tutorial starting<br>with the discriminator. Let $x$ be data representing an image.<br>$D(x)$ is the discriminator network which outputs the (scalar)<br>probability that $x$ came from training data rather than the<br>generator. Here, since we are dealing with images the input to<br>$D(x)$ is an image of CHW size 3x64x64. Intuitively, $D(x)$<br>should be HIGH when $x$ comes from training data and LOW when<br>$x$ comes from the generator. $D(x)$ can also be thought of<br>as a traditional binary classifier.</p><p>For the generator’s notation, let $z$ be a latent space vector<br>sampled from a standard normal distribution. $G(z)$ represents the<br>generator function which maps the latent vector $z$ to data-space.<br>The goal of $G$ is to estimate the distribution that the training<br>data comes from ($p_{data}$) so it can generate fake samples from<br>that estimated distribution ($p_g$).</p><p>So, $D(G(z))$ is the probability (scalar) that the output of the<br>generator $G$ is a real image. As described in <code>Goodfellow’spaper &lt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&gt;</code>__,<br>$D$ and $G$ play a minimax game in which $D$ tries to<br>maximize the probability it correctly classifies reals and fakes<br>($logD(x)$), and $G$ tries to minimize the probability that<br>$D$ will predict its outputs are fake ($log(1-D(G(x)))$).<br>From the paper, the GAN loss function is</p><p>$\begin{align}\underset{G}{\text{min}} \underset{D}{\text{max}}V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}\big[logD(x)\big] + \mathbb{E}_{z\sim p_{z}(z)}\big[log(1-D(G(z)))\big]\end{align}$</p><p>In theory, the solution to this minimax game is where<br>$p_g = p_{data}$, and the discriminator guesses randomly if the<br>inputs are real or fake. However, the convergence theory of GANs is<br>still being actively researched and in reality models do not always<br>train to this point.</p><p>What is a DCGAN?</p><p>A DCGAN is a direct extension of the GAN described above, except that it<br>explicitly uses convolutional and convolutional-transpose layers in the<br>discriminator and generator, respectively. It was first described by<br>Radford et. al. in the paper <code>Unsupervised Representation Learning WithDeep Convolutional Generative AdversarialNetworks &lt;https://arxiv.org/pdf/1511.06434.pdf&gt;</code>. The discriminator<br>is made up of strided<br><code>convolution &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code><br>layers, <code>batchnorm &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d&gt;</code><br>layers, and<br><code>LeakyReLU &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU&gt;</code><br>activations. The input is a 3x64x64 input image and the output is a<br>scalar probability that the input is from the real data distribution.<br>The generator is comprised of<br><code>convolutional-transpose &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d&gt;</code><br>layers, batch norm layers, and<br><code>ReLU &lt;https://pytorch.org/docs/stable/nn.html#relu&gt;</code>__ activations. The<br>input is a latent vector, $z$, that is drawn from a standard<br>normal distribution and the output is a 3x64x64 RGB image. The strided<br>conv-transpose layers allow the latent vector to be transformed into a<br>volume with the same shape as an image. In the paper, the authors also<br>give some tips about how to setup the optimizers, how to calculate the<br>loss functions, and how to initialize the model weights, all of which<br>will be explained in the coming sections.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="comment">#%matplotlib inline</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed for reproducibility</span></span><br><span class="line">manualSeed = <span class="number">999</span></span><br><span class="line"><span class="comment">#manualSeed = random.randint(1, 10000) # use if you want new results</span></span><br><span class="line">print(<span class="string">"Random Seed: "</span>, manualSeed)</span><br><span class="line">random.seed(manualSeed)</span><br><span class="line">torch.manual_seed(manualSeed)</span><br></pre></td></tr></table></figure><pre><code>Random Seed:  999&lt;torch._C.Generator at 0x268c24f1e50&gt;</code></pre><h2 id="Inputs"><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h2><p>Let’s define some inputs for the run:</p><ul><li><strong>dataroot</strong> - the path to the root of the dataset folder. We will<br>talk more about the dataset in the next section</li><li><strong>workers</strong> - the number of worker threads for loading the data with<br>the DataLoader</li><li><strong>batch_size</strong> - the batch size used in training. The DCGAN paper<br>uses a batch size of 128</li><li><strong>image_size</strong> - the spatial size of the images used for training.<br>This implementation defaults to 64x64. If another size is desired,<br>the structures of D and G must be changed. See<br><code>here &lt;https://github.com/pytorch/examples/issues/70&gt;</code>__ for more<br>details</li><li><strong>nc</strong> - number of color channels in the input images. For color<br>images this is 3</li><li><strong>nz</strong> - length of latent vector</li><li><strong>ngf</strong> - relates to the depth of feature maps carried through the<br>generator</li><li><strong>ndf</strong> - sets the depth of feature maps propagated through the<br>discriminator</li><li><strong>num_epochs</strong> - number of training epochs to run. Training for<br>longer will probably lead to better results but will also take much<br>longer</li><li><strong>lr</strong> - learning rate for training. As described in the DCGAN paper,<br>this number should be 0.0002</li><li><strong>beta1</strong> - beta1 hyperparameter for Adam optimizers. As described in<br>paper, this number should be 0.5</li><li><strong>ngpu</strong> - number of GPUs available. If this is 0, code will run in<br>CPU mode. If this number is greater than 0 it will run on that number<br>of GPUs</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Root directory for dataset</span></span><br><span class="line">dataroot = <span class="string">"data/celeba"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of workers for dataloader</span></span><br><span class="line">workers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch size during training</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spatial size of training images. All images will be resized to this</span></span><br><span class="line"><span class="comment">#   size using a transformer.</span></span><br><span class="line">image_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of channels in the training images. For color images this is 3</span></span><br><span class="line">nc = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of z latent vector (i.e. size of generator input)</span></span><br><span class="line">nz = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in generator</span></span><br><span class="line">ngf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of feature maps in discriminator</span></span><br><span class="line">ndf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of training epochs</span></span><br><span class="line"><span class="comment"># num_epochs = 5</span></span><br><span class="line">num_epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate for optimizers</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Beta1 hyperparam for Adam optimizers</span></span><br><span class="line">beta1 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of GPUs available. Use 0 for CPU mode.</span></span><br><span class="line">ngpu = <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>In this tutorial we will use the <code>Celeb-A Facesdataset &lt;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&gt;</code><strong> which can<br>be downloaded at the linked site, or in <code>GoogleDrive &lt;https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg&gt;</code></strong>.<br>The dataset will download as a file named <em>img_align_celeba.zip</em>. Once<br>downloaded, create a directory named <em>celeba</em> and extract the zip file<br>into that directory. Then, set the <em>dataroot</em> input for this notebook to<br>the <em>celeba</em> directory you just created. The resulting directory<br>structure should be:</p><p>::</p><p>   /path/to/celeba<br>       -&gt; img_align_celeba<br>           -&gt; 188242.jpg<br>           -&gt; 173822.jpg<br>           -&gt; 284702.jpg<br>           -&gt; 537394.jpg<br>              …</p><p>This is an important step because we will be using the ImageFolder<br>dataset class, which requires there to be subdirectories in the<br>dataset’s root folder. Now, we can create the dataset, create the<br>dataloader, set the device to run on, and finally visualize some of the<br>training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can use an image folder dataset the way we have it setup.</span></span><br><span class="line"><span class="comment"># Create the dataset</span></span><br><span class="line">dataset = dset.ImageFolder(root=dataroot,</span><br><span class="line">                           transform=transforms.Compose([</span><br><span class="line">                               transforms.Resize(image_size),</span><br><span class="line">                               transforms.CenterCrop(image_size),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">                           ]))</span><br><span class="line"><span class="comment"># Create the dataloader</span></span><br><span class="line">dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>, num_workers=workers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot some training images</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Training Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">2</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x268c49247f0&gt;</code></pre><p><img src="/2020/07/24/Pytorch-Image-DCGAN%E6%95%99%E7%A8%8B/output_7_1.png" alt="png"></p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>With our input parameters set and the dataset prepared, we can now get<br>into the implementation. We will start with the weigth initialization<br>strategy, then talk about the generator, discriminator, loss functions,<br>and training loop in detail.</p><p>Weight Initialization</p><p>From the DCGAN paper, the authors specify that all model weights shall<br>be randomly initialized from a Normal distribution with mean=0,<br>stdev=0.02. The <code>weights_init</code> function takes an initialized model as<br>input and reinitializes all convolutional, convolutional-transpose, and<br>batch normalization layers to meet this criteria. This function is<br>applied to the models immediately after initialization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># custom weights initialization called on netG and netD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        nn.init.normal_(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        nn.init.constant_(m.bias.data, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>Generator</p><p>The generator, $G$, is designed to map the latent space vector<br>($z$) to data-space. Since our data are images, converting<br>$z$ to data-space means ultimately creating a RGB image with the<br>same size as the training images (i.e. 3x64x64). In practice, this is<br>accomplished through a series of strided two dimensional convolutional<br>transpose layers, each paired with a 2d batch norm layer and a relu<br>activation. The output of the generator is fed through a tanh function<br>to return it to the input data range of $[-1,1]$. It is worth<br>noting the existence of the batch norm functions after the<br>conv-transpose layers, as this is a critical contribution of the DCGAN<br>paper. These layers help with the flow of gradients during training. An<br>image of the generator from the DCGAN paper is shown below.</p><p>.. figure:: /_static/img/dcgan_generator.png<br>   :alt: dcgan_generator</p><p>Notice, the how the inputs we set in the input section (<em>nz</em>, <em>ngf</em>, and<br><em>nc</em>) influence the generator architecture in code. <em>nz</em> is the length<br>of the z input vector, <em>ngf</em> relates to the size of the feature maps<br>that are propagated through the generator, and <em>nc</em> is the number of<br>channels in the output image (set to 3 for RGB images). Below is the<br>code for the generator.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generator Code</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is Z, going into a convolution</span></span><br><span class="line">            nn.ConvTranspose2d( nz, ngf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">8</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*8) x 4 x 4</span></span><br><span class="line">            nn.ConvTranspose2d(ngf * <span class="number">8</span>, ngf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">4</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*4) x 8 x 8</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">4</span>, ngf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf*2) x 16 x 16</span></span><br><span class="line">            nn.ConvTranspose2d( ngf * <span class="number">2</span>, ngf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ngf) x 32 x 32</span></span><br><span class="line">            nn.ConvTranspose2d( ngf, nc, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">            <span class="comment"># state size. (nc) x 64 x 64</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure><p>Now, we can instantiate the generator and apply the <code>weights_init</code><br>function. Check out the printed model to see how the generator object is<br>structured.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the generator</span></span><br><span class="line">netG = Generator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netG = nn.DataParallel(netG, list(range(ngpu)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netG.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netG)</span><br></pre></td></tr></table></figure><pre><code>Generator(  (main): Sequential(    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (2): ReLU(inplace=True)    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (5): ReLU(inplace=True)    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (8): ReLU(inplace=True)    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (11): ReLU(inplace=True)    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (13): Tanh()  ))</code></pre><p>Discriminator</p><p>As mentioned, the discriminator, $D$, is a binary classification<br>network that takes an image as input and outputs a scalar probability<br>that the input image is real (as opposed to fake). Here, $D$ takes<br>a 3x64x64 input image, processes it through a series of Conv2d,<br>BatchNorm2d, and LeakyReLU layers, and outputs the final probability<br>through a Sigmoid activation function. This architecture can be extended<br>with more layers if necessary for the problem, but there is significance<br>to the use of the strided convolution, BatchNorm, and LeakyReLUs. The<br>DCGAN paper mentions it is a good practice to use strided convolution<br>rather than pooling to downsample because it lets the network learn its<br>own pooling function. Also batch norm and leaky relu functions promote<br>healthy gradient flow which is critical for the learning process of both<br>$G$ and $D$.</p><p>Discriminator Code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ngpu)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.ngpu = ngpu</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># input is (nc) x 64 x 64</span></span><br><span class="line">            nn.Conv2d(nc, ndf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf) x 32 x 32</span></span><br><span class="line">            nn.Conv2d(ndf, ndf * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">2</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*2) x 16 x 16</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">2</span>, ndf * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">4</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*4) x 8 x 8</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">4</span>, ndf * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf * <span class="number">8</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># state size. (ndf*8) x 4 x 4</span></span><br><span class="line">            nn.Conv2d(ndf * <span class="number">8</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.main(input)</span><br></pre></td></tr></table></figure><p>Now, as with the generator, we can create the discriminator, apply the<br><code>weights_init</code> function, and print the model’s structure.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the Discriminator</span></span><br><span class="line">netD = Discriminator(ngpu).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handle multi-gpu if desired</span></span><br><span class="line"><span class="keyword">if</span> (device.type == <span class="string">'cuda'</span>) <span class="keyword">and</span> (ngpu &gt; <span class="number">1</span>):</span><br><span class="line">    netD = nn.DataParallel(netD, list(range(ngpu)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Apply the weights_init function to randomly initialize all weights</span></span><br><span class="line"><span class="comment">#  to mean=0, stdev=0.2.</span></span><br><span class="line">netD.apply(weights_init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model</span></span><br><span class="line">print(netD)</span><br></pre></td></tr></table></figure><pre><code>Discriminator(  (main): Sequential(    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (1): LeakyReLU(negative_slope=0.2, inplace=True)    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (4): LeakyReLU(negative_slope=0.2, inplace=True)    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (7): LeakyReLU(negative_slope=0.2, inplace=True)    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (10): LeakyReLU(negative_slope=0.2, inplace=True)    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)    (12): Sigmoid()  ))</code></pre><p>Loss Functions and Optimizers</p><p>With $D$ and $G$ setup, we can specify how they learn<br>through the loss functions and optimizers. We will use the Binary Cross<br>Entropy loss<br>(<code>BCELoss &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss&gt;</code>__)<br>function which is defined in PyTorch as:</p><p>\begin{align}\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]\end{align}</p><p>Notice how this function provides the calculation of both log components<br>in the objective function (i.e. $log(D(x))$ and<br>$log(1-D(G(z)))$). We can specify what part of the BCE equation to<br>use with the $y$ input. This is accomplished in the training loop<br>which is coming up soon, but it is important to understand how we can<br>choose which component we wish to calculate just by changing $y$<br>(i.e. GT labels).</p><p>Next, we define our real label as 1 and the fake label as 0. These<br>labels will be used when calculating the losses of $D$ and<br>$G$, and this is also the convention used in the original GAN<br>paper. Finally, we set up two separate optimizers, one for $D$ and<br>one for $G$. As specified in the DCGAN paper, both are Adam<br>optimizers with learning rate 0.0002 and Beta1 = 0.5. For keeping track<br>of the generator’s learning progression, we will generate a fixed batch<br>of latent vectors that are drawn from a Gaussian distribution<br>(i.e. fixed_noise) . In the training loop, we will periodically input<br>this fixed_noise into $G$, and over the iterations we will see<br>images form out of the noise.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize BCELoss function</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create batch of latent vectors that we will use to visualize</span></span><br><span class="line"><span class="comment">#  the progression of the generator</span></span><br><span class="line">fixed_noise = torch.randn(<span class="number">64</span>, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Establish convention for real and fake labels during training</span></span><br><span class="line">real_label = <span class="number">1</span></span><br><span class="line">fake_label = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup Adam optimizers for both G and D</span></span><br><span class="line">optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure><p>Training</p><p>Finally, now that we have all of the parts of the GAN framework defined,<br>we can train it. Be mindful that training GANs is somewhat of an art<br>form, as incorrect hyperparameter settings lead to mode collapse with<br>little explanation of what went wrong. Here, we will closely follow<br>Algorithm 1 from Goodfellow’s paper, while abiding by some of the best<br>practices shown in <code>ganhacks &lt;https://github.com/soumith/ganhacks&gt;</code>__.<br>Namely, we will “construct different mini-batches for real and fake”<br>images, and also adjust G’s objective function to maximize<br>$logD(G(z))$. Training is split up into two main parts. Part 1<br>updates the Discriminator and Part 2 updates the Generator.</p><p><strong>Part 1 - Train the Discriminator</strong></p><p>Recall, the goal of training the discriminator is to maximize the<br>probability of correctly classifying a given input as real or fake. In<br>terms of Goodfellow, we wish to “update the discriminator by ascending<br>its stochastic gradient”. Practically, we want to maximize<br>$log(D(x)) + log(1-D(G(z)))$. Due to the separate mini-batch<br>suggestion from ganhacks, we will calculate this in two steps. First, we<br>will construct a batch of real samples from the training set, forward<br>pass through $D$, calculate the loss ($log(D(x))$), then<br>calculate the gradients in a backward pass. Secondly, we will construct<br>a batch of fake samples with the current generator, forward pass this<br>batch through $D$, calculate the loss ($log(1-D(G(z)))$),<br>and <em>accumulate</em> the gradients with a backward pass. Now, with the<br>gradients accumulated from both the all-real and all-fake batches, we<br>call a step of the Discriminator’s optimizer.</p><p><strong>Part 2 - Train the Generator</strong></p><p>As stated in the original paper, we want to train the Generator by<br>minimizing $log(1-D(G(z)))$ in an effort to generate better fakes.<br>As mentioned, this was shown by Goodfellow to not provide sufficient<br>gradients, especially early in the learning process. As a fix, we<br>instead wish to maximize $log(D(G(z)))$. In the code we accomplish<br>this by: classifying the Generator output from Part 1 with the<br>Discriminator, computing G’s loss <em>using real labels as GT</em>, computing<br>G’s gradients in a backward pass, and finally updating G’s parameters<br>with an optimizer step. It may seem counter-intuitive to use the real<br>labels as GT labels for the loss function, but this allows us to use the<br>$log(x)$ part of the BCELoss (rather than the $log(1-x)$<br>part) which is exactly what we want.</p><p>Finally, we will do some statistic reporting and at the end of each<br>epoch we will push our fixed_noise batch through the generator to<br>visually track the progress of G’s training. The training statistics<br>reported are:</p><ul><li><strong>Loss_D</strong> - discriminator loss calculated as the sum of losses for<br>the all real and all fake batches ($log(D(x)) + log(D(G(z)))$).</li><li><strong>Loss_G</strong> - generator loss calculated as $log(D(G(z)))$</li><li><strong>D(x)</strong> - the average output (across the batch) of the discriminator<br>for the all real batch. This should start close to 1 then<br>theoretically converge to 0.5 when G gets better. Think about why<br>this is.</li><li><strong>D(G(z))</strong> - average discriminator outputs for the all fake batch.<br>The first number is before D is updated and the second number is<br>after D is updated. These numbers should start near 0 and converge to<br>0.5 as G gets better. Think about why this is.</li></ul><p><strong>Note:</strong> This step might take a while, depending on how many epochs you<br>run and if you removed some data from the dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Loop</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Lists to keep track of progress</span></span><br><span class="line">img_list = []</span><br><span class="line">G_losses = []</span><br><span class="line">D_losses = []</span><br><span class="line">iters = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Starting Training Loop..."</span>)</span><br><span class="line"><span class="comment"># For each epoch</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># For each batch in the dataloader</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(dataloader, <span class="number">0</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        <span class="comment">## Train with all-real batch</span></span><br><span class="line">        netD.zero_grad()</span><br><span class="line">        <span class="comment"># Format batch</span></span><br><span class="line">        real_cpu = data[<span class="number">0</span>].to(device)</span><br><span class="line">        b_size = real_cpu.size(<span class="number">0</span>)</span><br><span class="line">        label = torch.full((b_size,), real_label, device=device)</span><br><span class="line">        <span class="comment"># Forward pass real batch through D</span></span><br><span class="line">        output = netD(real_cpu).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate loss on all-real batch</span></span><br><span class="line">        errD_real = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for D in backward pass</span></span><br><span class="line">        errD_real.backward()</span><br><span class="line">        D_x = output.mean().item()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Train with all-fake batch</span></span><br><span class="line">        <span class="comment"># Generate batch of latent vectors</span></span><br><span class="line">        noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        <span class="comment"># Generate fake image batch with G</span></span><br><span class="line">        fake = netG(noise)</span><br><span class="line">        label.fill_(fake_label)</span><br><span class="line">        <span class="comment"># Classify all fake batch with D</span></span><br><span class="line">        output = netD(fake.detach()).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate D's loss on the all-fake batch</span></span><br><span class="line">        errD_fake = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate the gradients for this batch</span></span><br><span class="line">        errD_fake.backward()</span><br><span class="line">        D_G_z1 = output.mean().item()</span><br><span class="line">        <span class="comment"># Add the gradients from the all-real and all-fake batches</span></span><br><span class="line">        errD = errD_real + errD_fake</span><br><span class="line">        <span class="comment"># Update D</span></span><br><span class="line">        optimizerD.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################</span></span><br><span class="line">        <span class="comment"># (2) Update G network: maximize log(D(G(z)))</span></span><br><span class="line">        <span class="comment">###########################</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        label.fill_(real_label)  <span class="comment"># fake labels are real for generator cost</span></span><br><span class="line">        <span class="comment"># Since we just updated D, perform another forward pass of all-fake batch through D</span></span><br><span class="line">        output = netD(fake).view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Calculate G's loss based on this output</span></span><br><span class="line">        errG = criterion(output, label)</span><br><span class="line">        <span class="comment"># Calculate gradients for G</span></span><br><span class="line">        errG.backward()</span><br><span class="line">        D_G_z2 = output.mean().item()</span><br><span class="line">        <span class="comment"># Update G</span></span><br><span class="line">        optimizerG.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Output training stats</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'</span></span><br><span class="line">                  % (epoch, num_epochs, i, len(dataloader),</span><br><span class="line">                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Save Losses for plotting later</span></span><br><span class="line">        G_losses.append(errG.item())</span><br><span class="line">        D_losses.append(errD.item())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check how the generator is doing by saving G's output on fixed_noise</span></span><br><span class="line">        <span class="keyword">if</span> (iters % <span class="number">500</span> == <span class="number">0</span>) <span class="keyword">or</span> ((epoch == num_epochs<span class="number">-1</span>) <span class="keyword">and</span> (i == len(dataloader)<span class="number">-1</span>)):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = netG(fixed_noise).detach().cpu()</span><br><span class="line">            img_list.append(vutils.make_grid(fake, padding=<span class="number">2</span>, normalize=<span class="literal">True</span>))</span><br><span class="line">            </span><br><span class="line">        iters += <span class="number">1</span></span><br></pre></td></tr></table></figure><pre><code>Starting Training Loop.....\aten\src\ATen\native\TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.[0/2][0/1583]    Loss_D: 1.8664    Loss_G: 4.9949    D(x): 0.5050    D(G(z)): 0.5928 / 0.0106[0/2][50/1583]    Loss_D: 0.1046    Loss_G: 7.1177    D(x): 0.9758    D(G(z)): 0.0124 / 0.0086[0/2][100/1583]    Loss_D: 1.0915    Loss_G: 12.3498    D(x): 0.9553    D(G(z)): 0.4722 / 0.0000[0/2][150/1583]    Loss_D: 1.7593    Loss_G: 5.9544    D(x): 0.3933    D(G(z)): 0.0053 / 0.0070[0/2][200/1583]    Loss_D: 0.8020    Loss_G: 5.9020    D(x): 0.6117    D(G(z)): 0.0295 / 0.0069[0/2][250/1583]    Loss_D: 0.6124    Loss_G: 4.1050    D(x): 0.7546    D(G(z)): 0.1926 / 0.0268[0/2][300/1583]    Loss_D: 0.9607    Loss_G: 1.8866    D(x): 0.5101    D(G(z)): 0.0519 / 0.2133[0/2][350/1583]    Loss_D: 0.5478    Loss_G: 3.3292    D(x): 0.7691    D(G(z)): 0.1973 / 0.0492[0/2][400/1583]    Loss_D: 1.2509    Loss_G: 1.3775    D(x): 0.4456    D(G(z)): 0.1254 / 0.2993[0/2][450/1583]    Loss_D: 0.4384    Loss_G: 5.5121    D(x): 0.7491    D(G(z)): 0.0298 / 0.0068[0/2][500/1583]    Loss_D: 0.4170    Loss_G: 3.6389    D(x): 0.7676    D(G(z)): 0.0610 / 0.0396[0/2][550/1583]    Loss_D: 0.5595    Loss_G: 4.2165    D(x): 0.8009    D(G(z)): 0.1784 / 0.0296[0/2][600/1583]    Loss_D: 1.2295    Loss_G: 3.3066    D(x): 0.5244    D(G(z)): 0.0907 / 0.0752[0/2][650/1583]    Loss_D: 0.5091    Loss_G: 4.7135    D(x): 0.7118    D(G(z)): 0.0292 / 0.0203[0/2][700/1583]    Loss_D: 0.3912    Loss_G: 2.7194    D(x): 0.8198    D(G(z)): 0.1221 / 0.1098[0/2][750/1583]    Loss_D: 0.7578    Loss_G: 7.3482    D(x): 0.9471    D(G(z)): 0.4270 / 0.0015[0/2][800/1583]    Loss_D: 0.7080    Loss_G: 5.8282    D(x): 0.9395    D(G(z)): 0.4118 / 0.0053[0/2][850/1583]    Loss_D: 0.5755    Loss_G: 3.2657    D(x): 0.6967    D(G(z)): 0.0784 / 0.0637[0/2][900/1583]    Loss_D: 1.0720    Loss_G: 6.0690    D(x): 0.8734    D(G(z)): 0.5405 / 0.0047[0/2][950/1583]    Loss_D: 0.6901    Loss_G: 5.7612    D(x): 0.9313    D(G(z)): 0.3993 / 0.0064[0/2][1000/1583]    Loss_D: 0.2473    Loss_G: 5.5474    D(x): 0.8535    D(G(z)): 0.0574 / 0.0123[0/2][1050/1583]    Loss_D: 0.6581    Loss_G: 5.6885    D(x): 0.9537    D(G(z)): 0.4081 / 0.0069[0/2][1100/1583]    Loss_D: 0.5447    Loss_G: 2.8289    D(x): 0.7523    D(G(z)): 0.1252 / 0.0966[0/2][1150/1583]    Loss_D: 0.3936    Loss_G: 3.4201    D(x): 0.8383    D(G(z)): 0.1639 / 0.0528[0/2][1200/1583]    Loss_D: 0.7676    Loss_G: 5.3321    D(x): 0.8581    D(G(z)): 0.3643 / 0.0112[0/2][1250/1583]    Loss_D: 0.4362    Loss_G: 3.9937    D(x): 0.8223    D(G(z)): 0.1671 / 0.0321[0/2][1300/1583]    Loss_D: 0.6602    Loss_G: 5.5204    D(x): 0.9106    D(G(z)): 0.3806 / 0.0072[0/2][1350/1583]    Loss_D: 0.6352    Loss_G: 3.2255    D(x): 0.7404    D(G(z)): 0.1788 / 0.0708[0/2][1400/1583]    Loss_D: 0.7936    Loss_G: 3.5125    D(x): 0.7248    D(G(z)): 0.2855 / 0.0474[0/2][1450/1583]    Loss_D: 1.0550    Loss_G: 1.2678    D(x): 0.7036    D(G(z)): 0.3793 / 0.3957[0/2][1500/1583]    Loss_D: 0.4235    Loss_G: 2.8506    D(x): 0.7756    D(G(z)): 0.1001 / 0.0801[0/2][1550/1583]    Loss_D: 0.4839    Loss_G: 4.1835    D(x): 0.8839    D(G(z)): 0.2465 / 0.0254[1/2][0/1583]    Loss_D: 0.7391    Loss_G: 3.9735    D(x): 0.8016    D(G(z)): 0.3263 / 0.0284[1/2][50/1583]    Loss_D: 0.5051    Loss_G: 3.8654    D(x): 0.7839    D(G(z)): 0.1756 / 0.0337[1/2][100/1583]    Loss_D: 0.4857    Loss_G: 4.5489    D(x): 0.8677    D(G(z)): 0.2420 / 0.0209[1/2][150/1583]    Loss_D: 0.6025    Loss_G: 4.4404    D(x): 0.8519    D(G(z)): 0.2920 / 0.0212[1/2][200/1583]    Loss_D: 0.4301    Loss_G: 4.4767    D(x): 0.8909    D(G(z)): 0.2399 / 0.0190[1/2][250/1583]    Loss_D: 1.2600    Loss_G: 7.6782    D(x): 0.9744    D(G(z)): 0.6415 / 0.0013[1/2][300/1583]    Loss_D: 0.5044    Loss_G: 3.7002    D(x): 0.8408    D(G(z)): 0.2446 / 0.0375[1/2][350/1583]    Loss_D: 0.4184    Loss_G: 3.2221    D(x): 0.7736    D(G(z)): 0.0924 / 0.0649[1/2][400/1583]    Loss_D: 0.5320    Loss_G: 4.6695    D(x): 0.9051    D(G(z)): 0.3072 / 0.0150[1/2][450/1583]    Loss_D: 0.3804    Loss_G: 3.3363    D(x): 0.7888    D(G(z)): 0.0978 / 0.0636[1/2][500/1583]    Loss_D: 0.4293    Loss_G: 4.2911    D(x): 0.9014    D(G(z)): 0.2399 / 0.0226[1/2][550/1583]    Loss_D: 0.3940    Loss_G: 2.7648    D(x): 0.7634    D(G(z)): 0.0777 / 0.0929[1/2][600/1583]    Loss_D: 0.4044    Loss_G: 3.3666    D(x): 0.8438    D(G(z)): 0.1664 / 0.0598[1/2][650/1583]    Loss_D: 0.3879    Loss_G: 3.4838    D(x): 0.8517    D(G(z)): 0.1754 / 0.0455[1/2][700/1583]    Loss_D: 0.4487    Loss_G: 3.6364    D(x): 0.8773    D(G(z)): 0.2434 / 0.0370[1/2][750/1583]    Loss_D: 0.7588    Loss_G: 2.0882    D(x): 0.6144    D(G(z)): 0.1150 / 0.1773[1/2][800/1583]    Loss_D: 0.6134    Loss_G: 4.0046    D(x): 0.9100    D(G(z)): 0.3546 / 0.0278[1/2][850/1583]    Loss_D: 0.5061    Loss_G: 2.2267    D(x): 0.7046    D(G(z)): 0.0860 / 0.1488[1/2][900/1583]    Loss_D: 0.6032    Loss_G: 1.8834    D(x): 0.6518    D(G(z)): 0.0847 / 0.2023[1/2][950/1583]    Loss_D: 1.1199    Loss_G: 2.2135    D(x): 0.4166    D(G(z)): 0.0332 / 0.1791[1/2][1000/1583]    Loss_D: 0.8061    Loss_G: 2.2557    D(x): 0.5479    D(G(z)): 0.0442 / 0.1506[1/2][1050/1583]    Loss_D: 0.7723    Loss_G: 2.7941    D(x): 0.5652    D(G(z)): 0.0532 / 0.0968[1/2][1100/1583]    Loss_D: 0.6160    Loss_G: 1.4266    D(x): 0.6152    D(G(z)): 0.0460 / 0.2874[1/2][1150/1583]    Loss_D: 1.1706    Loss_G: 5.4761    D(x): 0.9509    D(G(z)): 0.6143 / 0.0088[1/2][1200/1583]    Loss_D: 0.5637    Loss_G: 2.2863    D(x): 0.7523    D(G(z)): 0.1901 / 0.1335[1/2][1250/1583]    Loss_D: 0.4913    Loss_G: 2.1290    D(x): 0.7336    D(G(z)): 0.1155 / 0.1592[1/2][1300/1583]    Loss_D: 0.4753    Loss_G: 2.9672    D(x): 0.8157    D(G(z)): 0.1986 / 0.0763[1/2][1350/1583]    Loss_D: 0.6133    Loss_G: 2.9954    D(x): 0.8253    D(G(z)): 0.2826 / 0.0687[1/2][1400/1583]    Loss_D: 0.4921    Loss_G: 3.1019    D(x): 0.8035    D(G(z)): 0.1985 / 0.0676</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Finally, lets check out how we did. Here, we will look at three<br>different results. First, we will see how D and G’s losses changed<br>during training. Second, we will visualize G’s output on the fixed_noise<br>batch for every epoch. And third, we will look at a batch of real data<br>next to a batch of fake data from G.</p><p><strong>Loss versus training iteration</strong></p><p>Below is a plot of D &amp; G’s losses versus training iterations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">"Generator and Discriminator Loss During Training"</span>)</span><br><span class="line">plt.plot(G_losses,label=<span class="string">"G"</span>)</span><br><span class="line">plt.plot(D_losses,label=<span class="string">"D"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"iterations"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Visualization of G’s progression</strong></p><p>Remember how we saved the generator’s output on the fixed_noise batch<br>after every epoch of training. Now, we can visualize the training<br>progression of G with an animation. Press the play button to start the<br>animation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%%capture</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">ims = [[plt.imshow(np.transpose(i,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)), animated=<span class="literal">True</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> img_list]</span><br><span class="line">ani = animation.ArtistAnimation(fig, ims, interval=<span class="number">1000</span>, repeat_delay=<span class="number">1000</span>, blit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">HTML(ani.to_jshtml())</span><br></pre></td></tr></table></figure><p><strong>Real Images vs. Fake Images</strong></p><p>Finally, lets take a look at some real images and fake images side by<br>side.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grab a batch of real images from the dataloader</span></span><br><span class="line">real_batch = next(iter(dataloader))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the real images</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Real Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(vutils.make_grid(real_batch[<span class="number">0</span>].to(device)[:<span class="number">64</span>], padding=<span class="number">5</span>, normalize=<span class="literal">True</span>).cpu(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the fake images from the last epoch</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Fake Images"</span>)</span><br><span class="line">plt.imshow(np.transpose(img_list[<span class="number">-1</span>],(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="Where-to-Go-Next"><a href="#Where-to-Go-Next" class="headerlink" title="Where to Go Next"></a>Where to Go Next</h2><p>We have reached the end of our journey, but there are several places you<br>could go from here. You could:</p><ul><li>Train for longer to see how good the results get</li><li>Modify this model to take a different dataset and possibly change the<br>size of the images and the model architecture</li><li>Check out some other cool GAN projects<br><code>here &lt;https://github.com/nashory/gans-awesome-applications&gt;</code>__</li><li>Create GANs that generate<br><code>music &lt;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&gt;</code>__</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 我不认识的单词</span></span><br></pre></td></tr></table></figure><p>Intuitively:直观地<br><code></code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Image-DCGAN教程:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Image" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Image-对抗样本生成</title>
    <link href="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/"/>
    <id>http://yoursite.com/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/</id>
    <published>2020-07-24T06:47:29.000Z</published>
    <updated>2020-07-24T08:44:57.984Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Image-对抗样本生成:<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Adversarial-Example-Generation"><a href="#Adversarial-Example-Generation" class="headerlink" title="Adversarial Example Generation"></a>Adversarial Example Generation</h1><p><strong>Author:</strong> <code>Nathan Inkawhich &lt;https://github.com/inkawhich&gt;</code>__</p><p>If you are reading this, hopefully you can appreciate how effective some<br>machine learning models are. Research is constantly pushing ML models to<br>be faster, more accurate, and more efficient. However, an often<br>overlooked aspect of designing and training models is security and<br>robustness, especially in the face of an adversary who wishes to fool<br>the model.</p><p>This tutorial will raise your awareness to the security vulnerabilities<br>of ML models, and will give insight into the hot topic of adversarial<br>machine learning. You may be surprised to find that adding imperceptible<br>perturbations to an image <em>can</em> cause drastically different model<br>performance. Given that this is a tutorial, we will explore the topic<br>via example on an image classifier. Specifically we will use one of the<br>first and most popular attack methods, the Fast Gradient Sign Attack<br>(FGSM), to fool an MNIST classifier.</p><h2 id="Threat-Model"><a href="#Threat-Model" class="headerlink" title="Threat Model"></a>Threat Model</h2><p>For context, there are many categories of adversarial attacks, each with<br>a different goal and assumption of the attacker’s knowledge. However, in<br>general the overarching goal is to add the least amount of perturbation<br>to the input data to cause the desired misclassification. There are<br>several kinds of assumptions of the attacker’s knowledge, two of which<br>are: <strong>white-box</strong> and <strong>black-box</strong>. A <em>white-box</em> attack assumes the<br>attacker has full knowledge and access to the model, including<br>architecture, inputs, outputs, and weights. A <em>black-box</em> attack assumes<br>the attacker only has access to the inputs and outputs of the model, and<br>knows nothing about the underlying architecture or weights. There are<br>also several types of goals, including <strong>misclassification</strong> and<br><strong>source/target misclassification</strong>. A goal of <em>misclassification</em> means<br>the adversary only wants the output classification to be wrong but does<br>not care what the new classification is. A <em>source/target<br>misclassification</em> means the adversary wants to alter an image that is<br>originally of a specific source class so that it is classified as a<br>specific target class.</p><p>In this case, the FGSM attack is a <em>white-box</em> attack with the goal of<br><em>misclassification</em>. With this background information, we can now<br>discuss the attack in detail.</p><p>Fast Gradient Sign Attack</p><p>One of the first and most popular adversarial attacks to date is<br>referred to as the <em>Fast Gradient Sign Attack (FGSM)</em> and is described<br>by Goodfellow et. al. in <code>Explaining and Harnessing AdversarialExamples &lt;https://arxiv.org/abs/1412.6572&gt;</code>__. The attack is remarkably<br>powerful, and yet intuitive. It is designed to attack neural networks by<br>leveraging the way they learn, <em>gradients</em>. The idea is simple, rather<br>than working to minimize the loss by adjusting the weights based on the<br>backpropagated gradients, the attack <em>adjusts the input data to maximize<br>the loss</em> based on the same backpropagated gradients. In other words,<br>the attack uses the gradient of the loss w.r.t the input data, then<br>adjusts the input data to maximize the loss.</p><p>Before we jump into the code, let’s look at the famous<br><code>FGSM &lt;https://arxiv.org/abs/1412.6572&gt;</code>__ panda example and extract<br>some notation.</p><p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_images/fgsm_panda_image.png" alt></p><p>From the figure, $\mathbf{x}$ is the original input image<br>correctly classified as a “panda”, $y$ is the ground truth label<br>for $\mathbf{x}$, $\mathbf{\theta}$ represents the model<br>parameters, and $J(\mathbf{\theta}, \mathbf{x}, y)$ is the loss<br>that is used to train the network. The attack backpropagates the<br>gradient back to the input data to calculate<br>$\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$. Then, it adjusts<br>the input data by a small step ($\epsilon$ or $0.007$ in the<br>picture) in the direction (i.e.<br>$sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))$) that will<br>maximize the loss. The resulting perturbed image, $x’$, is then<br><em>misclassified</em> by the target network as a “gibbon” when it is still<br>clearly a “panda”.</p><p>Hopefully now the motivation for this tutorial is clear, so lets jump<br>into the implementation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>In this section, we will discuss the input parameters for the tutorial,<br>define the model under attack, then code the attack and run some tests.</p><p>Inputs</p><p>There are only three inputs for this tutorial, and are defined as<br>follows:</p><ul><li><p><strong>epsilons</strong> - List of epsilon values to use for the run. It is<br>important to keep 0 in the list because it represents the model<br>performance on the original test set. Also, intuitively we would<br>expect the larger the epsilon, the more noticeable the perturbations<br>but the more effective the attack in terms of degrading model<br>accuracy. Since the data range here is $[0,1]$, no epsilon<br>value should exceed 1.</p></li><li><p><strong>pretrained_model</strong> - path to the pretrained MNIST model which was<br>trained with<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code><strong>.<br>For simplicity, download the pretrained model <code>here &lt;https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing&gt;</code></strong>.</p></li><li><p><strong>use_cuda</strong> - boolean flag to use CUDA if desired and available.<br>Note, a GPU with CUDA is not critical for this tutorial as a CPU will<br>not take much time.</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epsilons = [<span class="number">0</span>, <span class="number">.05</span>, <span class="number">.1</span>, <span class="number">.15</span>, <span class="number">.2</span>, <span class="number">.25</span>, <span class="number">.3</span>]</span><br><span class="line">pretrained_model = <span class="string">"data/lenet_mnist_model.pth"</span></span><br><span class="line">use_cuda=<span class="literal">True</span></span><br></pre></td></tr></table></figure><p>Model Under Attack</p><p>As mentioned, the model under attack is the same MNIST model from<br><code>pytorch/examples/mnist &lt;https://github.com/pytorch/examples/tree/master/mnist&gt;</code>__.<br>You may train and save your own MNIST model or you can download and use<br>the provided model. The <em>Net</em> definition and test dataloader here have<br>been copied from the MNIST example. The purpose of this section is to<br>define the model and dataloader, then initialize the model and load the<br>pretrained weights.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LeNet Model definition</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MNIST Test dataset and dataloader declaration</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            ])), </span><br><span class="line">        batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define what device we are using</span></span><br><span class="line">print(<span class="string">"CUDA Available: "</span>,torch.cuda.is_available())</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> (use_cuda <span class="keyword">and</span> torch.cuda.is_available()) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the network</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pretrained model</span></span><br><span class="line">model.load_state_dict(torch.load(pretrained_model, map_location=<span class="string">'cpu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the model in evaluation mode. In this case this is for the Dropout layers</span></span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure><pre><code>Using downloaded and verified file: data\MNIST\raw\train-images-idx3-ubyte.gzExtracting data\MNIST\raw\train-images-idx3-ubyte.gzUsing downloaded and verified file: data\MNIST\raw\train-labels-idx1-ubyte.gzExtracting data\MNIST\raw\train-labels-idx1-ubyte.gzUsing downloaded and verified file: data\MNIST\raw\t10k-images-idx3-ubyte.gzExtracting data\MNIST\raw\t10k-images-idx3-ubyte.gzUsing downloaded and verified file: data\MNIST\raw\t10k-labels-idx1-ubyte.gzExtracting data\MNIST\raw\t10k-labels-idx1-ubyte.gzProcessing...Done!CUDA Available:  FalseNet(  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))  (conv2_drop): Dropout2d(p=0.5)  (fc1): Linear(in_features=320, out_features=50, bias=True)  (fc2): Linear(in_features=50, out_features=10, bias=True))</code></pre><p>FGSM Attack</p><p>Now, we can define the function that creates the adversarial examples by<br>perturbing the original inputs. The <code>fgsm_attack</code> function takes three<br>inputs, <em>image</em> is the original clean image ($x$), <em>epsilon</em> is<br>the pixel-wise perturbation amount ($\epsilon$), and <em>data_grad</em><br>is gradient of the loss w.r.t the input image<br>($\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$). The function<br>then creates perturbed image as</p><p>\begin{align}perturbed_image = image + epsilon<em>sign(data_grad) = x + \epsilon </em> sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))\end{align}</p><p>Finally, in order to maintain the original range of the data, the<br>perturbed image is clipped to range $[0,1]$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FGSM attack code</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fgsm_attack</span><span class="params">(image, epsilon, data_grad)</span>:</span></span><br><span class="line">    <span class="comment"># Collect the element-wise sign of the data gradient</span></span><br><span class="line">    sign_data_grad = data_grad.sign()</span><br><span class="line">    <span class="comment"># Create the perturbed image by adjusting each pixel of the input image</span></span><br><span class="line">    perturbed_image = image + epsilon*sign_data_grad</span><br><span class="line">    <span class="comment"># Adding clipping to maintain [0,1] range</span></span><br><span class="line">    perturbed_image = torch.clamp(perturbed_image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Return the perturbed image</span></span><br><span class="line">    <span class="keyword">return</span> perturbed_image</span><br></pre></td></tr></table></figure><p>Testing Function</p><p>Finally, the central result of this tutorial comes from the <code>test</code><br>function. Each call to this test function performs a full test step on<br>the MNIST test set and reports a final accuracy. However, notice that<br>this function also takes an <em>epsilon</em> input. This is because the<br><code>test</code> function reports the accuracy of a model that is under attack<br>from an adversary with strength $\epsilon$. More specifically, for<br>each sample in the test set, the function computes the gradient of the<br>loss w.r.t the input data ($data_grad$), creates a perturbed<br>image with <code>fgsm_attack</code> ($perturbed_data$), then checks to see<br>if the perturbed example is adversarial. In addition to testing the<br>accuracy of the model, the function also saves and returns some<br>successful adversarial examples to be visualized later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">( model, device, test_loader, epsilon )</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Accuracy counter</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    adv_examples = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop over all examples in test set</span></span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Send the data and label to the device</span></span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set requires_grad attribute of tensor. Important for Attack</span></span><br><span class="line">        data.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass the data through the model</span></span><br><span class="line">        output = model(data)</span><br><span class="line">        init_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the initial prediction is wrong, dont bother attacking, just move on</span></span><br><span class="line">        <span class="keyword">if</span> init_pred.item() != target.item():</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the loss</span></span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero all existing gradients</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate gradients of model in backward pass</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Collect datagrad</span></span><br><span class="line">        data_grad = data.grad.data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Call FGSM Attack</span></span><br><span class="line">        perturbed_data = fgsm_attack(data, epsilon, data_grad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Re-classify the perturbed image</span></span><br><span class="line">        output = model(perturbed_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for success</span></span><br><span class="line">        final_pred = output.max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">        <span class="keyword">if</span> final_pred.item() == target.item():</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Special case for saving 0 epsilon examples</span></span><br><span class="line">            <span class="keyword">if</span> (epsilon == <span class="number">0</span>) <span class="keyword">and</span> (len(adv_examples) &lt; <span class="number">5</span>):</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Save some adv examples for visualization later</span></span><br><span class="line">            <span class="keyword">if</span> len(adv_examples) &lt; <span class="number">5</span>:</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate final accuracy for this epsilon</span></span><br><span class="line">    final_acc = correct/float(len(test_loader))</span><br><span class="line">    print(<span class="string">"Epsilon: &#123;&#125;\tTest Accuracy = &#123;&#125; / &#123;&#125; = &#123;&#125;"</span>.format(epsilon, correct, len(test_loader), final_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the accuracy and an adversarial example</span></span><br><span class="line">    <span class="keyword">return</span> final_acc, adv_examples</span><br></pre></td></tr></table></figure><p>Run Attack</p><p>The last part of the implementation is to actually run the attack. Here,<br>we run a full test step for each epsilon value in the <em>epsilons</em> input.<br>For each epsilon we also save the final accuracy and some successful<br>adversarial examples to be plotted in the coming sections. Notice how<br>the printed accuracies decrease as the epsilon value increases. Also,<br>note the $\epsilon=0$ case represents the original test accuracy,<br>with no attack.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">accuracies = []</span><br><span class="line">examples = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run test for each epsilon</span></span><br><span class="line"><span class="keyword">for</span> eps <span class="keyword">in</span> epsilons:</span><br><span class="line">    acc, ex = test(model, device, test_loader, eps)</span><br><span class="line">    accuracies.append(acc)</span><br><span class="line">    examples.append(ex)</span><br></pre></td></tr></table></figure><pre><code>Epsilon: 0    Test Accuracy = 9810 / 10000 = 0.981Epsilon: 0.05    Test Accuracy = 9426 / 10000 = 0.9426Epsilon: 0.1    Test Accuracy = 8510 / 10000 = 0.851Epsilon: 0.15    Test Accuracy = 6826 / 10000 = 0.6826Epsilon: 0.2    Test Accuracy = 4301 / 10000 = 0.4301Epsilon: 0.25    Test Accuracy = 2082 / 10000 = 0.2082Epsilon: 0.3    Test Accuracy = 869 / 10000 = 0.0869</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Accuracy vs Epsilon</p><p>The first result is the accuracy versus epsilon plot. As alluded to<br>earlier, as epsilon increases we expect the test accuracy to decrease.<br>This is because larger epsilons mean we take a larger step in the<br>direction that will maximize the loss. Notice the trend in the curve is<br>not linear even though the epsilon values are linearly spaced. For<br>example, the accuracy at $\epsilon=0.05$ is only about 4% lower<br>than $\epsilon=0$, but the accuracy at $\epsilon=0.2$ is 25%<br>lower than $\epsilon=0.15$. Also, notice the accuracy of the model<br>hits random accuracy for a 10-class classifier between<br>$\epsilon=0.25$ and $\epsilon=0.3$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(epsilons, accuracies, <span class="string">"*-"</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">1.1</span>, step=<span class="number">0.1</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, <span class="number">.35</span>, step=<span class="number">0.05</span>))</span><br><span class="line">plt.title(<span class="string">"Accuracy vs Epsilon"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epsilon"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_15_0.png" alt="png"></p><p>Sample Adversarial Examples</p><p>Remember the idea of no free lunch? In this case, as epsilon increases<br>the test accuracy decreases <strong>BUT</strong> the perturbations become more easily<br>perceptible. In reality, there is a tradeoff between accuracy<br>degredation and perceptibility that an attacker must consider. Here, we<br>show some examples of successful adversarial examples at each epsilon<br>value. Each row of the plot shows a different epsilon value. The first<br>row is the $\epsilon=0$ examples which represent the original<br>“clean” images with no perturbation. The title of each image shows the<br>“original classification -&gt; adversarial classification.” Notice, the<br>perturbations start to become evident at $\epsilon=0.15$ and are<br>quite evident at $\epsilon=0.3$. However, in all cases humans are<br>still capable of identifying the correct class despite the added noise.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot several examples of adversarial samples at each epsilon</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(epsilons)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(examples[i])):</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.subplot(len(epsilons),len(examples[<span class="number">0</span>]),cnt)</span><br><span class="line">        plt.xticks([], [])</span><br><span class="line">        plt.yticks([], [])</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            plt.ylabel(<span class="string">"Eps: &#123;&#125;"</span>.format(epsilons[i]), fontsize=<span class="number">14</span>)</span><br><span class="line">        orig,adv,ex = examples[i][j]</span><br><span class="line">        plt.title(<span class="string">"&#123;&#125; -&gt; &#123;&#125;"</span>.format(orig, adv))</span><br><span class="line">        plt.imshow(ex, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/output_17_0.png" alt="png"></p><h2 id="Where-to-go-next"><a href="#Where-to-go-next" class="headerlink" title="Where to go next?"></a>Where to go next?</h2><p>Hopefully this tutorial gives some insight into the topic of adversarial<br>machine learning. There are many potential directions to go from here.<br>This attack represents the very beginning of adversarial attack research<br>and since there have been many subsequent ideas for how to attack and<br>defend ML models from an adversary. In fact, at NIPS 2017 there was an<br>adversarial attack and defense competition and many of the methods used<br>in the competition are described in this paper: <code>Adversarial Attacks andDefences Competition &lt;https://arxiv.org/pdf/1804.00097.pdf&gt;</code>__. The work<br>on defense also leads into the idea of making machine learning models<br>more <em>robust</em> in general, to both naturally perturbed and adversarially<br>crafted inputs.</p><p>Another direction to go is adversarial attacks and defense in different<br>domains. Adversarial research is not limited to the image domain, check<br>out <code>this &lt;https://arxiv.org/pdf/1801.01944.pdf&gt;</code>__ attack on<br>speech-to-text models. But perhaps the best way to learn more about<br>adversarial machine learning is to get your hands dirty. Try to<br>implement a different attack from the NIPS 2017 competition, and see how<br>it differs from FGSM. Then, try to defend the model from your own<br>attacks.</p><h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">overlooked:被忽视</span><br><span class="line">robustness:健壮性&#x2F;鲁棒性</span><br><span class="line">vulnerabilities:漏洞</span><br><span class="line">imperceptible:不可察觉的</span><br><span class="line">drastically:剧烈地</span><br><span class="line">via:通过</span><br><span class="line">remarkably:显着地</span><br><span class="line">intuitive:直觉的</span><br><span class="line">tradeoff:交易</span><br><span class="line">perturbations:摄动,扰动</span><br><span class="line">adversarial:对抗的</span><br><span class="line">However, in general the overarching goal is to add the least amount of perturbation to the input data to cause the desired misclassification.:一般来说，总体目标是向输入数据添加最少的扰动量，从而导致所需的错误分类</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Image-对抗样本生成:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Image" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Image-计算机视觉迁移学习</title>
    <link href="http://yoursite.com/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-07-24T06:47:08.000Z</published>
    <updated>2020-07-24T07:09:25.672Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Image-计算机视觉迁移学习:</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Transfer-Learning-for-Computer-Vision-Tutorial"><a href="#Transfer-Learning-for-Computer-Vision-Tutorial" class="headerlink" title="Transfer Learning for Computer Vision Tutorial"></a>Transfer Learning for Computer Vision Tutorial</h1><p><strong>Author</strong>: <code>Sasank Chilamkurthy &lt;https://chsasank.github.io&gt;</code>_</p><p>In this tutorial, you will learn how to train a convolutional neural network for<br>image classification using transfer learning. You can read more about the transfer<br>learning at <code>cs231n notes &lt;https://cs231n.github.io/transfer-learning/&gt;</code>__</p><p>Quoting these notes,</p><pre><code>In practice, very few people train an entire Convolutional Networkfrom scratch (with random initialization), because it is relativelyrare to have a dataset of sufficient size. Instead, it is common topretrain a ConvNet on a very large dataset (e.g. ImageNet, whichcontains 1.2 million images with 1000 categories), and then use theConvNet either as an initialization or a fixed feature extractor forthe task of interest.</code></pre><p>These two major transfer learning scenarios look as follows:</p><ul><li><strong>Finetuning the convnet</strong>: Instead of random initializaion, we<br>initialize the network with a pretrained network, like the one that is<br>trained on imagenet 1000 dataset. Rest of the training looks as<br>usual.</li><li><strong>ConvNet as fixed feature extractor</strong>: Here, we will freeze the weights<br>for all of the network except that of the final fully connected<br>layer. This last fully connected layer is replaced with a new one<br>with random weights and only this layer is trained.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># License: BSD</span></span><br><span class="line"><span class="comment"># Author: Sasank Chilamkurthy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure><h2 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h2><p>We will use torchvision and torch.utils.data packages for loading the<br>data.</p><p>The problem we’re going to solve today is to train a model to classify<br><strong>ants</strong> and <strong>bees</strong>. We have about 120 training images each for ants and bees.<br>There are 75 validation images for each class. Usually, this is a very<br>small dataset to generalize upon, if trained from scratch. Since we<br>are using transfer learning, we should be able to generalize reasonably<br>well.</p><p>This dataset is a very small subset of imagenet.</p><p>.. Note ::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/hymenoptera_data.zip&gt;</code>_<br>   and extract it to the current directory.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data augmentation and normalization for training</span></span><br><span class="line"><span class="comment"># Just normalization for validation</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">'val'</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">'data/hymenoptera_data'</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">                                          data_transforms[x])</span><br><span class="line">                  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">dataset_sizes = &#123;x: len(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">class_names = image_datasets[<span class="string">'train'</span>].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><p>Visualize a few images</p><p>Let’s visualize a few training images so as to understand the data<br>augmentations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch of training data</span></span><br><span class="line">inputs, classes = next(iter(dataloaders[<span class="string">'train'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a grid from batch</span></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=[class_names[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_6_0.png" alt="png"></p><h2 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h2><p>Now, let’s write a general function to train a model. Here, we will<br>illustrate:</p><ul><li>Scheduling the learning rate</li><li>Saving the best model</li></ul><p>In the following, parameter <code>scheduler</code> is an LR scheduler object from<br><code>torch.optim.lr_scheduler</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter gradients</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># statistics</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>Visualizing the model predictions</p><p>Generic function to display predictions for a few images</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span><span class="params">(model, num_images=<span class="number">6</span>)</span>:</span></span><br><span class="line">    was_training = model.training</span><br><span class="line">    model.eval()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(dataloaders[<span class="string">'val'</span>]):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(inputs.size()[<span class="number">0</span>]):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">'off'</span>)</span><br><span class="line">                ax.set_title(<span class="string">'predicted: &#123;&#125;'</span>.format(class_names[preds[j]]))</span><br><span class="line">                imshow(inputs.cpu().data[j])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train(mode=was_training)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train(mode=was_training)</span><br></pre></td></tr></table></figure><h2 id="Finetuning-the-convnet"><a href="#Finetuning-the-convnet" class="headerlink" title="Finetuning the convnet"></a>Finetuning the convnet</h2><p>Load a pretrained model and reset final fully connected layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"><span class="comment"># Here the size of each output sample is set to 2.</span></span><br><span class="line"><span class="comment"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 0/24----------train Loss: 0.5563 Acc: 0.7131val Loss: 0.3475 Acc: 0.8693Epoch 1/24----------train Loss: 0.4089 Acc: 0.8156val Loss: 0.2445 Acc: 0.9085Epoch 2/24----------train Loss: 0.5546 Acc: 0.7910val Loss: 0.2798 Acc: 0.8758Epoch 3/24----------train Loss: 0.4420 Acc: 0.8238val Loss: 0.2220 Acc: 0.9085Epoch 4/24----------train Loss: 0.5138 Acc: 0.8279val Loss: 0.4449 Acc: 0.8497Epoch 5/24----------train Loss: 0.4944 Acc: 0.8320val Loss: 0.2964 Acc: 0.9281Epoch 6/24----------train Loss: 0.6059 Acc: 0.7541val Loss: 0.2792 Acc: 0.8889Epoch 7/24----------train Loss: 0.4492 Acc: 0.8279val Loss: 0.2148 Acc: 0.9085Epoch 8/24----------train Loss: 0.3162 Acc: 0.8730val Loss: 0.2214 Acc: 0.9281Epoch 9/24----------train Loss: 0.2760 Acc: 0.8730val Loss: 0.2317 Acc: 0.9281Epoch 10/24----------train Loss: 0.2800 Acc: 0.8811val Loss: 0.2063 Acc: 0.9216Epoch 11/24----------train Loss: 0.2789 Acc: 0.8975val Loss: 0.2132 Acc: 0.9281Epoch 12/24----------train Loss: 0.2112 Acc: 0.9180val Loss: 0.2114 Acc: 0.9346Epoch 13/24----------train Loss: 0.3116 Acc: 0.8811val Loss: 0.2009 Acc: 0.9346Epoch 14/24----------train Loss: 0.2907 Acc: 0.8975val Loss: 0.1990 Acc: 0.9346Epoch 15/24----------train Loss: 0.2431 Acc: 0.9098val Loss: 0.2149 Acc: 0.9346Epoch 16/24----------train Loss: 0.2203 Acc: 0.9180val Loss: 0.2014 Acc: 0.9346Epoch 17/24----------train Loss: 0.2727 Acc: 0.8689val Loss: 0.1924 Acc: 0.9346Epoch 18/24----------train Loss: 0.2276 Acc: 0.9139val Loss: 0.1987 Acc: 0.9281Epoch 19/24----------train Loss: 0.1850 Acc: 0.9180val Loss: 0.2287 Acc: 0.9346Epoch 20/24----------train Loss: 0.2624 Acc: 0.8893val Loss: 0.2368 Acc: 0.9281Epoch 21/24----------train Loss: 0.2524 Acc: 0.8975val Loss: 0.2231 Acc: 0.9346Epoch 22/24----------train Loss: 0.2732 Acc: 0.8730val Loss: 0.1966 Acc: 0.9346Epoch 23/24----------train Loss: 0.3067 Acc: 0.8811val Loss: 0.1995 Acc: 0.9346Epoch 24/24----------train Loss: 0.2301 Acc: 0.8934val Loss: 0.1958 Acc: 0.9281Training complete in 80m 47sBest val Acc: 0.934641</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_0.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_1.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_2.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_3.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_4.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_14_5.png" alt="png"></p><h2 id="ConvNet-as-fixed-feature-extractor"><a href="#ConvNet-as-fixed-feature-extractor" class="headerlink" title="ConvNet as fixed feature extractor"></a>ConvNet as fixed feature extractor</h2><p>Here, we need to freeze all the network except the final layer. We need<br>to set <code>requires_grad == False</code> to freeze the parameters so that the<br>gradients are not computed in <code>backward()</code>.</p><p>You can read more about this in the documentation<br><code>here &lt;https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward&gt;</code>__.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></span><br><span class="line"><span class="comment"># opposed to before.</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>Train and evaluate</p><p>On CPU this will take about half the time compared to previous scenario.<br>This is expected as gradients don’t need to be computed for most of the<br>network. However, forward does need to be computed.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 0/24----------train Loss: 0.6604 Acc: 0.6516val Loss: 0.4662 Acc: 0.7582Epoch 1/24----------train Loss: 0.4774 Acc: 0.7910val Loss: 0.2465 Acc: 0.9020Epoch 2/24----------train Loss: 0.7743 Acc: 0.7049val Loss: 0.3536 Acc: 0.8431Epoch 3/24----------train Loss: 0.4459 Acc: 0.8033val Loss: 0.1802 Acc: 0.9346Epoch 4/24----------train Loss: 0.8349 Acc: 0.6803val Loss: 0.3540 Acc: 0.8627Epoch 5/24----------train Loss: 0.4686 Acc: 0.8033val Loss: 0.1727 Acc: 0.9608Epoch 6/24----------train Loss: 0.5511 Acc: 0.7623val Loss: 0.3009 Acc: 0.8889Epoch 7/24----------train Loss: 0.3496 Acc: 0.8525val Loss: 0.1738 Acc: 0.9542Epoch 8/24----------train Loss: 0.3696 Acc: 0.8484val Loss: 0.1663 Acc: 0.9542Epoch 9/24----------train Loss: 0.2564 Acc: 0.8770val Loss: 0.1647 Acc: 0.9608Epoch 10/24----------train Loss: 0.3623 Acc: 0.8402val Loss: 0.1873 Acc: 0.9346Epoch 11/24----------train Loss: 0.3846 Acc: 0.8320val Loss: 0.1770 Acc: 0.9477Epoch 12/24----------train Loss: 0.3871 Acc: 0.8238val Loss: 0.1760 Acc: 0.9477Epoch 13/24----------train Loss: 0.3481 Acc: 0.8525val Loss: 0.1711 Acc: 0.9542Epoch 14/24----------train Loss: 0.3504 Acc: 0.8402val Loss: 0.1635 Acc: 0.9477Epoch 15/24----------train Loss: 0.4247 Acc: 0.8279val Loss: 0.1630 Acc: 0.9608Epoch 16/24----------train Loss: 0.3036 Acc: 0.8607val Loss: 0.1695 Acc: 0.9608Epoch 17/24----------train Loss: 0.2761 Acc: 0.8934val Loss: 0.1709 Acc: 0.9608Epoch 18/24----------train Loss: 0.4223 Acc: 0.8238val Loss: 0.1854 Acc: 0.9412Epoch 19/24----------train Loss: 0.3503 Acc: 0.8402val Loss: 0.1845 Acc: 0.9216Epoch 20/24----------train Loss: 0.2934 Acc: 0.8811val Loss: 0.1648 Acc: 0.9412Epoch 21/24----------train Loss: 0.3156 Acc: 0.8402val Loss: 0.1775 Acc: 0.9346Epoch 22/24----------train Loss: 0.4119 Acc: 0.8115val Loss: 0.1744 Acc: 0.9477Epoch 23/24----------train Loss: 0.2424 Acc: 0.8893val Loss: 0.1738 Acc: 0.9412Epoch 24/24----------train Loss: 0.3547 Acc: 0.8361val Loss: 0.1687 Acc: 0.9412Training complete in 46m 23sBest val Acc: 0.960784</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_0.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_1.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_2.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_3.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_4.png" alt="png"></p><p><img src="/2020/07/24/Pytorch-Image-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/output_19_5.png" alt="png"></p><h2 id="Further-Learning"><a href="#Further-Learning" class="headerlink" title="Further Learning"></a>Further Learning</h2><p>If you would like to learn more about the applications of transfer learning,<br>checkout our <code>Quantized Transfer Learning for Computer Vision Tutorial &lt;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&gt;</code>_.</p><h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sufficient:足够的</span><br><span class="line">Finetuning:微调</span><br><span class="line">extractor:提取器</span><br><span class="line">generalize:概括</span><br><span class="line">flip:翻转</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Image-计算机视觉迁移学习:&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Image" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Image-微调TorchVision对象检测</title>
    <link href="http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/</id>
    <published>2020-07-24T06:46:24.000Z</published>
    <updated>2020-07-24T08:44:59.684Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Image-微调TorchVision对象检测:<br><a id="more"></a></p><h1 id="TorchVision-0-3-Object-Detection-finetuning-tutorial"><a href="#TorchVision-0-3-Object-Detection-finetuning-tutorial" class="headerlink" title="TorchVision 0.3 Object Detection finetuning tutorial"></a>TorchVision 0.3 Object Detection finetuning tutorial</h1><p>For this tutorial, we will be finetuning a pre-trained <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a> model in the <a href="https://www.cis.upenn.edu/jshi/ped_html/" target="_blank" rel="noopener"><em>Penn-Fudan Database for Pedestrian Detection and Segmentation</em></a>. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.</p><p>First, we need to install <code>pycocotools</code>. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line">pip install cython</span><br><span class="line"><span class="comment"># Install pycocotools, the version by default in Colab</span></span><br><span class="line"><span class="comment"># has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354</span></span><br><span class="line">pip install -U <span class="string">'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</span></span><br></pre></td></tr></table></figure><pre><code>Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-h3isg2r5  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-h3isg2r5Requirement already satisfied, skipping upgrade: setuptools&gt;=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.1.0)Requirement already satisfied, skipping upgrade: cython&gt;=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)Requirement already satisfied, skipping upgrade: matplotlib&gt;=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.4.7)Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.8.1)Requirement already satisfied, skipping upgrade: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.18.5)Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (0.10.0)Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.2.0)Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.15.0)Building wheels for collected packages: pycocotools  Building wheel for pycocotools (setup.py) ... [?25l[?25hdone  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266460 sha256=3292fbae19c3df30ceb54183f71e1e7288d447743b1dcb8f88257833cf2f23e1  Stored in directory: /tmp/pip-ephem-wheel-cache-y2jf5d3p/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3aSuccessfully built pycocotoolsInstalling collected packages: pycocotools  Found existing installation: pycocotools 2.0.1    Uninstalling pycocotools-2.0.1:      Successfully uninstalled pycocotools-2.0.1Successfully installed pycocotools-2.0</code></pre><h2 id="Defining-the-Dataset"><a href="#Defining-the-Dataset" class="headerlink" title="Defining the Dataset"></a>Defining the Dataset</h2><p>The <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision reference scripts for training object detection, instance segmentation and person keypoint detection</a> allows for easily supporting adding new custom datasets.<br>The dataset should inherit from the standard <code>torch.utils.data.Dataset</code> class, and implement <code>__len__</code> and <code>__getitem__</code>.</p><p>The only specificity that we require is that the dataset <code>__getitem__</code> should return:</p><ul><li>image: a PIL Image of size (H, W)</li><li>target: a dict containing the following fields<ul><li><code>boxes</code> (<code>FloatTensor[N, 4]</code>): the coordinates of the <code>N</code> bounding boxes in <code>[x0, y0, x1, y1]</code> format, ranging from <code>0</code> to <code>W</code> and <code>0</code> to <code>H</code></li><li><code>labels</code> (<code>Int64Tensor[N]</code>): the label for each bounding box</li><li><code>image_id</code> (<code>Int64Tensor[1]</code>): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation</li><li><code>area</code> (<code>Tensor[N]</code>): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.</li><li><code>iscrowd</code> (<code>UInt8Tensor[N]</code>): instances with <code>iscrowd=True</code> will be ignored during evaluation.</li><li>(optionally) <code>masks</code> (<code>UInt8Tensor[N, H, W]</code>): The segmentation masks for each one of the objects</li><li>(optionally) <code>keypoints</code> (<code>FloatTensor[N, K, 3]</code>): For each one of the <code>N</code> objects, it contains the <code>K</code> keypoints in <code>[x, y, visibility]</code> format, defining the object. <code>visibility=0</code> means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt <code>references/detection/transforms.py</code> for your new keypoint representation</li></ul></li></ul><p>If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.</p><p>Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a <code>get_height_and_width</code> method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via <code>__getitem__</code> , which loads the image in memory and is slower than if a custom method is provided.</p><h3 id="Writing-a-custom-dataset-for-Penn-Fudan"><a href="#Writing-a-custom-dataset-for-Penn-Fudan" class="headerlink" title="Writing a custom dataset for Penn-Fudan"></a>Writing a custom dataset for Penn-Fudan</h3><p>Let’s write a dataset for the Penn-Fudan dataset.</p><p>First, let’s download and extract the data, present in a zip file at <a href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip" target="_blank" rel="noopener">https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the Penn-Fudan dataset</span></span><br><span class="line">wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .</span><br><span class="line"><span class="comment"># extract it in the current folder</span></span><br><span class="line">unzip PennFudanPed.zip</span><br></pre></td></tr></table></figure><pre><code>--2020-07-23 13:46:08--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zipResolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::dConnecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 53723336 (51M) [application/zip]Saving to: ‘PennFudanPed.zip’PennFudanPed.zip    100%[===================&gt;]  51.23M  1009KB/s    in 48s     2020-07-23 13:46:58 (1.06 MB/s) - ‘PennFudanPed.zip’ saved [53723336/53723336]--2020-07-23 13:46:58--  http://./Resolving . (.)... failed: No address associated with hostname.wget: unable to resolve host address ‘.’FINISHED --2020-07-23 13:46:58--Total wall clock time: 50sDownloaded: 1 files, 51M in 48s (1.06 MB/s)Archive:  PennFudanPed.zip   creating: PennFudanPed/  inflating: PennFudanPed/added-object-list.txt     creating: PennFudanPed/Annotation/  inflating: PennFudanPed/Annotation/FudanPed00001.txt    inflating: PennFudanPed/Annotation/FudanPed00002.txt    inflating: PennFudanPed/Annotation/FudanPed00003.txt    inflating: PennFudanPed/Annotation/FudanPed00004.txt    inflating: PennFudanPed/Annotation/FudanPed00005.txt    inflating: PennFudanPed/Annotation/FudanPed00006.txt    inflating: PennFudanPed/Annotation/FudanPed00007.txt    inflating: PennFudanPed/Annotation/FudanPed00008.txt    inflating: PennFudanPed/Annotation/FudanPed00009.txt    inflating: PennFudanPed/Annotation/FudanPed00010.txt    inflating: PennFudanPed/Annotation/FudanPed00011.txt    inflating: PennFudanPed/Annotation/FudanPed00012.txt    inflating: PennFudanPed/Annotation/FudanPed00013.txt    inflating: PennFudanPed/Annotation/FudanPed00014.txt    inflating: PennFudanPed/Annotation/FudanPed00015.txt    inflating: PennFudanPed/Annotation/FudanPed00016.txt    inflating: PennFudanPed/Annotation/FudanPed00017.txt    inflating: PennFudanPed/Annotation/FudanPed00018.txt    inflating: PennFudanPed/Annotation/FudanPed00019.txt    inflating: PennFudanPed/Annotation/FudanPed00020.txt    inflating: PennFudanPed/Annotation/FudanPed00021.txt    inflating: PennFudanPed/Annotation/FudanPed00022.txt    inflating: PennFudanPed/Annotation/FudanPed00023.txt    inflating: PennFudanPed/Annotation/FudanPed00024.txt    inflating: PennFudanPed/Annotation/FudanPed00025.txt    inflating: PennFudanPed/Annotation/FudanPed00026.txt    inflating: PennFudanPed/Annotation/FudanPed00027.txt    inflating: PennFudanPed/Annotation/FudanPed00028.txt    inflating: PennFudanPed/Annotation/FudanPed00029.txt    inflating: PennFudanPed/Annotation/FudanPed00030.txt    inflating: PennFudanPed/Annotation/FudanPed00031.txt    inflating: PennFudanPed/Annotation/FudanPed00032.txt    inflating: PennFudanPed/Annotation/FudanPed00033.txt    inflating: PennFudanPed/Annotation/FudanPed00034.txt    inflating: PennFudanPed/Annotation/FudanPed00035.txt    inflating: PennFudanPed/Annotation/FudanPed00036.txt    inflating: PennFudanPed/Annotation/FudanPed00037.txt    inflating: PennFudanPed/Annotation/FudanPed00038.txt    inflating: PennFudanPed/Annotation/FudanPed00039.txt    inflating: PennFudanPed/Annotation/FudanPed00040.txt    inflating: PennFudanPed/Annotation/FudanPed00041.txt    inflating: PennFudanPed/Annotation/FudanPed00042.txt    inflating: PennFudanPed/Annotation/FudanPed00043.txt    inflating: PennFudanPed/Annotation/FudanPed00044.txt    inflating: PennFudanPed/Annotation/FudanPed00045.txt    inflating: PennFudanPed/Annotation/FudanPed00046.txt    inflating: PennFudanPed/Annotation/FudanPed00047.txt    inflating: PennFudanPed/Annotation/FudanPed00048.txt    inflating: PennFudanPed/Annotation/FudanPed00049.txt    inflating: PennFudanPed/Annotation/FudanPed00050.txt    inflating: PennFudanPed/Annotation/FudanPed00051.txt    inflating: PennFudanPed/Annotation/FudanPed00052.txt    inflating: PennFudanPed/Annotation/FudanPed00053.txt    inflating: PennFudanPed/Annotation/FudanPed00054.txt    inflating: PennFudanPed/Annotation/FudanPed00055.txt    inflating: PennFudanPed/Annotation/FudanPed00056.txt    inflating: PennFudanPed/Annotation/FudanPed00057.txt    inflating: PennFudanPed/Annotation/FudanPed00058.txt    inflating: PennFudanPed/Annotation/FudanPed00059.txt    inflating: PennFudanPed/Annotation/FudanPed00060.txt    inflating: PennFudanPed/Annotation/FudanPed00061.txt    inflating: PennFudanPed/Annotation/FudanPed00062.txt    inflating: PennFudanPed/Annotation/FudanPed00063.txt    inflating: PennFudanPed/Annotation/FudanPed00064.txt    inflating: PennFudanPed/Annotation/FudanPed00065.txt    inflating: PennFudanPed/Annotation/FudanPed00066.txt    inflating: PennFudanPed/Annotation/FudanPed00067.txt    inflating: PennFudanPed/Annotation/FudanPed00068.txt    inflating: PennFudanPed/Annotation/FudanPed00069.txt    inflating: PennFudanPed/Annotation/FudanPed00070.txt    inflating: PennFudanPed/Annotation/FudanPed00071.txt    inflating: PennFudanPed/Annotation/FudanPed00072.txt    inflating: PennFudanPed/Annotation/FudanPed00073.txt    inflating: PennFudanPed/Annotation/FudanPed00074.txt    inflating: PennFudanPed/Annotation/PennPed00001.txt    inflating: PennFudanPed/Annotation/PennPed00002.txt    inflating: PennFudanPed/Annotation/PennPed00003.txt    inflating: PennFudanPed/Annotation/PennPed00004.txt    inflating: PennFudanPed/Annotation/PennPed00005.txt    inflating: PennFudanPed/Annotation/PennPed00006.txt    inflating: PennFudanPed/Annotation/PennPed00007.txt    inflating: PennFudanPed/Annotation/PennPed00008.txt    inflating: PennFudanPed/Annotation/PennPed00009.txt    inflating: PennFudanPed/Annotation/PennPed00010.txt    inflating: PennFudanPed/Annotation/PennPed00011.txt    inflating: PennFudanPed/Annotation/PennPed00012.txt    inflating: PennFudanPed/Annotation/PennPed00013.txt    inflating: PennFudanPed/Annotation/PennPed00014.txt    inflating: PennFudanPed/Annotation/PennPed00015.txt    inflating: PennFudanPed/Annotation/PennPed00016.txt    inflating: PennFudanPed/Annotation/PennPed00017.txt    inflating: PennFudanPed/Annotation/PennPed00018.txt    inflating: PennFudanPed/Annotation/PennPed00019.txt    inflating: PennFudanPed/Annotation/PennPed00020.txt    inflating: PennFudanPed/Annotation/PennPed00021.txt    inflating: PennFudanPed/Annotation/PennPed00022.txt    inflating: PennFudanPed/Annotation/PennPed00023.txt    inflating: PennFudanPed/Annotation/PennPed00024.txt    inflating: PennFudanPed/Annotation/PennPed00025.txt    inflating: PennFudanPed/Annotation/PennPed00026.txt    inflating: PennFudanPed/Annotation/PennPed00027.txt    inflating: PennFudanPed/Annotation/PennPed00028.txt    inflating: PennFudanPed/Annotation/PennPed00029.txt    inflating: PennFudanPed/Annotation/PennPed00030.txt    inflating: PennFudanPed/Annotation/PennPed00031.txt    inflating: PennFudanPed/Annotation/PennPed00032.txt    inflating: PennFudanPed/Annotation/PennPed00033.txt    inflating: PennFudanPed/Annotation/PennPed00034.txt    inflating: PennFudanPed/Annotation/PennPed00035.txt    inflating: PennFudanPed/Annotation/PennPed00036.txt    inflating: PennFudanPed/Annotation/PennPed00037.txt    inflating: PennFudanPed/Annotation/PennPed00038.txt    inflating: PennFudanPed/Annotation/PennPed00039.txt    inflating: PennFudanPed/Annotation/PennPed00040.txt    inflating: PennFudanPed/Annotation/PennPed00041.txt    inflating: PennFudanPed/Annotation/PennPed00042.txt    inflating: PennFudanPed/Annotation/PennPed00043.txt    inflating: PennFudanPed/Annotation/PennPed00044.txt    inflating: PennFudanPed/Annotation/PennPed00045.txt    inflating: PennFudanPed/Annotation/PennPed00046.txt    inflating: PennFudanPed/Annotation/PennPed00047.txt    inflating: PennFudanPed/Annotation/PennPed00048.txt    inflating: PennFudanPed/Annotation/PennPed00049.txt    inflating: PennFudanPed/Annotation/PennPed00050.txt    inflating: PennFudanPed/Annotation/PennPed00051.txt    inflating: PennFudanPed/Annotation/PennPed00052.txt    inflating: PennFudanPed/Annotation/PennPed00053.txt    inflating: PennFudanPed/Annotation/PennPed00054.txt    inflating: PennFudanPed/Annotation/PennPed00055.txt    inflating: PennFudanPed/Annotation/PennPed00056.txt    inflating: PennFudanPed/Annotation/PennPed00057.txt    inflating: PennFudanPed/Annotation/PennPed00058.txt    inflating: PennFudanPed/Annotation/PennPed00059.txt    inflating: PennFudanPed/Annotation/PennPed00060.txt    inflating: PennFudanPed/Annotation/PennPed00061.txt    inflating: PennFudanPed/Annotation/PennPed00062.txt    inflating: PennFudanPed/Annotation/PennPed00063.txt    inflating: PennFudanPed/Annotation/PennPed00064.txt    inflating: PennFudanPed/Annotation/PennPed00065.txt    inflating: PennFudanPed/Annotation/PennPed00066.txt    inflating: PennFudanPed/Annotation/PennPed00067.txt    inflating: PennFudanPed/Annotation/PennPed00068.txt    inflating: PennFudanPed/Annotation/PennPed00069.txt    inflating: PennFudanPed/Annotation/PennPed00070.txt    inflating: PennFudanPed/Annotation/PennPed00071.txt    inflating: PennFudanPed/Annotation/PennPed00072.txt    inflating: PennFudanPed/Annotation/PennPed00073.txt    inflating: PennFudanPed/Annotation/PennPed00074.txt    inflating: PennFudanPed/Annotation/PennPed00075.txt    inflating: PennFudanPed/Annotation/PennPed00076.txt    inflating: PennFudanPed/Annotation/PennPed00077.txt    inflating: PennFudanPed/Annotation/PennPed00078.txt    inflating: PennFudanPed/Annotation/PennPed00079.txt    inflating: PennFudanPed/Annotation/PennPed00080.txt    inflating: PennFudanPed/Annotation/PennPed00081.txt    inflating: PennFudanPed/Annotation/PennPed00082.txt    inflating: PennFudanPed/Annotation/PennPed00083.txt    inflating: PennFudanPed/Annotation/PennPed00084.txt    inflating: PennFudanPed/Annotation/PennPed00085.txt    inflating: PennFudanPed/Annotation/PennPed00086.txt    inflating: PennFudanPed/Annotation/PennPed00087.txt    inflating: PennFudanPed/Annotation/PennPed00088.txt    inflating: PennFudanPed/Annotation/PennPed00089.txt    inflating: PennFudanPed/Annotation/PennPed00090.txt    inflating: PennFudanPed/Annotation/PennPed00091.txt    inflating: PennFudanPed/Annotation/PennPed00092.txt    inflating: PennFudanPed/Annotation/PennPed00093.txt    inflating: PennFudanPed/Annotation/PennPed00094.txt    inflating: PennFudanPed/Annotation/PennPed00095.txt    inflating: PennFudanPed/Annotation/PennPed00096.txt     creating: PennFudanPed/PedMasks/  inflating: PennFudanPed/PedMasks/FudanPed00001_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00002_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00003_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00004_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00005_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00006_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00007_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00008_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00009_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00010_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00011_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00012_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00013_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00014_mask.png   extracting: PennFudanPed/PedMasks/FudanPed00015_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00016_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00017_mask.png   extracting: PennFudanPed/PedMasks/FudanPed00018_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00019_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00020_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00021_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00022_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00023_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00024_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00025_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00026_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00027_mask.png   extracting: PennFudanPed/PedMasks/FudanPed00028_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00029_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00030_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00031_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00032_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00033_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00034_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00035_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00036_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00037_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00038_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00039_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00040_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00041_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00042_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00043_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00044_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00045_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00046_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00047_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00048_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00049_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00050_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00051_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00052_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00053_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00054_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00055_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00056_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00057_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00058_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00059_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00060_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00061_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00062_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00063_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00064_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00065_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00066_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00067_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00068_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00069_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00070_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00071_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00072_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00073_mask.png    inflating: PennFudanPed/PedMasks/FudanPed00074_mask.png    inflating: PennFudanPed/PedMasks/PennPed00001_mask.png    inflating: PennFudanPed/PedMasks/PennPed00002_mask.png    inflating: PennFudanPed/PedMasks/PennPed00003_mask.png    inflating: PennFudanPed/PedMasks/PennPed00004_mask.png    inflating: PennFudanPed/PedMasks/PennPed00005_mask.png    inflating: PennFudanPed/PedMasks/PennPed00006_mask.png    inflating: PennFudanPed/PedMasks/PennPed00007_mask.png    inflating: PennFudanPed/PedMasks/PennPed00008_mask.png    inflating: PennFudanPed/PedMasks/PennPed00009_mask.png    inflating: PennFudanPed/PedMasks/PennPed00010_mask.png    inflating: PennFudanPed/PedMasks/PennPed00011_mask.png    inflating: PennFudanPed/PedMasks/PennPed00012_mask.png    inflating: PennFudanPed/PedMasks/PennPed00013_mask.png    inflating: PennFudanPed/PedMasks/PennPed00014_mask.png    inflating: PennFudanPed/PedMasks/PennPed00015_mask.png    inflating: PennFudanPed/PedMasks/PennPed00016_mask.png    inflating: PennFudanPed/PedMasks/PennPed00017_mask.png    inflating: PennFudanPed/PedMasks/PennPed00018_mask.png    inflating: PennFudanPed/PedMasks/PennPed00019_mask.png    inflating: PennFudanPed/PedMasks/PennPed00020_mask.png    inflating: PennFudanPed/PedMasks/PennPed00021_mask.png    inflating: PennFudanPed/PedMasks/PennPed00022_mask.png    inflating: PennFudanPed/PedMasks/PennPed00023_mask.png    inflating: PennFudanPed/PedMasks/PennPed00024_mask.png    inflating: PennFudanPed/PedMasks/PennPed00025_mask.png    inflating: PennFudanPed/PedMasks/PennPed00026_mask.png    inflating: PennFudanPed/PedMasks/PennPed00027_mask.png    inflating: PennFudanPed/PedMasks/PennPed00028_mask.png    inflating: PennFudanPed/PedMasks/PennPed00029_mask.png    inflating: PennFudanPed/PedMasks/PennPed00030_mask.png    inflating: PennFudanPed/PedMasks/PennPed00031_mask.png    inflating: PennFudanPed/PedMasks/PennPed00032_mask.png    inflating: PennFudanPed/PedMasks/PennPed00033_mask.png    inflating: PennFudanPed/PedMasks/PennPed00034_mask.png    inflating: PennFudanPed/PedMasks/PennPed00035_mask.png    inflating: PennFudanPed/PedMasks/PennPed00036_mask.png    inflating: PennFudanPed/PedMasks/PennPed00037_mask.png    inflating: PennFudanPed/PedMasks/PennPed00038_mask.png    inflating: PennFudanPed/PedMasks/PennPed00039_mask.png    inflating: PennFudanPed/PedMasks/PennPed00040_mask.png    inflating: PennFudanPed/PedMasks/PennPed00041_mask.png    inflating: PennFudanPed/PedMasks/PennPed00042_mask.png    inflating: PennFudanPed/PedMasks/PennPed00043_mask.png    inflating: PennFudanPed/PedMasks/PennPed00044_mask.png    inflating: PennFudanPed/PedMasks/PennPed00045_mask.png    inflating: PennFudanPed/PedMasks/PennPed00046_mask.png    inflating: PennFudanPed/PedMasks/PennPed00047_mask.png    inflating: PennFudanPed/PedMasks/PennPed00048_mask.png    inflating: PennFudanPed/PedMasks/PennPed00049_mask.png    inflating: PennFudanPed/PedMasks/PennPed00050_mask.png    inflating: PennFudanPed/PedMasks/PennPed00051_mask.png    inflating: PennFudanPed/PedMasks/PennPed00052_mask.png   extracting: PennFudanPed/PedMasks/PennPed00053_mask.png    inflating: PennFudanPed/PedMasks/PennPed00054_mask.png    inflating: PennFudanPed/PedMasks/PennPed00055_mask.png    inflating: PennFudanPed/PedMasks/PennPed00056_mask.png    inflating: PennFudanPed/PedMasks/PennPed00057_mask.png    inflating: PennFudanPed/PedMasks/PennPed00058_mask.png    inflating: PennFudanPed/PedMasks/PennPed00059_mask.png    inflating: PennFudanPed/PedMasks/PennPed00060_mask.png    inflating: PennFudanPed/PedMasks/PennPed00061_mask.png   extracting: PennFudanPed/PedMasks/PennPed00062_mask.png    inflating: PennFudanPed/PedMasks/PennPed00063_mask.png    inflating: PennFudanPed/PedMasks/PennPed00064_mask.png    inflating: PennFudanPed/PedMasks/PennPed00065_mask.png   extracting: PennFudanPed/PedMasks/PennPed00066_mask.png    inflating: PennFudanPed/PedMasks/PennPed00067_mask.png    inflating: PennFudanPed/PedMasks/PennPed00068_mask.png   extracting: PennFudanPed/PedMasks/PennPed00069_mask.png   extracting: PennFudanPed/PedMasks/PennPed00070_mask.png    inflating: PennFudanPed/PedMasks/PennPed00071_mask.png   extracting: PennFudanPed/PedMasks/PennPed00072_mask.png    inflating: PennFudanPed/PedMasks/PennPed00073_mask.png   extracting: PennFudanPed/PedMasks/PennPed00074_mask.png    inflating: PennFudanPed/PedMasks/PennPed00075_mask.png    inflating: PennFudanPed/PedMasks/PennPed00076_mask.png    inflating: PennFudanPed/PedMasks/PennPed00077_mask.png    inflating: PennFudanPed/PedMasks/PennPed00078_mask.png    inflating: PennFudanPed/PedMasks/PennPed00079_mask.png    inflating: PennFudanPed/PedMasks/PennPed00080_mask.png    inflating: PennFudanPed/PedMasks/PennPed00081_mask.png    inflating: PennFudanPed/PedMasks/PennPed00082_mask.png   extracting: PennFudanPed/PedMasks/PennPed00083_mask.png   extracting: PennFudanPed/PedMasks/PennPed00084_mask.png    inflating: PennFudanPed/PedMasks/PennPed00085_mask.png   extracting: PennFudanPed/PedMasks/PennPed00086_mask.png    inflating: PennFudanPed/PedMasks/PennPed00087_mask.png    inflating: PennFudanPed/PedMasks/PennPed00088_mask.png   extracting: PennFudanPed/PedMasks/PennPed00089_mask.png   extracting: PennFudanPed/PedMasks/PennPed00090_mask.png    inflating: PennFudanPed/PedMasks/PennPed00091_mask.png    inflating: PennFudanPed/PedMasks/PennPed00092_mask.png    inflating: PennFudanPed/PedMasks/PennPed00093_mask.png    inflating: PennFudanPed/PedMasks/PennPed00094_mask.png    inflating: PennFudanPed/PedMasks/PennPed00095_mask.png   extracting: PennFudanPed/PedMasks/PennPed00096_mask.png     creating: PennFudanPed/PNGImages/  inflating: PennFudanPed/PNGImages/FudanPed00001.png    inflating: PennFudanPed/PNGImages/FudanPed00002.png    inflating: PennFudanPed/PNGImages/FudanPed00003.png    inflating: PennFudanPed/PNGImages/FudanPed00004.png    inflating: PennFudanPed/PNGImages/FudanPed00005.png    inflating: PennFudanPed/PNGImages/FudanPed00006.png    inflating: PennFudanPed/PNGImages/FudanPed00007.png    inflating: PennFudanPed/PNGImages/FudanPed00008.png    inflating: PennFudanPed/PNGImages/FudanPed00009.png    inflating: PennFudanPed/PNGImages/FudanPed00010.png    inflating: PennFudanPed/PNGImages/FudanPed00011.png    inflating: PennFudanPed/PNGImages/FudanPed00012.png    inflating: PennFudanPed/PNGImages/FudanPed00013.png    inflating: PennFudanPed/PNGImages/FudanPed00014.png    inflating: PennFudanPed/PNGImages/FudanPed00015.png    inflating: PennFudanPed/PNGImages/FudanPed00016.png    inflating: PennFudanPed/PNGImages/FudanPed00017.png    inflating: PennFudanPed/PNGImages/FudanPed00018.png    inflating: PennFudanPed/PNGImages/FudanPed00019.png    inflating: PennFudanPed/PNGImages/FudanPed00020.png    inflating: PennFudanPed/PNGImages/FudanPed00021.png    inflating: PennFudanPed/PNGImages/FudanPed00022.png    inflating: PennFudanPed/PNGImages/FudanPed00023.png    inflating: PennFudanPed/PNGImages/FudanPed00024.png    inflating: PennFudanPed/PNGImages/FudanPed00025.png    inflating: PennFudanPed/PNGImages/FudanPed00026.png    inflating: PennFudanPed/PNGImages/FudanPed00027.png    inflating: PennFudanPed/PNGImages/FudanPed00028.png    inflating: PennFudanPed/PNGImages/FudanPed00029.png    inflating: PennFudanPed/PNGImages/FudanPed00030.png    inflating: PennFudanPed/PNGImages/FudanPed00031.png    inflating: PennFudanPed/PNGImages/FudanPed00032.png    inflating: PennFudanPed/PNGImages/FudanPed00033.png    inflating: PennFudanPed/PNGImages/FudanPed00034.png    inflating: PennFudanPed/PNGImages/FudanPed00035.png    inflating: PennFudanPed/PNGImages/FudanPed00036.png    inflating: PennFudanPed/PNGImages/FudanPed00037.png    inflating: PennFudanPed/PNGImages/FudanPed00038.png    inflating: PennFudanPed/PNGImages/FudanPed00039.png    inflating: PennFudanPed/PNGImages/FudanPed00040.png    inflating: PennFudanPed/PNGImages/FudanPed00041.png    inflating: PennFudanPed/PNGImages/FudanPed00042.png    inflating: PennFudanPed/PNGImages/FudanPed00043.png    inflating: PennFudanPed/PNGImages/FudanPed00044.png    inflating: PennFudanPed/PNGImages/FudanPed00045.png    inflating: PennFudanPed/PNGImages/FudanPed00046.png    inflating: PennFudanPed/PNGImages/FudanPed00047.png    inflating: PennFudanPed/PNGImages/FudanPed00048.png    inflating: PennFudanPed/PNGImages/FudanPed00049.png    inflating: PennFudanPed/PNGImages/FudanPed00050.png    inflating: PennFudanPed/PNGImages/FudanPed00051.png    inflating: PennFudanPed/PNGImages/FudanPed00052.png    inflating: PennFudanPed/PNGImages/FudanPed00053.png    inflating: PennFudanPed/PNGImages/FudanPed00054.png    inflating: PennFudanPed/PNGImages/FudanPed00055.png    inflating: PennFudanPed/PNGImages/FudanPed00056.png    inflating: PennFudanPed/PNGImages/FudanPed00057.png    inflating: PennFudanPed/PNGImages/FudanPed00058.png    inflating: PennFudanPed/PNGImages/FudanPed00059.png    inflating: PennFudanPed/PNGImages/FudanPed00060.png    inflating: PennFudanPed/PNGImages/FudanPed00061.png    inflating: PennFudanPed/PNGImages/FudanPed00062.png    inflating: PennFudanPed/PNGImages/FudanPed00063.png    inflating: PennFudanPed/PNGImages/FudanPed00064.png    inflating: PennFudanPed/PNGImages/FudanPed00065.png    inflating: PennFudanPed/PNGImages/FudanPed00066.png    inflating: PennFudanPed/PNGImages/FudanPed00067.png    inflating: PennFudanPed/PNGImages/FudanPed00068.png    inflating: PennFudanPed/PNGImages/FudanPed00069.png    inflating: PennFudanPed/PNGImages/FudanPed00070.png    inflating: PennFudanPed/PNGImages/FudanPed00071.png    inflating: PennFudanPed/PNGImages/FudanPed00072.png    inflating: PennFudanPed/PNGImages/FudanPed00073.png    inflating: PennFudanPed/PNGImages/FudanPed00074.png    inflating: PennFudanPed/PNGImages/PennPed00001.png    inflating: PennFudanPed/PNGImages/PennPed00002.png    inflating: PennFudanPed/PNGImages/PennPed00003.png    inflating: PennFudanPed/PNGImages/PennPed00004.png    inflating: PennFudanPed/PNGImages/PennPed00005.png    inflating: PennFudanPed/PNGImages/PennPed00006.png    inflating: PennFudanPed/PNGImages/PennPed00007.png    inflating: PennFudanPed/PNGImages/PennPed00008.png    inflating: PennFudanPed/PNGImages/PennPed00009.png    inflating: PennFudanPed/PNGImages/PennPed00010.png    inflating: PennFudanPed/PNGImages/PennPed00011.png    inflating: PennFudanPed/PNGImages/PennPed00012.png    inflating: PennFudanPed/PNGImages/PennPed00013.png    inflating: PennFudanPed/PNGImages/PennPed00014.png    inflating: PennFudanPed/PNGImages/PennPed00015.png    inflating: PennFudanPed/PNGImages/PennPed00016.png    inflating: PennFudanPed/PNGImages/PennPed00017.png    inflating: PennFudanPed/PNGImages/PennPed00018.png    inflating: PennFudanPed/PNGImages/PennPed00019.png    inflating: PennFudanPed/PNGImages/PennPed00020.png    inflating: PennFudanPed/PNGImages/PennPed00021.png    inflating: PennFudanPed/PNGImages/PennPed00022.png    inflating: PennFudanPed/PNGImages/PennPed00023.png    inflating: PennFudanPed/PNGImages/PennPed00024.png    inflating: PennFudanPed/PNGImages/PennPed00025.png    inflating: PennFudanPed/PNGImages/PennPed00026.png    inflating: PennFudanPed/PNGImages/PennPed00027.png    inflating: PennFudanPed/PNGImages/PennPed00028.png    inflating: PennFudanPed/PNGImages/PennPed00029.png    inflating: PennFudanPed/PNGImages/PennPed00030.png    inflating: PennFudanPed/PNGImages/PennPed00031.png    inflating: PennFudanPed/PNGImages/PennPed00032.png    inflating: PennFudanPed/PNGImages/PennPed00033.png    inflating: PennFudanPed/PNGImages/PennPed00034.png    inflating: PennFudanPed/PNGImages/PennPed00035.png    inflating: PennFudanPed/PNGImages/PennPed00036.png    inflating: PennFudanPed/PNGImages/PennPed00037.png    inflating: PennFudanPed/PNGImages/PennPed00038.png    inflating: PennFudanPed/PNGImages/PennPed00039.png    inflating: PennFudanPed/PNGImages/PennPed00040.png    inflating: PennFudanPed/PNGImages/PennPed00041.png    inflating: PennFudanPed/PNGImages/PennPed00042.png    inflating: PennFudanPed/PNGImages/PennPed00043.png    inflating: PennFudanPed/PNGImages/PennPed00044.png    inflating: PennFudanPed/PNGImages/PennPed00045.png    inflating: PennFudanPed/PNGImages/PennPed00046.png    inflating: PennFudanPed/PNGImages/PennPed00047.png    inflating: PennFudanPed/PNGImages/PennPed00048.png    inflating: PennFudanPed/PNGImages/PennPed00049.png    inflating: PennFudanPed/PNGImages/PennPed00050.png    inflating: PennFudanPed/PNGImages/PennPed00051.png    inflating: PennFudanPed/PNGImages/PennPed00052.png    inflating: PennFudanPed/PNGImages/PennPed00053.png    inflating: PennFudanPed/PNGImages/PennPed00054.png    inflating: PennFudanPed/PNGImages/PennPed00055.png    inflating: PennFudanPed/PNGImages/PennPed00056.png    inflating: PennFudanPed/PNGImages/PennPed00057.png    inflating: PennFudanPed/PNGImages/PennPed00058.png    inflating: PennFudanPed/PNGImages/PennPed00059.png    inflating: PennFudanPed/PNGImages/PennPed00060.png    inflating: PennFudanPed/PNGImages/PennPed00061.png    inflating: PennFudanPed/PNGImages/PennPed00062.png    inflating: PennFudanPed/PNGImages/PennPed00063.png    inflating: PennFudanPed/PNGImages/PennPed00064.png    inflating: PennFudanPed/PNGImages/PennPed00065.png    inflating: PennFudanPed/PNGImages/PennPed00066.png    inflating: PennFudanPed/PNGImages/PennPed00067.png    inflating: PennFudanPed/PNGImages/PennPed00068.png    inflating: PennFudanPed/PNGImages/PennPed00069.png    inflating: PennFudanPed/PNGImages/PennPed00070.png    inflating: PennFudanPed/PNGImages/PennPed00071.png    inflating: PennFudanPed/PNGImages/PennPed00072.png    inflating: PennFudanPed/PNGImages/PennPed00073.png    inflating: PennFudanPed/PNGImages/PennPed00074.png    inflating: PennFudanPed/PNGImages/PennPed00075.png    inflating: PennFudanPed/PNGImages/PennPed00076.png    inflating: PennFudanPed/PNGImages/PennPed00077.png    inflating: PennFudanPed/PNGImages/PennPed00078.png    inflating: PennFudanPed/PNGImages/PennPed00079.png    inflating: PennFudanPed/PNGImages/PennPed00080.png    inflating: PennFudanPed/PNGImages/PennPed00081.png    inflating: PennFudanPed/PNGImages/PennPed00082.png    inflating: PennFudanPed/PNGImages/PennPed00083.png    inflating: PennFudanPed/PNGImages/PennPed00084.png    inflating: PennFudanPed/PNGImages/PennPed00085.png    inflating: PennFudanPed/PNGImages/PennPed00086.png    inflating: PennFudanPed/PNGImages/PennPed00087.png    inflating: PennFudanPed/PNGImages/PennPed00088.png    inflating: PennFudanPed/PNGImages/PennPed00089.png    inflating: PennFudanPed/PNGImages/PennPed00090.png    inflating: PennFudanPed/PNGImages/PennPed00091.png    inflating: PennFudanPed/PNGImages/PennPed00092.png    inflating: PennFudanPed/PNGImages/PennPed00093.png    inflating: PennFudanPed/PNGImages/PennPed00094.png    inflating: PennFudanPed/PNGImages/PennPed00095.png    inflating: PennFudanPed/PNGImages/PennPed00096.png    inflating: PennFudanPed/readme.txt  </code></pre><p>Let’s have a look at the dataset and how it is layed down.</p><p>The data is structured as follows<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PennFudanPed&#x2F;</span><br><span class="line">  PedMasks&#x2F;</span><br><span class="line">    FudanPed00001_mask.png</span><br><span class="line">    FudanPed00002_mask.png</span><br><span class="line">    FudanPed00003_mask.png</span><br><span class="line">    FudanPed00004_mask.png</span><br><span class="line">    ...</span><br><span class="line">  PNGImages&#x2F;</span><br><span class="line">    FudanPed00001.png</span><br><span class="line">    FudanPed00002.png</span><br><span class="line">    FudanPed00003.png</span><br><span class="line">    FudanPed00004.png</span><br></pre></td></tr></table></figure></p><p>Here is one example of an image in the dataset, with its corresponding instance segmentation mask</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">Image.open(<span class="string">'PennFudanPed/PNGImages/FudanPed00001.png'</span>)</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_6_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mask = Image.open(<span class="string">'PennFudanPed/PedMasks/FudanPed00001_mask.png'</span>)</span><br><span class="line"><span class="comment"># each mask instance has a different color, from zero to N, where</span></span><br><span class="line"><span class="comment"># N is the number of instances. In order to make visualization easier,</span></span><br><span class="line"><span class="comment"># let's adda color palette to the mask.</span></span><br><span class="line">mask.putpalette([</span><br><span class="line">    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># black background</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment"># index 1 is red</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>, <span class="comment"># index 2 is yellow</span></span><br><span class="line">    <span class="number">255</span>, <span class="number">153</span>, <span class="number">0</span>, <span class="comment"># index 3 is orange</span></span><br><span class="line">])</span><br><span class="line">mask</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_7_0.png" alt="png"></p><p>So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a <code>torch.utils.data.Dataset</code> class for this dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PennFudanDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms=None)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        <span class="comment"># load all image files, sorting them to</span></span><br><span class="line">        <span class="comment"># ensure that they are aligned</span></span><br><span class="line">        self.imgs = list(sorted(os.listdir(os.path.join(root, <span class="string">"PNGImages"</span>))))</span><br><span class="line">        self.masks = list(sorted(os.listdir(os.path.join(root, <span class="string">"PedMasks"</span>))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="comment"># load images ad masks</span></span><br><span class="line">        img_path = os.path.join(self.root, <span class="string">"PNGImages"</span>, self.imgs[idx])</span><br><span class="line">        mask_path = os.path.join(self.root, <span class="string">"PedMasks"</span>, self.masks[idx])</span><br><span class="line">        img = Image.open(img_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        <span class="comment"># note that we haven't converted the mask to RGB,</span></span><br><span class="line">        <span class="comment"># because each color corresponds to a different instance</span></span><br><span class="line">        <span class="comment"># with 0 being background</span></span><br><span class="line">        mask = Image.open(mask_path)</span><br><span class="line"></span><br><span class="line">        mask = np.array(mask)</span><br><span class="line">        <span class="comment"># instances are encoded as different colors</span></span><br><span class="line">        obj_ids = np.unique(mask)</span><br><span class="line">        <span class="comment"># first id is the background, so remove it</span></span><br><span class="line">        obj_ids = obj_ids[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split the color-encoded mask into a set</span></span><br><span class="line">        <span class="comment"># of binary masks</span></span><br><span class="line">        masks = mask == obj_ids[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get bounding box coordinates for each mask</span></span><br><span class="line">        num_objs = len(obj_ids)</span><br><span class="line">        boxes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_objs):</span><br><span class="line">            pos = np.where(masks[i])</span><br><span class="line">            xmin = np.min(pos[<span class="number">1</span>])</span><br><span class="line">            xmax = np.max(pos[<span class="number">1</span>])</span><br><span class="line">            ymin = np.min(pos[<span class="number">0</span>])</span><br><span class="line">            ymax = np.max(pos[<span class="number">0</span>])</span><br><span class="line">            boxes.append([xmin, ymin, xmax, ymax])</span><br><span class="line"></span><br><span class="line">        boxes = torch.as_tensor(boxes, dtype=torch.float32)</span><br><span class="line">        <span class="comment"># there is only one class</span></span><br><span class="line">        labels = torch.ones((num_objs,), dtype=torch.int64)</span><br><span class="line">        masks = torch.as_tensor(masks, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        image_id = torch.tensor([idx])</span><br><span class="line">        area = (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]) * (boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>])</span><br><span class="line">        <span class="comment"># suppose all instances are not crowd</span></span><br><span class="line">        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">        target = &#123;&#125;</span><br><span class="line">        target[<span class="string">"boxes"</span>] = boxes</span><br><span class="line">        target[<span class="string">"labels"</span>] = labels</span><br><span class="line">        target[<span class="string">"masks"</span>] = masks</span><br><span class="line">        target[<span class="string">"image_id"</span>] = image_id</span><br><span class="line">        target[<span class="string">"area"</span>] = area</span><br><span class="line">        target[<span class="string">"iscrowd"</span>] = iscrowd</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img, target = self.transforms(img, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure><p>That’s all for the dataset. Let’s see how the outputs are structured for this dataset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed/'</span>)</span><br><span class="line">dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>(&lt;PIL.Image.Image image mode=RGB size=559x536 at 0x7FEBFE8767F0&gt;, {&#39;area&#39;: tensor([35358., 36225.]), &#39;boxes&#39;: tensor([[159., 181., 301., 430.],          [419., 170., 534., 485.]]), &#39;image_id&#39;: tensor([0]), &#39;iscrowd&#39;: tensor([0, 0]), &#39;labels&#39;: tensor([1, 1]), &#39;masks&#39;: tensor([[[0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           ...,           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0]],          [[0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           ...,           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})</code></pre><p>So we can see that by default, the dataset returns a <code>PIL.Image</code> and a dictionary<br>containing several fields, including <code>boxes</code>, <code>labels</code> and <code>masks</code>.</p><h2 id="Defining-your-model"><a href="#Defining-your-model" class="headerlink" title="Defining your model"></a>Defining your model</h2><p>In this tutorial, we will be using <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>, which is based on top of <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.</p><p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image03.png" alt="Faster R-CNN"></p><p>Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.</p><p><img src="https://yiyibooks.cn/__trs__/yiyibooks/pytorch_131/_static/img/tv_tutorial/tv_image04.png" alt="Mask R-CNN"></p><p>There are two common situations where one might want to modify one of the available models in torchvision modelzoo.<br>The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).</p><p>Let’s go see how we would do one or another in the following sections.</p><h3 id="1-Finetuning-from-a-pretrained-model"><a href="#1-Finetuning-from-a-pretrained-model" class="headerlink" title="1 - Finetuning from a pretrained model"></a>1 - Finetuning from a pretrained model</h3><p>Let’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection.faster_rcnn import FastRCNNPredictor</span><br><span class="line"></span><br><span class="line"># load a model pre-trained pre-trained on COCO</span><br><span class="line">model &#x3D; torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained&#x3D;True)</span><br><span class="line"></span><br><span class="line"># replace the classifier with a new one, that has</span><br><span class="line"># num_classes which is user-defined</span><br><span class="line">num_classes &#x3D; 2  # 1 class (person) + background</span><br><span class="line"># get number of input features for the classifier</span><br><span class="line">in_features &#x3D; model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line"># replace the pre-trained head with a new one</span><br><span class="line">model.roi_heads.box_predictor &#x3D; FastRCNNPredictor(in_features, num_classes)</span><br></pre></td></tr></table></figure></p><h3 id="2-Modifying-the-model-to-add-a-different-backbone"><a href="#2-Modifying-the-model-to-add-a-different-backbone" class="headerlink" title="2 - Modifying the model to add a different backbone"></a>2 - Modifying the model to add a different backbone</h3><p>Another common situation arises when the user wants to replace the backbone of a detection<br>model with a different one. For example, the current default backbone (ResNet-50) might be too big for some applications, and smaller models might be necessary.</p><p>Here is how we would go into leveraging the functions provided by torchvision to modify a backbone.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection import FasterRCNN</span><br><span class="line">from torchvision.models.detection.rpn import AnchorGenerator</span><br><span class="line"></span><br><span class="line"># load a pre-trained model for classification and return</span><br><span class="line"># only the features</span><br><span class="line">backbone &#x3D; torchvision.models.mobilenet_v2(pretrained&#x3D;True).features</span><br><span class="line"># FasterRCNN needs to know the number of</span><br><span class="line"># output channels in a backbone. For mobilenet_v2, it&#39;s 1280</span><br><span class="line"># so we need to add it here</span><br><span class="line">backbone.out_channels &#x3D; 1280</span><br><span class="line"></span><br><span class="line"># let&#39;s make the RPN generate 5 x 3 anchors per spatial</span><br><span class="line"># location, with 5 different sizes and 3 different aspect</span><br><span class="line"># ratios. We have a Tuple[Tuple[int]] because each feature</span><br><span class="line"># map could potentially have different sizes and</span><br><span class="line"># aspect ratios </span><br><span class="line">anchor_generator &#x3D; AnchorGenerator(sizes&#x3D;((32, 64, 128, 256, 512),),</span><br><span class="line">                                   aspect_ratios&#x3D;((0.5, 1.0, 2.0),))</span><br><span class="line"></span><br><span class="line"># let&#39;s define what are the feature maps that we will</span><br><span class="line"># use to perform the region of interest cropping, as well as</span><br><span class="line"># the size of the crop after rescaling.</span><br><span class="line"># if your backbone returns a Tensor, featmap_names is expected to</span><br><span class="line"># be [0]. More generally, the backbone should return an</span><br><span class="line"># OrderedDict[Tensor], and in featmap_names you can choose which</span><br><span class="line"># feature maps to use.</span><br><span class="line">roi_pooler &#x3D; torchvision.ops.MultiScaleRoIAlign(featmap_names&#x3D;[0],</span><br><span class="line">                                                output_size&#x3D;7,</span><br><span class="line">                                                sampling_ratio&#x3D;2)</span><br><span class="line"></span><br><span class="line"># put the pieces together inside a FasterRCNN model</span><br><span class="line">model &#x3D; FasterRCNN(backbone,</span><br><span class="line">                   num_classes&#x3D;2,</span><br><span class="line">                   rpn_anchor_generator&#x3D;anchor_generator,</span><br><span class="line">                   box_roi_pool&#x3D;roi_pooler)</span><br></pre></td></tr></table></figure><h3 id="An-Instance-segmentation-model-for-PennFudan-Dataset"><a href="#An-Instance-segmentation-model-for-PennFudan-Dataset" class="headerlink" title="An Instance segmentation model for PennFudan Dataset"></a>An Instance segmentation model for PennFudan Dataset</h3><p>In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.</p><p>Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> FastRCNNPredictor</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.mask_rcnn <span class="keyword">import</span> MaskRCNNPredictor</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_instance_segmentation_model</span><span class="params">(num_classes)</span>:</span></span><br><span class="line">    <span class="comment"># load an instance segmentation model pre-trained on COCO</span></span><br><span class="line">    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the number of input features for the classifier</span></span><br><span class="line">    in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    <span class="comment"># replace the pre-trained head with a new one</span></span><br><span class="line">    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now get the number of input features for the mask classifier</span></span><br><span class="line">    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels</span><br><span class="line">    hidden_layer = <span class="number">256</span></span><br><span class="line">    <span class="comment"># and replace the mask predictor with a new one</span></span><br><span class="line">    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,</span><br><span class="line">                                                       hidden_layer,</span><br><span class="line">                                                       num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>That’s it, this will make model be ready to be trained and evaluated on our custom dataset.</p><h2 id="Training-and-evaluation-functions"><a href="#Training-and-evaluation-functions" class="headerlink" title="Training and evaluation functions"></a>Training and evaluation functions</h2><p>In <code>references/detection/,</code> we have a number of helper functions to simplify training and evaluating detection models.<br>Here, we will use <code>references/detection/engine.py</code>, <code>references/detection/utils.py</code> and <code>references/detection/transforms.py</code>.</p><p>Let’s copy those files (and their dependencies) in here so that they are available in the notebook</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%%shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download TorchVision repo to use some files from</span></span><br><span class="line"><span class="comment"># references/detection</span></span><br><span class="line">git clone https://github.com/pytorch/vision.git</span><br><span class="line">cd vision</span><br><span class="line">git checkout v0<span class="number">.3</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">cp references/detection/utils.py ../</span><br><span class="line">cp references/detection/transforms.py ../</span><br><span class="line">cp references/detection/coco_eval.py ../</span><br><span class="line">cp references/detection/engine.py ../</span><br><span class="line">cp references/detection/coco_utils.py ../</span><br></pre></td></tr></table></figure><pre><code>Cloning into &#39;vision&#39;...remote: Enumerating objects: 20, done.[Kremote: Counting objects: 100% (20/20), done.[Kremote: Compressing objects: 100% (20/20), done.[Kremote: Total 9278 (delta 7), reused 3 (delta 0), pack-reused 9258[KReceiving objects: 100% (9278/9278), 11.24 MiB | 9.51 MiB/s, done.Resolving deltas: 100% (6426/6426), done.Note: checking out &#39;v0.3.0&#39;.You are in &#39;detached HEAD&#39; state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by performing another checkout.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -b with the checkout command again. Example:  git checkout -b &lt;new-branch-name&gt;HEAD is now at be37608 version check against PyTorch&#39;s CUDA version</code></pre><p>Let’s write some helper functions for data augmentation / transformation, which leverages the functions in <code>refereces/detection</code> that we have just copied:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> engine <span class="keyword">import</span> train_one_epoch, evaluate</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(train)</span>:</span></span><br><span class="line">    transforms = []</span><br><span class="line">    <span class="comment"># converts the image, a PIL image, into a PyTorch Tensor</span></span><br><span class="line">    transforms.append(T.ToTensor())</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        <span class="comment"># during training, randomly flip the training images</span></span><br><span class="line">        <span class="comment"># and ground-truth for data augmentation</span></span><br><span class="line">        transforms.append(T.RandomHorizontalFlip(<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> T.Compose(transforms)</span><br></pre></td></tr></table></figure><h4 id="Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model"><a href="#Note-that-we-do-not-need-to-add-a-mean-std-normalization-nor-image-rescaling-in-the-data-transforms-as-those-are-handled-internally-by-the-Mask-R-CNN-model" class="headerlink" title="Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."></a>Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.</h4><h3 id="Putting-everything-together"><a href="#Putting-everything-together" class="headerlink" title="Putting everything together"></a>Putting everything together</h3><p>We now have the dataset class, the models and the data transforms. Let’s instantiate them</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use our dataset and defined transformations</span></span><br><span class="line">dataset = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">True</span>))</span><br><span class="line">dataset_test = PennFudanDataset(<span class="string">'PennFudanPed'</span>, get_transform(train=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split the dataset in train and test set</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">indices = torch.randperm(len(dataset)).tolist()</span><br><span class="line">dataset = torch.utils.data.Subset(dataset, indices[:<span class="number">-50</span>])</span><br><span class="line">dataset_test = torch.utils.data.Subset(dataset_test, indices[<span class="number">-50</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># define training and validation data loaders</span></span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br><span class="line"></span><br><span class="line">data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">    dataset_test, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>,</span><br><span class="line">    collate_fn=utils.collate_fn)</span><br></pre></td></tr></table></figure><p>Now let’s instantiate the model and the optimizer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># our dataset has two classes only - background and person</span></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get the model using our helper function</span></span><br><span class="line">model = get_instance_segmentation_model(num_classes)</span><br><span class="line"><span class="comment"># move model to the right device</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct an optimizer</span></span><br><span class="line">params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=<span class="number">0.005</span>,</span><br><span class="line">                            momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and a learning rate scheduler which decreases the learning rate by</span></span><br><span class="line"><span class="comment"># 10x every 3 epochs</span></span><br><span class="line">lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,</span><br><span class="line">                                               step_size=<span class="number">3</span>,</span><br><span class="line">                                               gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><pre><code>Downloading: &quot;https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth&quot; to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pthHBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value=&#39;&#39;)))</code></pre><p>And now let’s train the model for 10 epochs, evaluating at the end of every epoch.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># let's train it for 10 epochs</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># train for one epoch, printing every 10 iterations</span></span><br><span class="line">    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># update the learning rate</span></span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    <span class="comment"># evaluate on the test dataset</span></span><br><span class="line">    evaluate(model, data_loader_test, device=device)</span><br></pre></td></tr></table></figure><pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details.   warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:    nonzero(Tensor input, *, Tensor out)Consider using one of the following signatures instead:    nonzero(Tensor input, *, bool as_tuple)Epoch: [0]  [ 0/60]  eta: 0:02:18  lr: 0.000090  loss: 3.5827 (3.5827)  loss_classifier: 0.7385 (0.7385)  loss_box_reg: 0.1523 (0.1523)  loss_mask: 2.6620 (2.6620)  loss_objectness: 0.0224 (0.0224)  loss_rpn_box_reg: 0.0076 (0.0076)  time: 2.3152  data: 0.2933  max mem: 2303Epoch: [0]  [10/60]  eta: 0:01:14  lr: 0.000936  loss: 1.5605 (2.1212)  loss_classifier: 0.4479 (0.4976)  loss_box_reg: 0.1826 (0.1906)  loss_mask: 0.9259 (1.4017)  loss_objectness: 0.0224 (0.0208)  loss_rpn_box_reg: 0.0090 (0.0105)  time: 1.4865  data: 0.0356  max mem: 2860Epoch: [0]  [20/60]  eta: 0:00:57  lr: 0.001783  loss: 0.8700 (1.4312)  loss_classifier: 0.2338 (0.3409)  loss_box_reg: 0.1579 (0.1731)  loss_mask: 0.4010 (0.8836)  loss_objectness: 0.0191 (0.0216)  loss_rpn_box_reg: 0.0099 (0.0120)  time: 1.3888  data: 0.0096  max mem: 2861Epoch: [0]  [30/60]  eta: 0:00:43  lr: 0.002629  loss: 0.5382 (1.1211)  loss_classifier: 0.0968 (0.2569)  loss_box_reg: 0.1155 (0.1598)  loss_mask: 0.2489 (0.6751)  loss_objectness: 0.0105 (0.0176)  loss_rpn_box_reg: 0.0099 (0.0117)  time: 1.4144  data: 0.0095  max mem: 3596Epoch: [0]  [40/60]  eta: 0:00:28  lr: 0.003476  loss: 0.4041 (0.9495)  loss_classifier: 0.0690 (0.2099)  loss_box_reg: 0.1090 (0.1521)  loss_mask: 0.2121 (0.5609)  loss_objectness: 0.0038 (0.0142)  loss_rpn_box_reg: 0.0118 (0.0124)  time: 1.4593  data: 0.0098  max mem: 3596Epoch: [0]  [50/60]  eta: 0:00:14  lr: 0.004323  loss: 0.3387 (0.8263)  loss_classifier: 0.0496 (0.1785)  loss_box_reg: 0.0833 (0.1393)  loss_mask: 0.1797 (0.4837)  loss_objectness: 0.0035 (0.0122)  loss_rpn_box_reg: 0.0118 (0.0128)  time: 1.4368  data: 0.0101  max mem: 3596Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.2567 (0.7367)  loss_classifier: 0.0392 (0.1566)  loss_box_reg: 0.0545 (0.1240)  loss_mask: 0.1464 (0.4332)  loss_objectness: 0.0020 (0.0106)  loss_rpn_box_reg: 0.0109 (0.0122)  time: 1.4374  data: 0.0101  max mem: 3596Epoch: [0] Total time: 0:01:26 (1.4425 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3444 (0.3444)  evaluator_time: 0.0059 (0.0059)  time: 0.4881  data: 0.1360  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)  time: 0.3262  data: 0.0053  max mem: 3596Test: Total time: 0:00:16 (0.3309 s / it)Averaged stats: model_time: 0.3135 (0.3116)  evaluator_time: 0.0048 (0.0088)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.698 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.901 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.756 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.759IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.700 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.886 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.716 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.741 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.745 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.751Epoch: [1]  [ 0/60]  eta: 0:01:39  lr: 0.005000  loss: 0.1716 (0.1716)  loss_classifier: 0.0231 (0.0231)  loss_box_reg: 0.0309 (0.0309)  loss_mask: 0.1041 (0.1041)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0124 (0.0124)  time: 1.6632  data: 0.3040  max mem: 3596Epoch: [1]  [10/60]  eta: 0:01:14  lr: 0.005000  loss: 0.2137 (0.2460)  loss_classifier: 0.0314 (0.0385)  loss_box_reg: 0.0309 (0.0406)  loss_mask: 0.1438 (0.1540)  loss_objectness: 0.0011 (0.0017)  loss_rpn_box_reg: 0.0113 (0.0111)  time: 1.4996  data: 0.0356  max mem: 3596Epoch: [1]  [20/60]  eta: 0:00:59  lr: 0.005000  loss: 0.2565 (0.2636)  loss_classifier: 0.0484 (0.0464)  loss_box_reg: 0.0338 (0.0442)  loss_mask: 0.1639 (0.1582)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0123 (0.0131)  time: 1.4682  data: 0.0102  max mem: 3596Epoch: [1]  [30/60]  eta: 0:00:43  lr: 0.005000  loss: 0.2174 (0.2409)  loss_classifier: 0.0349 (0.0410)  loss_box_reg: 0.0266 (0.0365)  loss_mask: 0.1426 (0.1502)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0080 (0.0115)  time: 1.4462  data: 0.0105  max mem: 3596Epoch: [1]  [40/60]  eta: 0:00:29  lr: 0.005000  loss: 0.1930 (0.2327)  loss_classifier: 0.0274 (0.0406)  loss_box_reg: 0.0189 (0.0334)  loss_mask: 0.1380 (0.1463)  loss_objectness: 0.0007 (0.0015)  loss_rpn_box_reg: 0.0075 (0.0109)  time: 1.4625  data: 0.0096  max mem: 3596Epoch: [1]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.2011 (0.2291)  loss_classifier: 0.0344 (0.0409)  loss_box_reg: 0.0253 (0.0325)  loss_mask: 0.1287 (0.1427)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0079 (0.0115)  time: 1.5020  data: 0.0099  max mem: 3596Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1680 (0.2220)  loss_classifier: 0.0294 (0.0398)  loss_box_reg: 0.0148 (0.0302)  loss_mask: 0.1265 (0.1394)  loss_objectness: 0.0011 (0.0016)  loss_rpn_box_reg: 0.0075 (0.0110)  time: 1.4470  data: 0.0098  max mem: 3596Epoch: [1] Total time: 0:01:27 (1.4662 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3606 (0.3606)  evaluator_time: 0.0046 (0.0046)  time: 0.5041  data: 0.1374  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)  time: 0.3234  data: 0.0053  max mem: 3596Test: Total time: 0:00:16 (0.3253 s / it)Averaged stats: model_time: 0.3171 (0.3077)  evaluator_time: 0.0046 (0.0070)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.772 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.357 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.821 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.747 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.987 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.891 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.789 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.789 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794Epoch: [2]  [ 0/60]  eta: 0:01:38  lr: 0.005000  loss: 0.1239 (0.1239)  loss_classifier: 0.0123 (0.0123)  loss_box_reg: 0.0068 (0.0068)  loss_mask: 0.0971 (0.0971)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.6336  data: 0.2465  max mem: 3596Epoch: [2]  [10/60]  eta: 0:01:12  lr: 0.005000  loss: 0.1866 (0.1751)  loss_classifier: 0.0283 (0.0308)  loss_box_reg: 0.0135 (0.0162)  loss_mask: 0.1129 (0.1189)  loss_objectness: 0.0007 (0.0011)  loss_rpn_box_reg: 0.0072 (0.0082)  time: 1.4478  data: 0.0310  max mem: 3596Epoch: [2]  [20/60]  eta: 0:00:55  lr: 0.005000  loss: 0.1433 (0.1623)  loss_classifier: 0.0203 (0.0253)  loss_box_reg: 0.0094 (0.0129)  loss_mask: 0.1074 (0.1162)  loss_objectness: 0.0003 (0.0008)  loss_rpn_box_reg: 0.0046 (0.0071)  time: 1.3800  data: 0.0095  max mem: 3596Epoch: [2]  [30/60]  eta: 0:00:42  lr: 0.005000  loss: 0.1621 (0.1821)  loss_classifier: 0.0218 (0.0294)  loss_box_reg: 0.0101 (0.0170)  loss_mask: 0.1160 (0.1257)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0077 (0.0088)  time: 1.4109  data: 0.0095  max mem: 3596Epoch: [2]  [40/60]  eta: 0:00:28  lr: 0.005000  loss: 0.1841 (0.1834)  loss_classifier: 0.0286 (0.0291)  loss_box_reg: 0.0157 (0.0164)  loss_mask: 0.1288 (0.1278)  loss_objectness: 0.0005 (0.0012)  loss_rpn_box_reg: 0.0081 (0.0088)  time: 1.4780  data: 0.0099  max mem: 3596Epoch: [2]  [50/60]  eta: 0:00:14  lr: 0.005000  loss: 0.1970 (0.1878)  loss_classifier: 0.0279 (0.0295)  loss_box_reg: 0.0173 (0.0175)  loss_mask: 0.1317 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0083 (0.0092)  time: 1.4749  data: 0.0099  max mem: 3596Epoch: [2]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.1872 (0.1894)  loss_classifier: 0.0279 (0.0307)  loss_box_reg: 0.0173 (0.0177)  loss_mask: 0.1296 (0.1301)  loss_objectness: 0.0008 (0.0015)  loss_rpn_box_reg: 0.0094 (0.0095)  time: 1.5513  data: 0.0099  max mem: 3596Epoch: [2] Total time: 0:01:28 (1.4738 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:21  model_time: 0.3020 (0.3020)  evaluator_time: 0.0047 (0.0047)  time: 0.4358  data: 0.1272  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)  time: 0.3183  data: 0.0053  max mem: 3596Test: Total time: 0:00:16 (0.3203 s / it)Averaged stats: model_time: 0.3124 (0.3039)  evaluator_time: 0.0037 (0.0061)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.932 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.476 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.821 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.921 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.760 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.345 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.788 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.650 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798Epoch: [3]  [ 0/60]  eta: 0:01:55  lr: 0.000500  loss: 0.1690 (0.1690)  loss_classifier: 0.0193 (0.0193)  loss_box_reg: 0.0098 (0.0098)  loss_mask: 0.1339 (0.1339)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0058 (0.0058)  time: 1.9331  data: 0.4201  max mem: 3596Epoch: [3]  [10/60]  eta: 0:01:19  lr: 0.000500  loss: 0.1668 (0.1790)  loss_classifier: 0.0294 (0.0273)  loss_box_reg: 0.0102 (0.0148)  loss_mask: 0.1203 (0.1283)  loss_objectness: 0.0005 (0.0010)  loss_rpn_box_reg: 0.0058 (0.0076)  time: 1.5992  data: 0.0464  max mem: 3596Epoch: [3]  [20/60]  eta: 0:01:01  lr: 0.000500  loss: 0.1635 (0.1723)  loss_classifier: 0.0225 (0.0257)  loss_box_reg: 0.0088 (0.0133)  loss_mask: 0.1203 (0.1243)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0061 (0.0079)  time: 1.5232  data: 0.0096  max mem: 3596Epoch: [3]  [30/60]  eta: 0:00:44  lr: 0.000500  loss: 0.1603 (0.1683)  loss_classifier: 0.0212 (0.0251)  loss_box_reg: 0.0083 (0.0121)  loss_mask: 0.1198 (0.1228)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0060 (0.0073)  time: 1.4131  data: 0.0097  max mem: 3596Epoch: [3]  [40/60]  eta: 0:00:29  lr: 0.000500  loss: 0.1603 (0.1725)  loss_classifier: 0.0266 (0.0268)  loss_box_reg: 0.0093 (0.0127)  loss_mask: 0.1150 (0.1239)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0069 (0.0082)  time: 1.4049  data: 0.0097  max mem: 3596Epoch: [3]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1715 (0.1754)  loss_classifier: 0.0266 (0.0267)  loss_box_reg: 0.0109 (0.0134)  loss_mask: 0.1232 (0.1261)  loss_objectness: 0.0005 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0083)  time: 1.4872  data: 0.0099  max mem: 3596Epoch: [3]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1509 (0.1709)  loss_classifier: 0.0256 (0.0263)  loss_box_reg: 0.0093 (0.0126)  loss_mask: 0.1055 (0.1231)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0076 (0.0081)  time: 1.4687  data: 0.0096  max mem: 3596Epoch: [3] Total time: 0:01:28 (1.4791 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3690 (0.3690)  evaluator_time: 0.0046 (0.0046)  time: 0.5078  data: 0.1324  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)  time: 0.3199  data: 0.0051  max mem: 3596Test: Total time: 0:00:16 (0.3224 s / it)Averaged stats: model_time: 0.3145 (0.3060)  evaluator_time: 0.0038 (0.0060)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.509 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.377 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.869IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.755 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.917 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.765 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.801 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.805Epoch: [4]  [ 0/60]  eta: 0:01:27  lr: 0.000500  loss: 0.1045 (0.1045)  loss_classifier: 0.0070 (0.0070)  loss_box_reg: 0.0029 (0.0029)  loss_mask: 0.0902 (0.0902)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0043 (0.0043)  time: 1.4538  data: 0.2039  max mem: 3596Epoch: [4]  [10/60]  eta: 0:01:12  lr: 0.000500  loss: 0.1510 (0.1583)  loss_classifier: 0.0209 (0.0197)  loss_box_reg: 0.0101 (0.0126)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0007)  loss_rpn_box_reg: 0.0071 (0.0075)  time: 1.4482  data: 0.0278  max mem: 3596Epoch: [4]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1510 (0.1582)  loss_classifier: 0.0209 (0.0215)  loss_box_reg: 0.0073 (0.0110)  loss_mask: 0.1107 (0.1178)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0060 (0.0066)  time: 1.3827  data: 0.0100  max mem: 3596Epoch: [4]  [30/60]  eta: 0:00:41  lr: 0.000500  loss: 0.1427 (0.1624)  loss_classifier: 0.0227 (0.0243)  loss_box_reg: 0.0077 (0.0117)  loss_mask: 0.1005 (0.1176)  loss_objectness: 0.0005 (0.0014)  loss_rpn_box_reg: 0.0058 (0.0074)  time: 1.3627  data: 0.0097  max mem: 3596Epoch: [4]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1472 (0.1611)  loss_classifier: 0.0255 (0.0253)  loss_box_reg: 0.0085 (0.0114)  loss_mask: 0.1079 (0.1161)  loss_objectness: 0.0004 (0.0013)  loss_rpn_box_reg: 0.0063 (0.0071)  time: 1.4537  data: 0.0094  max mem: 3596Epoch: [4]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1548 (0.1612)  loss_classifier: 0.0250 (0.0249)  loss_box_reg: 0.0079 (0.0113)  loss_mask: 0.1106 (0.1161)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0068 (0.0077)  time: 1.4913  data: 0.0094  max mem: 3596Epoch: [4]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1548 (0.1647)  loss_classifier: 0.0250 (0.0256)  loss_box_reg: 0.0079 (0.0121)  loss_mask: 0.1106 (0.1179)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0079 (0.0080)  time: 1.4724  data: 0.0097  max mem: 3596Epoch: [4] Total time: 0:01:26 (1.4357 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3692 (0.3692)  evaluator_time: 0.0045 (0.0045)  time: 0.5054  data: 0.1301  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)  time: 0.3202  data: 0.0050  max mem: 3596Test: Total time: 0:00:16 (0.3220 s / it)Averaged stats: model_time: 0.3175 (0.3059)  evaluator_time: 0.0036 (0.0061)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.813 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.944 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.520 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.825 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.861 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.868IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.920 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.809 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815Epoch: [5]  [ 0/60]  eta: 0:02:13  lr: 0.000500  loss: 0.1545 (0.1545)  loss_classifier: 0.0223 (0.0223)  loss_box_reg: 0.0059 (0.0059)  loss_mask: 0.1200 (0.1200)  loss_objectness: 0.0004 (0.0004)  loss_rpn_box_reg: 0.0059 (0.0059)  time: 2.2323  data: 0.5519  max mem: 3596Epoch: [5]  [10/60]  eta: 0:01:10  lr: 0.000500  loss: 0.1409 (0.1489)  loss_classifier: 0.0178 (0.0211)  loss_box_reg: 0.0076 (0.0094)  loss_mask: 0.1140 (0.1118)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0057 (0.0061)  time: 1.4098  data: 0.0540  max mem: 3596Epoch: [5]  [20/60]  eta: 0:00:55  lr: 0.000500  loss: 0.1379 (0.1454)  loss_classifier: 0.0189 (0.0208)  loss_box_reg: 0.0076 (0.0088)  loss_mask: 0.1032 (0.1091)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0054 (0.0061)  time: 1.3437  data: 0.0070  max mem: 3596Epoch: [5]  [30/60]  eta: 0:00:42  lr: 0.000500  loss: 0.1430 (0.1597)  loss_classifier: 0.0240 (0.0246)  loss_box_reg: 0.0092 (0.0114)  loss_mask: 0.1032 (0.1154)  loss_objectness: 0.0005 (0.0008)  loss_rpn_box_reg: 0.0069 (0.0075)  time: 1.4147  data: 0.0100  max mem: 3596Epoch: [5]  [40/60]  eta: 0:00:28  lr: 0.000500  loss: 0.1503 (0.1609)  loss_classifier: 0.0242 (0.0243)  loss_box_reg: 0.0102 (0.0117)  loss_mask: 0.1148 (0.1163)  loss_objectness: 0.0004 (0.0008)  loss_rpn_box_reg: 0.0083 (0.0078)  time: 1.4826  data: 0.0101  max mem: 3596Epoch: [5]  [50/60]  eta: 0:00:14  lr: 0.000500  loss: 0.1397 (0.1571)  loss_classifier: 0.0198 (0.0237)  loss_box_reg: 0.0075 (0.0107)  loss_mask: 0.1017 (0.1144)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0066 (0.0075)  time: 1.4890  data: 0.0096  max mem: 3596Epoch: [5]  [59/60]  eta: 0:00:01  lr: 0.000500  loss: 0.1422 (0.1581)  loss_classifier: 0.0197 (0.0241)  loss_box_reg: 0.0066 (0.0107)  loss_mask: 0.1042 (0.1149)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0064 (0.0076)  time: 1.5030  data: 0.0094  max mem: 3596Epoch: [5] Total time: 0:01:27 (1.4584 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3761 (0.3761)  evaluator_time: 0.0041 (0.0041)  time: 0.5475  data: 0.1655  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)  time: 0.3195  data: 0.0049  max mem: 3596Test: Total time: 0:00:16 (0.3235 s / it)Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0040 (0.0060)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.865 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814Epoch: [6]  [ 0/60]  eta: 0:01:53  lr: 0.000050  loss: 0.1645 (0.1645)  loss_classifier: 0.0255 (0.0255)  loss_box_reg: 0.0121 (0.0121)  loss_mask: 0.1195 (0.1195)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.8885  data: 0.2840  max mem: 3596Epoch: [6]  [10/60]  eta: 0:01:17  lr: 0.000050  loss: 0.1414 (0.1454)  loss_classifier: 0.0206 (0.0221)  loss_box_reg: 0.0056 (0.0076)  loss_mask: 0.1039 (0.1099)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0058 (0.0053)  time: 1.5460  data: 0.0336  max mem: 3596Epoch: [6]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1414 (0.1516)  loss_classifier: 0.0206 (0.0241)  loss_box_reg: 0.0065 (0.0101)  loss_mask: 0.1030 (0.1104)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0059 (0.0066)  time: 1.5057  data: 0.0092  max mem: 3596Epoch: [6]  [30/60]  eta: 0:00:45  lr: 0.000050  loss: 0.1479 (0.1531)  loss_classifier: 0.0255 (0.0261)  loss_box_reg: 0.0087 (0.0099)  loss_mask: 0.1030 (0.1098)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0072 (0.0068)  time: 1.4797  data: 0.0104  max mem: 3596Epoch: [6]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1493 (0.1593)  loss_classifier: 0.0255 (0.0267)  loss_box_reg: 0.0087 (0.0111)  loss_mask: 0.1043 (0.1137)  loss_objectness: 0.0005 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0072)  time: 1.4498  data: 0.0104  max mem: 3596Epoch: [6]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1440 (0.1584)  loss_classifier: 0.0234 (0.0261)  loss_box_reg: 0.0088 (0.0110)  loss_mask: 0.1129 (0.1135)  loss_objectness: 0.0004 (0.0006)  loss_rpn_box_reg: 0.0069 (0.0073)  time: 1.4308  data: 0.0097  max mem: 3596Epoch: [6]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1440 (0.1588)  loss_classifier: 0.0216 (0.0260)  loss_box_reg: 0.0080 (0.0110)  loss_mask: 0.1118 (0.1140)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0070 (0.0073)  time: 1.4508  data: 0.0095  max mem: 3596Epoch: [6] Total time: 0:01:28 (1.4739 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3615 (0.3615)  evaluator_time: 0.0038 (0.0038)  time: 0.5174  data: 0.1505  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)  time: 0.3199  data: 0.0057  max mem: 3596Test: Total time: 0:00:16 (0.3225 s / it)Averaged stats: model_time: 0.3161 (0.3058)  evaluator_time: 0.0038 (0.0059)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.351 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.738 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814Epoch: [7]  [ 0/60]  eta: 0:01:47  lr: 0.000050  loss: 0.1122 (0.1122)  loss_classifier: 0.0151 (0.0151)  loss_box_reg: 0.0039 (0.0039)  loss_mask: 0.0920 (0.0920)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.7859  data: 0.4771  max mem: 3596Epoch: [7]  [10/60]  eta: 0:01:11  lr: 0.000050  loss: 0.1316 (0.1467)  loss_classifier: 0.0157 (0.0221)  loss_box_reg: 0.0052 (0.0086)  loss_mask: 0.1004 (0.1097)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0037 (0.0053)  time: 1.4340  data: 0.0504  max mem: 3596Epoch: [7]  [20/60]  eta: 0:01:00  lr: 0.000050  loss: 0.1570 (0.1557)  loss_classifier: 0.0288 (0.0274)  loss_box_reg: 0.0085 (0.0100)  loss_mask: 0.1075 (0.1104)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0066 (0.0068)  time: 1.4943  data: 0.0092  max mem: 3596Epoch: [7]  [30/60]  eta: 0:00:44  lr: 0.000050  loss: 0.1447 (0.1519)  loss_classifier: 0.0257 (0.0255)  loss_box_reg: 0.0076 (0.0093)  loss_mask: 0.1062 (0.1092)  loss_objectness: 0.0003 (0.0010)  loss_rpn_box_reg: 0.0071 (0.0068)  time: 1.4985  data: 0.0104  max mem: 3596Epoch: [7]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1418 (0.1546)  loss_classifier: 0.0222 (0.0251)  loss_box_reg: 0.0068 (0.0098)  loss_mask: 0.1095 (0.1120)  loss_objectness: 0.0002 (0.0008)  loss_rpn_box_reg: 0.0054 (0.0069)  time: 1.4120  data: 0.0099  max mem: 3596Epoch: [7]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1616 (0.1590)  loss_classifier: 0.0232 (0.0253)  loss_box_reg: 0.0082 (0.0107)  loss_mask: 0.1132 (0.1150)  loss_objectness: 0.0002 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0072)  time: 1.4197  data: 0.0096  max mem: 3596Epoch: [7]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1474 (0.1592)  loss_classifier: 0.0230 (0.0256)  loss_box_reg: 0.0060 (0.0106)  loss_mask: 0.1101 (0.1150)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0057 (0.0072)  time: 1.4280  data: 0.0095  max mem: 3596Epoch: [7] Total time: 0:01:27 (1.4505 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3761 (0.3761)  evaluator_time: 0.0044 (0.0044)  time: 0.5139  data: 0.1315  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)  time: 0.3181  data: 0.0050  max mem: 3596Test: Total time: 0:00:16 (0.3223 s / it)Averaged stats: model_time: 0.3125 (0.3060)  evaluator_time: 0.0039 (0.0059)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.823 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.946 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.875IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.923 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.382 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.352 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814Epoch: [8]  [ 0/60]  eta: 0:01:59  lr: 0.000050  loss: 0.1533 (0.1533)  loss_classifier: 0.0187 (0.0187)  loss_box_reg: 0.0076 (0.0076)  loss_mask: 0.1242 (0.1242)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0027 (0.0027)  time: 1.9847  data: 0.7161  max mem: 3596Epoch: [8]  [10/60]  eta: 0:01:14  lr: 0.000050  loss: 0.1533 (0.1537)  loss_classifier: 0.0227 (0.0245)  loss_box_reg: 0.0076 (0.0104)  loss_mask: 0.1094 (0.1121)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0047 (0.0061)  time: 1.4974  data: 0.0700  max mem: 3596Epoch: [8]  [20/60]  eta: 0:00:58  lr: 0.000050  loss: 0.1512 (0.1592)  loss_classifier: 0.0217 (0.0230)  loss_box_reg: 0.0084 (0.0112)  loss_mask: 0.1071 (0.1168)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0070 (0.0072)  time: 1.4405  data: 0.0081  max mem: 3596Epoch: [8]  [30/60]  eta: 0:00:43  lr: 0.000050  loss: 0.1390 (0.1557)  loss_classifier: 0.0217 (0.0237)  loss_box_reg: 0.0084 (0.0111)  loss_mask: 0.1021 (0.1130)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0075 (0.0071)  time: 1.4221  data: 0.0102  max mem: 3596Epoch: [8]  [40/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1438 (0.1632)  loss_classifier: 0.0257 (0.0257)  loss_box_reg: 0.0086 (0.0117)  loss_mask: 0.1112 (0.1172)  loss_objectness: 0.0003 (0.0012)  loss_rpn_box_reg: 0.0076 (0.0075)  time: 1.4616  data: 0.0096  max mem: 3596Epoch: [8]  [50/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1613 (0.1628)  loss_classifier: 0.0265 (0.0265)  loss_box_reg: 0.0104 (0.0116)  loss_mask: 0.1130 (0.1157)  loss_objectness: 0.0005 (0.0011)  loss_rpn_box_reg: 0.0076 (0.0080)  time: 1.5084  data: 0.0096  max mem: 3596Epoch: [8]  [59/60]  eta: 0:00:01  lr: 0.000050  loss: 0.1426 (0.1593)  loss_classifier: 0.0209 (0.0254)  loss_box_reg: 0.0060 (0.0107)  loss_mask: 0.1046 (0.1148)  loss_objectness: 0.0003 (0.0011)  loss_rpn_box_reg: 0.0056 (0.0073)  time: 1.4303  data: 0.0097  max mem: 3596Epoch: [8] Total time: 0:01:27 (1.4531 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:27  model_time: 0.3871 (0.3871)  evaluator_time: 0.0041 (0.0041)  time: 0.5413  data: 0.1481  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)  time: 0.3191  data: 0.0052  max mem: 3596Test: Total time: 0:00:16 (0.3230 s / it)Averaged stats: model_time: 0.3111 (0.3065)  evaluator_time: 0.0040 (0.0059)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.380 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.867 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814Epoch: [9]  [ 0/60]  eta: 0:01:35  lr: 0.000005  loss: 0.1384 (0.1384)  loss_classifier: 0.0122 (0.0122)  loss_box_reg: 0.0035 (0.0035)  loss_mask: 0.1193 (0.1193)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0031 (0.0031)  time: 1.5976  data: 0.1940  max mem: 3596Epoch: [9]  [10/60]  eta: 0:01:12  lr: 0.000005  loss: 0.1391 (0.1678)  loss_classifier: 0.0229 (0.0239)  loss_box_reg: 0.0087 (0.0123)  loss_mask: 0.1188 (0.1247)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0055 (0.0064)  time: 1.4416  data: 0.0261  max mem: 3596Epoch: [9]  [20/60]  eta: 0:00:57  lr: 0.000005  loss: 0.1595 (0.1658)  loss_classifier: 0.0253 (0.0262)  loss_box_reg: 0.0106 (0.0121)  loss_mask: 0.1154 (0.1203)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0056 (0.0066)  time: 1.4367  data: 0.0094  max mem: 3596Epoch: [9]  [30/60]  eta: 0:00:43  lr: 0.000005  loss: 0.1595 (0.1680)  loss_classifier: 0.0256 (0.0275)  loss_box_reg: 0.0088 (0.0125)  loss_mask: 0.1150 (0.1199)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0060 (0.0074)  time: 1.4554  data: 0.0095  max mem: 3596Epoch: [9]  [40/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1449 (0.1605)  loss_classifier: 0.0212 (0.0258)  loss_box_reg: 0.0070 (0.0111)  loss_mask: 0.1049 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0064 (0.0069)  time: 1.4446  data: 0.0094  max mem: 3596Epoch: [9]  [50/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1504 (0.1591)  loss_classifier: 0.0195 (0.0256)  loss_box_reg: 0.0083 (0.0109)  loss_mask: 0.1037 (0.1149)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0071)  time: 1.4887  data: 0.0095  max mem: 3596Epoch: [9]  [59/60]  eta: 0:00:01  lr: 0.000005  loss: 0.1527 (0.1602)  loss_classifier: 0.0224 (0.0256)  loss_box_reg: 0.0083 (0.0108)  loss_mask: 0.1102 (0.1160)  loss_objectness: 0.0002 (0.0007)  loss_rpn_box_reg: 0.0061 (0.0072)  time: 1.4984  data: 0.0097  max mem: 3596Epoch: [9] Total time: 0:01:27 (1.4592 s / it)creating index...index created!Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3668 (0.3668)  evaluator_time: 0.0042 (0.0042)  time: 0.5024  data: 0.1296  max mem: 3596Test:  [49/50]  eta: 0:00:00  model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)  time: 0.3215  data: 0.0061  max mem: 3596Test: Total time: 0:00:16 (0.3233 s / it)Averaged stats: model_time: 0.3142 (0.3063)  evaluator_time: 0.0039 (0.0059)Accumulating evaluation results...DONE (t=0.01s).Accumulating evaluation results...DONE (t=0.01s).IoU metric: bbox Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.822 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.955 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.832 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.874IoU metric: segm Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.774 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.814</code></pre><p>Now that training has finished, let’s have a look at what it actually predicts in a test image</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pick one image from the test set</span></span><br><span class="line">img, _ = dataset_test[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># put the model in evaluation mode</span></span><br><span class="line">model.eval()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    prediction = model([img.to(device)])</span><br></pre></td></tr></table></figure><pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details.   warnings.warn(&quot;The default behavior for interpolate/upsample with float scale_factor will change &quot;</code></pre><p>Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.<br>The dictionary contains the predictions for the image we passed. In this case, we can see that it contains <code>boxes</code>, <code>labels</code>, <code>masks</code> and <code>scores</code> as fields.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction</span><br></pre></td></tr></table></figure><pre><code>[{&#39;boxes&#39;: tensor([[ 59.6432,  41.9334, 195.6993, 327.8640],          [276.4631,  22.6867, 290.8581,  73.6079]], device=&#39;cuda:0&#39;),  &#39;labels&#39;: tensor([1, 1], device=&#39;cuda:0&#39;),  &#39;masks&#39;: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.],            ...,            [0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.]]],          [[[0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.],            ...,            [0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.],            [0., 0., 0.,  ..., 0., 0., 0.]]]], device=&#39;cuda:0&#39;),  &#39;scores&#39;: tensor([0.9991, 0.8170], device=&#39;cuda:0&#39;)}]</code></pre><p>Let’s inspect the image and the predicted segmentation masks.</p><p>For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in <code>[C, H, W]</code> format.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image.fromarray(img.mul(<span class="number">255</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).byte().numpy())</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_31_0.png" alt="png"></p><p>And let’s now visualize the top predicted segmentation mask. The masks are predicted as <code>[N, 1, H, W]</code>, where <code>N</code> is the number of predictions, and are probability maps between 0-1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image.fromarray(prediction[<span class="number">0</span>][<span class="string">'masks'</span>][<span class="number">0</span>, <span class="number">0</span>].mul(<span class="number">255</span>).byte().cpu().numpy())</span><br></pre></td></tr></table></figure><p><img src="/2020/07/24/Pytorch-Image-%E5%BE%AE%E8%B0%83TorchVision%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/output_33_0.png" alt="png"></p><p>Looks pretty good!</p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset.<br>For that, you wrote a <code>torch.utils.data.Dataset</code> class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.</p><p>For a more complete example, which includes multi-machine / multi-gpu training, check <code>references/detection/train.py</code>, which is present in the <a href="https://github.com/pytorch/vision/tree/v0.3.0/references/detection" target="_blank" rel="noopener">torchvision GitHub repo</a>. </p><p>#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">coordinates:坐标, 座标,协调, 配合, 接应</span><br><span class="line">segmentation:分割</span><br><span class="line">backbone:主干</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Image-微调TorchVision对象检测:&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Image" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Image/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Learning-cifar10tutorial-visualizing</title>
    <link href="http://yoursite.com/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/"/>
    <id>http://yoursite.com/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/</id>
    <published>2020-07-23T09:27:53.000Z</published>
    <updated>2020-07-23T13:40:30.363Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Learning-cifar10tutorial-visualizing</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Training-a-Classifier"><a href="#Training-a-Classifier" class="headerlink" title="Training a Classifier"></a>Training a Classifier</h1><p>This is it. You have seen how to define neural networks, compute loss and make<br>updates to the weights of the network.</p><p>Now you might be thinking,</p><h2 id="What-about-data"><a href="#What-about-data" class="headerlink" title="What about data?"></a>What about data?</h2><p>Generally, when you have to deal with image, text, audio or video data,<br>you can use standard python packages that load data into a numpy array.<br>Then you can convert this array into a <code>torch.*Tensor</code>.</p><ul><li>For images, packages such as Pillow, OpenCV are useful</li><li>For audio, packages such as scipy and librosa</li><li>For text, either raw Python or Cython based loading, or NLTK and<br>SpaCy are useful</li></ul><p>Specifically for vision, we have created a package called<br><code>torchvision</code>, that has data loaders for common datasets such as<br>Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,<br><code>torchvision.datasets</code> and <code>torch.utils.data.DataLoader</code>.</p><p>This provides a huge convenience and avoids writing boilerplate code.</p><p>For this tutorial, we will use the CIFAR10 dataset.<br>It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,<br>‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of<br>size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.</p><p><img src="https://pytorch.org/tutorials/_images/cifar10.png" alt></p><p>   cifar10</p><h2 id="Training-an-image-classifier"><a href="#Training-an-image-classifier" class="headerlink" title="Training an image classifier"></a>Training an image classifier</h2><p>We will do the following steps in order:</p><ol><li><p>Load and normalizing the CIFAR10 training and test datasets using<br><code>torchvision</code></p></li><li><p>Define a Convolutional Neural Network</p></li><li><p>Define a loss function</p></li><li><p>Train the network on the training data</p></li><li><p>Test the network on the test data</p></li><li><p>Loading and normalizing CIFAR10</p></li></ol><hr><p>Using <code>torchvision</code>, it’s extremely easy to load CIFAR10.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure><p>The output of torchvision datasets are PILImage images of range [0, 1].<br>We transform them to Tensors of normalized range [-1, 1].</p><div class="alert alert-info"><h4>Note</h4><p>If running on Windows and you get a BrokenPipeError, try setting    the num_worker of torch.utils.data.DataLoader() to 0.</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure><pre><code>Files already downloaded and verifiedFiles already downloaded and verified</code></pre><p>Let us show some of the training images, for fun.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_6_0.png" alt="png"></p><pre><code>horse   cat  deer   cat</code></pre><ol start="2"><li>Define a Convolutional Neural Network</li></ol><hr><p>Copy the neural network from the Neural Networks section before and modify it to<br>take 3-channel images (instead of 1-channel images as it was defined).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure><ol start="3"><li>Define a Loss function and optimizer</li></ol><hr><p>Let’s use a Classification Cross-Entropy loss and SGD with momentum.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><ol start="4"><li>Train the network</li></ol><hr><p>This is when things start to get interesting.<br>We simply have to loop over our data iterator, and feed the inputs to the<br>network and optimize.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure><pre><code>[1,  2000] loss: 2.173[1,  4000] loss: 1.818[1,  6000] loss: 1.647[1,  8000] loss: 1.545[1, 10000] loss: 1.490[1, 12000] loss: 1.436[2,  2000] loss: 1.384[2,  4000] loss: 1.348[2,  6000] loss: 1.341[2,  8000] loss: 1.306[2, 10000] loss: 1.292[2, 12000] loss: 1.283Finished Training</code></pre><p>Let’s quickly save our trained model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH = <span class="string">'./cifar_net.pth'</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure><p>See <code>here &lt;https://pytorch.org/docs/stable/notes/serialization.html&gt;</code>_<br>for more details on saving PyTorch models.</p><ol start="5"><li>Test the network on the test data</li></ol><hr><p>We have trained the network for 2 passes over the training dataset.<br>But we need to check if the network has learnt anything at all.</p><p>We will check this by predicting the class label that the neural network<br>outputs, and checking it against the ground-truth. If the prediction is<br>correct, we add the sample to the list of correct predictions.</p><p>Okay, first step. Let us display an image from the test set to get familiar.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_16_0.png" alt="png"></p><pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p>Next, let’s load back in our saved model (note: saving and re-loading the model<br>wasn’t necessary here, we only did it to illustrate how to do so):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure><pre><code>IncompatibleKeys(missing_keys=[], unexpected_keys=[])</code></pre><p>Okay, now let us see what the neural network thinks these examples above are:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure><p>The outputs are energies for the 10 classes.<br>The higher the energy for a class, the more the network<br>thinks that the image is of the particular class.<br>So, let’s get the index of the highest energy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><pre><code>Predicted:    cat plane plane plane</code></pre><p>The results seem pretty good.</p><p>Let us look at how the network performs on the whole dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><pre><code>Accuracy of the network on the 10000 test images: 53 %</code></pre><p>That looks way better than chance, which is 10% accuracy (randomly picking<br>a class out of 10 classes).<br>Seems like the network learnt something.</p><p>Hmmm, what are the classes that performed well, and the classes that did<br>not perform well:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure><pre><code>Accuracy of plane : 71 %Accuracy of   car : 57 %Accuracy of  bird : 26 %Accuracy of   cat : 32 %Accuracy of  deer : 52 %Accuracy of   dog : 40 %Accuracy of  frog : 72 %Accuracy of horse : 74 %Accuracy of  ship : 57 %Accuracy of truck : 53 %</code></pre><p>Okay, so what next?</p><p>How do we run these neural networks on the GPU?</p><h2 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h2><p>Just like how you transfer a Tensor onto the GPU, you transfer the neural<br>net onto the GPU.</p><p>Let’s first define our device as the first visible cuda device if we have<br>CUDA available:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure><pre><code>cpu</code></pre><p>The rest of this section assumes that <code>device</code> is a CUDA device.</p><p>Then these methods will recursively go over all modules and convert their<br>parameters and buffers to CUDA tensors:</p><p>.. code:: python</p><pre><code>net.to(device)</code></pre><p>Remember that you will have to send the inputs and targets at every step<br>to the GPU too:</p><p>.. code:: python</p><pre><code>inputs, labels = data[0].to(device), data[1].to(device)</code></pre><p>Why dont I notice MASSIVE speedup compared to CPU? Because your network<br>is really small.</p><p><strong>Exercise:</strong> Try increasing the width of your network (argument 2 of<br>the first <code>nn.Conv2d</code>, and argument 1 of the second <code>nn.Conv2d</code> –<br>they need to be the same number), see what kind of speedup you get.</p><p><strong>Goals achieved</strong>:</p><ul><li>Understanding PyTorch’s Tensor library and neural networks at a high level.</li><li>Train a small neural network to classify images</li></ul><h2 id="Training-on-multiple-GPUs"><a href="#Training-on-multiple-GPUs" class="headerlink" title="Training on multiple GPUs"></a>Training on multiple GPUs</h2><p>If you want to see even more MASSIVE speedup using all of your GPUs,<br>please check out :doc:<code>data_parallel_tutorial</code>.</p><h2 id="Where-do-I-go-next"><a href="#Where-do-I-go-next" class="headerlink" title="Where do I go next?"></a>Where do I go next?</h2><ul><li>:doc:<code>Train neural nets to play video games &lt;/intermediate/reinforcement_q_learning&gt;</code></li><li><code>Train a state-of-the-art ResNet network on imagenet</code>_</li><li><code>Train a face generator using Generative Adversarial Networks</code>_</li><li><code>Train a word-level language model using Recurrent LSTM networks</code>_</li><li><code>More examples</code>_</li><li><code>More tutorials</code>_</li><li><code>Discuss PyTorch on the Forums</code>_</li><li><code>Chat with other users on Slack</code>_</li></ul><h2 id="VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD"><a href="#VISUALIZING-MODELS-DATA-AND-TRAINING-WITH-TENSORBOARD" class="headerlink" title="VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD"></a>VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD</h2><p>In the 60 Minute Blitz, we show you how to load in data, feed it through a model we define as a subclass of nn.Module, train this model on training data, and test it on test data. To see what’s happening, we print out some statistics as the model is training to get a sense for whether training is progressing. However, we can do much better than that: PyTorch integrates with TensorBoard, a tool designed for visualizing the results of neural network training runs. This tutorial illustrates some of its functionality, using the Fashion-MNIST dataset which can be read into PyTorch using torchvision.datasets.</p><p>In this tutorial, we’ll learn how to:</p><ul><li>Read in data and with appropriate transforms (nearly identical to the prior tutorial).</li><li>Set up TensorBoard.</li><li>Write to TensorBoard.</li><li>Inspect a model architecture using TensorBoard.</li></ul><p>Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code<br>Specifically, on point #5, we’ll see:</p><ul><li>A couple of ways to inspect our training data</li><li>How to track our model’s performance as it trains</li><li>How to assess our model’s performance once it is trained.</li><li>We’ll begin with similar boilerplate code as in the CIFAR-10 tutorial:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># transforms</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># datasets</span></span><br><span class="line">trainset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line">testset = torchvision.datasets.FashionMNIST(<span class="string">'./data'</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataloaders</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># constant for classes</span></span><br><span class="line">classes = (<span class="string">'T-shirt/top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,</span><br><span class="line">        <span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle Boot'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function to show an image</span></span><br><span class="line"><span class="comment"># (used in the `plot_classes_preds` function below)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matplotlib_imshow</span><span class="params">(img, one_channel=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        img = img.mean(dim=<span class="number">0</span>)</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        plt.imshow(npimg, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><pre><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\train-images-idx3-ubyte.gz100.0%Extracting ./data\FashionMNIST\raw\train-images-idx3-ubyte.gzDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gz111.0%Extracting ./data\FashionMNIST\raw\train-labels-idx1-ubyte.gzDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz100.0%Extracting ./data\FashionMNIST\raw\t10k-images-idx3-ubyte.gzDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz159.1%Extracting ./data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gzProcessing...Done!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. TensorBoard setup</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">'runs/fashion_mnist_experiment_1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Writing to TensorBoard</span></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">matplotlib_imshow(img_grid, one_channel=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># write to tensorboard</span></span><br><span class="line">writer.add_image(<span class="string">'four_fashion_mnist_images'</span>, img_grid)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Now running: tensorboard --logdir=runs</span></span><br><span class="line"><span class="comment"># from the command line and then navigating to https://localhost:6006</span></span><br></pre></td></tr></table></figure><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_32_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Inspect the model using TensorBoard</span></span><br><span class="line">writer.add_graph(net, images)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Adding a “Projector” to TensorBoard</span></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_n_random</span><span class="params">(data, labels, n=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Selects n random datapoints and their corresponding labels from a dataset</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> len(data) == len(labels)</span><br><span class="line"></span><br><span class="line">    perm = torch.randperm(len(data))</span><br><span class="line">    <span class="keyword">return</span> data[perm][:n], labels[perm][:n]</span><br><span class="line"></span><br><span class="line"><span class="comment"># select random images and their target indices</span></span><br><span class="line">images, labels = select_n_random(trainset.data, trainset.targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the class labels for each image</span></span><br><span class="line">class_labels = [classes[lab] <span class="keyword">for</span> lab <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="comment"># log embeddings</span></span><br><span class="line">features = images.view(<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>)</span><br><span class="line">writer.add_embedding(features,</span><br><span class="line">                    metadata=class_labels,</span><br><span class="line">                    label_img=images.unsqueeze(<span class="number">1</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5. Tracking model training with TensorBoard</span></span><br><span class="line"><span class="comment"># helper functions</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">images_to_probs</span><span class="params">(net, images)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates predictions and corresponding probabilities from a trained</span></span><br><span class="line"><span class="string">    network and a list of images</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    output = net(images)</span><br><span class="line">    <span class="comment"># convert output probabilities to predicted class</span></span><br><span class="line">    _, preds_tensor = torch.max(output, <span class="number">1</span>)</span><br><span class="line">    preds = np.squeeze(preds_tensor.numpy())</span><br><span class="line">    <span class="keyword">return</span> preds, [F.softmax(el, dim=<span class="number">0</span>)[i].item() <span class="keyword">for</span> i, el <span class="keyword">in</span> zip(preds, output)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_classes_preds</span><span class="params">(net, images, labels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generates matplotlib Figure using a trained network, along with images</span></span><br><span class="line"><span class="string">    and labels from a batch, that shows the network's top prediction along</span></span><br><span class="line"><span class="string">    with its probability, alongside the actual label, coloring this</span></span><br><span class="line"><span class="string">    information based on whether the prediction was correct or not.</span></span><br><span class="line"><span class="string">    Uses the "images_to_probs" function.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    preds, probs = images_to_probs(net, images)</span><br><span class="line">    <span class="comment"># plot the images in the batch, along with predicted and true labels</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">48</span>))</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> np.arange(<span class="number">4</span>):</span><br><span class="line">        ax = fig.add_subplot(<span class="number">1</span>, <span class="number">4</span>, idx+<span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        matplotlib_imshow(images[idx], one_channel=<span class="literal">True</span>)</span><br><span class="line">        ax.set_title(<span class="string">"&#123;0&#125;, &#123;1:.1f&#125;%\n(label: &#123;2&#125;)"</span>.format(</span><br><span class="line">            classes[preds[idx]],</span><br><span class="line">            probs[idx] * <span class="number">100.0</span>,</span><br><span class="line">            classes[labels[idx]]),</span><br><span class="line">                    color=(<span class="string">"green"</span> <span class="keyword">if</span> preds[idx]==labels[idx].item() <span class="keyword">else</span> <span class="string">"red"</span>))</span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line">running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">999</span>:    <span class="comment"># every 1000 mini-batches...</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log the running loss</span></span><br><span class="line">            writer.add_scalar(<span class="string">'training loss'</span>,</span><br><span class="line">                            running_loss / <span class="number">1000</span>,</span><br><span class="line">                            epoch * len(trainloader) + i)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log a Matplotlib Figure showing the model's predictions on a</span></span><br><span class="line">            <span class="comment"># random mini-batch</span></span><br><span class="line">            writer.add_figure(<span class="string">'predictions vs. actuals'</span>,</span><br><span class="line">                            plot_classes_preds(net, inputs, labels),</span><br><span class="line">                            global_step=epoch * len(trainloader) + i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure><pre><code>Finished Training</code></pre><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_1.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_2.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_3.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_4.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_5.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_6.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_7.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_8.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_9.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_10.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_11.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_12.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_13.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_14.png" alt="png"></p><p><img src="/2020/07/23/Pytorch-Learning-cifar10tutorial-visualizing/output_35_15.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6. Assessing trained models with TensorBoard</span></span><br><span class="line"><span class="comment"># 1. gets the probability predictions in a test_size x num_classes Tensor</span></span><br><span class="line"><span class="comment"># 2. gets the preds in a test_size Tensor</span></span><br><span class="line"><span class="comment"># takes ~10 seconds to run</span></span><br><span class="line">class_probs = []</span><br><span class="line">class_preds = []</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        output = net(images)</span><br><span class="line">        class_probs_batch = [F.softmax(el, dim=<span class="number">0</span>) <span class="keyword">for</span> el <span class="keyword">in</span> output]</span><br><span class="line">        _, class_preds_batch = torch.max(output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        class_probs.append(class_probs_batch)</span><br><span class="line">        class_preds.append(class_preds_batch)</span><br><span class="line"></span><br><span class="line">test_probs = torch.cat([torch.stack(batch) <span class="keyword">for</span> batch <span class="keyword">in</span> class_probs])</span><br><span class="line">test_preds = torch.cat(class_preds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_pr_curve_tensorboard</span><span class="params">(class_index, test_probs, test_preds, global_step=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Takes in a "class_index" from 0 to 9 and plots the corresponding</span></span><br><span class="line"><span class="string">    precision-recall curve</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tensorboard_preds = test_preds == class_index</span><br><span class="line">    tensorboard_probs = test_probs[:, class_index]</span><br><span class="line"></span><br><span class="line">    writer.add_pr_curve(classes[class_index],</span><br><span class="line">                        tensorboard_preds,</span><br><span class="line">                        tensorboard_probs,</span><br><span class="line">                        global_step=global_step)</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot all the pr curves</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">    add_pr_curve_tensorboard(i, test_probs, test_preds)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Learning-cifar10tutorial-visualizing&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Learning" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Learning-torch.nn</title>
    <link href="http://yoursite.com/2020/07/23/Pytorch-Learning-torch-nn/"/>
    <id>http://yoursite.com/2020/07/23/Pytorch-Learning-torch-nn/</id>
    <published>2020-07-23T09:25:48.000Z</published>
    <updated>2020-07-23T13:40:47.444Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Learning-torch.nn</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="What-is-torch-nn-really"><a href="#What-is-torch-nn-really" class="headerlink" title="What is torch.nn really?"></a>What is <code>torch.nn</code> <em>really</em>?</h1><p>by Jeremy Howard, <code>fast.ai &lt;https://www.fast.ai&gt;</code>_. Thanks to Rachel Thomas and Francisco Ingham.</p><p>We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,<br>click the link at the top of the page.</p><p>PyTorch provides the elegantly designed modules and classes <code>torch.nn &lt;https://pytorch.org/docs/stable/nn.html&gt;</code>_ ,<br><code>torch.optim &lt;https://pytorch.org/docs/stable/optim.html&gt;</code>_ ,<br><code>Dataset &lt;https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset&gt;</code>_ ,<br>and <code>DataLoader &lt;https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader&gt;</code>_<br>to help you create and train neural networks.<br>In order to fully utilize their power and customize<br>them for your problem, you need to really understand exactly what they’re<br>doing. To develop this understanding, we will first train basic neural net<br>on the MNIST data set without using any features from these models; we will<br>initially only use the most basic PyTorch tensor functionality. Then, we will<br>incrementally add one feature from <code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, or<br><code>DataLoader</code> at a time, showing exactly what each piece does, and how it<br>works to make the code either more concise, or more flexible.</p><p><strong>This tutorial assumes you already have PyTorch installed, and are familiar<br>with the basics of tensor operations.</strong> (If you’re familiar with Numpy array<br>operations, you’ll find the PyTorch tensor operations used here nearly identical).</p><h2 id="MNIST-data-setup"><a href="#MNIST-data-setup" class="headerlink" title="MNIST data setup"></a>MNIST data setup</h2><p>We will use the classic <code>MNIST &lt;http://deeplearning.net/data/mnist/&gt;</code>_ dataset,<br>which consists of black-and-white images of hand-drawn digits (between 0 and 9).</p><p>We will use <code>pathlib &lt;https://docs.python.org/3/library/pathlib.html&gt;</code>_<br>for dealing with paths (part of the Python 3 standard library), and will<br>download the dataset using<br><code>requests &lt;http://docs.python-requests.org/en/master/&gt;</code>_. We will only<br>import modules when we use them, so you can see exactly what’s being<br>used at each point.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">"data"</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">"mnist"</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">"http://deeplearning.net/data/mnist/"</span></span><br><span class="line">FILENAME = <span class="string">"mnist.pkl.gz"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).open(<span class="string">"wb"</span>).write(content)</span><br></pre></td></tr></table></figure><p>This dataset is in numpy array format, and has been stored using pickle,<br>a python-specific format for serializing data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.open((PATH / FILENAME).as_posix(), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">"latin-1"</span>)</span><br></pre></td></tr></table></figure><p>Each image is 28 x 28, and is being stored as a flattened row of length<br>784 (=28x28). Let’s take a look at one; we need to reshape it to 2d<br>first.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">"gray"</span>)</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure><pre><code>(50000, 784)</code></pre><p><img src="/2020/07/23/Pytorch-Learning-torch-nn/output_7_1.png" alt="png"></p><p>PyTorch uses <code>torch.tensor</code>, rather than numpy arrays, so we need to<br>convert our data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = map(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line">x_train, x_train.shape, y_train.min(), y_train.max()</span><br><span class="line">print(x_train, y_train)</span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.min(), y_train.max())</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.],        ...,        [0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])torch.Size([50000, 784])tensor(0) tensor(9)</code></pre><h2 id="Neural-net-from-scratch-no-torch-nn"><a href="#Neural-net-from-scratch-no-torch-nn" class="headerlink" title="Neural net from scratch (no torch.nn)"></a>Neural net from scratch (no torch.nn)</h2><p>Let’s first create a model using nothing but PyTorch tensor operations. We’re assuming<br>you’re already familiar with the basics of neural networks. (If you’re not, you can<br>learn them at <code>course.fast.ai &lt;https://course.fast.ai&gt;</code>_).</p><p>PyTorch provides methods to create random or zero-filled tensors, which we will<br>use to create our weights and bias for a simple linear model. These are just regular<br>tensors, with one very special addition: we tell PyTorch that they require a<br>gradient. This causes PyTorch to record all of the operations done on the tensor,<br>so that it can calculate the gradient during back-propagation <em>automatically</em>!</p><p>For the weights, we set <code>requires_grad</code> <strong>after</strong> the initialization, since we<br>don’t want that step included in the gradient. (Note that a trailling <code>_</code> in<br>PyTorch signifies that the operation is performed in-place.)</p><div class="alert alert-info"><h4>Note</h4><p>We are initializing the weights here with   `Xavier initialisation <http: proceedings.mlr.press v9 glorot10a glorot10a.pdf>`_   (by multiplying with 1/sqrt(n)).</http:></p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Thanks to PyTorch’s ability to calculate gradients automatically, we can<br>use any standard Python function (or callable object) as a model! So<br>let’s just write a plain matrix multiplication and broadcasted addition<br>to create a simple linear model. We also need an activation function, so<br>we’ll write <code>log_softmax</code> and use it. Remember: although PyTorch<br>provides lots of pre-written loss functions, activation functions, and<br>so forth, you can easily write your own using plain python. PyTorch will<br>even create fast GPU or vectorized CPU code for your function<br>automatically.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x - x.exp().sum(<span class="number">-1</span>).log().unsqueeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure><p>In the above, the <code>@</code> stands for the dot product operation. We will call<br>our function on one batch of data (in this case, 64 images).  This is<br>one <em>forward pass</em>.  Note that our predictions won’t be any better than<br>random at this stage, since we start with random weights.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bs = <span class="number">64</span>  <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs]  <span class="comment"># a mini-batch from x</span></span><br><span class="line">preds = model(xb)  <span class="comment"># predictions</span></span><br><span class="line">preds[<span class="number">0</span>], preds.shape</span><br><span class="line">print(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure><pre><code>tensor([-2.2669, -2.6024, -2.8454, -1.5665, -2.7687, -2.2455, -2.6885, -2.4918,        -2.1065, -2.1682], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])</code></pre><p>As you see, the <code>preds</code> tensor contains not only the tensor values, but also a<br>gradient function. We’ll use this later to do backprop.</p><p>Let’s implement negative log-likelihood to use as the loss function<br>(again, we can just use standard Python):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nll</span><span class="params">(input, target)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -input[range(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure><p>Let’s check our loss with our random model, so we can see if we improve<br>after a backprop pass later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line">print(loss_func(preds, yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(2.2549, grad_fn=&lt;NegBackward&gt;)</code></pre><p>Let’s also implement a function to calculate the accuracy of our model.<br>For each prediction, if the index with the largest value matches the<br>target value, then the prediction was correct.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(out, yb)</span>:</span></span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).float().mean()</span><br></pre></td></tr></table></figure><p>Let’s check the accuracy of our random model, so we can see if our<br>accuracy improves as our loss improves.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(accuracy(preds, yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(0.1562)</code></pre><p>We can now run a training loop.  For each iteration, we will:</p><ul><li>select a mini-batch of data (of size <code>bs</code>)</li><li>use the model to make predictions</li><li>calculate the loss</li><li><code>loss.backward()</code> updates the gradients of the model, in this case, <code>weights</code><br>and <code>bias</code>.</li></ul><p>We now use these gradients to update the weights and bias.  We do this<br>within the <code>torch.no_grad()</code> context manager, because we do not want these<br>actions to be recorded for our next calculation of the gradient.  You can read<br>more about how PyTorch’s Autograd records operations<br><code>here &lt;https://pytorch.org/docs/stable/notes/autograd.html&gt;</code>_.</p><p>We then set the<br>gradients to zero, so that we are ready for the next loop.<br>Otherwise, our gradients would record a running tally of all the operations<br>that had happened (i.e. <code>loss.backward()</code> <em>adds</em> the gradients to whatever is<br>already stored, rather than replacing them).</p><p>.. tip:: You can use the standard python debugger to step through PyTorch<br>   code, allowing you to check the various variable values at each step.<br>   Uncomment <code>set_trace()</code> below to try it out.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># how many epochs to train for</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line"><span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure><p>That’s it: we’ve created and trained a minimal neural network (in this case, a<br>logistic regression, since we have no hidden layers) entirely from scratch!</p><p>Let’s check the loss and accuracy and compare those to what we got<br>earlier. We expect that the loss will have decreased and accuracy to<br>have increased, and they have.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(0.0781, grad_fn=&lt;NegBackward&gt;) tensor(1.)</code></pre><h2 id="Using-torch-nn-functional"><a href="#Using-torch-nn-functional" class="headerlink" title="Using torch.nn.functional"></a>Using torch.nn.functional</h2><p>We will now refactor our code, so that it does the same thing as before, only<br>we’ll start taking advantage of PyTorch’s <code>nn</code> classes to make it more concise<br>and flexible. At each step from here, we should be making our code one or more<br>of: shorter, more understandable, and/or more flexible.</p><p>The first and easiest step is to make our code shorter by replacing our<br>hand-written activation and loss functions with those from <code>torch.nn.functional</code><br>(which is generally imported into the namespace <code>F</code> by convention). This module<br>contains all the functions in the <code>torch.nn</code> library (whereas other parts of the<br>library contain classes). As well as a wide range of loss and activation<br>functions, you’ll also find here some convenient functions for creating neural<br>nets, such as pooling functions. (There are also functions for doing convolutions,<br>linear layers, etc, but as we’ll see, these are usually better handled using<br>other parts of the library.)</p><p>If you’re using negative log likelihood loss and log softmax activation,<br>then Pytorch provides a single function <code>F.cross_entropy</code> that combines<br>the two. So we can even remove the activation function from our model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure><p>Note that we no longer call <code>log_softmax</code> in the <code>model</code> function. Let’s<br>confirm that our loss and accuracy are the same as before:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(0.0781, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)</code></pre><h2 id="Refactor-using-nn-Module"><a href="#Refactor-using-nn-Module" class="headerlink" title="Refactor using nn.Module"></a>Refactor using nn.Module</h2><p>Next up, we’ll use <code>nn.Module</code> and <code>nn.Parameter</code>, for a clearer and more<br>concise training loop. We subclass <code>nn.Module</code> (which itself is a class and<br>able to keep track of state).  In this case, we want to create a class that<br>holds our weights, bias, and method for the forward step.  <code>nn.Module</code> has a<br>number of attributes and methods (such as <code>.parameters()</code> and <code>.zero_grad()</code>)<br>which we will be using.</p><div class="alert alert-info"><h4>Note</h4><p>``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a   class we'll be using a lot. ``nn.Module`` is not to be confused with the Python   concept of a (lowercase ``m``) `module <https: 3 docs.python.org tutorial modules.html>`_,   which is a file of Python code that can be imported.</https:></p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure><p>Since we’re now using an object instead of just using a function, we<br>first have to instantiate our model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure><p>Now we can calculate the loss in the same way as before. Note that<br><code>nn.Module</code> objects are used as if they are functions (i.e they are<br><em>callable</em>), but behind the scenes Pytorch will call our <code>forward</code><br>method automatically.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(2.4768, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>Previously for our training loop we had to update the values for each parameter<br>by name, and manually zero out the grads for each parameter separately, like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    weights -&#x3D; weights.grad * lr</span><br><span class="line">    bias -&#x3D; bias.grad * lr</span><br><span class="line">    weights.grad.zero_()</span><br><span class="line">    bias.grad.zero_()</span><br></pre></td></tr></table></figure><p>Now we can take advantage of model.parameters() and model.zero_grad() (which<br>are both defined by PyTorch for <code>nn.Module</code>) to make those steps more concise<br>and less prone to the error of forgetting some of our parameters, particularly<br>if we had a more complicated model:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure><p>We’ll wrap our little training loop in a <code>fit</code> function so we can run it<br>again later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure><p>Let’s double-check that our loss has gone down:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(0.0860, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-nn-Linear"><a href="#Refactor-using-nn-Linear" class="headerlink" title="Refactor using nn.Linear"></a>Refactor using nn.Linear</h2><p>We continue to refactor our code.  Instead of manually defining and<br>initializing <code>self.weights</code> and <code>self.bias</code>, and calculating <code>xb  @self.weights + self.bias</code>, we will instead use the Pytorch class<br><code>nn.Linear &lt;https://pytorch.org/docs/stable/nn.html#linear-layers&gt;</code>_ for a<br>linear layer, which does all that for us. Pytorch has many types of<br>predefined layers that can greatly simplify our code, and often makes it<br>faster too.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure><p>We instantiate our model and calculate the loss in the same way as before:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(2.3752, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>We are still able to use our same <code>fit</code> method as before.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fit()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(0.0814, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-optim"><a href="#Refactor-using-optim" class="headerlink" title="Refactor using optim"></a>Refactor using optim</h2><p>Pytorch also has a package with various optimization algorithms, <code>torch.optim</code>.<br>We can use the <code>step</code> method from our optimizer to take a forward step, instead<br>of manually updating each parameter.</p><p>This will let us replace our previous manually coded optimization step:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): </span><br><span class="line">        p -&#x3D; p.grad * lr</span><br><span class="line">        model.zero_grad()</span><br></pre></td></tr></table></figure><p>and instead use just:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">opt.step()</span><br><span class="line">opt.zero_grad()</span><br></pre></td></tr></table></figure><p>(<code>optim.zero_grad()</code> resets the gradient to 0 and we need to call it before<br>computing the gradient for the next minibatch.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure><p>We’ll define a little function to create our model and optimizer so we<br>can reuse it in the future.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(2.2501, grad_fn=&lt;NllLossBackward&gt;)tensor(0.0822, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-Dataset"><a href="#Refactor-using-Dataset" class="headerlink" title="Refactor using Dataset"></a>Refactor using Dataset</h2><p>PyTorch has an abstract Dataset class.  A Dataset can be anything that has<br>a <code>__len__</code> function (called by Python’s standard <code>len</code> function) and<br>a <code>__getitem__</code> function as a way of indexing into it.<br><code>This tutorial &lt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&gt;</code>_<br>walks through a nice example of creating a custom <code>FacialLandmarkDataset</code> class<br>as a subclass of <code>Dataset</code>.</p><p>PyTorch’s <code>TensorDataset &lt;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset&gt;</code>_<br>is a Dataset wrapping tensors. By defining a length and way of indexing,<br>this also gives us a way to iterate, index, and slice along the first<br>dimension of a tensor. This will make it easier to access both the<br>independent and dependent variables in the same line as we train.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br></pre></td></tr></table></figure><p>Both <code>x_train</code> and <code>y_train</code> can be combined in a single <code>TensorDataset</code>,<br>which will be easier to iterate over and slice.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure><p>Previously, we had to iterate through minibatches of x and y values separately:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xb &#x3D; x_train[start_i:end_i]</span><br><span class="line">yb &#x3D; y_train[start_i:end_i]</span><br></pre></td></tr></table></figure><p>Now, we can do these two steps together:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xb,yb &#x3D; train_ds[i*bs : i*bs+bs]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(0.0801, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-DataLoader"><a href="#Refactor-using-DataLoader" class="headerlink" title="Refactor using DataLoader"></a>Refactor using DataLoader</h2><p>Pytorch’s <code>DataLoader</code> is responsible for managing batches. You can<br>create a <code>DataLoader</code> from any <code>Dataset</code>. <code>DataLoader</code> makes it easier<br>to iterate over batches. Rather than having to use <code>train_ds[i*bs : i*bs+bs]</code>,<br>the DataLoader gives us each minibatch automatically.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure><p>Previously, our loop iterated over batches (xb, yb) like this:<br>::<br>      for i in range((n-1)//bs + 1):<br>          xb,yb = train_ds[i<em>bs : i</em>bs+bs]<br>          pred = model(xb)</p><p>Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:<br>::<br>      for xb,yb in train_dl:<br>          pred = model(xb)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure><pre><code>tensor(0.0824, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>Thanks to Pytorch’s <code>nn.Module</code>, <code>nn.Parameter</code>, <code>Dataset</code>, and <code>DataLoader</code>,<br>our training loop is now dramatically smaller and easier to understand. Let’s<br>now try to add the basic features necessary to create effecive models in practice.</p><h2 id="Add-validation"><a href="#Add-validation" class="headerlink" title="Add validation"></a>Add validation</h2><p>In section 1, we were just trying to get a reasonable training loop set up for<br>use on our training data.  In reality, you <strong>always</strong> should also have<br>a <code>validation set &lt;https://www.fast.ai/2017/11/13/validation-sets/&gt;</code>_, in order<br>to identify if you are overfitting.</p><p>Shuffling the training data is<br><code>important &lt;https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks&gt;</code>_<br>to prevent correlation between batches and overfitting. On the other hand, the<br>validation loss will be identical whether we shuffle the validation set or not.<br>Since shuffling takes extra time, it makes no sense to shuffle the validation data.</p><p>We’ll use a batch size for the validation set that is twice as large as<br>that for the training set. This is because the validation set does not<br>need backpropagation and thus takes less memory (it doesn’t need to<br>store the gradients). We take advantage of this to use a larger batch<br>size and compute the loss more quickly.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>We will calculate and print the validation loss at the end of each epoch.</p><p>(Note that we always call <code>model.train()</code> before training, and <code>model.eval()</code><br>before inference, because these are used by layers such as <code>nn.BatchNorm2d</code><br>and <code>nn.Dropout</code> to ensure appropriate behaviour for these different phases.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = sum(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line"></span><br><span class="line">    print(epoch, valid_loss / len(valid_dl))</span><br></pre></td></tr></table></figure><pre><code>0 tensor(0.3169)1 tensor(0.4910)</code></pre><h2 id="Create-fit-and-get-data"><a href="#Create-fit-and-get-data" class="headerlink" title="Create fit() and get_data()"></a>Create fit() and get_data()</h2><p>We’ll now do a little refactoring of our own. Since we go through a similar<br>process twice of calculating the loss for both the training set and the<br>validation set, let’s make that into its own function, <code>loss_batch</code>, which<br>computes the loss for one batch.</p><p>We pass an optimizer in for the training set, and use it to perform<br>backprop.  For the validation set, we don’t pass an optimizer, so the<br>method doesn’t perform backprop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_batch</span><span class="params">(model, loss_func, xb, yb, opt=None)</span>:</span></span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), len(xb)</span><br></pre></td></tr></table></figure><p><code>fit</code> runs the necessary operations to train our model and compute the<br>training and validation losses for each epoch.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(epochs, model, loss_func, opt, train_dl, valid_dl)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = zip(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)</span><br><span class="line"></span><br><span class="line">        print(epoch, val_loss)</span><br></pre></td></tr></table></figure><p><code>get_data</code> returns dataloaders for the training and validation sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(train_ds, valid_ds, bs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>Now, our whole process of obtaining the data loaders and fitting the<br>model can be run in 3 lines of code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure><pre><code>0 0.307052354145050051 0.31455935287475584</code></pre><p>You can use these basic 3 lines of code to train a wide variety of models.<br>Let’s see if we can use them to train a convolutional neural network (CNN)!</p><h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>We are now going to build our neural network with three convolutional layers.<br>Because none of the functions in the previous section assume anything about<br>the model form, we’ll be able to use them to train a CNN without any modification.</p><p>We will use Pytorch’s predefined<br><code>Conv2d &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&gt;</code>_ class<br>as our convolutional layer. We define a CNN with 3 convolutional layers.<br>Each convolution is followed by a ReLU.  At the end, we perform an<br>average pooling.  (Note that <code>view</code> is PyTorch’s version of numpy’s<br><code>reshape</code>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        xb = xb.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> xb.view(<span class="number">-1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure><p><code>Momentum &lt;https://cs231n.github.io/neural-networks-3/#sgd&gt;</code>_ is a variation on<br>stochastic gradient descent that takes previous updates into account as well<br>and generally leads to faster training.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure><pre><code>0 0.88983625602722171 0.7796085683822632</code></pre><h2 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h2><p><code>torch.nn</code> has another handy class we can use to simply our code:<br><code>Sequential &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential&gt;</code>_ .<br>A <code>Sequential</code> object runs each of the modules contained within it, in a<br>sequential manner. This is a simpler way of writing our neural network.</p><p>To take advantage of this, we need to be able to easily define a<br><strong>custom layer</strong> from a given function.  For instance, PyTorch doesn’t<br>have a <code>view</code> layer, and we need to create one for our network. <code>Lambda</code><br>will create a layer that we can then use when defining a network with<br><code>Sequential</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure><p>The model created with <code>Sequential</code> is simply:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure><pre><code>0 0.47461769504547121 0.3015344765663147</code></pre><h2 id="Wrapping-DataLoader"><a href="#Wrapping-DataLoader" class="headerlink" title="Wrapping DataLoader"></a>Wrapping DataLoader</h2><p>Our CNN is fairly concise, but it only works with MNIST, because:</p><ul><li>It assumes the input is a 28*28 long vector</li><li>It assumes that the final CNN grid size is 4*4 (since that’s the average<br>pooling kernel size we used)</li></ul><p>Let’s get rid of these two assumptions, so our model works with any 2d<br>single channel image. First, we can remove the initial Lambda layer but<br>moving the data preprocessing into a generator:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WrappedDataLoader</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, func)</span>:</span></span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        batches = iter(self.dl)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> batches:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure><p>Next, we can replace <code>nn.AvgPool2d</code> with <code>nn.AdaptiveAvgPool2d</code>, which<br>allows us to define the size of the <em>output</em> tensor we want, rather than<br>the <em>input</em> tensor we have. As a result, our model will work with any<br>size input.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>Let’s try it out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure><pre><code>0 0.439469513940811131 0.2360862446308136</code></pre><h2 id="Using-your-GPU"><a href="#Using-your-GPU" class="headerlink" title="Using your GPU"></a>Using your GPU</h2><p>If you’re lucky enough to have access to a CUDA-capable GPU (you can<br>rent one for about $0.50/hour from most cloud providers) you can<br>use it to speed up your code. First check that your GPU is working in<br>Pytorch:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><p>And then create a device object for it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dev = torch.device(</span><br><span class="line">    <span class="string">"cuda"</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><p>Let’s update <code>preprocess</code> to move batches to the GPU:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(dev), y.to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure><p>Finally, we can move our model to the GPU.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>You should find it runs faster now:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure><pre><code>0 0.200874821186065671 0.21629996614456176</code></pre><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>We now have a general data pipeline and training loop which you can use for<br>training many types of models using Pytorch. To see how simple training a model<br>can now be, take a look at the <code>mnist_sample</code> sample notebook.</p><p>Of course, there are many things you’ll want to add, such as data augmentation,<br>hyperparameter tuning, monitoring training, transfer learning, and so forth.<br>These features are available in the fastai library, which has been developed<br>using the same design approach shown in this tutorial, providing a natural<br>next step for practitioners looking to take their models further.</p><p>We promised at the start of this tutorial we’d explain through example each of<br><code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, and <code>DataLoader</code>. So let’s summarize<br>what we’ve seen:</p><ul><li><p><strong>torch.nn</strong></p><ul><li><code>Module</code>: creates a callable which behaves like a function, but can also<br>contain state(such as neural net layer weights). It knows what <code>Parameter</code> (s) it<br>contains and can zero all their gradients, loop through them for weight updates, etc.</li><li><code>Parameter</code>: a wrapper for a tensor that tells a <code>Module</code> that it has weights<br>that need updating during backprop. Only tensors with the <code>requires_grad</code> attribute set are updated</li><li><code>functional</code>: a module(usually imported into the <code>F</code> namespace by convention)<br>which contains activation functions, loss functions, etc, as well as non-stateful<br>versions of layers such as convolutional and linear layers.</li></ul></li><li><p><code>torch.optim</code>: Contains optimizers such as <code>SGD</code>, which update the weights<br>of <code>Parameter</code> during the backward step</p></li><li><p><code>Dataset</code>: An abstract interface of objects with a <code>__len__</code> and a <code>__getitem__</code>,<br>including classes provided with Pytorch such as <code>TensorDataset</code></p></li><li><p><code>DataLoader</code>: Takes any <code>Dataset</code> and creates an iterator which returns batches of data.</p></li></ul><h2 id="我不会的单词"><a href="#我不会的单词" class="headerlink" title="我不会的单词"></a>我不会的单词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">utilize:利用</span><br><span class="line">customize:定制</span><br><span class="line">refactor:重构</span><br><span class="line">identical:相同的</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Learning-torch.nn&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Learning" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Learning-examples</title>
    <link href="http://yoursite.com/2020/07/23/Pytorch-Learning-examples/"/>
    <id>http://yoursite.com/2020/07/23/Pytorch-Learning-examples/</id>
    <published>2020-07-23T09:25:32.000Z</published>
    <updated>2020-07-23T13:40:33.971Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Learning-examples</p><a id="more"></a><ul><li>1.<a href="#header1">warm-up:numpy</a></li><li>2.<a href="#header2">pytorch:tensors</a></li><li>3.<a href="#header3">pytorch:tensors and autograd</a></li><li>4.<a href="#header4">pytorch:defining new autograd functions</a></li><li>5.<a href="#header5">tensorflow1.x:static graphs</a></li><li>6.<a href="#header6">pytorch:nn</a></li><li>7.<a href="#header7">pytorch:optim</a></li><li>8.<a href="#header8">pytorch:custom nn modules</a></li><li>9.<a href="#header9">pytorch:control flow+weight sharing</a></li></ul><h1 id="warm-up-numpy"><a href="#warm-up-numpy" class="headerlink" title="warm-up:numpy"></a><span id="header1">warm-up:numpy</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="Warm-up-numpy"><a href="#Warm-up-numpy" class="headerlink" title="Warm-up: numpy"></a>Warm-up: numpy</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x using Euclidean error.</p><p>This implementation uses numpy to manually compute the forward pass, loss, and<br>backward pass.</p><p>A numpy array is a generic n-dimensional array; it does not know anything about<br>deep learning or gradients or computational graphs, and is just a way to perform<br>generic numeric computations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><pre><code>0 38502658.121784671 39021030.5051500352 43783005.359469283 43513345.957547724 33130647.5743021565 18494184.5827089066 8309147.3120929747 3709728.90043023378 1973222.00640194479 1292160.302042167910 969207.669652336611 777313.216624471112 642885.445805720413 539862.096494385614 457706.3691039313615 390776.9998036827316 335541.366044388217 289534.969840669718 250999.089117504719 218440.892216002120 190801.0350538508221 167229.4111161939222 147030.1499517079823 129644.5566975234324 114634.8557394440125 101654.5091088093826 90379.6691015803427 80547.5042060514128 71942.3023425903129 64389.5389746246530 57755.01160866779531 51903.6147844508232 46747.3782186621833 42180.3667054835634 38120.1014324445335 34504.836786208536 31283.62582576260337 28409.55482622988338 25832.8441065927139 23520.94490195371340 21443.9287248615841 19573.4961957905442 17887.5296118543143 16364.90191715814244 14986.99907487606145 13739.2178249793446 12607.39291010569847 11579.32265338763948 10644.06182831764549 9792.95616820409150 9016.95690620057851 8309.61938771363652 7664.01398516503953 7073.44980079116654 6532.83392081704255 6037.53846405572256 5583.27482231634557 5166.315037295702558 4783.43896688735659 4431.26511821845660 4107.30625067173561 3809.17324428442462 3534.306552632403563 3281.38248719660964 3048.272032777972565 2832.961363890582666 2634.055248654373467 2450.120868078801368 2280.003861210682869 2122.502034812571870 1976.614076376976571 1841.48028219392272 1716.143614435584673 1599.849821051512774 1491.92510360600675 1391.7478816340876 1298.70082166170277 1212.21245078229978 1131.794269697062679 1057.043004330298880 987.492519928740681 922.746029889500582 862.463745582051883 806.33529070793284 754.034789026812985 705.303259019786886 659.864058368210987 617.471504349213588 577.936932077719989 541.038271159059690 506.6213968822449491 474.4614646023590692 444.428985022876193 416.383171450924194 390.178126241481895 365.6841615686796696 342.782824305860997 321.3745103325069398 301.3434434552800799 282.6049616675032100 265.0769405906856101 248.66706624922423102 233.31182982470165103 218.93579583019107104 205.47399586723316105 192.8673639685237106 181.05784775853311107 169.9914161272534108 159.62205201703807109 149.90414548125696110 140.7986893389537111 132.2586907147042112 124.25279696970381113 116.74222194734885114 109.70210950592357115 103.09497001058955116 96.89525543441061117 91.07884065063246118 85.6198765958778119 80.49732520749234120 75.6882961733577121 71.1722154368565122 66.93235677110607123 62.95241596773887124 59.21332801738676125 55.70220136122839126 52.40276978766286127 49.30324491666052128 46.39219465065453129 43.6561631420701130 41.08445097890545131 38.66705716574056132 36.39569090451589133 34.2596896820596134 32.25129366141559135 30.36277332849072136 28.58840749936592137 26.91886404849481138 25.348295684128104139 23.871887568407733140 22.482392230639924141 21.1752612429013142 19.94540814626695143 18.78848359075504144 17.699455611505552145 16.674867891142245146 15.710568962031482147 14.802825886952098148 13.94842074029503149 13.144227723348116150 12.387131550628439151 11.674026481428772152 11.002689146006661153 10.370609949309603154 9.775509369377593155 9.215072226656517156 8.687075426828947157 8.189767782525552158 7.7214576865624025159 7.280233710481657160 6.864642688917494161 6.473008103216368162 6.104105173833666163 5.7564916113754085164 5.428879681554832165 5.120147993533515166 4.829259142691539167 4.555145740046442168 4.29672213329475169 4.053143156190888170 3.8235214202376655171 3.607138244094545172 3.4031196927940224173 3.210757574413539174 3.0293765387034064175 2.858454621478234176 2.697234384156207177 2.545177266862872178 2.4017939117722404179 2.2665997813870664180 2.1391195167593566181 2.0188693271562754182 1.9054496369340477183 1.7984763417415828184 1.6975705115269373185 1.6023729047397333186 1.512573512380922187 1.4278650471502181188 1.347969970837449189 1.272567002922919190 1.2014204465432794191 1.1342855863955332192 1.0709568915888343193 1.0111820179227746194 0.9547746744541861195 0.9015532749111042196 0.8513302278674784197 0.803922014568537198 0.7591740066091374199 0.7169375303267338200 0.6770877114086349201 0.6394607979618921202 0.6039480138637082203 0.5704178896870162204 0.5387711941311117205 0.5088920505641997206 0.4806800135074349207 0.45404935956164216208 0.4289117734184146209 0.40517486327826213210 0.38275697069924214211 0.36158944936278636212 0.3416070794747825213 0.3227344916114124214 0.3049141167911339215 0.28808300182326085216 0.27219060149023244217 0.2571773488343734218 0.24299758077475503219 0.22960605968706874220 0.21696181922245522221 0.20501866875786806222 0.19373590402638502223 0.1830764093023291224 0.17300985254584955225 0.1635004063397864226 0.15451596252804456227 0.14602859536330753228 0.13801270444499963229 0.1304395543791902230 0.12328307118542486231 0.1165215280650118232 0.11013498048221826233 0.10409915585381448234 0.09839683048227854235 0.09300849009893092236 0.08791893479291592237 0.08310796594355277238 0.07856156604359649239 0.07426508307950577240 0.07020565731703925241 0.06636946700613897242 0.0627437660794708243 0.059317781060088316244 0.05608034093448906245 0.05301955241121613246 0.050126674069563906247 0.04739256213401111248 0.04480928906259292249 0.04236691748155608250 0.04005879599601969251 0.03787661761938381252 0.03581475257492655253 0.033864928740949526254 0.032021804201541854255 0.030279858100986354256 0.02863334928312954257 0.02707658789186648258 0.025604569332906155259 0.024213249395320283260 0.022897970661672578261 0.02165425187746097262 0.020478737432682297263 0.01936719332160696264 0.018316430865928372265 0.017322619307502316266 0.016383050617920083267 0.015494734481326376268 0.014655032299327592269 0.013860819190983916270 0.013109809002567857271 0.0123997755042141272 0.011728358290906498273 0.011093355906296416274 0.01049298268330884275 0.009925290535664889276 0.009388360895355642277 0.008880549450207519278 0.008400420664047465279 0.007946325800609208280 0.00751686776061394281 0.007110688095051999282 0.006726592064373132283 0.00636331544306615284 0.006019761179348237285 0.005694800059913335286 0.005387424438387055287 0.005096730276146309288 0.004821747870498604289 0.004561741644547909290 0.004315728687198646291 0.004083088241466739292 0.0038630342216639736293 0.0036548280743543514294 0.003457908894412565295 0.0032716544897889873296 0.003095480430783593297 0.0029287903501173454298 0.0027711283850350016299 0.002622002726801867300 0.0024808885834647024301 0.0023473968087844807302 0.00222113284310873303 0.00210170542574699304 0.0019887074497667136305 0.0018817802500325769306 0.00178065116010868307 0.0016849552143364992308 0.0015944091801416442309 0.001508753706017779310 0.0014277397746070208311 0.0013510695821452332312 0.0012785229777043136313 0.0012098928983483854314 0.0011449625641886667315 0.0010835156717760649316 0.001025377588941091317 0.0009703758384736077318 0.0009183361752611588319 0.0008690850720838991320 0.0008224940700348837321 0.0007784029943592372322 0.0007366791771033156323 0.0006971989990605096324 0.0006598426281655082325 0.000624495741732001326 0.0005910439292796761327 0.0005593973773407745328 0.0005294403512899198329 0.0005010976939705757330 0.00047427317778801455331 0.0004488877470686361332 0.0004248669939759654333 0.0004021385047171281334 0.0003806279081745162335 0.0003602664168058468336 0.00034099755680728995337 0.0003227653070463328338 0.00030550707403562225339 0.0002891754204576711340 0.000273721587865668341 0.00025909406293294767342 0.0002452471530517434343 0.0002321429824444712344 0.00021974540966147302345 0.00020800723976222412346 0.00019689755859322988347 0.00018638458674544666348 0.00017643522452308918349 0.00016701530053816744350 0.0001581003279303452351 0.00014966407110653316352 0.00014167707307891997353 0.00013411790417482185354 0.00012696356477874673355 0.00012019230146127977356 0.00011378157083006711357 0.00010771346551538498358 0.0001019705019676451359 9.653404961343065e-05360 9.138760257508527e-05361 8.651709721760377e-05362 8.19070094203869e-05363 7.754226881202036e-05364 7.341041687150536e-05365 6.94995078743375e-05366 6.579726835351665e-05367 6.229309750532338e-05368 5.897611236256165e-05369 5.583565153701364e-05370 5.286297313654044e-05371 5.004834708849745e-05372 4.738444170826265e-05373 4.486241750382573e-05374 4.2474866618345255e-05375 4.0215019660048885e-05376 3.807518640145299e-05377 3.604963907599714e-05378 3.4131911558009186e-05379 3.231655671510322e-05380 3.059770246224207e-05381 2.8970757618958097e-05382 2.743057310133526e-05383 2.5972006538400987e-05384 2.459140169431031e-05385 2.3284245013094282e-05386 2.2046767101248254e-05387 2.0874974705148337e-05388 1.976572632623195e-05389 1.8715764940418173e-05390 1.7721415862570632e-05391 1.6779971953419194e-05392 1.588871637152896e-05393 1.5044836588187851e-05394 1.4245828226986497e-05395 1.3489410778399332e-05396 1.2773364399028685e-05397 1.2095139789484804e-05398 1.145299443496586e-05399 1.0845077903682161e-05400 1.0269508212283454e-05401 9.724527789368986e-06402 9.208527492374383e-06403 8.720045436070135e-06404 8.2573575243078e-06405 7.819257808270196e-06406 7.404463436549593e-06407 7.011821523303816e-06408 6.639943046336263e-06409 6.2878326555936834e-06410 5.954457206733875e-06411 5.638725815406013e-06412 5.339791165439266e-06413 5.056690012109548e-06414 4.7886913275527965e-06415 4.5348871524205895e-06416 4.294550962428522e-06417 4.0669989344986414e-06418 3.851507524029607e-06419 3.6474147842461815e-06420 3.4541567135768535e-06421 3.2712066098541254e-06422 3.0979501309951946e-06423 2.9338623570734417e-06424 2.778486859288849e-06425 2.6313579995304816e-06426 2.492005877958706e-06427 2.3600400344687442e-06428 2.235117848833567e-06429 2.1168180481419086e-06430 2.0047497870126444e-06431 1.8986381787503396e-06432 1.7981531121369732e-06433 1.7029874912083225e-06434 1.6128687459205246e-06435 1.5275476848984372e-06436 1.4467329944515942e-06437 1.3701865914534269e-06438 1.2977020898043846e-06439 1.2290629088171652e-06440 1.164051733130194e-06441 1.1024831847468408e-06442 1.0441903271914875e-06443 9.889751182032595e-07444 9.366772125412096e-07445 8.871553290035977e-07446 8.402520019677647e-07447 7.958308104042071e-07448 7.53763518098307e-07449 7.139244346971557e-07450 6.761960655207478e-07451 6.404595540956439e-07452 6.066124906303541e-07453 5.74556854302957e-07454 5.442030621013854e-07455 5.154497792919255e-07456 4.882203577154002e-07457 4.6243074105866146e-07458 4.380049551012981e-07459 4.1486836737937236e-07460 3.929552794591328e-07461 3.722082406282778e-07462 3.525512434892848e-07463 3.339350935587304e-07464 3.163035075274776e-07465 2.9960560558970377e-07466 2.8378833766961356e-07467 2.688083814724989e-07468 2.5461985965457267e-07469 2.4117912639785815e-07470 2.2845076123088963e-07471 2.1639404311547854e-07472 2.049768154586423e-07473 1.9416035947365062e-07474 1.8391578287894605e-07475 1.742122640542393e-07476 1.6502075323866398e-07477 1.5631602307949188e-07478 1.4807025488750153e-07479 1.4026124873333745e-07480 1.3286339655833968e-07481 1.2585529347481789e-07482 1.1921849306789818e-07483 1.1293151084301514e-07484 1.069766166825837e-07485 1.0133619692710931e-07486 9.599472852904284e-08487 9.093341825332011e-08488 8.613939196599216e-08489 8.159862515209676e-08490 7.729751975546753e-08491 7.322345528572764e-08492 6.936479915203619e-08493 6.57099781685289e-08494 6.224692745752194e-08495 5.896657665795129e-08496 5.585950062511546e-08497 5.291656163586972e-08498 5.012926872369238e-08499 4.748859133659835e-08</code></pre><h1 id="pytorch-tensors"><a href="#pytorch-tensors" class="headerlink" title="pytorch:tensors"></a><span id="header2">pytorch:tensors</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p><p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p><p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p><p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><pre><code>99 784.6785888671875199 5.850834846496582299 0.07988587021827698399 0.0017072007758542895499 0.00015852594515308738</code></pre><h1 id="pytorch-tensors-and-autograd"><a href="#pytorch-tensors-and-autograd" class="headerlink" title="pytorch:tensors and autograd"></a><span id="header3">pytorch:tensors and autograd</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Tensors-and-autograd"><a href="#PyTorch-Tensors-and-autograd" class="headerlink" title="PyTorch: Tensors and autograd"></a>PyTorch: Tensors and autograd</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p><p>This implementation computes the forward pass using operations on PyTorch<br>Tensors, and uses PyTorch autograd to compute gradients.</p><p>A PyTorch Tensor represents a node in a computational graph. If <code>x</code> is a<br>Tensor that has <code>x.requires_grad=True</code> then <code>x.grad</code> is another Tensor<br>holding the gradient of <code>x</code> with respect to some scalar value.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></span><br><span class="line"><span class="comment"># with respect to these Tensors during the backward pass.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Tensors during the backward pass.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Tensors; these</span></span><br><span class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></span><br><span class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></span><br><span class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss using operations on Tensors.</span></span><br><span class="line">    <span class="comment"># Now loss is a Tensor of shape (1,)</span></span><br><span class="line">    <span class="comment"># loss.item() gets the scalar value held in the loss.</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></span><br><span class="line">    <span class="comment"># gradient of loss with respect to all Tensors with requires_grad=True.</span></span><br><span class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Tensors holding the gradient</span></span><br><span class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually update weights using gradient descent. Wrap in torch.no_grad()</span></span><br><span class="line">    <span class="comment"># because weights have requires_grad=True, but we don't need to track this</span></span><br><span class="line">    <span class="comment"># in autograd.</span></span><br><span class="line">    <span class="comment"># An alternative way is to operate on weight.data and weight.grad.data.</span></span><br><span class="line">    <span class="comment"># Recall that tensor.data gives a tensor that shares the storage with</span></span><br><span class="line">    <span class="comment"># tensor, but doesn't track history.</span></span><br><span class="line">    <span class="comment"># You can also use torch.optim.SGD to achieve this.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure><pre><code>99 850.6499633789062199 5.497010231018066299 0.0542689710855484399 0.0009686618577688932499 0.000102342150057666</code></pre><h1 id="pytorch-defining-new-autograd-functions"><a href="#pytorch-defining-new-autograd-functions" class="headerlink" title="pytorch:defining new autograd functions"></a><span id="header4">pytorch:defining new autograd functions</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Defining-New-autograd-Functions"><a href="#PyTorch-Defining-New-autograd-Functions" class="headerlink" title="PyTorch: Defining New autograd Functions"></a>PyTorch: Defining New autograd Functions</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p><p>This implementation computes the forward pass using operations on PyTorch<br>Variables, and uses PyTorch autograd to compute gradients.</p><p>In this implementation we implement our own custom autograd function to perform<br>the ReLU function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs.</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights.</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># To apply our Function, we use Function.apply method. We alias this as 'relu'.</span></span><br><span class="line">    relu = MyReLU.apply</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure><pre><code>99 173.57586669921875199 0.16617316007614136299 0.0004797253059223294399 3.693650069180876e-05499 1.2812281056540087e-05</code></pre><h1 id="pytorch-static-graphs"><a href="#pytorch-static-graphs" class="headerlink" title="pytorch:static graphs"></a><span id="header5">pytorch:static graphs</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Tensors-1"><a href="#PyTorch-Tensors-1" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>A fully-connected ReLU network with one hidden layer and no biases, trained to<br>predict y from x by minimizing squared Euclidean distance.</p><p>This implementation uses PyTorch tensors to manually compute the forward pass,<br>loss, and backward pass.</p><p>A PyTorch Tensor is basically the same as a numpy array: it does not know<br>anything about deep learning or computational graphs or gradients, and is just<br>a generic n-dimensional array to be used for arbitrary numeric computation.</p><p>The biggest difference between a numpy array and a PyTorch Tensor is that<br>a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,<br>just cast the Tensor to a cuda datatype.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># device = torch.device("cuda:0") # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><pre><code>99 784.6785888671875199 5.850834846496582299 0.07988587021827698399 0.0017072007758542895499 0.00015852594515308738</code></pre><h1 id="pytorch-nn"><a href="#pytorch-nn" class="headerlink" title="pytorch:nn"></a><span id="header6">pytorch:nn</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p><p>This implementation uses the nn package from PyTorch to build the network.<br>PyTorch autograd makes it easy to define computational graphs and take gradients,<br>but raw autograd can be a bit too low-level for defining complex neural networks;<br>this is where the nn package can help. The nn package defines a set of Modules,<br>which you can think of as a neural network layer that has produces output from<br>input and may have some trainable weights.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></span><br><span class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></span><br><span class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></span><br><span class="line"><span class="comment"># linear function, and holds internal Tensors for its weight and bias.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></span><br><span class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></span><br><span class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></span><br><span class="line">    <span class="comment"># doing so you pass a Tensor of input data to the Module and it produces</span></span><br><span class="line">    <span class="comment"># a Tensor of output data.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss. We pass Tensors containing the predicted and true</span></span><br><span class="line">    <span class="comment"># values of y, and the loss function returns a Tensor containing the</span></span><br><span class="line">    <span class="comment"># loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></span><br><span class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></span><br><span class="line">    <span class="comment"># in Tensors with requires_grad=True, so this call will compute gradients for</span></span><br><span class="line">    <span class="comment"># all learnable parameters in the model.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Tensor, so</span></span><br><span class="line">    <span class="comment"># we can access its gradients like we did before.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param -= learning_rate * param.grad</span><br></pre></td></tr></table></figure><pre><code>99 2.456005573272705199 0.04037925601005554299 0.001298694172874093399 5.4667791118845344e-05499 2.6507393613428576e-06</code></pre><h1 id="pytorch-optim"><a href="#pytorch-optim" class="headerlink" title="pytorch:optim"></a><span id="header7">pytorch:optim</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p><p>This implementation uses the nn package from PyTorch to build the network.</p><p>Rather than manually updating the weights of the model as we have been doing,<br>we use the optim package to define an Optimizer that will update the weights<br>for us. The optim package defines many optimization algorithms that are commonly<br>used for deep learning, including SGD+momentum, RMSProp, Adam, etc.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></span><br><span class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></span><br><span class="line"><span class="comment"># optimization algorithms. The first argument to the Adam constructor tells the</span></span><br><span class="line"><span class="comment"># optimizer which Tensors it should update.</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></span><br><span class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable</span></span><br><span class="line">    <span class="comment"># weights of the model). This is because by default, gradients are</span></span><br><span class="line">    <span class="comment"># accumulated in buffers( i.e, not overwritten) whenever .backward()</span></span><br><span class="line">    <span class="comment"># is called. Checkout docs of torch.autograd.backward for more details.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><pre><code>99 40.16399383544922199 0.3977137506008148299 0.001604456570930779399 1.6438591046608053e-05499 8.815557350771996e-08</code></pre><h1 id="pytorch-custom-nn-modules"><a href="#pytorch-custom-nn-modules" class="headerlink" title="pytorch:custom nn modules"></a><span id="header8">pytorch:custom nn modules</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch: Custom nn Modules"></a>PyTorch: Custom nn Modules</h2><p>A fully-connected ReLU network with one hidden layer, trained to predict y from x<br>by minimizing squared Euclidean distance.</p><p>This implementation defines the model as a custom Module subclass. Whenever you<br>want a model more complex than a simple sequence of existing Modules you will<br>need to define your model this way.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as</span></span><br><span class="line"><span class="string">        member variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward function we accept a Tensor of input data and we must return</span></span><br><span class="line"><span class="string">        a Tensor of output data. We can use Modules defined in the constructor as</span></span><br><span class="line"><span class="string">        well as arbitrary operators on Tensors.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.linear2(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></span><br><span class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></span><br><span class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h1 id="pytorch-control-flow-weight-sharing"><a href="#pytorch-control-flow-weight-sharing" class="headerlink" title="pytorch:control flow+weight sharing"></a><span id="header9">pytorch:control flow+weight sharing</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch: Control Flow + Weight Sharing"></a>PyTorch: Control Flow + Weight Sharing</h2><p>To showcase the power of PyTorch dynamic graphs, we will implement a very strange<br>model: a fully-connected ReLU network that on each forward pass randomly chooses<br>a number between 1 and 4 and has that many hidden layers, reusing the same<br>weights multiple times to compute the innermost hidden layers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we construct three nn.Linear instances that we will use</span></span><br><span class="line"><span class="string">        in the forward pass.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(DynamicNet, self).__init__()</span><br><span class="line">        self.input_linear = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.middle_linear = torch.nn.Linear(H, H)</span><br><span class="line">        self.output_linear = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</span></span><br><span class="line"><span class="string">        and reuse the middle_linear Module that many times to compute hidden layer</span></span><br><span class="line"><span class="string">        representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Since each forward pass builds a dynamic computation graph, we can use normal</span></span><br><span class="line"><span class="string">        Python control-flow operators like loops or conditional statements when</span></span><br><span class="line"><span class="string">        defining the forward pass of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Here we also see that it is perfectly safe to reuse the same Module many</span></span><br><span class="line"><span class="string">        times when defining a computational graph. This is a big improvement from Lua</span></span><br><span class="line"><span class="string">        Torch, where each Module could be used only once.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</span><br><span class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.output_linear(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = DynamicNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></span><br><span class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><pre><code>99 18.901199340820312199 7.5054731369018555299 1.1003623008728027399 0.8731748461723328499 2.003668785095215</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Learning-examples&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Learning" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Learning-neural_newworks</title>
    <link href="http://yoursite.com/2020/07/23/Pytorch-Learning-neural-newworks/"/>
    <id>http://yoursite.com/2020/07/23/Pytorch-Learning-neural-newworks/</id>
    <published>2020-07-23T08:04:49.000Z</published>
    <updated>2020-07-23T13:40:37.938Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Learning-autograd:</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><p>Neural networks can be constructed using the <code>torch.nn</code> package.</p><p>Now that you had a glimpse of <code>autograd</code>, <code>nn</code> depends on<br><code>autograd</code> to define models and differentiate them.<br>An <code>nn.Module</code> contains layers, and a method <code>forward(input)</code> that<br>returns the <code>output</code>.</p><p>For example, look at this network that classifies digit images:</p><p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt></p><p>   convnet</p><p>It is a simple feed-forward network. It takes the input, feeds it<br>through several layers one after the other, and then finally gives the<br>output.</p><p>A typical training procedure for a neural network is as follows:</p><ul><li>Define the neural network that has some learnable parameters (or<br>weights)</li><li>Iterate over a dataset of inputs</li><li>Process input through the network</li><li>Compute the loss (how far is the output from being correct)</li><li>Propagate gradients back into the network’s parameters</li><li>Update the weights of the network, typically using a simple update rule:<br><code>weight = weight - learning_rate * gradient</code></li></ul><h2 id="Define-the-network"><a href="#Define-the-network" class="headerlink" title="Define the network"></a>Define the network</h2><p>Let’s define this network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)  <span class="comment"># 6*6 from image dimension </span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))  (fc1): Linear(in_features=576, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>You just have to define the <code>forward</code> function, and the <code>backward</code><br>function (where gradients are computed) is automatically defined for you<br>using <code>autograd</code>.<br>You can use any of the Tensor operations in the <code>forward</code> function.</p><p>The learnable parameters of a model are returned by <code>net.parameters()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's .weight</span></span><br></pre></td></tr></table></figure><pre><code>10torch.Size([6, 1, 3, 3])</code></pre><p>Let’s try a random 32x32 input.<br>Note: expected input size of this net (LeNet) is 32x32. To use this net on<br>the MNIST dataset, please resize the images from the dataset to 32x32.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.0416,  0.0926, -0.0761, -0.0135, -0.0745, -0.0158,  0.0696, -0.0040,         -0.0099, -0.1799]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>Zero the gradient buffers of all parameters and backprops with random<br>gradients:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><div class="alert alert-info">    <h4>Note</h4><pre><code>torch.nn only supports mini-batches. The entire torch.nnpackage only supports inputs that are a mini-batch of samples, and nota single sample.For example, nn.Conv2d will take in a 4D Tensor ofnSamples x nChannels x Height x Width.If you have a single sample, just use input.unsqueeze(0) to adda fake batch dimension.</code></pre></div><p>Before proceeding further, let’s recap all the classes you’ve seen so far.</p><p><strong>Recap:</strong></p><ul><li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd<br>operations like <code>backward()</code>. Also <em>holds the gradient</em> w.r.t. the<br>tensor.</li><li><code>nn.Module</code> - Neural network module. <em>Convenient way of<br>encapsulating parameters</em>, with helpers for moving them to GPU,<br>exporting, loading, etc.</li><li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically<br>registered as a parameter when assigned as an attribute to a</em><br><code>Module</code>.</li><li><code>autograd.Function</code> - Implements <em>forward and backward definitions<br>of an autograd operation</em>. Every <code>Tensor</code> operation creates at<br>least a single <code>Function</code> node that connects to functions that<br>created a <code>Tensor</code> and <em>encodes its history</em>.</li></ul><p><strong>At this point, we covered:</strong></p><ul><li>Defining a neural network</li><li>Processing inputs and calling backward</li></ul><p><strong>Still Left:</strong></p><ul><li>Computing the loss</li><li>Updating the weights of the network</li></ul><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>A loss function takes the (output, target) pair of inputs, and computes a<br>value that estimates how far away the output is from the target.</p><p>There are several different<br><code>loss functions &lt;https://pytorch.org/docs/nn.html#loss-functions&gt;</code>_ under the<br>nn package .<br>A simple loss is: <code>nn.MSELoss</code> which computes the mean-squared error<br>between the input and the target.</p><p>For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(0.9128, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>Now, if you follow <code>loss</code> in the backward direction, using its<br><code>.grad_fn</code> attribute, you will see a graph of computations that looks<br>like this:</p><p>::</p><pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear      -&gt; MSELoss      -&gt; loss</code></pre><p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated<br>w.r.t. the loss, and all Tensors in the graph that has <code>requires_grad=True</code><br>will have their <code>.grad</code> Tensor accumulated with the gradient.</p><p>For illustration, let us follow a few steps backward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure><pre><code>&lt;MseLossBackward object at 0x000001F109346C88&gt;&lt;AddmmBackward object at 0x000001F109346EB8&gt;&lt;AccumulateGrad object at 0x000001F109346C88&gt;</code></pre><h2 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a>Backprop</h2><p>To backpropagate the error all we have to do is to <code>loss.backward()</code>.<br>You need to clear the existing gradients though, else gradients will be<br>accumulated to existing gradients.</p><p>Now we shall call <code>loss.backward()</code>, and have a look at conv1’s bias<br>gradients before and after the backward.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><pre><code>conv1.bias.grad before backwardtensor([0., 0., 0., 0., 0., 0.])conv1.bias.grad after backwardtensor([-0.0032, -0.0131,  0.0148,  0.0334, -0.0327, -0.0073])</code></pre><p>Now, we have seen how to use loss functions.</p><p><strong>Read Later:</strong></p><p>  The neural network package contains various modules and loss functions<br>  that form the building blocks of deep neural networks. A full list with<br>  documentation is <code>here &lt;https://pytorch.org/docs/nn&gt;</code>_.</p><p><strong>The only thing left to learn is:</strong></p><ul><li>Updating the weights of the network</li></ul><h2 id="Update-the-weights"><a href="#Update-the-weights" class="headerlink" title="Update the weights"></a>Update the weights</h2><p>The simplest update rule used in practice is the Stochastic Gradient<br>Descent (SGD):</p><pre><code>``weight = weight - learning_rate * gradient``</code></pre><p>We can implement this using simple Python code:</p><p>.. code:: python</p><pre><code>learning_rate = 0.01for f in net.parameters():    f.data.sub_(f.grad.data * learning_rate)</code></pre><p>However, as you use neural networks, you want to use various different<br>update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.<br>To enable this, we built a small package: <code>torch.optim</code> that<br>implements all these methods. Using it is very simple:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure><p>.. Note::</p><pre><code>Observe how gradient buffers had to be manually set to zero using``optimizer.zero_grad()``. This is because gradients are accumulatedas explained in the `Backprop`_ section.</code></pre><h2 id="我不认识的单词"><a href="#我不认识的单词" class="headerlink" title="我不认识的单词"></a>我不认识的单词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">feed-forward:前向</span><br><span class="line">glimpse:一瞥</span><br><span class="line">proce:进行</span><br><span class="line">recap:回顾</span><br><span class="line">encapsulat:封装</span><br><span class="line">assign:分配</span><br><span class="line">For illustration:为了说明</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Learning-autograd:&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Learning" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Learning-autograd</title>
    <link href="http://yoursite.com/2020/07/23/Pytorch-Learning-autograd/"/>
    <id>http://yoursite.com/2020/07/23/Pytorch-Learning-autograd/</id>
    <published>2020-07-23T08:03:14.000Z</published>
    <updated>2020-07-23T13:40:22.453Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Learning-autograd:</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Autograd-Automatic-Differentiation"><a href="#Autograd-Automatic-Differentiation" class="headerlink" title="Autograd: Automatic Differentiation"></a>Autograd: Automatic Differentiation</h1><p>Central to all neural networks in PyTorch is the <code>autograd</code> package.<br>Let’s first briefly visit this, and we will then go to training our<br>first neural network.</p><p>The <code>autograd</code> package provides automatic differentiation for all operations<br>on Tensors. It is a define-by-run framework, which means that your backprop is<br>defined by how your code is run, and that every single iteration can be<br>different.</p><p>Let us see this in more simple terms with some examples.</p><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><code>torch.Tensor</code> is the central class of the package. If you set its attribute<br><code>.requires_grad</code> as <code>True</code>, it starts to track all operations on it. When<br>you finish your computation you can call <code>.backward()</code> and have all the<br>gradients computed automatically. The gradient for this tensor will be<br>accumulated into <code>.grad</code> attribute.</p><p>To stop a tensor from tracking history, you can call <code>.detach()</code> to detach<br>it from the computation history, and to prevent future computation from being<br>tracked.</p><p>To prevent tracking history (and using memory), you can also wrap the code block<br>in <code>with torch.no_grad():</code>. This can be particularly helpful when evaluating a<br>model because the model may have trainable parameters with<br><code>requires_grad=True</code>, but for which we don’t need the gradients.</p><p>There’s one more class which is very important for autograd<br>implementation - a <code>Function</code>.</p><p><code>Tensor</code> and <code>Function</code> are interconnected and build up an acyclic<br>graph, that encodes a complete history of computation. Each tensor has<br>a <code>.grad_fn</code> attribute that references a <code>Function</code> that has created<br>the <code>Tensor</code> (except for Tensors created by the user - their<br><code>grad_fn is None</code>).</p><p>If you want to compute the derivatives, you can call <code>.backward()</code> on<br>a <code>Tensor</code>. If <code>Tensor</code> is a scalar (i.e. it holds a one element<br>data), you don’t need to specify any arguments to <code>backward()</code>,<br>however if it has more elements, you need to specify a <code>gradient</code><br>argument that is a tensor of matching shape.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>Create a tensor and set <code>requires_grad=True</code> to track computation with it</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1.],        [1., 1.]], requires_grad=True)</code></pre><p>Do a tensor operation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>tensor([[3., 3.],        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p><code>y</code> was created as a result of an operation, so it has a <code>grad_fn</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure><pre><code>&lt;AddBackward0 object at 0x0000015370CAB438&gt;</code></pre><p>Do more operations on <code>y</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure><pre><code>tensor([[27., 27.],        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><p><code>.requires_grad_( ... )</code> changes an existing Tensor’s <code>requires_grad</code><br>flag in-place. The input flag defaults to <code>False</code> if not given.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure><pre><code>FalseTrue&lt;SumBackward0 object at 0x000001536D5A24E0&gt;</code></pre><h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>Let’s backprop now.<br>Because <code>out</code> contains a single scalar, <code>out.backward()</code> is<br>equivalent to <code>out.backward(torch.tensor(1.))</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure><p>Print gradients d(out)/dx</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor([[4.5000, 4.5000],        [4.5000, 4.5000]])</code></pre><p>You should have got a matrix of <code>4.5</code>. Let’s call the <code>out</code><br><em>Tensor</em> “$o$”.<br>We have that $o = \frac{1}{4}\sum_i z_i$,<br>$z_i = 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$.<br>Therefore,<br>$\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)$, hence<br>$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$.</p><p>Mathematically, if you have a vector valued function $\vec{y}=f(\vec{x})$,<br>then the gradient of $\vec{y}$ with respect to $\vec{x}$<br>is a Jacobian matrix:</p><p>\begin{align}J=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\<br>   \vdots &amp; \ddots &amp; \vdots\<br>   \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p><p>Generally speaking, <code>torch.autograd</code> is an engine for computing<br>vector-Jacobian product. That is, given any vector<br>$v=\left(\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m}\end{array}\right)^{T}$,<br>compute the product $v^{T}\cdot J$. If $v$ happens to be<br>the gradient of a scalar function $l=g\left(\vec{y}\right)$,<br>that is,<br>$v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$,<br>then by the chain rule, the vector-Jacobian product would be the<br>gradient of $l$ with respect to $\vec{x}$:</p><p>\begin{align}J^{T}\cdot v=\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}\<br>   \vdots &amp; \ddots &amp; \vdots\<br>   \frac{\partial y_{1}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\left(\begin{array}{c}<br>   \frac{\partial l}{\partial y_{1}}\<br>   \vdots\<br>   \frac{\partial l}{\partial y_{m}}<br>   \end{array}\right)=\left(\begin{array}{c}<br>   \frac{\partial l}{\partial x_{1}}\<br>   \vdots\<br>   \frac{\partial l}{\partial x_{n}}<br>   \end{array}\right)\end{align}</p><p>(Note that $v^{T}\cdot J$ gives a row vector which can be<br>treated as a column vector by taking $J^{T}\cdot v$.)</p><p>This characteristic of vector-Jacobian product makes it very<br>convenient to feed external gradients into a model that has<br>non-scalar output.</p><p>Now let’s take a look at an example of vector-Jacobian product:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>tensor([849.9269, 245.4694, 633.8903], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>Now in this case <code>y</code> is no longer a scalar. <code>torch.autograd</code><br>could not compute the full Jacobian directly, but if we just<br>want the vector-Jacobian product, simply pass the vector to<br><code>backward</code> as argument:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])</code></pre><p>You can also stop autograd from tracking history on Tensors<br>with <code>.requires_grad=True</code> either by wrapping the code block in<br><code>with torch.no_grad():</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure><pre><code>TrueTrueFalse</code></pre><p>Or by using <code>.detach()</code> to get a new Tensor with the same<br>content but that does not require gradients:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">y = x.detach()</span><br><span class="line">print(y.requires_grad)</span><br><span class="line">print(x.eq(y).all())</span><br></pre></td></tr></table></figure><pre><code>TrueFalsetensor(1, dtype=torch.uint8)</code></pre><p><strong>Read Later:</strong></p><p>Document about <code>autograd.Function</code> is at<br><a href="https://pytorch.org/docs/stable/autograd.html#function" target="_blank" rel="noopener">https://pytorch.org/docs/stable/autograd.html#function</a></p><h2 id="不认识的单词"><a href="#不认识的单词" class="headerlink" title="不认识的单词"></a>不认识的单词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Automatic:自动的</span><br><span class="line">briefly:短暂的</span><br><span class="line">track:跟踪</span><br><span class="line">specify:指明</span><br><span class="line">accumulate:累加</span><br><span class="line">particularly:尤其,格外的</span><br><span class="line">interconnect:互相连接</span><br><span class="line">acyclic:非循环的,无环的</span><br><span class="line">Mathematically:数学上</span><br><span class="line">Jacobian:雅可比</span><br><span class="line">scalar:标量</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Learning-autograd:&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Learning" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-Learning-tensor</title>
    <link href="http://yoursite.com/2020/07/23/Pytorch-Learning-tensor/"/>
    <id>http://yoursite.com/2020/07/23/Pytorch-Learning-tensor/</id>
    <published>2020-07-23T08:00:23.000Z</published>
    <updated>2020-07-23T13:40:41.997Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch-Learning-tensor:</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="What-is-PyTorch"><a href="#What-is-PyTorch" class="headerlink" title="What is PyTorch?"></a>What is PyTorch?</h1><p>It’s a Python-based scientific computing package targeted at two sets of<br>audiences:</p><ul><li>A replacement for NumPy to use the power of GPUs</li><li>a deep learning research platform that provides maximum flexibility<br>and speed</li></ul><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>Tensors</p><p>Tensors are similar to NumPy’s ndarrays, with the addition being that<br>Tensors can also be used on a GPU to accelerate computing.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><div class="alert alert-info"><h4>Note</h4><p>An uninitialized matrix is declared,    but does not contain definite known    values before it is used. When an    uninitialized matrix is created,    whatever values were in the allocated    memory at the time will appear as the initial values.</p></div><p>Construct a 5x3 matrix, uninitialized:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([[-3.4374e-14,  7.1046e-43, -3.4374e-14],        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],        [-3.4374e-14,  7.1046e-43, -3.4374e-14],        [ 7.1046e-43, -3.4374e-14,  7.1046e-43],        [-3.4374e-14,  7.1046e-43, -3.4374e-14]])</code></pre><p>Construct a randomly initialized matrix:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.6385, 0.8264, 0.0737],        [0.1567, 0.5029, 0.7141],        [0.8297, 0.3453, 0.2860],        [0.0158, 0.3826, 0.7823],        [0.3434, 0.0977, 0.1530]])</code></pre><p>Construct a matrix filled zeros and of dtype long:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]])</code></pre><p>Construct a tensor directly from data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([5.5000, 3.0000])</code></pre><p>or create a tensor based on an existing tensor. These methods<br>will reuse properties of the input tensor, e.g. dtype, unless<br>new values are provided by user</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)      <span class="comment"># new_* methods take in sizes</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)    <span class="comment"># override dtype!</span></span><br><span class="line">print(x)                                      <span class="comment"># result has the same size</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]], dtype=torch.float64)tensor([[ 2.0072,  0.0294,  0.1776],        [-0.3961, -1.7436, -0.1741],        [ 0.7820,  0.5535, -0.0059],        [-1.9826, -0.7387, -0.3942],        [ 0.3501,  0.5796, -1.3633]])</code></pre><p>Get its size:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><pre><code>torch.Size([5, 3])</code></pre><div class="alert alert-info"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div><p>Operations</p><p>There are multiple syntaxes for operations. In the following<br>example, we will take a look at the addition operation.</p><p>Addition: syntax 1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],        [-1.7379e-01, -9.1149e-01,  7.2974e-01],        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],        [-1.4469e+00, -1.0496e-01, -1.7707e-03],        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: syntax 2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],        [-1.7379e-01, -9.1149e-01,  7.2974e-01],        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],        [-1.4469e+00, -1.0496e-01, -1.7707e-03],        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: providing an output tensor as argument</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],        [-1.7379e-01, -9.1149e-01,  7.2974e-01],        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],        [-1.4469e+00, -1.0496e-01, -1.7707e-03],        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><p>Addition: in-place</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 2.6168e+00,  9.4984e-01,  4.0212e-01],        [-1.7379e-01, -9.1149e-01,  7.2974e-01],        [ 7.8210e-01,  1.0687e+00,  6.7449e-01],        [-1.4469e+00, -1.0496e-01, -1.7707e-03],        [ 7.3285e-01,  1.0422e+00, -1.0675e+00]])</code></pre><div class="alert alert-info"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div><p>You can use standard NumPy-like indexing with all bells and whistles!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>tensor([ 0.0294, -1.7436,  0.5535, -0.7387,  0.5796])</code></pre><p>Resizing: If you want to resize/reshape tensor, you can use <code>torch.view</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure><pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>If you have a one element tensor, use <code>.item()</code> to get the value as a<br>Python number</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure><pre><code>tensor([1.0191])1.0191349983215332</code></pre><p><strong>Read later:</strong></p><p>  100+ Tensor operations, including transposing, indexing, slicing,<br>  mathematical operations, linear algebra, random numbers, etc.,<br>  are described<br>  <code>here &lt;https://pytorch.org/docs/torch&gt;</code>_.</p><h2 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h2><p>Converting a Torch Tensor to a NumPy array and vice versa is a breeze.</p><p>The Torch Tensor and NumPy array will share their underlying memory<br>locations (if the Torch Tensor is on CPU), and changing one will change<br>the other.</p><p>Converting a Torch Tensor to a NumPy Array</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><pre><code>tensor([1., 1., 1., 1., 1.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><pre><code>[1. 1. 1. 1. 1.]</code></pre><p>See how the numpy array changed in value.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><pre><code>tensor([2., 2., 2., 2., 2.])[2. 2. 2. 2. 2.]</code></pre><p>Converting NumPy Array to Torch Tensor</p><p>See how changing the np array changed the Torch Tensor automatically</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><pre><code>[2. 2. 2. 2. 2.]tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><p>All the Tensors on the CPU except a CharTensor support converting to<br>NumPy and back.</p><h2 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h2><p>Tensors can be moved onto any device using the <code>.to</code> method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="comment"># We will use ``torch.device`` objects to move tensors in and out of GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># a CUDA device object</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` can also change dtype together!</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">allocate:分配</span><br><span class="line">mutate:变异</span><br><span class="line">Bridge:桥</span><br><span class="line">vice versa:反之亦然</span><br><span class="line">underlying:底层的</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch-Learning-tensor:&lt;/p&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch1.5.1官网教程" scheme="http://yoursite.com/tags/pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B/"/>
    
      <category term="Pytorch1.5.1官网教程-Learning" scheme="http://yoursite.com/tags/Pytorch1-5-1%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战-泰坦尼克号获救预测</title>
    <link href="http://yoursite.com/2020/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B/"/>
    <id>http://yoursite.com/2020/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B/</id>
    <published>2020-07-15T02:43:32.000Z</published>
    <updated>2020-07-15T02:46:47.711Z</updated>
    
    <content type="html"><![CDATA[<p>数据集:<a href="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/titanic_train.csv" target="_blank" rel="noopener">titanic_train.csv</a></p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">titianic = pandas.read_csv(<span class="string">'../data/titanic_train.csv'</span>)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>male</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>female</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>C</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>female</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1</td>      <td>1</td>      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>      <td>female</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>113803</td>      <td>53.1000</td>      <td>C123</td>      <td>S</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>0</td>      <td>3</td>      <td>Allen, Mr. William Henry</td>      <td>male</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>373450</td>      <td>8.0500</td>      <td>NaN</td>      <td>S</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titianic[<span class="string">'Age'</span>]=titianic[<span class="string">'Age'</span>].fillna(titianic[<span class="string">'Age'</span>].median())</span><br><span class="line">print(titianic.describe())</span><br></pre></td></tr></table></figure><pre><code>       PassengerId    Survived      Pclass         Age       SibSp  \count   891.000000  891.000000  891.000000  891.000000  891.000000   mean    446.000000    0.383838    2.308642   29.361582    0.523008   std     257.353842    0.486592    0.836071   13.019697    1.102743   min       1.000000    0.000000    1.000000    0.420000    0.000000   25%     223.500000    0.000000    2.000000   22.000000    0.000000   50%     446.000000    0.000000    3.000000   28.000000    0.000000   75%     668.500000    1.000000    3.000000   35.000000    1.000000   max     891.000000    1.000000    3.000000   80.000000    8.000000               Parch        Fare  count  891.000000  891.000000  mean     0.381594   32.204208  std      0.806057   49.693429  min      0.000000    0.000000  25%      0.000000    7.910400  50%      0.000000   14.454200  75%      0.000000   31.000000  max      6.000000  512.329200  </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>male</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>female</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>C</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>female</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1</td>      <td>1</td>      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>      <td>female</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>113803</td>      <td>53.1000</td>      <td>C123</td>      <td>S</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>0</td>      <td>3</td>      <td>Allen, Mr. William Henry</td>      <td>male</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>373450</td>      <td>8.0500</td>      <td>NaN</td>      <td>S</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(titianic[<span class="string">'Sex'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Sex'</span>] = titianic[<span class="string">'Sex'</span>].map(&#123;<span class="string">'male'</span>:<span class="number">0</span>,<span class="string">'female'</span>:<span class="number">1</span>&#125;)</span><br><span class="line"><span class="comment"># 把male变成0，把female变成1</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "male", "Sex"] = 0</span></span><br><span class="line"><span class="comment"># titanic.loc[titanic["Sex"] == "female", "Sex"] = 1</span></span><br></pre></td></tr></table></figure><pre><code>[&apos;male&apos; &apos;female&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>0</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>1</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>C</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>1</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1</td>      <td>1</td>      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>      <td>1</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>113803</td>      <td>53.1000</td>      <td>C123</td>      <td>S</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>0</td>      <td>3</td>      <td>Allen, Mr. William Henry</td>      <td>0</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>373450</td>      <td>8.0500</td>      <td>NaN</td>      <td>S</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(titianic[<span class="string">'Embarked'</span>].unique())</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>)</span><br><span class="line">titianic[<span class="string">'Embarked'</span>]=titianic[<span class="string">'Embarked'</span>].map(&#123;<span class="string">'S'</span>:<span class="number">0</span>,<span class="string">'C'</span>:<span class="number">1</span>,<span class="string">'Q'</span>:<span class="number">2</span>&#125;)</span><br><span class="line">titianic[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><pre><code>[&apos;S&apos; &apos;C&apos; &apos;Q&apos; nan]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>0</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>1</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>1</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1</td>      <td>1</td>      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>      <td>1</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>113803</td>      <td>53.1000</td>      <td>C123</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>0</td>      <td>3</td>      <td>Allen, Mr. William Henry</td>      <td>0</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>373450</td>      <td>8.0500</td>      <td>NaN</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">predictors = [<span class="string">'Pclass'</span>,<span class="string">'Sex'</span>,<span class="string">'Age'</span>,<span class="string">'SibSp'</span>,<span class="string">'Parch'</span>,<span class="string">'Fare'</span>,<span class="string">'Embarked'</span>]</span><br><span class="line">x_data = titianic[predictors]</span><br><span class="line">y_data = titianic[<span class="string">'Survived'</span>]</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">x_data = scaler.fit_transform(x_data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">LR = LogisticRegression()</span><br><span class="line">scores = model_selection.cross_val_score(LR,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>0.7901234567901234</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">20</span>,<span class="number">10</span>),max_iter=<span class="number">1000</span>)</span><br><span class="line">scores = model_selection.cross_val_score(mlp,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)0.8002244668911335</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line">knn = neighbors.KNeighborsClassifier(<span class="number">21</span>)</span><br><span class="line">scores=model_selection.cross_val_score(knn,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>0.8125701459034792</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">dtree = tree.DecisionTreeClassifier(max_depth=<span class="number">5</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(dtree,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>0.8080808080808081</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">RF1 = RandomForestClassifier(random_state=<span class="number">1</span>,n_estimators=<span class="number">10</span>,min_samples_split=<span class="number">2</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF1,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>0.7991021324354657</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RF2 = RandomForestClassifier(n_estimators=<span class="number">100</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">scores = model_selection.cross_val_score(RF2,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>0.8125701459034792</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line">bagging_clf = BaggingClassifier(RF2,n_estimators=<span class="number">20</span>)</span><br><span class="line">scores=model_selection.cross_val_score(bagging_clf,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>0.819304152637486</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">adaboost = AdaBoostClassifier(bagging_clf,n_estimators=<span class="number">10</span>)</span><br><span class="line">scores=model_selection.cross_val_score(adaboost,x_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>0.8181818181818182</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line">sclf = StackingClassifier(classifiers=[bagging_clf,mlp,LR],</span><br><span class="line">                         meta_classifier=LogisticRegression())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)0.819304152637486</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sclf2 = VotingClassifier([(<span class="string">'adaboost'</span>,adaboost),(<span class="string">'mlp'</span>,mlp),(<span class="string">'LR'</span>,LR),(<span class="string">'knn'</span>,knn),(<span class="string">'dtree'</span>,dtree)])</span><br><span class="line"></span><br><span class="line">scores = model_selection.cross_val_score(sclf2,x_data,y_data,y_data,cv=<span class="number">3</span>)</span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)c:\users\18025\appdata\local\programs\python\python37\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)0.8159371492704826</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据集:&lt;a href=&quot;http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/data/titanic_train.csv&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;titanic_train.csv&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习实战" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>opencv中的图像处理5</title>
    <link href="http://yoursite.com/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%865/"/>
    <id>http://yoursite.com/2020/07/14/opencv%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%865/</id>
    <published>2020-07-14T04:03:19.000Z</published>
    <updated>2020-07-14T04:09:17.202Z</updated>
    
    <content type="html"><![CDATA[<ul><li>11.<a href="#header1">傅里叶变换</a></li><li>12.<a href="#header2">模板匹配</a></li><li>13.<a href="#header3">霍夫线变换</a></li><li>14.<a href="#header4">霍夫圈变换</a></li><li>15.<a href="#header5">图像分割与Watershed算法</a></li><li>16.<a href="#header6">交互式前景提取使用GrabCut算法</a><a id="more"></a></li></ul><h1 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a><span id="header1">傅里叶变换</span></h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>在本节中，我们将学习 </p><ul><li>使用OpenCV查找图像的傅立叶变换 </li><li>利用Numpy中可用的FFT函数 </li><li>傅立叶变换的某些应用程序 </li><li>我们将看到以下函数：cv.dft()，cv.idft()等</li></ul><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>傅立叶变换用于分析各种滤波器的频率特性。</p><p>对于图像，使用<strong>2D离散傅里叶变换</strong>(DFT)查找频域。</p><p>一种称为<strong>快速傅立叶变换</strong>(FFT)的快速算法用于DFT的计算。</p><p>关于这些的详细信息可以在任何图像处理或信号处理教科书中找到。请参阅其他资源部分。</p><p>对于正弦信号x(t)=Asin(2πft)，我们可以说f是信号的频率，如果采用其频域，则可以看到f的尖峰。</p><p>如果对信号进行采样以形成离散信号，我们将获得相同的频域，但是在[−π，π]或[0,2π]范围内（对于N点DFT为[0，N]）是周期性的。</p><p>您可以将图像视为在两个方向上采样的信号。因此，在X和Y方向都进行傅立叶变换，可以得到图像的频率表示。</p><p>更直观地说，对于正弦信号，如果幅度在短时间内变化如此之快，则可以说它是高频信号。</p><p>如果变化缓慢，则为低频信号。您可以将相同的想法扩展到图像。</p><p>图像中的振幅在哪里急剧变化？在边缘点或噪声。因此，可以说边缘和噪声是图像中的高频内容。</p><p>如果幅度没有太大变化，则它是低频分量。（一些链接已添加到“其他资源”，其中通过示例直观地说明了频率变换）。</p><p>现在，我们将看到如何找到傅立叶变换。</p><h2 id="Numpy中的傅里叶变换"><a href="#Numpy中的傅里叶变换" class="headerlink" title="Numpy中的傅里叶变换"></a>Numpy中的傅里叶变换</h2><p>首先，我们将看到如何使用Numpy查找傅立叶变换。</p><p>Numpy具有FFT软件包来执行此操作。np.fft.fft2()为我们提供了频率转换，它将是一个复杂的数组。</p><ul><li>它的第一个参数是输入图像，即灰度图像。</li><li>第二个参数是可选的，它决定输出数组的大小。如果它大于输入图像的大小，则在计算FFT之前用零填充输入图像。如果小于输入图像，将裁切输入图像。如果未传递任何参数，则输出数组的大小将与输入的大小相同。</li></ul><p>现在，一旦获得结果，零频率分量（DC分量）将位于左上角。</p><p>如果要使其居中，则需要在两个方向上将结果都移动N2。</p><p>只需通过函数<strong>np.fft.fftshift</strong>()即可完成。（它更容易分析）。找到频率变换后，就可以找到幅度谱。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">f &#x3D; np.fft.fft2(img)</span><br><span class="line">fshift &#x3D; np.fft.fftshift(f)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(np.abs(fshift))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Result look like below:<br>结果看起来像下面这样:<br><img src="http://qiniu.aihubs.net/fft1.jpg" alt></p><p>看，您可以在中心看到更多白色区域，这表明低频内容更多。</p><p>因此，您发现了频率变换现在，您可以在频域中进行一些操作，例如高通滤波和重建图像，即找到逆DFT。</p><p>为此，您只需用尺寸为60x60的矩形窗口遮罩即可消除低频。</p><p>然后，使用<strong>np.fft.ifftshift</strong>()应用反向移位，以使DC分量再次出现在左上角。</p><p>然后使用<strong>np.ifft2</strong>()函数找到逆FFT。同样，结果将是一个复数。您可以采用其绝对值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;&#x2F;2 , cols&#x2F;&#x2F;2</span><br><span class="line">fshift[crow-30:crow+31, ccol-30:ccol+31] &#x3D; 0</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; np.fft.ifft2(f_ishift)</span><br><span class="line">img_back &#x3D; np.real(img_back)</span><br><span class="line">plt.subplot(131),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(132),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Image after HPF&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(133),plt.imshow(img_back)</span><br><span class="line">plt.title(&#39;Result in JET&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果看起来像下面这样：<br><img src="http://qiniu.aihubs.net/fft2.jpg" alt></p><p>结果表明高通滤波是边缘检测操作。</p><p>这就是我们在“图像渐变”一章中看到的。</p><p>这也表明大多数图像数据都存在于频谱的低频区域。</p><p>无论如何，我们已经看到了如何在Numpy中找到DFT，IDFT等。</p><p>现在，让我们看看如何在OpenCV中进行操作。 </p><p>如果您仔细观察结果，尤其是最后一张JET颜色的图像，您会看到一些伪像（我用红色箭头标记的一个实例）。</p><p>它在那里显示出一些波纹状结构，称为<strong>振铃效应</strong>。</p><p>这是由我们用于遮罩的矩形窗口引起的。此掩码转换为正弦形状，从而导致此问题。</p><p>因此，矩形窗口不用于过滤。更好的选择是高斯窗口。</p><h2 id="OpenCV中的傅里叶变换"><a href="#OpenCV中的傅里叶变换" class="headerlink" title="OpenCV中的傅里叶变换"></a>OpenCV中的傅里叶变换</h2><p>OpenCV为此提供了<strong>cv.dft</strong>()和<strong>cv.idft</strong>()函数。它返回与前一个相同的结果，但是有两个通道。</p><p>第一个通道是结果的实部，第二个通道是结果的虚部。输入图像首先应转换为np.float32。我们来看看怎么做。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">dft &#x3D; cv.dft(np.float32(img),flags &#x3D; cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">dft_shift &#x3D; np.fft.fftshift(dft)</span><br><span class="line">magnitude_spectrum &#x3D; 20*np.log(cv.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(magnitude_spectrum, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>注意 您还可以使用<strong>cv.cartToPolar</strong>()，它在单个镜头中同时返回幅值和相位</p><p>现在我们要做DFT的逆变换。在上一节中，我们创建了一个HPF，这次我们将看到如何删除图像中的高频内容，即我们将LPF应用到图像中。</p><p>它实际上模糊了图像。为此，我们首先创建一个高值(1)在低频部分，即我们过滤低频内容，0在高频区。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">rows, cols &#x3D; img.shape</span><br><span class="line">crow,ccol &#x3D; rows&#x2F;2 , cols&#x2F;2</span><br><span class="line"># 首先创建一个掩码，中心正方形为1，其余全为零</span><br><span class="line">mask &#x3D; np.zeros((rows,cols,2),np.uint8)</span><br><span class="line">mask[crow-30:crow+30, ccol-30:ccol+30] &#x3D; 1</span><br><span class="line"># 应用掩码和逆DFT</span><br><span class="line">fshift &#x3D; dft_shift*mask</span><br><span class="line">f_ishift &#x3D; np.fft.ifftshift(fshift)</span><br><span class="line">img_back &#x3D; cv.idft(f_ishift)</span><br><span class="line">img_back &#x3D; cv.magnitude(img_back[:,:,0],img_back[:,:,1])</span><br><span class="line">plt.subplot(121),plt.imshow(img, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Input Image&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(122),plt.imshow(img_back, cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">plt.title(&#39;Magnitude Spectrum&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://qiniu.aihubs.net/fft4.jpg" alt></p><p>注意 通常，OpenCV函数<strong>cv.dft</strong>()和<strong>cv.idft</strong>()比Numpy函数更快。但是Numpy函数更容易使用。有关性能问题的更多细节，请参见下面的部分。</p><h2 id="DFT的性能优化"><a href="#DFT的性能优化" class="headerlink" title="DFT的性能优化"></a>DFT的性能优化</h2><p>对于某些数组尺寸，DFT的计算性能较好。当数组大小为2的幂时，速度最快。</p><p>对于大小为2、3和5的乘积的数组，也可以非常有效地进行处理。</p><p>因此，如果您担心代码的性能，可以在找到DFT之前将数组的大小修改为任何最佳大小(通过填充零)。对</p><p>于OpenCV，您必须手动填充零。但是对于Numpy，您指定FFT计算的新大小，它将自动为您填充零。</p><p>那么如何找到最优的大小呢?OpenCV为此提供了一个函数，cv.getOptimalDFTSize()。它同时适用于<strong>cv.dft</strong>()和<strong>np.fft.fft2</strong>()。让我们使用IPython魔术命令timeit来检查它们的性能。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [16]: img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">In [17]: rows,cols &#x3D; img.shape</span><br><span class="line">In [18]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(rows,cols))</span><br><span class="line">342 548</span><br><span class="line">In [19]: nrows &#x3D; cv.getOptimalDFTSize(rows)</span><br><span class="line">In [20]: ncols &#x3D; cv.getOptimalDFTSize(cols)</span><br><span class="line">In [21]: print(&quot;&#123;&#125; &#123;&#125;&quot;.format(nrows,ncols))</span><br><span class="line">360 576</span><br></pre></td></tr></table></figure><p>参见，将大小(342,548)修改为(360，576)。现在让我们用零填充（对于OpenCV），并找到其DFT计算性能。您可以通过创建一个新的零数组并将数据复制到其中来完成此操作，或者使用<strong>cv.copyMakeBorder</strong>()。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nimg &#x3D; np.zeros((nrows,ncols))</span><br><span class="line">nimg[:rows,:cols] &#x3D; img</span><br></pre></td></tr></table></figure><p>或者:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">right &#x3D; ncols - cols</span><br><span class="line">bottom &#x3D; nrows - rows</span><br><span class="line">bordertype &#x3D; cv.BORDER_CONSTANT ＃只是为了避免PDF文件中的行中断</span><br><span class="line">nimg &#x3D; cv.copyMakeBorder(img,0,bottom,0,right,bordertype, value &#x3D; 0)</span><br></pre></td></tr></table></figure><p>现在，我们计算Numpy函数的DFT性能比较：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [22]: %timeit fft1 &#x3D; np.fft.fft2(img)</span><br><span class="line">10 loops, best of 3: 40.9 ms per loop</span><br><span class="line">In [23]: %timeit fft2 &#x3D; np.fft.fft2(img,[nrows,ncols])</span><br><span class="line">100 loops, best of 3: 10.4 ms per loop</span><br></pre></td></tr></table></figure><p>它显示了4倍的加速。现在，我们将尝试使用OpenCV函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [24]: %timeit dft1&#x3D; cv.dft(np.float32(img),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 13.5 ms per loop</span><br><span class="line">In [27]: %timeit dft2&#x3D; cv.dft(np.float32(nimg),flags&#x3D;cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">100 loops, best of 3: 3.11 ms per loop</span><br></pre></td></tr></table></figure><p>它还显示了4倍的加速。您还可以看到OpenCV函数比Numpy函数快3倍左右。也可以对逆FFT进行测试，这留给您练习。</p><h2 id="为什么拉普拉斯算子是高通滤波器？"><a href="#为什么拉普拉斯算子是高通滤波器？" class="headerlink" title="为什么拉普拉斯算子是高通滤波器？"></a>为什么拉普拉斯算子是高通滤波器？</h2><p>在一个论坛上也有人提出了类似的问题。问题是，为什么拉普拉斯变换是高通滤波器?</p><p>为什么Sobel是HPF?等。第一个答案是关于傅里叶变换的。对于更大的FFT只需要拉普拉斯变换。分析下面的代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"># 没有缩放参数的简单均值滤波器</span><br><span class="line">mean_filter &#x3D; np.ones((3,3))</span><br><span class="line"># 创建高斯滤波器</span><br><span class="line">x &#x3D; cv.getGaussianKernel(5,10)</span><br><span class="line">gaussian &#x3D; x*x.T</span><br><span class="line"># 不同的边缘检测滤波器</span><br><span class="line"># x方向上的scharr</span><br><span class="line">scharr &#x3D; np.array([[-3, 0, 3],</span><br><span class="line">                   [-10,0,10],</span><br><span class="line">                   [-3, 0, 3]])</span><br><span class="line"># x方向上的sobel</span><br><span class="line">sobel_x&#x3D; np.array([[-1, 0, 1],</span><br><span class="line">                   [-2, 0, 2],</span><br><span class="line">                   [-1, 0, 1]])</span><br><span class="line"># y方向上的sobel</span><br><span class="line">sobel_y&#x3D; np.array([[-1,-2,-1],</span><br><span class="line">                   [0, 0, 0],</span><br><span class="line">                   [1, 2, 1]])</span><br><span class="line"># 拉普拉斯变换</span><br><span class="line">laplacian&#x3D;np.array([[0, 1, 0],</span><br><span class="line">                    [1,-4, 1],</span><br><span class="line">                    [0, 1, 0]])</span><br><span class="line">filters &#x3D; [mean_filter, gaussian, laplacian, sobel_x, sobel_y, scharr]</span><br><span class="line">filter_name &#x3D; [&#39;mean_filter&#39;, &#39;gaussian&#39;,&#39;laplacian&#39;, &#39;sobel_x&#39;, \</span><br><span class="line">                &#39;sobel_y&#39;, &#39;scharr_x&#39;]</span><br><span class="line">fft_filters &#x3D; [np.fft.fft2(x) for x in filters]</span><br><span class="line">fft_shift &#x3D; [np.fft.fftshift(y) for y in fft_filters]</span><br><span class="line">mag_spectrum &#x3D; [np.log(np.abs(z)+1) for z in fft_shift]</span><br><span class="line">for i in range(6):</span><br><span class="line">    plt.subplot(2,3,i+1),plt.imshow(mag_spectrum[i],cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(filter_name[i]), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>看看结果：<br><img src="http://qiniu.aihubs.net/fft5.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="模板匹配"><a href="#模板匹配" class="headerlink" title="模板匹配"></a><span id="header2">模板匹配</span></h1><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>在本章中，您将学习 - 使用模板匹配在图像中查找对象 - 你将看到以下功能：cv.matchTemplate()，cv.minMaxLoc()</p><h2 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h2><p>模板匹配是一种用于在较大图像中搜索和查找模板图像位置的方法。</p><p>为此，OpenCV带有一个函数<strong>cv.matchTemplate</strong>()。 </p><p>它只是将模板图​​像滑动到输入图像上（就像在2D卷积中一样），然后在模板图像下比较模板和输入图像的拼图。 </p><p>OpenCV中实现了几种比较方法。（您可以检查文档以了解更多详细信息）。它返回一个灰度图像，其中每个像素表示该像素的邻域与模板匹配的程度。</p><p>如果输入图像的大小为(WxH)，而模板图像的大小为(wxh)，则输出图像的大小将为(W-w + 1，H-h + 1)。得到结果后，可以使用<strong>cv.minMaxLoc</strong>()函数查找最大/最小值在哪。将其作为矩形的左上角，并以(w，h)作为矩形的宽度和高度。该矩形是您模板的区域。</p><p>注意 如果使用<strong>cv.TM_SQDIFF</strong>作为比较方法，则最小值提供最佳匹配。</p><h2 id="OpenCV中的模板匹配"><a href="#OpenCV中的模板匹配" class="headerlink" title="OpenCV中的模板匹配"></a>OpenCV中的模板匹配</h2><p>作为示例，我们将在梅西的照片中搜索他的脸。所以我创建了一个模板，如下所示： <img src="http://qiniu.aihubs.net/messi_face.jpg" alt> 我们将尝试所有比较方法，以便我们可以看到它们的结果如何：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;,0)</span><br><span class="line">img2 &#x3D; img.copy()</span><br><span class="line">template &#x3D; cv.imread(&#39;template.jpg&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line"># 列表中所有的6种比较方法</span><br><span class="line">methods &#x3D; [&#39;cv.TM_CCOEFF&#39;, &#39;cv.TM_CCOEFF_NORMED&#39;, &#39;cv.TM_CCORR&#39;,</span><br><span class="line">            &#39;cv.TM_CCORR_NORMED&#39;, &#39;cv.TM_SQDIFF&#39;, &#39;cv.TM_SQDIFF_NORMED&#39;]</span><br><span class="line">for meth in methods:</span><br><span class="line">    img &#x3D; img2.copy()</span><br><span class="line">    method &#x3D; eval(meth)</span><br><span class="line">    # 应用模板匹配</span><br><span class="line">    res &#x3D; cv.matchTemplate(img,template,method)</span><br><span class="line">    min_val, max_val, min_loc, max_loc &#x3D; cv.minMaxLoc(res)</span><br><span class="line">    # 如果方法是TM_SQDIFF或TM_SQDIFF_NORMED，则取最小值</span><br><span class="line">    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:</span><br><span class="line">        top_left &#x3D; min_loc</span><br><span class="line">    else:</span><br><span class="line">        top_left &#x3D; max_loc</span><br><span class="line">    bottom_right &#x3D; (top_left[0] + w, top_left[1] + h)</span><br><span class="line">    cv.rectangle(img,top_left, bottom_right, 255, 2)</span><br><span class="line">    plt.subplot(121),plt.imshow(res,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Matching Result&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.subplot(122),plt.imshow(img,cmap &#x3D; &#39;gray&#39;)</span><br><span class="line">    plt.title(&#39;Detected Point&#39;), plt.xticks([]), plt.yticks([])</span><br><span class="line">    plt.suptitle(meth)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>查看以下结果：</p><ul><li>cv.TM_CCOEFF<img src="http://qiniu.aihubs.net/template_ccoeff_1.jpg" alt></li><li>cv.TM_CCOEFF_NORMED<img src="http://qiniu.aihubs.net/template_ccoeffn_2.jpg" alt></li><li>cv.TM_CCORR<img src="http://qiniu.aihubs.net/template_ccorr_3.jpg" alt></li><li>cv.TM_CCORR_NORMED<img src="http://qiniu.aihubs.net/template_ccorrn_4.jpg" alt></li><li>cv.TM_SQDIFF<img src="http://qiniu.aihubs.net/template_sqdiff_5.jpg" alt></li><li>cv.TM_SQDIFF_NORMED<img src="http://qiniu.aihubs.net/template_sqdiffn_6.jpg" alt><br>使用<strong>cv.TM_CCORR</strong>的结果并不理想。</li></ul><h2 id="多对象的模板匹配"><a href="#多对象的模板匹配" class="headerlink" title="多对象的模板匹配"></a>多对象的模板匹配</h2><p>在上一节中，我们在图像中搜索了梅西的脸，该脸在图像中仅出现一次。</p><p>假设您正在搜索具有多次出现的对象，则<strong>cv.minMaxLoc</strong>()不会为您提供所有位置。</p><p>在这种情况下，我们将使用阈值化。因此，在此示例中，我们将使用著名游戏<strong>Mario</strong>的屏幕截图，并在其中找到硬币。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img_rgb &#x3D; cv.imread(&#39;mario.png&#39;)</span><br><span class="line">img_gray &#x3D; cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)</span><br><span class="line">template &#x3D; cv.imread(&#39;mario_coin.png&#39;,0)</span><br><span class="line">w, h &#x3D; template.shape[::-1]</span><br><span class="line">res &#x3D; cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)</span><br><span class="line">threshold &#x3D; 0.8</span><br><span class="line">loc &#x3D; np.where( res &gt;&#x3D; threshold)</span><br><span class="line">for pt in zip(*loc[::-1]):</span><br><span class="line">    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)</span><br><span class="line">cv.imwrite(&#39;res.png&#39;,img_rgb)</span><br></pre></td></tr></table></figure><p>结果:<br><img src="http://qiniu.aihubs.net/res_mario.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="霍夫线变换"><a href="#霍夫线变换" class="headerlink" title="霍夫线变换"></a><span id="header3">霍夫线变换</span></h1><h2 id="目标-2"><a href="#目标-2" class="headerlink" title="目标"></a>目标</h2><p>在这一章当中， </p><ul><li>我们将了解霍夫变换的概念。 </li><li>我们将看到如何使用它来检测图像中的线条。 </li><li>我们将看到以下函数：cv.HoughLines()，cv.HoughLinesP()</li></ul><h2 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h2><p>如果可以用数学形式表示形状，则霍夫变换是一种检测任何形状的流行技术。</p><p>即使形状有些破损或变形，也可以检测出形状。我们将看到它如何作用于一条线。</p><p>一条线可以表示为y=mx+c或以参数形式表示为ρ=xcosθ+ysinθ，</p><p>其中ρ是从原点到该线的垂直距离，</p><p>而θ是由该垂直线和水平轴形成的角度以逆时针方向测量（该方向随您如何表示坐标系而变化。此表示形式在OpenCV中使用）。</p><p>查看下面的图片：<br><img src="http://qiniu.aihubs.net/1.png" alt></p><p>因此，如果线在原点下方通过，则它将具有正的ρ且角度小于180。如果线在原点上方，则将角度取为小于180，而不是大于180的角度。</p><p>ρ取负值。任何垂直线将具有0度，水平线将具有90度。</p><p>现在，让我们看一下霍夫变换如何处理线条。任何一条线都可以用(ρ，θ)这两个术语表示。</p><p>因此，首先创建2D数组或累加器（以保存两个参数的值），并将其初始设置为0。</p><p>让行表示ρ，列表示θ。阵列的大小取决于所需的精度。假设您希望角度的精度为1度，则需要180列。</p><p>对于ρ，最大距离可能是图像的对角线长度。因此，以一个像素精度为准，行数可以是图像的对角线长度。</p><p>考虑一个100x100的图像，中间有一条水平线。</p><p>取直线的第一点。您知道它的(x，y)值。</p><p>现在在线性方程式中，将值θ= 0,1,2，….. 180放进去，然后检查得到ρ。</p><p>对于每对(ρ，θ)，在累加器中对应的(ρ，θ)单元格将值增加1。所以现在在累加器中，单元格(50,90)= 1以及其他一些单元格。</p><p>现在，对行的第二个点。执行与上述相同的操作。递增(ρ，θ)对应的单元格中的值。</p><p>这次，单元格(50,90)=2。实际上，您正在对(ρ，θ)值进行投票。</p><p>您对线路上的每个点都继续执行此过程。</p><p>在每个点上，单元格(50,90)都会增加或投票，而其他单元格可能会或可能不会投票。</p><p>这样一来，最后，单元格(50,90)的投票数将最高。</p><p>因此，如果您在累加器中搜索最大票数，则将获得(50,90)值，该值表示该图像中的一条线与原点的距离为50，角度为90度。</p><p>在下面的动画中很好地显示了该图片(图片提供：Amos Storkey)<br><img src="http://qiniu.aihubs.net/houghlinesdemo.gif" alt></p><p>这就是霍夫变换对线条的工作方式。它很简单，也许您可​​以自己使用Numpy来实现它。</p><p>下图显示了累加器。某些位置的亮点表示它们是图像中可能的线条的参数。</p><p><img src="http://qiniu.aihubs.net/houghlines2.jpg" alt></p><h2 id="OpenCV中的霍夫曼变换"><a href="#OpenCV中的霍夫曼变换" class="headerlink" title="OpenCV中的霍夫曼变换"></a>OpenCV中的霍夫曼变换</h2><p>上面说明的所有内容都封装在OpenCV函数<strong>cv.HoughLines</strong>()中。</p><p>它只是返回一个：math:(rho，theta)值的数组。ρ以像素为单位，θ以弧度为单位。</p><p>第一个参数，输入图像应该是二进制图像，因此在应用霍夫变换之前，请应用阈值或使用Canny边缘检测。</p><p>第二和第三参数分别是ρ和θ精度。</p><p>第四个参数是阈值，这意味着应该将其视为行的最低投票。</p><p>请记住，票数取决于线上的点数。因此，它表示应检测到的最小线长。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLines(edges,1,np.pi&#x2F;180,200)</span><br><span class="line">for line in lines:</span><br><span class="line">    rho,theta &#x3D; line[0]</span><br><span class="line">    a &#x3D; np.cos(theta)</span><br><span class="line">    b &#x3D; np.sin(theta)</span><br><span class="line">    x0 &#x3D; a*rho</span><br><span class="line">    y0 &#x3D; b*rho</span><br><span class="line">    x1 &#x3D; int(x0 + 1000*(-b))</span><br><span class="line">    y1 &#x3D; int(y0 + 1000*(a))</span><br><span class="line">    x2 &#x3D; int(x0 - 1000*(-b))</span><br><span class="line">    y2 &#x3D; int(y0 - 1000*(a))</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,0,255),2)</span><br><span class="line">cv.imwrite(&#39;houghlines3.jpg&#39;,img)</span><br></pre></td></tr></table></figure><p><img src="http://qiniu.aihubs.net/houghlines3.jpg" alt></p><h2 id="概率霍夫变换"><a href="#概率霍夫变换" class="headerlink" title="概率霍夫变换"></a>概率霍夫变换</h2><p>在霍夫变换中，您可以看到，即使对于带有两个参数的行，也需要大量计算。</p><p>概率霍夫变换是我们看到的霍夫变换的优化。它没有考虑所有要点。</p><p>取而代之的是，它仅采用随机的点子集，足以进行线检测。</p><p>只是我们必须降低阈值。参见下图，比较了霍夫空间中的霍夫变换和概率霍夫变换。<br><img src="http://qiniu.aihubs.net/houghlines4.png" alt></p><p>OpenCV的实现基于Matas,J.和Galambos,C.和Kittler, J.V.使用渐进概率霍夫变换对行进行的稳健检测[145]。</p><p>使用的函数是<strong>cv.HoughLinesP</strong>()。它有两个新的论点。 </p><ul><li>minLineLength - 最小行长。小于此长度的线段将被拒绝。 </li><li>maxLineGap - 线段之间允许将它们视为一条线的最大间隙。</li></ul><p>最好的是，它直接返回行的两个端点。在以前的情况下，您仅获得线的参数，并且必须找到所有点。在这里，一切都是直接而简单的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line">img &#x3D; cv.imread(cv.samples.findFile(&#39;sudoku.png&#39;))</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">edges &#x3D; cv.Canny(gray,50,150,apertureSize &#x3D; 3)</span><br><span class="line">lines &#x3D; cv.HoughLinesP(edges,1,np.pi&#x2F;180,100,minLineLength&#x3D;100,maxLineGap&#x3D;10)</span><br><span class="line">for line in lines:</span><br><span class="line">    x1,y1,x2,y2 &#x3D; line[0]</span><br><span class="line">    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)</span><br><span class="line">cv.imwrite(&#39;houghlines5.jpg&#39;,img)</span><br></pre></td></tr></table></figure><p>看到如下结果：<br><img src="http://qiniu.aihubs.net/houghlines5.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="霍夫圈变换"><a href="#霍夫圈变换" class="headerlink" title="霍夫圈变换"></a><span id="header4">霍夫圈变换</span></h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><p>在本章中， </p><ul><li>我们将学习使用霍夫变换来查找图像中的圆。 </li><li>我们将看到以下函数：cv.HoughCircles()</li></ul><p>理论<br>圆在数学上表示为$(x−x_{center})^2+(y−y_{center})^2=r^2$，其中$(x_{center},y_{center})$是圆的中心，r是圆的半径。从等式中，我们可以看到我们有3个参数，因此我们需要3D累加器进行霍夫变换，这将非常低效。因此，OpenCV使用更加技巧性的方法，即使用边缘的梯度信息的<strong>Hough梯度方法</strong>。</p><p>我们在这里使用的函数是<strong>cv.HoughCircles</strong>()。它有很多参数，这些参数在文档中有很好的解释。因此，我们直接转到代码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">img &#x3D; cv.imread(&#39;opencv-logo-white.png&#39;,0)</span><br><span class="line">img &#x3D; cv.medianBlur(img,5)</span><br><span class="line">cimg &#x3D; cv.cvtColor(img,cv.COLOR_GRAY2BGR)</span><br><span class="line">circles &#x3D; cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20,</span><br><span class="line">                            param1&#x3D;50,param2&#x3D;30,minRadius&#x3D;0,maxRadius&#x3D;0)</span><br><span class="line">circles &#x3D; np.uint16(np.around(circles))</span><br><span class="line">for i in circles[0,:]:</span><br><span class="line">    # 绘制外圆</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2)</span><br><span class="line">    # 绘制圆心</span><br><span class="line">    cv.circle(cimg,(i[0],i[1]),2,(0,0,255),3)</span><br><span class="line">cv.imshow(&#39;detected circles&#39;,cimg)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><p><img src="http://qiniu.aihubs.net/houghcircles2.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="图像分割与Watershed算法"><a href="#图像分割与Watershed算法" class="headerlink" title="图像分割与Watershed算法"></a><span id="header5">图像分割与Watershed算法</span></h1><h2 id="目标-3"><a href="#目标-3" class="headerlink" title="目标"></a>目标</h2><p>在本章中， - 我们将学习使用分水岭算法实现基于标记的图像分割 - 我们将看到：cv.watershed()</p><h2 id="理论-3"><a href="#理论-3" class="headerlink" title="理论"></a>理论</h2><p>任何灰度图像都可以看作是一个地形表面，其中高强度表示山峰，低强度表示山谷。</p><p>你开始用不同颜色的水(标签)填充每个孤立的山谷(局部最小值)。</p><p>随着水位的上升，根据附近的山峰(坡度)，来自不同山谷的水明显会开始合并，颜色也不同。</p><p>为了避免这种情况，你要在水融合的地方建造屏障。你继续填满水，建造障碍，直到所有的山峰都在水下。</p><p>然后你创建的屏障将返回你的分割结果。这就是Watershed背后的“思想”。</p><p>你可以访问Watershed的CMM网页，了解它与一些动画的帮助。</p><p>但是这种方法会由于图像中的噪声或其他不规则性而产生过度分割的结果。</p><p>因此OpenCV实现了一个基于标记的分水岭算法，你可以指定哪些是要合并的山谷点，哪些不是。</p><p>这是一个交互式的图像分割。我们所做的是给我们知道的对象赋予不同的标签。</p><p>用一种颜色(或强度)标记我们确定为前景或对象的区域，用另一种颜色标记我们确定为背景或非对象的区域，最后用0标记我们不确定的区域。</p><p>这是我们的标记。然后应用分水岭算法。然后我们的标记将使用我们给出的标签进行更新，对象的边界值将为-1。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>下面我们将看到一个有关如何使用距离变换和分水岭来分割相互接触的对象的示例。</p><p>考虑下面的硬币图像，硬币彼此接触。即使你设置阈值，它也会彼此接触。<br><img src="http://qiniu.aihubs.net/water_coins.jpg" alt></p><p>我们先从寻找硬币的近似估计开始。因此，我们可以使用Otsu的二值化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;coins.png&#39;)</span><br><span class="line">gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh &#x3D; cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)</span><br></pre></td></tr></table></figure><p><img src="http://qiniu.aihubs.net/water_thresh.jpg" alt></p><p>现在我们需要去除图像中的任何白点噪声。为此，我们可以使用形态学扩张。</p><p>要去除对象中的任何小孔，我们可以使用形态学侵蚀。因此，现在我们可以确定，靠近对象中心的区域是前景，而离对象中心很远的区域是背景。</p><p>我们不确定的唯一区域是硬币的边界区域。</p><p>因此，我们需要提取我们可确定为硬币的区域。侵蚀会去除边界像素。</p><p>因此，无论剩余多少，我们都可以肯定它是硬币。如果物体彼此不接触，那将起作用。</p><p>但是，由于它们彼此接触，因此另一个好选择是找到距离变换并应用适当的阈值。</p><p>接下来，我们需要找到我们确定它们不是硬币的区域。</p><p>为此，我们扩张了结果。膨胀将对象边界增加到背景。</p><p>这样，由于边界区域已删除，因此我们可以确保结果中背景中的任何区域实际上都是背景。参见下图。<br><img src="http://qiniu.aihubs.net/water_fgbg.jpg" alt></p><p>剩下的区域是我们不知道的区域，无论是硬币还是背景。分水岭算法应该找到它。</p><p>这些区域通常位于前景和背景相遇（甚至两个不同的硬币相遇）的硬币边界附近。我们称之为边界。可以通过从sure_bg区域中减去sure_fg区域来获得。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 噪声去除</span><br><span class="line">kernel &#x3D; np.ones((3,3),np.uint8)</span><br><span class="line">opening &#x3D; cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations &#x3D; 2)</span><br><span class="line"># 确定背景区域</span><br><span class="line">sure_bg &#x3D; cv.dilate(opening,kernel,iterations&#x3D;3)</span><br><span class="line"># 寻找前景区域</span><br><span class="line">dist_transform &#x3D; cv.distanceTransform(opening,cv.DIST_L2,5)</span><br><span class="line">ret, sure_fg &#x3D; cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)</span><br><span class="line"># 找到未知区域</span><br><span class="line">sure_fg &#x3D; np.uint8(sure_fg)</span><br><span class="line">unknown &#x3D; cv.subtract(sure_bg,sure_fg)</span><br></pre></td></tr></table></figure><p>查看结果。在阈值图像中，我们得到了一些硬币区域，我们确定它们是硬币，并且现在已分离它们。（在某些情况下，你可能只对前景分割感兴趣，而不对分离相互接触的对象感兴趣。在那种情况下，你无需使用距离变换，只需侵蚀就足够了。侵蚀只是提取确定前景区域的另一种方法。）</p><p><img src="http://qiniu.aihubs.net/water_dt.jpg" alt></p><p>现在我们可以确定哪些是硬币的区域，哪些是背景。</p><p>因此，我们创建了标记（它的大小与原始图像的大小相同，但具有int32数据类型），并标记其中的区域。</p><p>我们肯定知道的区域（无论是前景还是背景）都标有任何正整数，但是带有不同的整数，而我们不确定的区域则保留为零。</p><p>为此，我们使用<strong>cv.connectedComponents</strong>()。它用0标记图像的背景，然后其他对象用从1开始的整数标记。</p><p>但是我们知道，如果背景标记为0，则分水岭会将其视为未知区域。所以我们想用不同的整数来标记它。相反，我们将未知定义的未知区域标记为0。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 类别标记</span><br><span class="line">ret, markers &#x3D; cv.connectedComponents(sure_fg)</span><br><span class="line"># 为所有的标记加1，保证背景是0而不是1</span><br><span class="line">markers &#x3D; markers+1</span><br><span class="line"># 现在让所有的未知区域为0</span><br><span class="line">markers[unknown&#x3D;&#x3D;255] &#x3D; 0</span><br></pre></td></tr></table></figure><p>参见JET colormap中显示的结果。深蓝色区域显示未知区域。当然,硬币的颜色不同。剩下,肯定为背景的区域显示在较浅的蓝色，跟未知区域相比。<br><img src="http://qiniu.aihubs.net/water_marker.jpg" alt></p><p>现在我们的标记已准备就绪。现在是最后一步的时候了，使用分水岭算法。然后标记图像将被修改。边界区域将标记为-1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">markers &#x3D; cv.watershed(img,markers) </span><br><span class="line">img[markers &#x3D;&#x3D; -1] &#x3D; [255,0,0]</span><br></pre></td></tr></table></figure><p>请参阅下面的结果。对某些硬币，它们接触的区域被正确地分割，而对于某些硬币，却不是。</p><p><img src="http://qiniu.aihubs.net/water_result.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="交互式前景提取使用GrabCut算法"><a href="#交互式前景提取使用GrabCut算法" class="headerlink" title="交互式前景提取使用GrabCut算法"></a><span id="header6">交互式前景提取使用GrabCut算法</span></h1><h2 id="目标-4"><a href="#目标-4" class="headerlink" title="目标"></a>目标</h2><p>在本章中， - 我们将看到GrabCut算法来提取图像中的前景 - 我们将为此创建一个交互式应用程序。</p><h2 id="理论-4"><a href="#理论-4" class="headerlink" title="理论"></a>理论</h2><p>GrabCut算法由英国微软研究院的Carsten Rother，Vladimir Kolmogorov和Andrew Blake设计。</p><p>在他们的论文“GrabCut”中：使用迭代图割的交互式前景提取。需要用最少的用户交互进行前景提取的算法，结果是GrabCut。</p><p>从用户角度来看，它是如何工作的？</p><p>最初，用户在前景区域周围绘制一个矩形（前景区域应完全位于矩形内部）。</p><p>然后，算法会对其进行迭代分割，以获得最佳结果。</p><p>做完了但在某些情况下，分割可能不会很好，例如，可能已将某些前景区域标记为背景，反之亦然。在这种情况下，需要用户进行精修。</p><p>只需在图像错误分割区域上画些笔画。笔画基本上说 “嘿，该区域应该是前景，你将其标记为背景，在下一次迭代中对其进行校正”或与背景相反。</p><p>然后在下一次迭代中，你将获得更好的结果。</p><p>参见下图。</p><p>第一名球员和橄榄球被封闭在一个蓝色矩形中。然后用白色笔划（表示前景）和黑色笔划（表示背景）进行最后的修饰。而且我们得到了不错的结果。</p><p><img src="http://qiniu.aihubs.net/grabcut_output1.jpg" alt></p><p>那么背景发生了什么呢？ </p><ul><li>用户输入矩形。此矩形外部的所有内容都将作为背景（这是在矩形应包含所有对象之前提到的原因）。矩形内的所有内容都是未知的。同样，任何指定前景和背景的用户输入都被视为硬标签，这意味着它们在此过程中不会更改。 </li><li>计算机根据我们提供的数据进行初始标记。它标记前景和背景像素（或对其进行硬标记），现在使用高斯混合模型(GMM)对前景和背景进行建模。 </li><li>根据我们提供的数据，GMM可以学习并创建新的像素分布。也就是说，未知像素根据颜色统计上与其他硬标记像素的关系而被标记为可能的前景或可能的背景（就像聚类一样）。 </li><li>根据此像素分布构建图形。图中的节点为像素。添加了另外两个节点，即“源”节点和“接收器”节点。每个前景像素都连接到源节点，每个背景像素都连接到接收器节点。 </li><li>通过像素是前景/背景的概率来定义将像素连接到源节点/末端节点的边缘的权重。像素之间的权重由边缘信息或像素相似度定义。如果像素颜色差异很大，则它们之间的边缘将变低。 </li><li>然后使用mincut算法对图进行分割。它将图切成具有最小成本函数的两个分离的源节点和宿节点。成本函数是被切割边缘的所有权重的总和。剪切后，连接到“源”节点的所有像素都变为前景，而连接到“接收器”节点的像素都变为背景。 </li><li>继续该过程，直到分类收敛为止。</li></ul><p>如下图所示（图片提供：<a href="http://www.cs.ru.ac.za/research/g02m1682/）" target="_blank" rel="noopener">http://www.cs.ru.ac.za/research/g02m1682/）</a><br><img src="http://qiniu.aihubs.net/grabcut_scheme.jpg" alt></p><p>示例<br>现在我们使用OpenCV进行抓取算法。OpenCV为此具有功能<strong>cv.grabCut</strong>()，我们将首先看到其参数： </p><ul><li>img - 输入图像 </li><li>mask - 这是一个掩码图像，在其中我们指定哪些区域是背景，前景或可能的背景/前景等。这是通过以下标志完成的：cv.GC_BGD,cv.GC_FGD, cv.GC_PR_BGD,cv.GC_PR_FGD，或直接将0,1,2,3传递给图像。 </li><li>rect - 它是矩形的坐标，其中包括前景对象，格式为(x,y,w,h) - bdgModel, fgdModel - 这些是算法内部使用的数组。你只需创建两个大小为(1,65)的np.float64类型零数组。 </li><li>iterCount - 算法应运行的迭代次数。 </li><li>model - 应该是<strong>cv.GC_INIT_WITH_RECT</strong>或<strong>cv.GC_INIT_WITH_MASK</strong>或两者结合，决定我们要绘制矩形还是最终的修饰笔触。</li></ul><p>首先让我们看看矩形模式。我们加载图像，创建类似的mask图像。 </p><p>我们创建<em>fgdModel</em>和<em>bgdModel</em>。我们给出矩形参数。一切都是直截了当的。</p><p>让算法运行5次迭代。模式应为<strong>cv.GC_INIT_WITH_RECT</strong>, 因为我们使用的是矩形。 </p><p>然后运行grabcut。修改mask图像。在新的mask图像中，像素将被标记有四个标记，分别表示上面指定的背景/前景。</p><p>因此，我们修改mask，使所有0像素和2像素都置为0（即背景），而所有1像素和3像素均置为1（即前景像素）。</p><p>现在，我们的最终mask已经准备就绪。只需将其与输入图像相乘即可得到分割的图像。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">img &#x3D; cv.imread(&#39;messi5.jpg&#39;)</span><br><span class="line">mask &#x3D; np.zeros(img.shape[:2],np.uint8)</span><br><span class="line">bgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">fgdModel &#x3D; np.zeros((1,65),np.float64)</span><br><span class="line">rect &#x3D; (50,50,450,290)</span><br><span class="line">cv.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv.GC_INIT_WITH_RECT)</span><br><span class="line">mask2 &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask2[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://qiniu.aihubs.net/grabcut_rect.jpg" alt></p><p>糟糕，梅西的头发不见了。谁会喜欢没有头发的梅西？我们需要把它找回来。</p><p>因此，我们将使用1像素（确保前景）进行精细修饰。</p><p>同时，一些不需要的地面也出现在图片里。我们需要删除它们。</p><p>在那里，我们给出了一些0像素的修饰（确保背景）。</p><p>因此，如现在所说，我们在以前的情况下修改生成的mask。</p><p>我实际上所做的是，我在paint应用程序中打开了输入图像，并在图像中添加了另一层。</p><p>使用画笔中的画笔工具，我在新图层上用白色标记了错过的前景（头发，鞋子，球等），而用白色标记了不需要的背景（例如logo，地面等）。</p><p>然后用灰色填充剩余的背景。</p><p>然后将该mask图像加载到OpenCV中，编辑我们在新添加的mask图像中具有相应值的原始mask图像。</p><p>检查以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">＃newmask是我手动标记过的mask图像</span><br><span class="line">newmask &#x3D; cv.imread(&#39;newmask.png&#39;,0)</span><br><span class="line"># 标记为白色（确保前景）的地方，更改mask &#x3D; 1</span><br><span class="line"># 标记为黑色（确保背景）的地方，更改mask &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 0] &#x3D; 0</span><br><span class="line">mask[newmask &#x3D;&#x3D; 255] &#x3D; 1</span><br><span class="line">mask, bgdModel, fgdModel &#x3D; cv.grabCut(img,mask,None,bgdModel,fgdModel,5,cv.GC_INIT_WITH_MASK)</span><br><span class="line">mask &#x3D; np.where((mask&#x3D;&#x3D;2)|(mask&#x3D;&#x3D;0),0,1).astype(&#39;uint8&#39;)</span><br><span class="line">img &#x3D; img*mask[:,:,np.newaxis]</span><br><span class="line">plt.imshow(img),plt.colorbar(),plt.show()</span><br></pre></td></tr></table></figure><p>就是这样了。在这里，你无需直接在rect模式下初始化，而可以直接进入mask模式。</p><p>只需用2像素或3像素（可能的背景/前景）标记mask图像中的矩形区域。</p><p>然后像在第二个示例中一样，将我们的sure_foreground标记为1像素。然后直接在mask模式下应用grabCut功能。<br><img src="http://qiniu.aihubs.net/grabcut_mask.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;11.&lt;a href=&quot;#header1&quot;&gt;傅里叶变换&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;12.&lt;a href=&quot;#header2&quot;&gt;模板匹配&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;13.&lt;a href=&quot;#header3&quot;&gt;霍夫线变换&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;14.&lt;a href=&quot;#header4&quot;&gt;霍夫圈变换&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;15.&lt;a href=&quot;#header5&quot;&gt;图像分割与Watershed算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;16.&lt;a href=&quot;#header6&quot;&gt;交互式前景提取使用GrabCut算法&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/categories/opencv/"/>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
      <category term="图像" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F/"/>
    
  </entry>
  
</feed>
