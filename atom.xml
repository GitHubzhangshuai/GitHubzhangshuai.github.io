<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张帅的Blog</title>
  
  <subtitle>用hexo搭建的简易博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-29T04:36:23.698Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhangshuai</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>动手实现卷积层和池化层</title>
    <link href="http://yoursite.com/2020/07/29/%E5%8A%A8%E6%89%8B%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%92%8C%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
    <id>http://yoursite.com/2020/07/29/%E5%8A%A8%E6%89%8B%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%92%8C%E6%B1%A0%E5%8C%96%E5%B1%82/</id>
    <published>2020-07-29T04:35:44.000Z</published>
    <updated>2020-07-29T04:36:23.698Z</updated>
    
    <content type="html"><![CDATA[<p>动手实现卷积层和池化层<br><a id="more"></a></p><h1 id="卷积层实现"><a href="#卷积层实现" class="headerlink" title="卷积层实现"></a>卷积层实现</h1><p>CNN中各层间传递的数据是4维数据。所谓4维数据，比如数据的形状是(10, 1, 28, 28)，<br>则它对应10个高为28、长为28、通道为1的数据。用Python来实现的话，如下所示。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x &#x3D; np.random.rand(10, 1, 28, 28) # 随机生成数据</span><br><span class="line">&gt;&gt;&gt; x.shape</span><br><span class="line">(10, 1, 28, 28)</span><br></pre></td></tr></table></figure><br>这里，如果要访问第1个数据，只要写x[0]就可以了（注意Python的索引是从0开始的）。同样地，用x[1]可以访问第2个数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x[0].shape # (1, 28, 28)</span><br><span class="line">&gt;&gt;&gt; x[1].shape # (1, 28, 28)</span><br></pre></td></tr></table></figure></p><p>如果要访问第1个数据的第1个通道的空间数据，可以写成下面这样。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x[0, 0] # 或者x[0][0]</span><br></pre></td></tr></table></figure></p><p>像这样，CNN中处理的是4维数据，因此卷积运算的实现看上去会很复杂，但是通过使用下面要介绍的im2col这个技巧，问题就会变得很简单。</p><h2 id="基于-im2col的展开"><a href="#基于-im2col的展开" class="headerlink" title="基于 im2col的展开"></a>基于 im2col的展开</h2><p>如果老老实实地实现卷积运算，估计要重复好几层的for语句。<br>这样的实现有点麻烦，而且，NumPy中存在使用for语句后处理变慢的缺点（NumPy中，访问元素时最好不要用for语句）。</p><p>这里，我们不使用for语句，而是使用im2col这个便利的函数进行简单的实现。</p><p>im2col是一个函数，将输入数据展开以适合滤波器（权重）。</p><p>如图7-17所示，对3维的输入数据应用im2col后，数据转换为2维矩阵（正确地讲，是把包含批数量的4维数据转换成了2维数据）。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B01.png" alt></p><p>im2col会把输入数据展开以适合滤波器（权重）。</p><p>具体地说，如图7-18所示，对于输入数据，将应用滤波器的区域（3维方块）横向展开为1列。</p><p>im2col会在所有应用滤波器的地方进行这个展开处理。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B02.png" alt></p><p>在图7-18中，为了便于观察，将步幅设置得很大，以使滤波器的应用区域不重叠。</p><p>而在实际的卷积运算中，滤波器的应用区域几乎都是重叠的。</p><p>在滤波器的应用区域重叠的情况下，使用im2col展开后，展开后的元素个数会多于原方块的元素个数。</p><p>因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。</p><p>但是，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处。</p><p>比如，在矩阵计算的库（线性代数库）等中，矩阵计算的实现已被高度最优化，可以高速地进行大矩阵的乘法运算。因此，通过归结到矩阵计算<br>上，可以有效地利用线性代数库。</p><p>im2col这个名称是“image to column”的缩写，翻译过来就是“从图像到矩阵”的意思。</p><p>Caffe、Chainer 等深度学习框架中有名为im2col的函数，并且在卷积层的实现中，都使用了im2col。</p><p>使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可（参照图7-19）。</p><p>这和全连接层的Affi ne层进行的处理基本相同。<br>如图7-19所示，基于im2col方式的输出结果是2维矩阵。</p><p>因为CNN中数据会保存为4维数组，所以要将2维输出数据转换为合适的形状。</p><p>以上就是卷积层的实现流程。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B03.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def im2col(input_data, filter_h, filter_w, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据</span><br><span class="line">    filter_h : 滤波器的高</span><br><span class="line">    filter_w : 滤波器的长</span><br><span class="line">    stride : 步幅</span><br><span class="line">    pad : 填充</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    col : 2维数组</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W &#x3D; input_data.shape</span><br><span class="line">    out_h &#x3D; (H + 2*pad - filter_h)&#x2F;&#x2F;stride + 1</span><br><span class="line">    out_w &#x3D; (W + 2*pad - filter_w)&#x2F;&#x2F;stride + 1</span><br><span class="line"></span><br><span class="line">    img &#x3D; np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], &#39;constant&#39;)</span><br><span class="line">    col &#x3D; np.zeros((N, C, filter_h, filter_w, out_h, out_w))</span><br><span class="line"></span><br><span class="line">    for y in range(filter_h):</span><br><span class="line">        y_max &#x3D; y + stride*out_h</span><br><span class="line">        for x in range(filter_w):</span><br><span class="line">            x_max &#x3D; x + stride*out_w</span><br><span class="line">            col[:, :, y, x, :, :] &#x3D; img[:, :, y:y_max:stride, x:x_max:stride]</span><br><span class="line"></span><br><span class="line">    col &#x3D; col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)</span><br><span class="line">    return col</span><br></pre></td></tr></table></figure><p>im2col会考虑滤波器大小、步幅、填充，将输入数据展开为2维数组。<br>现在，我们来实际使用一下这个im2col。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line">from common.util import im2col</span><br><span class="line">x1 &#x3D; np.random.rand(1, 3, 7, 7)</span><br><span class="line">col1 &#x3D; im2col(x1, 5, 5, stride&#x3D;1, pad&#x3D;0)</span><br><span class="line">print(col1.shape) # (9, 75)</span><br><span class="line">x2 &#x3D; np.random.rand(10, 3, 7, 7) # 10个数据</span><br><span class="line">col2 &#x3D; im2col(x2, 5, 5, stride&#x3D;1, pad&#x3D;0)</span><br><span class="line">print(col2.shape) # (90, 75)</span><br></pre></td></tr></table></figure></p><p>这里举了两个例子。第一个是批大小为1、通道为3的7 × 7的数据，第二个的批大小为10，数据形状和第一个相同。</p><p>分别对其应用im2col函数，在这两种情形下，第2维的元素个数均为75。这是滤波器（通道为3、大小为5 × 5）的元素个数的总和。批大小为1时，im2col的结果是(9, 75)。而第2个例子中批大小为10，所以保存了10倍的数据，即(90, 75)。现在使用im2col来实现卷积层。这里我们将卷积层实现为名为Convolution的类。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">class Convolution:</span><br><span class="line">    def __init__(self, W, b, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">        self.W &#x3D; W</span><br><span class="line">        self.b &#x3D; b</span><br><span class="line">        self.stride &#x3D; stride</span><br><span class="line">        self.pad &#x3D; pad</span><br><span class="line">        </span><br><span class="line">        # 中间数据（backward时使用）</span><br><span class="line">        self.x &#x3D; None   </span><br><span class="line">        self.col &#x3D; None</span><br><span class="line">        self.col_W &#x3D; None</span><br><span class="line">        </span><br><span class="line">        # 权重和偏置参数的梯度</span><br><span class="line">        self.dW &#x3D; None</span><br><span class="line">        self.db &#x3D; None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        FN, C, FH, FW &#x3D; self.W.shape</span><br><span class="line">        N, C, H, W &#x3D; x.shape</span><br><span class="line">        out_h &#x3D; 1 + int((H + 2*self.pad - FH) &#x2F; self.stride)</span><br><span class="line">        out_w &#x3D; 1 + int((W + 2*self.pad - FW) &#x2F; self.stride)</span><br><span class="line"></span><br><span class="line">        col &#x3D; im2col(x, FH, FW, self.stride, self.pad)</span><br><span class="line">        col_W &#x3D; self.W.reshape(FN, -1).T</span><br><span class="line"></span><br><span class="line">        out &#x3D; np.dot(col, col_W) + self.b</span><br><span class="line">        out &#x3D; out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)</span><br><span class="line"></span><br><span class="line">        self.x &#x3D; x</span><br><span class="line">        self.col &#x3D; col</span><br><span class="line">        self.col_W &#x3D; col_W</span><br><span class="line"></span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">    def backward(self, dout):</span><br><span class="line">        FN, C, FH, FW &#x3D; self.W.shape</span><br><span class="line">        dout &#x3D; dout.transpose(0,2,3,1).reshape(-1, FN)</span><br><span class="line"></span><br><span class="line">        self.db &#x3D; np.sum(dout, axis&#x3D;0)</span><br><span class="line">        self.dW &#x3D; np.dot(self.col.T, dout)</span><br><span class="line">        self.dW &#x3D; self.dW.transpose(1, 0).reshape(FN, C, FH, FW)</span><br><span class="line"></span><br><span class="line">        dcol &#x3D; np.dot(dout, self.col_W.T)</span><br><span class="line">        dx &#x3D; col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)</span><br><span class="line"></span><br><span class="line">        return dx</span><br></pre></td></tr></table></figure><p>卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。</p><p>滤波器是 (FN, C, FH, FW)的 4 维形状。另外，FN、C、FH、FW分别是 Filter Number（滤波器数量）、Channel、Filter Height、Filter Width的缩写。</p><p>这里用粗体字表示Convolution层的实现中的重要部分。在这些粗体字部分，用im2col展开输入数据，并用reshape将滤波器展开为2维数组。然后，计算展开后的矩阵的乘积。</p><p>展开滤波器的部分（代码段中的粗体字）如图7-19所示，将各个滤波器的方块纵向展开为1列。这里通过reshape(FN,-1)将参数指定为-1，这是<br>reshape的一个便利的功能。通过在reshape时指定为-1，reshape函数会自动计算-1维度上的元素个数，以使多维数组的元素个数前后一致。比如，<br>(10, 3, 5, 5)形状的数组的元素个数共有750个，指定reshape(10,-1)后，就会转换成(10, 75)形状的数组。</p><p>forward的实现中，最后会将输出大小转换为合适的形状。转换时使用了NumPy的transpose函数。transpose会更改多维数组的轴的顺序。如图7-20<br>所示，通过指定从0开始的索引（编号）序列，就可以更改轴的顺序。</p><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B04.png" alt></p><p>以上就是卷积层的forward处理的实现。</p><p>通过使用im2col进行展开，基本上可以像实现全连接层的Affine层一样来实现。</p><p>接下来是卷积层的反向传播的实现，因为和Affine层的实现有很多共通的地方，所以就不再介绍了。</p><p>但有一点需要注意，在进行卷积层的反向传播时，必须进行im2col的逆处理。这可以使用本书提供的col2im函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def col2im(col, input_shape, filter_h, filter_w, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    col :</span><br><span class="line">    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）</span><br><span class="line">    filter_h :</span><br><span class="line">    filter_w</span><br><span class="line">    stride</span><br><span class="line">    pad</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W &#x3D; input_shape</span><br><span class="line">    out_h &#x3D; (H + 2*pad - filter_h)&#x2F;&#x2F;stride + 1</span><br><span class="line">    out_w &#x3D; (W + 2*pad - filter_w)&#x2F;&#x2F;stride + 1</span><br><span class="line">    col &#x3D; col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)</span><br><span class="line"></span><br><span class="line">    img &#x3D; np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))</span><br><span class="line">    for y in range(filter_h):</span><br><span class="line">        y_max &#x3D; y + stride*out_h</span><br><span class="line">        for x in range(filter_w):</span><br><span class="line">            x_max &#x3D; x + stride*out_w</span><br><span class="line">            img[:, :, y:y_max:stride, x:x_max:stride] +&#x3D; col[:, :, y, x, :, :]</span><br><span class="line"></span><br><span class="line">    return img[:, :, pad:H + pad, pad:W + pad]</span><br></pre></td></tr></table></figure><br>来进行。除了使用col2im这一点，卷积层的反向传播和Affine层的实现方式都一样</p><h1 id="池化层实现"><a href="#池化层实现" class="headerlink" title="池化层实现"></a>池化层实现</h1><p>池化层的实现和卷积层相同，都使用im2col展开输入数据。</p><p>不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同。</p><p>具体地讲，如图7-21所示，池化的应用区域按通道单独展开。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B05.png" alt></p><p>像这样展开之后，只需对展开的矩阵求各行的最大值，并转换为合适的形状即可（图7-22）。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%AE%9E%E7%8E%B06.png" alt><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">class Pooling:</span><br><span class="line">    def __init__(self, pool_h, pool_w, stride&#x3D;1, pad&#x3D;0):</span><br><span class="line">        self.pool_h &#x3D; pool_h</span><br><span class="line">        self.pool_w &#x3D; pool_w</span><br><span class="line">        self.stride &#x3D; stride</span><br><span class="line">        self.pad &#x3D; pad</span><br><span class="line">        </span><br><span class="line">        self.x &#x3D; None</span><br><span class="line">        self.arg_max &#x3D; None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        N, C, H, W &#x3D; x.shape</span><br><span class="line">        out_h &#x3D; int(1 + (H - self.pool_h) &#x2F; self.stride)</span><br><span class="line">        out_w &#x3D; int(1 + (W - self.pool_w) &#x2F; self.stride)</span><br><span class="line"></span><br><span class="line">        col &#x3D; im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)</span><br><span class="line">        col &#x3D; col.reshape(-1, self.pool_h*self.pool_w)</span><br><span class="line"></span><br><span class="line">        arg_max &#x3D; np.argmax(col, axis&#x3D;1)</span><br><span class="line">        out &#x3D; np.max(col, axis&#x3D;1)</span><br><span class="line">        out &#x3D; out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)</span><br><span class="line"></span><br><span class="line">        self.x &#x3D; x</span><br><span class="line">        self.arg_max &#x3D; arg_max</span><br><span class="line"></span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">    def backward(self, dout):</span><br><span class="line">        dout &#x3D; dout.transpose(0, 2, 3, 1)</span><br><span class="line">        </span><br><span class="line">        pool_size &#x3D; self.pool_h * self.pool_w</span><br><span class="line">        dmax &#x3D; np.zeros((dout.size, pool_size))</span><br><span class="line">        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] &#x3D; dout.flatten()</span><br><span class="line">        dmax &#x3D; dmax.reshape(dout.shape + (pool_size,)) </span><br><span class="line">        </span><br><span class="line">        dcol &#x3D; dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)</span><br><span class="line">        dx &#x3D; col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)</span><br><span class="line">        </span><br><span class="line">        return dx</span><br></pre></td></tr></table></figure><br>池化层的实现按下面3个阶段进行。</p><ul><li>1.展开输入数据。</li><li>2.求各行的最大值。</li><li>3.转换为合适的输出大小。</li></ul><p>各阶段的实现都很简单，只有一两行代码</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;动手实现卷积层和池化层&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="deep-learning-from-scratch笔记" scheme="http://yoursite.com/tags/deep-learning-from-scratch%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>deep-learning-from-scratch笔记</title>
    <link href="http://yoursite.com/2020/07/29/deep-learning-from-scratch%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/07/29/deep-learning-from-scratch%E7%AC%94%E8%AE%B0/</id>
    <published>2020-07-29T03:07:01.000Z</published>
    <updated>2020-07-29T03:14:29.267Z</updated>
    
    <content type="html"><![CDATA[<p>deep-learning-from-scratch笔记<br><a id="more"></a></p><h1 id="sigmoid函数的平滑性对神经网络的学习具有重要意义。"><a href="#sigmoid函数的平滑性对神经网络的学习具有重要意义。" class="headerlink" title="sigmoid函数的平滑性对神经网络的学习具有重要意义。"></a>sigmoid函数的平滑性对神经网络的学习具有重要意义。</h1><p>识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。</p><p>出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。</p><p>阶跃函数的导数在绝大多数地方（除了0以外的地方）均为0。</p><p>也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微<br>小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。</p><p>sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。</p><p>另一个不同点是，相对于阶跃函数只能返回0或1，sigmoid函数可以返回0.731 …、0.880 …等实数（这一点和刚才的平滑性有关）。也就是说，感<br>知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。</p><p>sigmoid函数的导数在任何地方都不为0。这对神经网络的学习非常重要。得益于这个斜率不会为0的性质，神经网络的学习得以正确进行</p><h1 id="神经网络的激活函数必须使用非线性函数。"><a href="#神经网络的激活函数必须使用非线性函数。" class="headerlink" title="神经网络的激活函数必须使用非线性函数。"></a>神经网络的激活函数必须使用非线性函数。</h1><p>换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。</p><p>线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。</p><h1 id="实现-softmax函数时的注意事项"><a href="#实现-softmax函数时的注意事项" class="headerlink" title="实现 softmax函数时的注意事项"></a>实现 softmax函数时的注意事项</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/softmax1.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def softmax(a):</span><br><span class="line"> exp_a &#x3D; np.exp(a)</span><br><span class="line"> sum_exp_a &#x3D; np.sum(exp_a)</span><br><span class="line"> y &#x3D; exp_a &#x2F; sum_exp_a</span><br><span class="line"> return y</span><br></pre></td></tr></table></figure><p>上面的softmax函数的实现虽然正确描述了式，但在计算机的运算上有一定的缺陷。这个缺陷就是溢出问题。softmax函数的实现中要进行指<br>数函数的运算，但是此时指数函数的值很容易变得非常大。<br>比如，e的10次方的值会超过20000，e的1000次方会变成一个后面有40多个0的超大值，e的1000次方的结果会返回一个表示无穷大的inf。如果在这些超大值之间进行除法运算，结果会出现“不确定”的情况。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/softmax2.png" alt><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a &#x3D; np.array([1010, 1000, 990])</span><br><span class="line">&gt;&gt;&gt; np.exp(a) &#x2F; np.sum(np.exp(a)) # softmax函数的运算</span><br><span class="line">array([ nan, nan, nan]) # 没有被正确计算</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; c &#x3D; np.max(a) # 1010</span><br><span class="line">&gt;&gt;&gt; a - c</span><br><span class="line">array([ 0, -10, -20])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; np.exp(a - c) &#x2F; np.sum(np.exp(a - c))</span><br><span class="line">array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])</span><br></pre></td></tr></table></figure></p><p>如该例所示，通过减去输入信号中的最大值（上例中的c），我们发现原<br>本为nan（not a number，不确定）的地方，现在被正确计算了。综上，我们<br>可以像下面这样实现softmax函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def softmax(a):</span><br><span class="line"> c &#x3D; np.max(a)</span><br><span class="line"> exp_a &#x3D; np.exp(a - c) # 溢出对策</span><br><span class="line"> sum_exp_a &#x3D; np.sum(exp_a)</span><br><span class="line"> y &#x3D; exp_a &#x2F; sum_exp_a</span><br><span class="line"> return y</span><br></pre></td></tr></table></figure></p><h1 id="获得泛化能力是机器学习的最终目标。"><a href="#获得泛化能力是机器学习的最终目标。" class="headerlink" title="获得泛化能力是机器学习的最终目标。"></a>获得泛化能力是机器学习的最终目标。</h1><p>如果让我们自己来设计一个能将5正确分类的程序，就会意外地发现这是一个很难的问题。人可以简单地识别出5，但却很难明确说出是基于何种规律而识别出了5。</p><p>因此，与其绞尽脑汁，从零开始想出一个可以识别5的算法，不如考虑通过有效利用数据来解决这个问题。</p><p>一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。</p><p>图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。</p><p>机器学习的方法中，由机器从收集到的数据中找出规律性。与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担。</p><p>但是需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。</p><p>比如，为了区分狗的脸部，人们需要考虑与用于识别5的特征量不同的其他特征量。也就是说，即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量</p><p>神经网络的优点是对所有的问题都可以用同样的流程来解决。比如，不管要求解的问题是识别5，还是识别狗，抑或是识别人脸，神经网络都是通<br>过不断地学习所提供的数据，尝试发现待求解的问题的模式。也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端的学习。</p><p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的<br>能力。获得泛化能力是机器学习的最终目标。</p><h1 id="计算图优点"><a href="#计算图优点" class="headerlink" title="计算图优点"></a>计算图优点</h1><p>计算图到底有什么优点呢？</p><p>一个优点就在于前面所说的局部计算。无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题。</p><p>另一个优点是，利用计算图可以将中间的计算结果全部保存起来（比如，计算进行到2个苹果时的金额是200日元、加上消费税之前的金额650日元等）。</p><p>但是只有这些理由可能还无法令人信服。实际上，使用计算图最大的原因是，可以通过反向传播高效计算导数。</p><p>计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。</p><h1 id="推理和学习"><a href="#推理和学习" class="headerlink" title="推理和学习"></a>推理和学习</h1><p>神经网络中进行的处理有推理（inference）和学习两个阶段。</p><p>神经网络的推理通常不使用 Softmax层。</p><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%8E%A8%E7%90%86%E4%B8%8E%E5%AD%A6%E4%B9%A0.png" alt></p><p>比如，用上图网络进行推理时，会将最后一个 Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（Softmax层前面的 Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层。不过，神经网络的学习阶段则需要 Softmax层。</p><h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Momentum.png" alt><br>式中有αv这一项。在物体不受任何力时，该项承担使物体逐渐减速的任务（α设定为0.9之类的值），对应物理上的地面摩擦或空气阻力。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Momentum:</span><br><span class="line"> def __init__(self, lr&#x3D;0.01, momentum&#x3D;0.9):</span><br><span class="line"> self.lr &#x3D; lr</span><br><span class="line"> self.momentum &#x3D; momentum</span><br><span class="line"> self.v &#x3D; None</span><br><span class="line"> def update(self, params, grads):</span><br><span class="line"> if self.v is None:</span><br><span class="line"> self.v &#x3D; &#123;&#125;</span><br><span class="line"> for key, val in params.items():</span><br><span class="line"> self.v[key] &#x3D; np.zeros_like(val)</span><br><span class="line"> for key in params.keys():</span><br><span class="line"> self.v[key] &#x3D; self.momentum*self.v[key] - self.lr*grads[key]</span><br><span class="line"> params[key] +&#x3D; self.v[key]</span><br></pre></td></tr></table></figure><br>实例变量v会保存物体的速度。初始化时，v中什么都不保存，但当第一次调用update()时，v会以字典型变量的形式保存与参数结构相同的数据。</p><h1 id="AdaGrad和RMSProp"><a href="#AdaGrad和RMSProp" class="headerlink" title="AdaGrad和RMSProp"></a>AdaGrad和RMSProp</h1><p>在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/AdaGrad.png" alt><br>在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。而AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值。</p><p>AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新。为了改善这个问题，可以使用 RMSProp方法。RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class AdaGrad:</span><br><span class="line"> def __init__(self, lr&#x3D;0.01):</span><br><span class="line"> self.lr &#x3D; lr</span><br><span class="line"> self.h &#x3D; None</span><br><span class="line"> def update(self, params, grads):</span><br><span class="line"> if self.h is None:</span><br><span class="line"> self.h &#x3D; &#123;&#125;</span><br><span class="line"> for key, val in params.items():</span><br><span class="line"> self.h[key] &#x3D; np.zeros_like(val)</span><br><span class="line"> for key in params.keys():</span><br><span class="line"> self.h[key] +&#x3D; grads[key] * grads[key]</span><br><span class="line"> params[key] -&#x3D; self.lr * grads[key] &#x2F; (np.sqrt(self.h[key]) + 1e-7)</span><br></pre></td></tr></table></figure><p>这里需要注意的是，最后一行加上了微小值1e-7。这是为了防止当<br>self.h[key]中有0时，将0用作除数的情况。在很多深度学习的框架中，这<br>个微小值也可以设定为参数，但这里我们用的是1e-7这个固定值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class RMSprop:</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;RMSprop&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, lr&#x3D;0.01, decay_rate &#x3D; 0.99):</span><br><span class="line">        self.lr &#x3D; lr</span><br><span class="line">        self.decay_rate &#x3D; decay_rate</span><br><span class="line">        self.h &#x3D; None</span><br><span class="line">        </span><br><span class="line">    def update(self, params, grads):</span><br><span class="line">        if self.h is None:</span><br><span class="line">            self.h &#x3D; &#123;&#125;</span><br><span class="line">            for key, val in params.items():</span><br><span class="line">                self.h[key] &#x3D; np.zeros_like(val)</span><br><span class="line">            </span><br><span class="line">        for key in params.keys():</span><br><span class="line">            self.h[key] *&#x3D; self.decay_rate</span><br><span class="line">            self.h[key] +&#x3D; (1 - self.decay_rate) * grads[key] * grads[key]</span><br><span class="line">            params[key] -&#x3D; self.lr * grads[key] &#x2F; (np.sqrt(self.h[key]) + 1e-7)</span><br></pre></td></tr></table></figure></p><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam是2015年提出的新方法。它的理论有些复杂，直观地讲，就是融合了Momentum和AdaGrad的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。<br>此外，进行超参数的“偏置校正”也是Adam的特征。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class Adam:</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;Adam (http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1412.6980v8)&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, lr&#x3D;0.001, beta1&#x3D;0.9, beta2&#x3D;0.999):</span><br><span class="line">        self.lr &#x3D; lr</span><br><span class="line">        self.beta1 &#x3D; beta1</span><br><span class="line">        self.beta2 &#x3D; beta2</span><br><span class="line">        self.iter &#x3D; 0</span><br><span class="line">        self.m &#x3D; None</span><br><span class="line">        self.v &#x3D; None</span><br><span class="line">        </span><br><span class="line">    def update(self, params, grads):</span><br><span class="line">        if self.m is None:</span><br><span class="line">            self.m, self.v &#x3D; &#123;&#125;, &#123;&#125;</span><br><span class="line">            for key, val in params.items():</span><br><span class="line">                self.m[key] &#x3D; np.zeros_like(val)</span><br><span class="line">                self.v[key] &#x3D; np.zeros_like(val)</span><br><span class="line">        </span><br><span class="line">        self.iter +&#x3D; 1</span><br><span class="line">        lr_t  &#x3D; self.lr * np.sqrt(1.0 - self.beta2**self.iter) &#x2F; (1.0 - self.beta1**self.iter)         </span><br><span class="line">        </span><br><span class="line">        for key in params.keys():</span><br><span class="line">            #self.m[key] &#x3D; self.beta1*self.m[key] + (1-self.beta1)*grads[key]</span><br><span class="line">            #self.v[key] &#x3D; self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)</span><br><span class="line">            self.m[key] +&#x3D; (1 - self.beta1) * (grads[key] - self.m[key])</span><br><span class="line">            self.v[key] +&#x3D; (1 - self.beta2) * (grads[key]**2 - self.v[key])</span><br><span class="line">            </span><br><span class="line">            params[key] -&#x3D; lr_t * self.m[key] &#x2F; (np.sqrt(self.v[key]) + 1e-7)</span><br><span class="line">            </span><br><span class="line">            #unbias_m +&#x3D; (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias</span><br><span class="line">            #unbisa_b +&#x3D; (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias</span><br><span class="line">            #params[key] +&#x3D; self.lr * unbias_m &#x2F; (np.sqrt(unbisa_b) + 1e-7)</span><br></pre></td></tr></table></figure></p><h1 id="可以将权重初始值设为0吗"><a href="#可以将权重初始值设为0吗" class="headerlink" title="可以将权重初始值设为0吗"></a>可以将权重初始值设为0吗</h1><p>为什么不能将权重初始值设为0呢？</p><p>严格地说，为什么不能将权重初始值设成一样的值呢？</p><p>这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。</p><p>比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会<br>被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新（回忆一下“乘法节点的反向传播”的内容）。</p><p>因此，权重被更新为相同的值，并拥有了对称的值（重复的值）。这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。</p><h1 id="各层的激活值的分布都要求有适当的广度"><a href="#各层的激活值的分布都要求有适当的广度" class="headerlink" title="各层的激活值的分布都要求有适当的广度"></a>各层的激活值的分布都要求有适当的广度</h1><p>各层的激活值的分布都要求有适当的广度。为什么呢？</p><p>因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。</p><p>反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。</p><p>激活值的分布有所偏向，说明在表现力上会有很大问题。为什么这么说呢？</p><p>因为如果有多个神经元都输出几乎相同的值，那它们就没有存在的意义了。</p><p>比如，如果100个神经元都输出几乎相同的值，那么也可以由1个神经元来表达基本相同的事情。</p><p>因此，激活值在分布上有所偏向会出现“表现力受限”的问题。</p><h1 id="Batch-Norm"><a href="#Batch-Norm" class="headerlink" title="Batch Norm"></a>Batch Norm</h1><p>为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。</p><ul><li>可以使学习快速进行（可以增大学习率）。</li><li>不那么依赖初始值（对于初始值不用那么神经质）。</li><li>抑制过拟合（降低Dropout等的必要性）。</li></ul><p>Batch Norm的思路是调整各层的激活值分布使其拥有适当的广度。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Batch%20Norm.png" alt></p><h1 id="权值衰减"><a href="#权值衰减" class="headerlink" title="权值衰减"></a>权值衰减</h1><p>权值衰减是一直以来经常被使用的一种抑制过拟合的方法。</p><p>该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。</p><p>很多过拟合原本就是因为权重参数取值过大才发生的。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%9D%83%E5%80%BC%E8%A1%B0%E5%87%8F.png" alt></p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>作为抑制过拟合的方法，前面我们介绍了为损失函数加上权重的L2范数的权值衰减方法。</p><p>该方法可以简单地实现，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应对了。</p><p>在这种情况下，我们经常会使用Dropout方法。</p><p>Dropout是一种在学习的过程中随机删除神经元的方法。<br>训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递，</p><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/Dropout.png" alt><br>训练时，每传递一次数据，就会随机选择要删除的神经元。<br>然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Dropout:</span><br><span class="line"> def __init__(self, dropout_ratio&#x3D;0.5):</span><br><span class="line"> self.dropout_ratio &#x3D; dropout_ratio</span><br><span class="line"> self.mask &#x3D; None</span><br><span class="line"> def forward(self, x, train_flg&#x3D;True):</span><br><span class="line"> if train_flg:</span><br><span class="line"> self.mask &#x3D; np.random.rand(*x.shape) &gt; self.dropout_ratio</span><br><span class="line"> return x * self.mask</span><br><span class="line"> else:</span><br><span class="line"> return x * (1.0 - self.dropout_ratio)</span><br><span class="line"> def backward(self, dout):</span><br><span class="line"> return dout * self.mask</span><br></pre></td></tr></table></figure><p>这里的要点是，每次正向传播时，self.mask中都会以False的形式保存要删除的神经元。</p><p>self.mask会随机生成和x形状相同的数组，并将值比dropout_ratio大的元素设为True。反向传播时的行为和ReLU相同。</p><p>也就是说，正向传播时传递了信号的神经元，反向传播时按原样传递信号；正向传播时没有传递信号的神经元，反向传播时信号将停在那里。</p><h1 id="验证数据"><a href="#验证数据" class="headerlink" title="验证数据"></a>验证数据</h1><p>调整超参数时，必须使用超参数专用的确认数据。</p><p>用于调整超参数的数据，一般称为验证数据（validation data）。</p><p>我们使用这个验证数据来评估超参数的好坏。</p><h1 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF1.png" alt><br>卷积运算对输入数据应用滤波器。</p><p>在这个例子中，输入数据是有高长方向的形状的数据，滤波器也一样，有高长方向上的维度。</p><p>假设用（height, width）表示数据和滤波器的形状，则在本例中，输入大小是(4, 4)，滤波器大小是(3, 3)，输出大小是(2, 2)。</p><p>另外，有的文献中也会用“核”这个词来表示这里所说的“滤波器”。</p><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF2.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF3.png" alt></p><p>应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如下图所示，应用滤波器的窗口的间隔变为2个元素。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF4.png" alt></p><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF5.png" alt></p><p>之前的卷积运算的例子都是以有高、长方向的2维形状为对象的。</p><p>但是，图像是3维数据，除了高、长方向之外，还需要处理通道方向。</p><p>这里，我们按照与之前相同的顺序，看一下对加上了通道方向的3维数据进行卷积运算的例子。</p><p>图7-8是卷积运算的例子，图7-9是计算顺序。</p><p>这里以3通道的数据为例，展示了卷积运算的结果。和2维数据时（图7-3的例子）相比，可以发现纵深方向（通道方向）上特征图增加了。</p><p>通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF6.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF7.png" alt></p><p>将数据和滤波器结合长方体的方块来考虑，3维数据的卷积运算会很容易理解。</p><p>方块是如图7-10所示的3维长方体。把3维数据表示为多维数组时，书写顺序为（channel, height, width）。</p><p>比如，通道数为C、高度为H、长度为W的数据的形状可以写成（C, H, W）。</p><p>滤波器也一样，要按（channel, height, width）的顺序书写。</p><p>比如，通道数为C、滤波器高度为FH（Filter Height）、长度为FW（Filter Width）时，可以写成（C, FH, FW）。</p><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF8.png" alt><br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF9.png" alt></p><p>图7-11中，通过应用FN个滤波器，输出特征图也生成了FN个。</p><p>如果将这FN个特征图汇集在一起，就得到了形状为(FN, OH, OW)的方块。将这个方块传给下一层，就是CNN的处理流。</p><p>如图 7-11 所示，关于卷积运算的滤波器，也必须考虑滤波器的数量。</p><p>因此，作为4维数据，滤波器的权重数据要按(output_channel, input_channel, height, width)的顺序书写。</p><p>比如，通道数为3、大小为5 × 5的滤波器有20个时，可以写成(20, 3, 5, 5)。</p><p>卷积运算中（和全连接层一样）存在偏置。在图7-11的例子中，如果进一步追加偏置的加法运算处理，则结果如下面的图7-12所示。</p><p>图7-12中，每个通道只有一个偏置。这里，偏置的形状是(FN, 1, 1)，滤波器的输出结果的形状是(FN, OH, OW)。</p><p>这两个方块相加时，要对滤波器的输出结果(FN, OH, OW)按通道加上相同的偏置值。</p><p>另外，不同形状的方块相加时，可以基于NumPy的广播功能轻松实现<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF10.png" alt></p><p>神经网络的处理中进行了将输入数据打包的批处理。</p><p>之前的全连接神经网络的实现也对应了批处理，通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。</p><p>我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。</p><p>具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。</p><p>比如，将图7-12中的处理改成对N个数据进行批处理时，数据的形状如图7-13所示。</p><p>图7-13的批处理版的数据流中，在各个数据的开头添加了批用的维度。像这样，数据作为4维的形状在各层间传递。</p><p>这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E5%8D%B7%E7%A7%AF11.png" alt></p><h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%B1%A0%E5%8C%961.png" alt><br>图7-14的例子是按步幅2进行2 × 2的Max池化时的处理顺序。“Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。</p><p>如图所示，从2 × 2的区域中取出最大的元素。此外，这个例子中将步幅设为了2，所以2 × 2的窗口的移动间隔为2个元素。</p><p>另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如，3 × 3的窗口的步幅会设为3，4 × 4的窗口的步幅会设为4等。</p><p>池化层有以下特征。</p><ul><li>没有要学习的参数。池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。<br>通道数不发生变化</li><li>经过池化运算，输入数据和输出数据的通道数不会发生变化。如图7-15<br>所示，计算是按通道独立进行的。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%B1%A0%E5%8C%962.png" alt><br>对微小的位置变化具有鲁棒性（健壮）</li></ul><p>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。比如，3 × 3的池化的情况下，如图7-16所示，池化会吸收输入数据的偏差（根据数据的不同，结果有可能不一致）。<br><img src="http://zs-hexo-blog.oss-cn-beijing.aliyuncs.com/%E6%B1%A0%E5%8C%963.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;deep-learning-from-scratch笔记&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="deep-learning-from-scratch笔记" scheme="http://yoursite.com/tags/deep-learning-from-scratch%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(17)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-07-27T09:20:38.000Z</published>
    <updated>2020-07-27T11:03:36.468Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了概率图模型，首先从生成式模型与判别式模型的定义出发，引出了概率图模型的基本概念，即利用图结构来表达变量之间的依赖关系；接着分别介绍了隐马尔可夫模型、马尔可夫随机场、条件随机场、精确推断方法以及LDA话题模型：HMM主要围绕着评估/解码/学习这三个实际问题展开论述；MRF基于团和势函数的概念来定义联合概率分布；CRF引入两种特征函数对状态序列进行评价打分；变量消去与信念传播在给定联合概率分布后计算特定变量的边际分布；LDA话题模型则试图去推断给定文档所蕴含的话题分布。本篇将介绍最后一种学习算法—强化学习。<br><a id="more"></a><br>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:20px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="16、强化学习"><a href="#16、强化学习" class="headerlink" title="16、强化学习"></a><strong>16、强化学习</strong></h1><p><strong>强化学习</strong>（Reinforcement Learning，简称<strong>RL</strong>）是机器学习的一个重要分支，前段时间人机大战的主角AlphaGo正是以强化学习为核心技术。在强化学习中，包含两种基本的元素：<strong>状态</strong>与<strong>动作</strong>，<strong>在某个状态下执行某种动作，这便是一种策略</strong>，学习器要做的就是通过不断地探索学习，从而获得一个好的策略。例如：在围棋中，一种落棋的局面就是一种状态，若能知道每种局面下的最优落子动作，那就攻无不克/百战不殆了~</p><p>若将状态看作为属性，动作看作为标记，易知：<strong>监督学习和强化学习都是在试图寻找一个映射，从已知属性/状态推断出标记/动作</strong>，这样强化学习中的策略相当于监督学习中的分类/回归器。但在实际问题中，<strong>强化学习并没有监督学习那样的标记信息</strong>，通常都是在<strong>尝试动作后才能获得结果</strong>，因此强化学习是通过反馈的结果信息不断调整之前的策略，从而算法能够学习到：在什么样的状态下选择什么样的动作可以获得最好的结果。</p><h2 id="16-1-基本要素"><a href="#16-1-基本要素" class="headerlink" title="16.1 基本要素"></a><strong>16.1 基本要素</strong></h2><p>强化学习任务通常使用<strong>马尔可夫决策过程</strong>（Markov Decision Process，简称<strong>MDP</strong>）来描述，具体而言：机器处在一个环境中，每个状态为机器对当前环境的感知；机器只能通过动作来影响环境，当机器执行一个动作后，会使得环境按某种概率转移到另一个状态；同时，环境会根据潜在的奖赏函数反馈给机器一个奖赏。综合而言，强化学习主要包含四个要素：状态、动作、转移概率以及奖赏函数。</p><blockquote><p><strong>状态（X）</strong>：机器对环境的感知，所有可能的状态称为状态空间；<br><strong>动作（A）</strong>：机器所采取的动作，所有能采取的动作构成动作空间；<br><strong>转移概率（P）</strong>：当执行某个动作后，当前状态会以某种概率转移到另一个状态；<br><strong>奖赏函数（R）</strong>：在状态转移的同时，环境给反馈给机器一个奖赏。</p></blockquote><p><img src="https://s1.ax1x.com/2018/10/18/iwYOud.png" alt="iwYOud.png"></p><p>因此，<strong>强化学习的主要任务就是通过在环境中不断地尝试，根据尝试获得的反馈信息调整策略，最终生成一个较好的策略π，机器根据这个策略便能知道在什么状态下应该执行什么动作</strong>。常见的策略表示方法有以下两种：</p><blockquote><p><strong>确定性策略</strong>：π（x）=a，即在状态x下执行a动作；<br><strong>随机性策略</strong>：P=π（x,a），即在状态x下执行a动作的概率。</p></blockquote><p><strong>一个策略的优劣取决于长期执行这一策略后的累积奖赏</strong>，换句话说：可以使用累积奖赏来评估策略的好坏，最优策略则表示在初始状态下一直执行该策略后，最后的累积奖赏值最高。长期累积奖赏通常使用下述两种计算方法：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYH3D.png" alt="iwYH3D.png"></p><h2 id="16-2-K摇摆赌博机"><a href="#16-2-K摇摆赌博机" class="headerlink" title="16.2 K摇摆赌博机"></a><strong>16.2 K摇摆赌博机</strong></h2><p>首先我们考虑强化学习最简单的情形：仅考虑一步操作，即在状态x下只需执行一次动作a便能观察到奖赏结果。易知：欲最大化单步奖赏，我们需要知道每个动作带来的期望奖赏值，这样便能选择奖赏值最大的动作来执行。若每个动作的奖赏值为确定值，则只需要将每个动作尝试一遍即可，但大多数情形下，一个动作的奖赏值来源于一个概率分布，因此需要进行多次的尝试。</p><p>单步强化学习实质上是<strong>K-摇臂赌博机</strong>（K-armed bandit）的原型，一般我们<strong>尝试动作的次数是有限的</strong>，那如何利用有限的次数进行有效地探索呢？这里有两种基本的想法：</p><blockquote><p><strong>仅探索法</strong>：将尝试的机会平均分给每一个动作，即轮流执行，最终将每个动作的平均奖赏作为期望奖赏的近似值。<br><strong>仅利用法</strong>：将尝试的机会分给当前平均奖赏值最大的动作，隐含着让一部分人先富起来的思想。</p></blockquote><p>可以看出：上述<strong>两种方法是相互矛盾的</strong>，仅探索法能较好地估算每个动作的期望奖赏，但是没能根据当前的反馈结果调整尝试策略；仅利用法在每次尝试之后都更新尝试策略，符合强化学习的思（tao）维（lu），但容易找不到最优动作。因此需要在这两者之间进行折中。</p><h3 id="16-2-1-ε-贪心"><a href="#16-2-1-ε-贪心" class="headerlink" title="16.2.1 ε-贪心"></a><strong>16.2.1 ε-贪心</strong></h3><p><strong>ε-贪心法基于一个概率来对探索和利用进行折中</strong>，具体而言：在每次尝试时，以ε的概率进行探索，即以均匀概率随机选择一个动作；以1-ε的概率进行利用，即选择当前最优的动作。ε-贪心法只需记录每个动作的当前平均奖赏值与被选中的次数，便可以增量式更新。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYzUP.png" alt="iwYzUP.png"></p><h3 id="16-2-2-Softmax"><a href="#16-2-2-Softmax" class="headerlink" title="16.2.2 Softmax"></a><strong>16.2.2 Softmax</strong></h3><p><strong>Softmax算法则基于当前每个动作的平均奖赏值来对探索和利用进行折中，Softmax函数将一组值转化为一组概率</strong>，值越大对应的概率也越高，因此当前平均奖赏值越高的动作被选中的几率也越大。Softmax函数如下所示：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYbge.png" alt="iwYbge.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYqjH.png" alt="iwYqjH.png"></p><h2 id="16-3-有模型学习"><a href="#16-3-有模型学习" class="headerlink" title="16.3 有模型学习"></a><strong>16.3 有模型学习</strong></h2><p>若学习任务中的四个要素都已知，即状态空间、动作空间、转移概率以及奖赏函数都已经给出，这样的情形称为“<strong>有模型学习</strong>”。假设状态空间和动作空间均为有限，即均为离散值，这样我们不用通过尝试便可以对某个策略进行评估。</p><h3 id="16-3-1-策略评估"><a href="#16-3-1-策略评估" class="headerlink" title="16.3.1 策略评估"></a><strong>16.3.1 策略评估</strong></h3><p>前面提到：<strong>在模型已知的前提下，我们可以对任意策略的进行评估</strong>（后续会给出演算过程）。一般常使用以下两种值函数来评估某个策略的优劣：</p><blockquote><p><strong>状态值函数（V）</strong>：V（x），即从状态x出发，使用π策略所带来的累积奖赏；<br><strong>状态-动作值函数（Q）</strong>：Q（x,a），即从状态x出发，执行动作a后再使用π策略所带来的累积奖赏。</p></blockquote><p>根据累积奖赏的定义，我们可以引入T步累积奖赏与r折扣累积奖赏：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYjHI.png" alt="iwYjHI.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYXDA.png" alt="iwYXDA.png"></p><p>由于MDP具有马尔可夫性，即现在决定未来，将来和过去无关，我们很容易找到值函数的递归关系：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwtS4f.png" alt="iwtS4f.png"></p><p>类似地，对于r折扣累积奖赏可以得到：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYxEt.png" alt="iwYxEt.png"></p><p>易知：<strong>当模型已知时，策略的评估问题转化为一种动态规划问题</strong>，即以填表格的形式自底向上，先求解每个状态的单步累积奖赏，再求解每个状态的两步累积奖赏，一直迭代逐步求解出每个状态的T步累积奖赏。算法流程如下所示：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwt9C8.png" alt="iwt9C8.png"></p><p>对于状态-动作值函数，只需通过简单的转化便可得到：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwt3r9.png" alt="iwt3r9.png"></p><h3 id="16-3-2-策略改进"><a href="#16-3-2-策略改进" class="headerlink" title="16.3.2 策略改进"></a><strong>16.3.2 策略改进</strong></h3><p>理想的策略应能使得每个状态的累积奖赏之和最大，简单来理解就是：不管处于什么状态，只要通过该策略执行动作，总能得到较好的结果。因此对于给定的某个策略，我们需要对其进行改进，从而得到<strong>最优的值函数</strong>。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwtm5V.png" alt="iwtm5V.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwtZEq.png" alt="iwtZEq.png"></p><p>最优Bellman等式改进策略的方式为：<strong>将策略选择的动作改为当前最优的动作</strong>，而不是像之前那样对每种可能的动作进行求和。易知：选择当前最优动作相当于将所有的概率都赋给累积奖赏值最大的动作，因此每次改进都会使得值函数单调递增。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwtEbn.png" alt="iwtEbn.png"></p><p>将策略评估与策略改进结合起来，我们便得到了生成最优策略的方法：先给定一个随机策略，现对该策略进行评估，然后再改进，接着再评估/改进一直到策略收敛、不再发生改变。这便是策略迭代算法，算法流程如下所示：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwteU0.png" alt="iwteU0.png"></p><p>可以看出：策略迭代法在每次改进策略后都要对策略进行重新评估，因此比较耗时。若从最优化值函数的角度出发，即先迭代得到最优的值函数，再来计算如何改变策略，这便是值迭代算法，算法流程如下所示：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwtuCT.png" alt="iwtuCT.png"></p><h2 id="16-4-蒙特卡罗强化学习"><a href="#16-4-蒙特卡罗强化学习" class="headerlink" title="16.4 蒙特卡罗强化学习"></a><strong>16.4 蒙特卡罗强化学习</strong></h2><p>在现实的强化学习任务中，<strong>环境的转移函数与奖赏函数往往很难得知</strong>，因此我们需要考虑在不依赖于环境参数的条件下建立强化学习模型，这便是<strong>免模型学习</strong>。蒙特卡罗强化学习便是其中的一种经典方法。</p><p>由于模型参数未知，状态值函数不能像之前那样进行全概率展开，从而运用动态规划法求解。一种直接的方法便是通过采样来对策略进行评估/估算其值函数，<strong>蒙特卡罗强化学习正是基于采样来估计状态-动作值函数</strong>：对采样轨迹中的每一对状态-动作，记录其后的奖赏值之和，作为该状态-动作的一次累积奖赏，通过多次采样后，使用累积奖赏的平均作为状态-动作值的估计，并<strong>引入ε-贪心策略保证采样的多样性</strong>。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwt1KJ.png" alt="iwt1KJ.png"></p><p>在上面的算法流程中，被评估和被改进的都是同一个策略，因此称为<strong>同策略蒙特卡罗强化学习算法</strong>。引入ε-贪心仅是为了便于采样评估，而在使用策略时并不需要ε-贪心，那能否仅在评估时使用ε-贪心策略，而在改进时使用原始策略呢？这便是<strong>异策略蒙特卡罗强化学习算法</strong>。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwtK8U.png" alt="iwtK8U.png"></p><h2 id="16-5-AlphaGo原理浅析"><a href="#16-5-AlphaGo原理浅析" class="headerlink" title="16.5 AlphaGo原理浅析"></a><strong>16.5 AlphaGo原理浅析</strong></h2><p>本篇一开始便提到强化学习是AlphaGo的核心技术之一，刚好借着这个东风将AlphaGo的工作原理了解一番。正如人类下棋那般“<strong>手下一步棋，心想三步棋</strong>”，Alphago也正是这个思想，<strong>当处于一个状态时，机器会暗地里进行多次的尝试/采样，并基于反馈回来的结果信息改进估值函数，从而最终通过增强版的估值函数来选择最优的落子动作。</strong></p><p>其中便涉及到了三个主要的问题：<strong>（1）如何确定估值函数（2）如何进行采样（3）如何基于反馈信息改进估值函数</strong>，这正对应着AlphaGo的三大核心模块：<strong>深度学习</strong>、<strong>蒙特卡罗搜索树</strong>、<strong>强化学习</strong>。</p><blockquote><p><strong>1.深度学习（拟合估值函数）</strong></p></blockquote><p>由于围棋的状态空间巨大，像蒙特卡罗强化学习那样通过采样来确定值函数就行不通了。在围棋中，<strong>状态值函数可以看作为一种局面函数，状态-动作值函数可以看作一种策略函数</strong>，若我们能获得这两个估值函数，便可以根据这两个函数来完成：(1)衡量当前局面的价值；(2)选择当前最优的动作。那如何精确地估计这两个估值函数呢？<strong>这就用到了深度学习，通过大量的对弈数据自动学习出特征，从而拟合出估值函数。</strong></p><blockquote><p><strong>2.蒙特卡罗搜索树（采样）</strong></p></blockquote><p>蒙特卡罗树是一种经典的搜索框架，它通过反复地采样模拟对局来探索状态空间。具体表现在：从当前状态开始，利用策略函数尽可能选择当前最优的动作，同时也引入随机性来减小估值错误带来的负面影响，从而模拟棋局运行，使得棋盘达到终局或一定步数后停止。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwtM2F.png" alt="iwtM2F.png"></p><blockquote><p><strong>3.强化学习（调整估值函数）</strong></p></blockquote><p>在使用蒙特卡罗搜索树进行多次采样后，每次采样都会反馈后续的局面信息（利用局面函数进行评价），根据反馈回来的结果信息自动调整两个估值函数的参数，这便是强化学习的核心思想，最后基于改进后的策略函数选择出当前最优的落子动作。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwtQv4.png" alt="iwtQv4.png"></p><p>在此，强化学习就介绍完毕。同时也意味着大口小口地啃完了这个西瓜，十分记得去年双11之后立下这个Flag，现在回想起来，大半年的时间里在嚼瓜上还是花费了不少功夫。有人说：当你阐述的能让别人看懂才算是真的理解，有人说：在写的过程中能发现那些只看书发现不了的东西，自己最初的想法十分简单：当健忘症发作的时候，如果能看到之前按照自己思路写下的文字，回忆便会汹涌澎湃一些~</p><p>最后，感谢自己这大半年以来的坚持~Get busy living, or get busy dying!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了概率图模型，首先从生成式模型与判别式模型的定义出发，引出了概率图模型的基本概念，即利用图结构来表达变量之间的依赖关系；接着分别介绍了隐马尔可夫模型、马尔可夫随机场、条件随机场、精确推断方法以及LDA话题模型：HMM主要围绕着评估/解码/学习这三个实际问题展开论述；MRF基于团和势函数的概念来定义联合概率分布；CRF引入两种特征函数对状态序列进行评价打分；变量消去与信念传播在给定联合概率分布后计算特定变量的边际分布；LDA话题模型则试图去推断给定文档所蕴含的话题分布。本篇将介绍最后一种学习算法—强化学习。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(16)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-07-27T08:20:38.000Z</published>
    <updated>2020-07-27T11:09:55.864Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了半监督学习，首先从如何利用未标记样本所蕴含的分布信息出发，引入了半监督学习的基本概念，即训练数据同时包含有标记样本和未标记样本的学习方法；接着分别介绍了几种常见的半监督学习方法：生成式方法基于对数据分布的假设，利用未标记样本隐含的分布信息，使得对模型参数的估计更加准确；TSVM给未标记样本赋予伪标记，并通过不断调整易出错样本的标记得到最终输出；基于分歧的方法结合了集成学习的思想，通过多个学习器在不同视图上的协作，有效利用了未标记样本数据 ；最后半监督聚类则是借助已有的监督信息来辅助聚类的过程，带约束k-均值算法需检测当前样本划分是否满足约束关系，带标记k-均值算法则利用有标记样本指定初始类中心。本篇将讨论一种基于图的学习算法—概率图模型。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="15、概率图模型"><a href="#15、概率图模型" class="headerlink" title="15、概率图模型"></a><strong>15、概率图模型</strong></h1><p>现在再来谈谈机器学习的核心价值观，可以更通俗地理解为：<strong>根据一些已观察到的证据来推断未知</strong>，更具哲学性地可以阐述为：未来的发展总是遵循着历史的规律。其中<strong>基于概率的模型将学习任务归结为计算变量的概率分布</strong>，正如之前已经提到的：生成式模型先对联合分布进行建模，从而再来求解后验概率，例如：贝叶斯分类器先对联合分布进行最大似然估计，从而便可以计算类条件概率；判别式模型则是直接对条件分布进行建模。</p><p><strong>概率图模型</strong>（probabilistic graphical model）是一类用<strong>图结构</strong>来表达各属性之间相关关系的概率模型，一般而言：<strong>图中的一个结点表示一个或一组随机变量，结点之间的边则表示变量间的相关关系</strong>，从而形成了一张“<strong>变量关系图</strong>”。若使用有向的边来表达变量之间的依赖关系，这样的有向关系图称为<strong>贝叶斯网</strong>（Bayesian nerwork）或有向图模型；若使用无向边，则称为<strong>马尔可夫网</strong>（Markov network）或无向图模型。</p><h2 id="15-1-隐马尔可夫模型-HMM"><a href="#15-1-隐马尔可夫模型-HMM" class="headerlink" title="15.1 隐马尔可夫模型(HMM)"></a><strong>15.1 隐马尔可夫模型(HMM)</strong></h2><p>隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的一种贝叶斯网，在语音识别与自然语言处理领域上有着广泛的应用。HMM中的变量分为两组：<strong>状态变量</strong>与<strong>观测变量</strong>，其中状态变量一般是未知的，因此又称为“<strong>隐变量</strong>”，观测变量则是已知的输出值。在隐马尔可夫模型中，变量之间的依赖关系遵循如下两个规则：</p><blockquote><p><strong>1. 观测变量的取值仅依赖于状态变量</strong>；<br><strong>2. 下一个状态的取值仅依赖于当前状态</strong>，通俗来讲：<strong>现在决定未来，未来与过去无关</strong>，这就是著名的<strong>马尔可夫性</strong>。</p></blockquote><p><img src="https://s1.ax1x.com/2018/10/18/iwYPmR.png" alt="iwYPmR.png"></p><p>基于上述变量之间的依赖关系，我们很容易写出隐马尔可夫模型中所有变量的联合概率分布：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwY9X9.png" alt="iwY9X9.png"></p><p>易知：<strong>欲确定一个HMM模型需要以下三组参数</strong>：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYi01.png" alt="iwYi01.png"></p><p>当确定了一个HMM模型的三个参数后，便按照下面的规则来生成观测值序列：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYFTx.png" alt="iwYFTx.png"></p><p>在实际应用中，HMM模型的发力点主要体现在下述三个问题上：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYEtK.png" alt="iwYEtK.png"></p><h3 id="15-1-1-HMM评估问题"><a href="#15-1-1-HMM评估问题" class="headerlink" title="15.1.1 HMM评估问题"></a><strong>15.1.1 HMM评估问题</strong></h3><p>HMM评估问题指的是：<strong>给定了模型的三个参数与观测值序列，求该观测值序列出现的概率</strong>。例如：对于赌场问题，便可以依据骰子掷出的结果序列来计算该结果序列出现的可能性，若小概率的事件发生了则可认为赌场的骰子有作弊的可能。解决该问题使用的是<strong>前向算法</strong>，即步步为营，自底向上的方式逐步增加序列的长度，直到获得目标概率值。在前向算法中，定义了一个<strong>前向变量</strong>，即给定观察值序列且t时刻的状态为Si的概率：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYVfO.png" alt="iwYVfO.png"></p><p>基于前向变量，很容易得到该问题的递推关系及终止条件：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYAk6.png" alt="iwYAk6.png"></p><p>因此可使用动态规划法，从最小的子问题开始，通过填表格的形式一步一步计算出目标结果。</p><h3 id="15-1-2-HMM解码问题"><a href="#15-1-2-HMM解码问题" class="headerlink" title="15.1.2 HMM解码问题"></a><strong>15.1.2 HMM解码问题</strong></h3><p>HMM解码问题指的是：<strong>给定了模型的三个参数与观测值序列，求可能性最大的状态序列</strong>。例如：在语音识别问题中，人说话形成的数字信号对应着观测值序列，对应的具体文字则是状态序列，从数字信号转化为文字正是对应着根据观测值序列推断最有可能的状态值序列。解决该问题使用的是<strong>Viterbi算法</strong>，与前向算法十分类似地，Viterbi算法定义了一个<strong>Viterbi变量</strong>，也是采用动态规划的方法，自底向上逐步求解。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYepD.png" alt="iwYepD.png"></p><h3 id="15-1-3-HMM学习问题"><a href="#15-1-3-HMM学习问题" class="headerlink" title="15.1.3 HMM学习问题"></a><strong>15.1.3 HMM学习问题</strong></h3><p>HMM学习问题指的是：<strong>给定观测值序列，如何调整模型的参数使得该序列出现的概率最大</strong>。这便转化成了机器学习问题，即从给定的观测值序列中学习出一个HMM模型，<strong>该问题正是EM算法的经典案例之一</strong>。其思想也十分简单：对于给定的观测值序列，如果我们能够按照该序列潜在的规律来调整模型的三个参数，则可以使得该序列出现的可能性最大。假设状态值序列也已知，则很容易计算出与该序列最契合的模型参数：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYm1e.png" alt="iwYm1e.png"></p><p>但一般状态值序列都是不可观测的，且<strong>即使给定观测值序列与模型参数，状态序列仍然遭遇组合爆炸</strong>。因此上面这种简单的统计方法就行不通了，若将状态值序列看作为隐变量，这时便可以考虑使用EM算法来对该问题进行求解：</p><p>【1】首先对HMM模型的三个参数进行随机初始化；<br>【2】根据模型的参数与观测值序列，计算t时刻状态为i且t+1时刻状态为j的概率以及t时刻状态为i的概率。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYn6H.png" alt="iwYn6H.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYdns.png" alt="iwYdns.png"></p><p>【3】接着便可以对模型的三个参数进行重新估计：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYY9S.png" alt="iwYY9S.png"></p><p>【4】重复步骤2-3，直至三个参数值收敛，便得到了最终的HMM模型。</p><h2 id="15-2-马尔可夫随机场（MRF）"><a href="#15-2-马尔可夫随机场（MRF）" class="headerlink" title="15.2 马尔可夫随机场（MRF）"></a><strong>15.2 马尔可夫随机场（MRF）</strong></h2><p>马尔可夫随机场（Markov Random Field）是一种典型的马尔可夫网，即使用无向边来表达变量间的依赖关系。在马尔可夫随机场中，对于关系图中的一个子集，<strong>若任意两结点间都有边连接，则称该子集为一个团；若再加一个结点便不能形成团，则称该子集为极大团</strong>。MRF使用<strong>势函数</strong>来定义多个变量的概率分布函数，其中<strong>每个（极大）团对应一个势函数</strong>，一般团中的变量关系也体现在它所对应的极大团中，因此常常基于极大团来定义变量的联合概率分布函数。具体而言，若所有变量构成的极大团的集合为C，则MRF的联合概率函数可以定义为：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYGh8.png" alt="iwYGh8.png"></p><p>对于条件独立性，<strong>马尔可夫随机场通过分离集来实现条件独立</strong>，若A结点集必须经过C结点集才能到达B结点集，则称C为分离集。书上给出了一个简单情形下的条件独立证明过程，十分贴切易懂，此处不再展开。基于分离集的概念，得到了MRF的三个性质：</p><blockquote><p><strong>全局马尔可夫性</strong>：给定两个变量子集的分离集，则这两个变量子集条件独立。<br><strong>局部马尔可夫性</strong>：给定某变量的邻接变量，则该变量与其它变量条件独立。<br><strong>成对马尔可夫性</strong>：给定所有其他变量，两个非邻接变量条件独立。</p></blockquote><p><img src="https://s1.ax1x.com/2018/10/18/iwY07q.png" alt="iwY07q.png"></p><p>对于MRF中的势函数，势函数主要用于描述团中变量之间的相关关系，且要求为非负函数，直观来看：势函数需要在偏好的变量取值上函数值较大，例如：若x1与x2成正相关，则需要将这种关系反映在势函数的函数值中。一般我们常使用指数函数来定义势函数：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwY8tf.png" alt="iwY8tf.png"></p><h2 id="15-3-条件随机场（CRF）"><a href="#15-3-条件随机场（CRF）" class="headerlink" title="15.3 条件随机场（CRF）"></a><strong>15.3 条件随机场（CRF）</strong></h2><p>前面所讲到的<strong>隐马尔可夫模型和马尔可夫随机场都属于生成式模型，即对联合概率进行建模，条件随机场则是对条件分布进行建模</strong>。CRF试图在给定观测值序列后，对状态序列的概率分布进行建模，即P(y | x)。直观上看：CRF与HMM的解码问题十分类似，都是在给定观测值序列后，研究状态序列可能的取值。CRF可以有多种结构，只需保证状态序列满足马尔可夫性即可，一般我们常使用的是<strong>链式条件随机场</strong>：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYt1g.png" alt="iwYt1g.png"></p><p>与马尔可夫随机场定义联合概率类似地，CRF也通过团以及势函数的概念来定义条件概率P(y | x)。在给定观测值序列的条件下，链式条件随机场主要包含两种团结构：单个状态团及相邻状态团，通过引入两类特征函数便可以定义出目标条件概率：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYNcQ.png" alt="iwYNcQ.png"></p><p>以词性标注为例，如何判断给出的一个标注序列靠谱不靠谱呢？<strong>转移特征函数主要判定两个相邻的标注是否合理</strong>，例如：动词+动词显然语法不通；<strong>状态特征函数则判定观测值与对应的标注是否合理</strong>，例如： ly结尾的词—&gt;副词较合理。因此我们可以定义一个特征函数集合，用这个特征函数集合来为一个标注序列打分，并据此选出最靠谱的标注序列。也就是说，每一个特征函数（对应一种规则）都可以用来为一个标注序列评分，把集合中所有特征函数对同一个标注序列的评分综合起来，就是这个标注序列最终的评分值。可以看出：<strong>特征函数是一些经验的特性</strong>。</p><h2 id="15-4-学习与推断"><a href="#15-4-学习与推断" class="headerlink" title="15.4 学习与推断"></a><strong>15.4 学习与推断</strong></h2><p>对于生成式模型，通常我们都是先对变量的联合概率分布进行建模，接着再求出目标变量的<strong>边际分布</strong>（marginal distribution），那如何从联合概率得到边际分布呢？这便是学习与推断。下面主要介绍两种精确推断的方法：<strong>变量消去</strong>与<strong>信念传播</strong>。</p><h3 id="15-4-1-变量消去"><a href="#15-4-1-变量消去" class="headerlink" title="15.4.1 变量消去"></a><strong>15.4.1 变量消去</strong></h3><p>变量消去利用条件独立性来消减计算目标概率值所需的计算量，它通过运用<strong>乘法与加法的分配率</strong>，将对变量的积的求和问题转化为对部分变量交替进行求积与求和的问题，从而将每次的<strong>运算控制在局部</strong>，达到简化运算的目的。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYUXj.png" alt="iwYUXj.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYwBn.png" alt="iwYwBn.png"></p><h3 id="15-4-2-信念传播"><a href="#15-4-2-信念传播" class="headerlink" title="15.4.2 信念传播"></a><strong>15.4.2 信念传播</strong></h3><p>若将变量求和操作看作是一种消息的传递过程，信念传播可以理解成：<strong>一个节点在接收到所有其它节点的消息后才向另一个节点发送消息</strong>，同时当前节点的边际概率正比于他所接收的消息的乘积：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYDA0.png" alt="iwYDA0.png"></p><p>因此只需要经过下面两个步骤，便可以完成所有的消息传递过程。利用动态规划法的思想记录传递过程中的所有消息，当计算某个结点的边际概率分布时，只需直接取出传到该结点的消息即可，从而避免了计算多个边际分布时的冗余计算问题。</p><blockquote><p>1.指定一个根节点，从所有的叶节点开始向根节点传递消息，直到根节点收到所有邻接结点的消息<strong>（从叶到根）</strong>；<br>2.从根节点开始向叶节点传递消息，直到所有叶节点均收到消息<strong>（从根到叶）</strong>。</p></blockquote><p><img src="https://s1.ax1x.com/2018/10/18/iwYgc4.png" alt="iwYgc4.png"></p><h2 id="15-5-LDA话题模型"><a href="#15-5-LDA话题模型" class="headerlink" title="15.5 LDA话题模型"></a><strong>15.5 LDA话题模型</strong></h2><p>话题模型主要用于处理文本类数据，其中<strong>隐狄利克雷分配模型</strong>（Latent Dirichlet Allocation，简称LDA）是话题模型的杰出代表。在话题模型中，有以下几个基本概念：词（word）、文档（document）、话题（topic）。</p><blockquote><p><strong>词</strong>：最基本的离散单元；<br><strong>文档</strong>：由一组词组成，词在文档中不计顺序；<br><strong>话题</strong>：由一组特定的词组成，这组词具有较强的相关关系。</p></blockquote><p>在现实任务中，一般我们可以得出一个文档的词频分布，但不知道该文档对应着哪些话题，LDA话题模型正是为了解决这个问题。具体来说：<strong>LDA认为每篇文档包含多个话题，且其中每一个词都对应着一个话题</strong>。因此可以假设文档是通过如下方式生成：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwY2jJ.png" alt="iwY2jJ.png"></p><p>这样一个文档中的所有词都可以认为是通过话题模型来生成的，当已知一个文档的词频分布后（即一个N维向量，N为词库大小），则可以认为：<strong>每一个词频元素都对应着一个话题，而话题对应的词频分布则影响着该词频元素的大小</strong>。因此很容易写出LDA模型对应的联合概率函数：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwYc3F.png" alt="iwYc3F.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwYWu9.png" alt="iwYWu9.png"></p><p>从上图可以看出，LDA的三个表示层被三种颜色表示出来：</p><blockquote><p><strong>corpus-level（红色）：</strong> α和β表示语料级别的参数，也就是每个文档都一样，因此生成过程只采样一次。<br><strong>document-level（橙色）：</strong> θ是文档级别的变量，每个文档对应一个θ。<br><strong>word-level（绿色）：</strong> z和w都是单词级别变量，z由θ生成，w由z和β共同生成，一个单词w对应一个主题z。</p></blockquote><p>通过上面对LDA生成模型的讨论，可以知道<strong>LDA模型主要是想从给定的输入语料中学习训练出两个控制参数α和β</strong>，当学习出了这两个控制参数就确定了模型，便可以用来生成文档。其中α和β分别对应以下各个信息：</p><blockquote><p><strong>α</strong>：分布p(θ)需要一个向量参数，即Dirichlet分布的参数，用于生成一个主题θ向量；<br><strong>β</strong>：各个主题对应的单词概率分布矩阵p(w|z)。</p></blockquote><p>把w当做观察变量，θ和z当做隐藏变量，就可以通过EM算法学习出α和β，求解过程中遇到后验概率p(θ,z|w)无法直接求解，需要找一个似然函数下界来近似求解，原作者使用基于分解（factorization）假设的变分法（varialtional inference）进行计算，用到了EM算法。每次E-step输入α和β，计算似然函数，M-step最大化这个似然函数，算出α和β，不断迭代直到收敛。</p><p>在此，概率图模型就介绍完毕。上周受到协同训练的启发，让实验的小伙伴做了一个HMM的slides，结果扩充了好多知识，所以完成这篇笔记还是花费了不少功夫，还刚好赶上实验室没空调回到解放前的日子，可谓汗流之作…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了半监督学习，首先从如何利用未标记样本所蕴含的分布信息出发，引入了半监督学习的基本概念，即训练数据同时包含有标记样本和未标记样本的学习方法；接着分别介绍了几种常见的半监督学习方法：生成式方法基于对数据分布的假设，利用未标记样本隐含的分布信息，使得对模型参数的估计更加准确；TSVM给未标记样本赋予伪标记，并通过不断调整易出错样本的标记得到最终输出；基于分歧的方法结合了集成学习的思想，通过多个学习器在不同视图上的协作，有效利用了未标记样本数据 ；最后半监督聚类则是借助已有的监督信息来辅助聚类的过程，带约束k-均值算法需检测当前样本划分是否满足约束关系，带标记k-均值算法则利用有标记样本指定初始类中心。本篇将讨论一种基于图的学习算法—概率图模型。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(15)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-07-27T07:20:38.000Z</published>
    <updated>2020-07-27T11:03:27.448Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了机器学习的理论基础，首先从独立同分布引入泛化误差与经验误差，接着介绍了PAC可学习的基本概念，即以较大的概率学习出与目标概念近似的假设（泛化误差满足预设上限），对于有限假设空间：（1）可分情形时，假设空间都是PAC可学习的，即当样本满足一定的数量之后，总是可以在与训练集一致的假设中找出目标概念的近似；（2）不可分情形时，假设空间都是不可知PAC可学习的，即以较大概率学习出与当前假设空间中泛化误差最小的假设的有效近似（Hoeffding不等式）。对于无限假设空间，通过增长函数与VC维来描述其复杂度，若学习算法满足经验风险最小化原则，则任何VC维有限的假设空间都是（不可知）PAC可学习的，同时也给出了泛化误差界与样本复杂度。稳定性则考察的是输入发生变化时输出的波动，稳定性通过损失函数与假设空间的可学习理论联系在了一起。本篇将讨论一种介于监督与非监督学习之间的学习算法—半监督学习。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="14、半监督学习"><a href="#14、半监督学习" class="headerlink" title="14、半监督学习"></a><strong>14、半监督学习</strong></h1><p>前面我们一直围绕的都是监督学习与无监督学习，监督学习指的是训练样本包含标记信息的学习任务，例如：常见的分类与回归算法；无监督学习则是训练样本不包含标记信息的学习任务，例如：聚类算法。在实际生活中，常常会出现一部分样本有标记和较多样本无标记的情形，例如：做网页推荐时需要让用户标记出感兴趣的网页，但是少有用户愿意花时间来提供标记。若直接丢弃掉无标记样本集，使用传统的监督学习方法，常常会由于训练样本的不充足，使得其刻画总体分布的能力减弱，从而影响了学习器泛化性能。那如何利用未标记的样本数据呢？</p><p>一种简单的做法是通过专家知识对这些未标记的样本进行打标，但随之而来的就是巨大的人力耗费。若我们先使用有标记的样本数据集训练出一个学习器，再基于该学习器对未标记的样本进行预测，从中<strong>挑选出不确定性高或分类置信度低的样本来咨询专家并进行打标</strong>，最后使用扩充后的训练集重新训练学习器，这样便能大幅度降低标记成本，这便是<strong>主动学习</strong>（active learning），其目标是<strong>使用尽量少的/有价值的咨询来获得更好的性能</strong>。</p><p>显然，<strong>主动学习需要与外界进行交互/查询/打标，其本质上仍然属于一种监督学习</strong>。事实上，无标记样本虽未包含标记信息，但它们与有标记样本一样都是从总体中独立同分布采样得到，因此<strong>它们所包含的数据分布信息对学习器的训练大有裨益</strong>。如何让学习过程不依赖外界的咨询交互，自动利用未标记样本所包含的分布信息的方法便是<strong>半监督学习</strong>（semi-supervised learning），<strong>即训练集同时包含有标记样本数据和未标记样本数据</strong>。</p><p><img src="https://i.loli.net/2018/10/18/5bc856e39801d.png" alt="1.png"></p><p>此外，半监督学习还可以进一步划分为<strong>纯半监督学习</strong>和<strong>直推学习</strong>，两者的区别在于：前者假定训练数据集中的未标记数据并非待预测数据，而后者假定学习过程中的未标记数据就是待预测数据。主动学习、纯半监督学习以及直推学习三者的概念如下图所示：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwJFJS.png" alt="iwJFJS.png"></p><h2 id="14-1-生成式方法"><a href="#14-1-生成式方法" class="headerlink" title="14.1 生成式方法"></a><strong>14.1 生成式方法</strong></h2><p><strong>生成式方法</strong>（generative methods）是基于生成式模型的方法，即先对联合分布P（x,c）建模，从而进一步求解 P（c | x），<strong>此类方法假定样本数据服从一个潜在的分布，因此需要充分可靠的先验知识</strong>。例如：前面已经接触到的贝叶斯分类器与高斯混合聚类，都属于生成式模型。现假定总体是一个高斯混合分布，即由多个高斯分布组合形成，从而一个子高斯分布就代表一个类簇（类别）。高斯混合分布的概率密度函数如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc856e3b82dc.png" alt="3.png"></p><p>不失一般性，假设类簇与真实的类别按照顺序一一对应，即第i个类簇对应第i个高斯混合成分。与高斯混合聚类类似地，这里的主要任务也是估计出各个高斯混合成分的参数以及混合系数，不同的是：对于有标记样本，不再是可能属于每一个类簇，而是只能属于真实类标对应的特定类簇。</p><p><img src="https://i.loli.net/2018/10/18/5bc856e431d30.png" alt="4.png"></p><p>直观上来看，<strong>基于半监督的高斯混合模型有机地整合了贝叶斯分类器与高斯混合聚类的核心思想</strong>，有效地利用了未标记样本数据隐含的分布信息，从而使得参数的估计更加准确。同样地，这里也要召唤出之前的EM大法进行求解，首先对各个高斯混合成分的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类，有标记样本则直接属于特定类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新。</p><p><img src="https://i.loli.net/2018/10/18/5bc856e43ff08.png" alt="5.png"></p><p>当参数迭代更新收敛后，对于待预测样本x，便可以像贝叶斯分类器那样计算出样本属于每个类簇的后验概率，接着找出概率最大的即可：</p><p><img src="https://i.loli.net/2018/10/18/5bc856e3dfb1c.png" alt="6.png"></p><p>可以看出：基于生成式模型的方法十分依赖于对潜在数据分布的假设，即假设的分布要能和真实分布相吻合，否则利用未标记的样本数据反倒会在错误的道路上渐行渐远，从而降低学习器的泛化性能。因此，<strong>此类方法要求极强的领域知识和掐指观天的本领</strong>。</p><h2 id="14-2-半监督SVM"><a href="#14-2-半监督SVM" class="headerlink" title="14.2 半监督SVM"></a><strong>14.2 半监督SVM</strong></h2><p>监督学习中的SVM试图找到一个划分超平面，使得两侧支持向量之间的间隔最大，即“<strong>最大划分间隔</strong>”思想。对于半监督学习，S3VM则考虑超平面需穿过数据低密度的区域。TSVM是半监督支持向量机中的最著名代表，其核心思想是：尝试为未标记样本找到合适的标记指派，使得超平面划分后的间隔最大化。TSVM采用局部搜索的策略来进行迭代求解，即首先使用有标记样本集训练出一个初始SVM，接着使用该学习器对未标记样本进行打标，这样所有样本都有了标记，并基于这些有标记的样本重新训练SVM，之后再寻找易出错样本不断调整。整个算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc856e427830.png" alt="7.png"></p><p><img src="https://s1.ax1x.com/2018/10/18/iwJZss.png" alt="iwJZss.png"></p><h2 id="14-3-基于分歧的方法"><a href="#14-3-基于分歧的方法" class="headerlink" title="14.3 基于分歧的方法"></a><strong>14.3 基于分歧的方法</strong></h2><p>基于分歧的方法通过多个学习器之间的<strong>分歧（disagreement）/多样性（diversity）</strong>来利用未标记样本数据，协同训练就是其中的一种经典方法。<strong>协同训练最初是针对于多视图（multi-view）数据而设计的，多视图数据指的是样本对象具有多个属性集，每个属性集则对应一个试图</strong>。例如：电影数据中就包含画面类属性和声音类属性，这样画面类属性的集合就对应着一个视图。首先引入两个关于视图的重要性质：</p><blockquote><p><strong>相容性</strong>：即使用单个视图数据训练出的学习器的输出空间是一致的。例如都是{好，坏}、{+1,-1}等。<br><strong>互补性</strong>：即不同视图所提供的信息是互补/相辅相成的，实质上这里体现的就是集成学习的思想。</p></blockquote><p>协同训练正是很好地利用了多视图数据的“<strong>相容互补性</strong>”，其基本的思想是：首先基于有标记样本数据在每个视图上都训练一个初始分类器，然后让每个分类器去挑选分类置信度最高的样本并赋予标记，并将带有伪标记的样本数据传给另一个分类器去学习，从而<strong>你依我侬/共同进步</strong>。</p><p><img src="https://s1.ax1x.com/2018/10/18/iwJVMj.png" alt="iwJVMj.png"><br><img src="https://s1.ax1x.com/2018/10/18/iwJeLn.png" alt="iwJeLn.png"></p><h2 id="14-4-半监督聚类"><a href="#14-4-半监督聚类" class="headerlink" title="14.4 半监督聚类"></a><strong>14.4 半监督聚类</strong></h2><p>前面提到的几种方法都是借助无标记样本数据来辅助监督学习的训练过程，从而使得学习更加充分/泛化性能得到提升；半监督聚类则是借助已有的监督信息来辅助聚类的过程。一般而言，监督信息大致有两种类型：</p><blockquote><p><strong>必连与勿连约束</strong>：必连指的是两个样本必须在同一个类簇，勿连则是必不在同一个类簇。<br><strong>标记信息</strong>：少量的样本带有真实的标记。</p></blockquote><p>下面主要介绍两种基于半监督的K-Means聚类算法：第一种是数据集包含一些必连与勿连关系，另外一种则是包含少量带有标记的样本。两种算法的基本思想都十分的简单：对于带有约束关系的k-均值算法，在迭代过程中对每个样本划分类簇时，需要<strong>检测当前划分是否满足约束关系</strong>，若不满足则会将该样本划分到距离次小对应的类簇中，再继续检测是否满足约束关系，直到完成所有样本的划分。算法流程如下图所示：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwJAzQ.png" alt="iwJAzQ.png"></p><p>对于带有少量标记样本的k-均值算法，则可以<strong>利用这些有标记样本进行类中心的指定，同时在对样本进行划分时，不需要改变这些有标记样本的簇隶属关系</strong>，直接将其划分到对应类簇即可。算法流程如下所示：</p><p><img src="https://s1.ax1x.com/2018/10/18/iwJkRg.png" alt="iwJkRg.png"></p><p>在此，半监督学习就介绍完毕。十分有趣的是：半监督学习将前面许多知识模块联系在了一起，足以体现了作者编排的用心。结合本篇的新知识再来回想之前自己做过的一些研究，发现还是蹚了一些浑水，也许越是觉得过去的自己傻，越就是好的兆头吧~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了机器学习的理论基础，首先从独立同分布引入泛化误差与经验误差，接着介绍了PAC可学习的基本概念，即以较大的概率学习出与目标概念近似的假设（泛化误差满足预设上限），对于有限假设空间：（1）可分情形时，假设空间都是PAC可学习的，即当样本满足一定的数量之后，总是可以在与训练集一致的假设中找出目标概念的近似；（2）不可分情形时，假设空间都是不可知PAC可学习的，即以较大概率学习出与当前假设空间中泛化误差最小的假设的有效近似（Hoeffding不等式）。对于无限假设空间，通过增长函数与VC维来描述其复杂度，若学习算法满足经验风险最小化原则，则任何VC维有限的假设空间都是（不可知）PAC可学习的，同时也给出了泛化误差界与样本复杂度。稳定性则考察的是输入发生变化时输出的波动，稳定性通过损失函数与假设空间的可学习理论联系在了一起。本篇将讨论一种介于监督与非监督学习之间的学习算法—半监督学习。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(14)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/</id>
    <published>2020-07-27T06:20:38.000Z</published>
    <updated>2020-07-27T11:03:20.664Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了常用的特征选择方法及稀疏学习。首先从相关/无关特征出发引出了特征选择的基本概念，接着分别介绍了子集搜索与评价、过滤式、包裹式以及嵌入式四种类型的特征选择方法。子集搜索与评价使用的是一种优中生优的贪婪算法，即每次从候选特征子集中选出最优子集；过滤式方法计算一个相关统计量来评判特征的重要程度；包裹式方法将学习器作为特征选择的评价准则；嵌入式方法则是通过L1正则项将特征选择融入到学习器参数优化的过程中。最后介绍了稀疏表示与压缩感知的核心思想：稀疏表示利用稀疏矩阵的优良性质，试图通过某种方法找到原始稠密矩阵的合适稀疏表示；压缩感知则试图利用可稀疏表示的欠采样信息来恢复全部信息。本篇将讨论一种为机器学习提供理论保证的学习方法—计算学习理论。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="13、计算学习理论"><a href="#13、计算学习理论" class="headerlink" title="13、计算学习理论"></a><strong>13、计算学习理论</strong></h1><p>计算学习理论（computational learning theory）是通过“计算”来研究机器学习的理论，简而言之，其目的是分析学习任务的本质，例如：<strong>在什么条件下可进行有效的学习，需要多少训练样本能获得较好的精度等，从而为机器学习算法提供理论保证</strong>。</p><p>首先我们回归初心，再来谈谈经验误差和泛化误差。假设给定训练集D，其中所有的训练样本都服从一个未知的分布T，且它们都是在总体分布T中独立采样得到，即<strong>独立同分布</strong>（independent and identically distributed，i.i.d.），在《贝叶斯分类器》中我们已经提到：独立同分布是很多统计学习算法的基础假设，例如最大似然法，贝叶斯分类器，高斯混合聚类等，简单来理解独立同分布：每个样本都是从总体分布中独立采样得到，而没有拖泥带水。例如现在要进行问卷调查，要从总体人群中随机采样，看到一个美女你高兴地走过去，结果她男票突然冒了出来，说道：you jump，i jump，于是你本来只想调查一个人结果被强行撒了一把狗粮得到两份问卷，这样这两份问卷就不能称为独立同分布了，因为它们的出现具有强相关性。</p><p>回归正题，<strong>泛化误差指的是学习器在总体上的预测误差，经验误差则是学习器在某个特定数据集D上的预测误差</strong>。在实际问题中，往往我们并不能得到总体且数据集D是通过独立同分布采样得到的，因此我们常常使用经验误差作为泛化误差的近似。</p><p><img src="https://i.loli.net/2018/10/18/5bc854f38d4fe.png" alt="1.png"></p><h2 id="13-1-PAC学习"><a href="#13-1-PAC学习" class="headerlink" title="13.1 PAC学习"></a><strong>13.1 PAC学习</strong></h2><p>在高中课本中，我们将<strong>函数定义为：从自变量到因变量的一种映射；对于机器学习算法，学习器也正是为了寻找合适的映射规则</strong>，即如何从条件属性得到目标属性。从样本空间到标记空间存在着很多的映射，我们将每个映射称之为<strong>概念</strong>（concept），定义：</p><blockquote><p>若概念c对任何样本x满足c(x)=y，则称c为<strong>目标概念</strong>，即最理想的映射，所有的目标概念构成的集合称为<strong>“概念类”</strong>；<br>给定学习算法，它所有可能映射/概念的集合称为<strong>“假设空间”</strong>，其中单个的概念称为<strong>“假设”</strong>（hypothesis）；<br>若一个算法的假设空间包含目标概念，则称该数据集对该算法是<strong>可分</strong>（separable）的，亦称<strong>一致</strong>（consistent）的；<br>若一个算法的假设空间不包含目标概念，则称该数据集对该算法是<strong>不可分</strong>（non-separable）的，或称<strong>不一致</strong>（non-consistent）的。</p></blockquote><p>举个简单的例子：对于非线性分布的数据集，若使用一个线性分类器，则该线性分类器对应的假设空间就是空间中所有可能的超平面，显然假设空间不包含该数据集的目标概念，所以称数据集对该学习器是不可分的。给定一个数据集D，我们希望模型学得的假设h尽可能地与目标概念一致，这便是<strong>概率近似正确</strong> (Probably Approximately Correct，简称PAC)的来源，即以较大的概率学得模型满足误差的预设上限。</p><p><img src="https://i.loli.net/2018/10/18/5bc854f446f97.png" alt="2.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc854f482d0b.png" alt="3.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc854f47d006.png" alt="4.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc854f46ad91.png" alt="5.png"></p><p>上述关于PAC的几个定义层层相扣：定义12.1表达的是对于某种学习算法，如果能以一个置信度学得假设满足泛化误差的预设上限，则称该算法能PAC辨识概念类，即该算法的输出假设已经十分地逼近目标概念。定义12.2则将样本数量考虑进来，当样本超过一定数量时，学习算法总是能PAC辨识概念类，则称概念类为PAC可学习的。定义12.3将学习器运行时间也考虑进来，若运行时间为多项式时间，则称PAC学习算法。</p><p>显然，PAC学习中的一个关键因素就是<strong>假设空间的复杂度</strong>，对于某个学习算法，<strong>若假设空间越大，则其中包含目标概念的可能性也越大，但同时找到某个具体概念的难度也越大</strong>，一般假设空间分为有限假设空间与无限假设空间。</p><h2 id="13-2-有限假设空间"><a href="#13-2-有限假设空间" class="headerlink" title="13.2 有限假设空间"></a><strong>13.2 有限假设空间</strong></h2><h3 id="13-2-1-可分情形"><a href="#13-2-1-可分情形" class="headerlink" title="13.2.1 可分情形"></a><strong>13.2.1 可分情形</strong></h3><p>可分或一致的情形指的是：<strong>目标概念包含在算法的假设空间中</strong>。对于目标概念，在训练集D中的经验误差一定为0，因此首先我们可以想到的是：不断地剔除那些出现预测错误的假设，直到找到经验误差为0的假设即为目标概念。但<strong>由于样本集有限，可能会出现多个假设在D上的经验误差都为0，因此问题转化为：需要多大规模的数据集D才能让学习算法以置信度的概率从这些经验误差都为0的假设中找到目标概念的有效近似</strong>。</p><p><img src="https://i.loli.net/2018/10/18/5bc854f484f30.png" alt="6.png"></p><p>通过上式可以得知：<strong>对于可分情形的有限假设空间，目标概念都是PAC可学习的，即当样本数量满足上述条件之后，在与训练集一致的假设中总是可以在1-σ概率下找到目标概念的有效近似。</strong></p><h3 id="13-2-2-不可分情形"><a href="#13-2-2-不可分情形" class="headerlink" title="13.2.2 不可分情形"></a><strong>13.2.2 不可分情形</strong></h3><p>不可分或不一致的情形指的是：<strong>目标概念不存在于假设空间中</strong>，这时我们就不能像可分情形时那样从假设空间中寻找目标概念的近似。但<strong>当假设空间给定时，必然存一个假设的泛化误差最小，若能找出此假设的有效近似也不失为一个好的目标，这便是不可知学习(agnostic learning)的来源。</strong></p><p><img src="https://i.loli.net/2018/10/18/5bc854f485f2e.png" alt="7.png"></p><p>这时候便要用到<strong>Hoeffding不等式</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc854f46970a.png" alt="8.png"></p><p>对于假设空间中的所有假设，出现泛化误差与经验误差之差大于e的概率和为：</p><p><img src="https://i.loli.net/2018/10/18/5bc854f4114fd.png" alt="9.png"></p><p>因此，可令不等式的右边小于（等于）σ，便可以求出满足泛化误差与经验误差相差小于e所需的最少样本数，同时也可以求出泛化误差界。</p><p><img src="https://i.loli.net/2018/10/18/5bc854f440a02.png" alt="10.png"></p><h2 id="13-3-VC维"><a href="#13-3-VC维" class="headerlink" title="13.3 VC维"></a><strong>13.3 VC维</strong></h2><p>现实中的学习任务通常都是无限假设空间，例如d维实数域空间中所有的超平面等，因此要对此种情形进行可学习研究，需要度量<strong>假设空间的复杂度</strong>。这便是<strong>VC维</strong>（Vapnik-Chervonenkis dimension）的来源。在介绍VC维之前，需要引入两个概念：</p><blockquote><p><strong>增长函数</strong>：对于给定数据集D，假设空间中的每个假设都能对数据集的样本赋予标记，因此一个假设对应着一种打标结果，不同假设对D的打标结果可能是相同的，也可能是不同的。随着样本数量m的增大，假设空间对样本集D的打标结果也会增多，增长函数则表示假设空间对m个样本的数据集D打标的最大可能结果数，因此<strong>增长函数描述了假设空间的表示能力与复杂度。</strong></p><p><img src="https://i.loli.net/2018/10/18/5bc855ba970cd.png" alt="11.png"></p><p><strong>打散</strong>：例如对二分类问题来说，m个样本最多有2^m个可能结果，每种可能结果称为一种<strong>“对分”</strong>，若假设空间能实现数据集D的所有对分，则称数据集能被该假设空间打散。</p></blockquote><p><strong>因此尽管假设空间是无限的，但它对特定数据集打标的不同结果数是有限的，假设空间的VC维正是它能打散的最大数据集大小</strong>。通常这样来计算假设空间的VC维：若存在大小为d的数据集能被假设空间打散，但不存在任何大小为d+1的数据集能被假设空间打散，则其VC维为d。</p><p><img src="https://i.loli.net/2018/10/18/5bc855bb20c1e.png" alt="12.png"></p><p>同时书中给出了假设空间VC维与增长函数的两个关系：</p><p><img src="https://i.loli.net/2018/10/18/5bc855ba83eb8.png" alt="13.png"></p><p>直观来理解（1）式也十分容易： 首先假设空间的VC维是d，说明当m&lt;=d时，增长函数与2^m相等，例如：当m=d时，右边的组合数求和刚好等于2^d；而当m=d+1时，右边等于2^(d+1)-1，十分符合VC维的定义，同时也可以使用数学归纳法证明；（2）式则是由（1）式直接推导得出。</p><p>在有限假设空间中，根据Hoeffding不等式便可以推导得出学习算法的泛化误差界；但在无限假设空间中，由于假设空间的大小无法计算，只能通过增长函数来描述其复杂度，因此无限假设空间中的泛化误差界需要引入增长函数。</p><p><img src="https://i.loli.net/2018/10/18/5bc855babc890.png" alt="14.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc855ba5b2c3.png" alt="15.png"></p><p>上式给出了基于VC维的泛化误差界，同时也可以计算出满足条件需要的样本数（样本复杂度）。若学习算法满足<strong>经验风险最小化原则（ERM）</strong>，即学习算法的输出假设h在数据集D上的经验误差最小，可证明：<strong>任何VC维有限的假设空间都是（不可知）PAC可学习的，换而言之：若假设空间的最小泛化误差为0即目标概念包含在假设空间中，则是PAC可学习，若最小泛化误差不为0，则称为不可知PAC可学习。</strong></p><h2 id="13-4-稳定性"><a href="#13-4-稳定性" class="headerlink" title="13.4 稳定性"></a><strong>13.4 稳定性</strong></h2><p>稳定性考察的是当算法的输入发生变化时，输出是否会随之发生较大的变化，输入的数据集D有以下两种变化：</p><p><img src="https://i.loli.net/2018/10/18/5bc855badc5a8.png" alt="16.png"></p><p>若对数据集中的任何样本z，满足：</p><p><img src="https://i.loli.net/2018/10/18/5bc855ba59b06.png" alt="17.png"></p><p>即原学习器和剔除一个样本后生成的学习器对z的损失之差保持β稳定，称学习器关于损失函数满足<strong>β-均匀稳定性</strong>。同时若损失函数有上界，即原学习器对任何样本的损失函数不超过M，则有如下定理：</p><p><img src="https://i.loli.net/2018/10/18/5bc855babe7c3.png" alt="18.png"></p><p>事实上，<strong>若学习算法符合经验风险最小化原则（ERM）且满足β-均匀稳定性，则假设空间是可学习的</strong>。稳定性通过损失函数与假设空间的可学习联系在了一起，区别在于：假设空间关注的是经验误差与泛化误差，需要考虑到所有可能的假设；而稳定性只关注当前的输出假设。</p><p>在此，计算学习理论就介绍完毕，一看这个名字就知道这一章比较偏底层理论了，最终还是咬着牙看完了它，这里引用一段小文字来梳理一下现在的心情：“孤岂欲卿治经为博士邪？但当涉猎，见往事耳”，就当扩充知识体系吧~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了常用的特征选择方法及稀疏学习。首先从相关/无关特征出发引出了特征选择的基本概念，接着分别介绍了子集搜索与评价、过滤式、包裹式以及嵌入式四种类型的特征选择方法。子集搜索与评价使用的是一种优中生优的贪婪算法，即每次从候选特征子集中选出最优子集；过滤式方法计算一个相关统计量来评判特征的重要程度；包裹式方法将学习器作为特征选择的评价准则；嵌入式方法则是通过L1正则项将特征选择融入到学习器参数优化的过程中。最后介绍了稀疏表示与压缩感知的核心思想：稀疏表示利用稀疏矩阵的优良性质，试图通过某种方法找到原始稠密矩阵的合适稀疏表示；压缩感知则试图利用可稀疏表示的欠采样信息来恢复全部信息。本篇将讨论一种为机器学习提供理论保证的学习方法—计算学习理论。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(13)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-07-27T05:20:38.000Z</published>
    <updated>2020-07-27T11:03:15.146Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了经典的降维方法与度量学习，首先从“维数灾难”导致的样本稀疏以及距离难计算两大难题出发，引出了降维的概念，即通过某种数学变换将原始高维空间转变到一个低维的子空间，接着分别介绍了kNN、MDS、PCA、KPCA以及两种经典的流形学习方法，k近邻算法的核心在于k值的选取以及距离的度量，MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持，主成分分析试图找到一个低维超平面来表出原空间样本点，核化主成分分析先将样本点映射到高维空间，再在高维空间中使用线性降维的方法，从而解决了原空间样本非线性分布的情形，基于流形学习的降维则是一种“邻域保持”的思想，最后度量学习试图去学习出一个距离度量来等效降维的效果。本篇将讨论另一种常用方法—特征选择与稀疏学习。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="12、特征选择与稀疏学习"><a href="#12、特征选择与稀疏学习" class="headerlink" title="12、特征选择与稀疏学习"></a><strong>12、特征选择与稀疏学习</strong></h1><p>最近在看论文的过程中，发现对于数据集行和列的叫法颇有不同，故在介绍本篇之前，决定先将最常用的术语罗列一二，以后再见到了不管它脚扑朔还是眼迷离就能一眼识破真身了~对于数据集中的一个对象及组成对象的零件元素：</p><blockquote><p>统计学家常称它们为<strong>观测</strong>（<strong>observation</strong>）和<strong>变量</strong>（<strong>variable</strong>）；<br>数据库分析师则称其为<strong>记录</strong>（<strong>record</strong>）和<strong>字段</strong>（<strong>field</strong>）；<br>数据挖掘/机器学习学科的研究者则习惯把它们叫做<strong>样本</strong>/<strong>示例</strong>（<strong>example</strong>/<strong>instance</strong>）和<strong>属性</strong>/<strong>特征</strong>（<strong>attribute</strong>/<strong>feature</strong>）。</p></blockquote><p>回归正题，在机器学习中特征选择是一个重要的“<strong>数据预处理</strong>”（<strong>data</strong> <strong>preprocessing</strong>）过程，即试图从数据集的所有特征中挑选出与当前学习任务相关的特征子集，接着再利用数据子集来训练学习器；稀疏学习则是围绕着稀疏矩阵的优良性质，来完成相应的学习任务。</p><h2 id="12-1-子集搜索与评价"><a href="#12-1-子集搜索与评价" class="headerlink" title="12.1 子集搜索与评价"></a><strong>12.1 子集搜索与评价</strong></h2><p>一般地，我们可以用很多属性/特征来描述一个示例，例如对于一个人可以用性别、身高、体重、年龄、学历、专业、是否吃货等属性来描述，那现在想要训练出一个学习器来预测人的收入。根据生活经验易知：并不是所有的特征都与学习任务相关，例如年龄/学历/专业可能很大程度上影响了收入，身高/体重这些外貌属性也有较小的可能性影响收入，但像是否是一个地地道道的吃货这种属性就八杆子打不着了。因此我们只需要那些与学习任务紧密相关的特征，<strong>特征选择便是从给定的特征集合中选出相关特征子集的过程</strong>。</p><p>与上篇中降维技术有着异曲同工之处的是，特征选择也可以有效地解决维数灾难的难题。具体而言：<strong>降维从一定程度起到了提炼优质低维属性和降噪的效果，特征选择则是直接剔除那些与学习任务无关的属性而选择出最佳特征子集</strong>。若直接遍历所有特征子集，显然当维数过多时遭遇指数爆炸就行不通了；若采取从候选特征子集中不断迭代生成更优候选子集的方法，则时间复杂度大大减小。这时就涉及到了两个关键环节：<strong>1.如何生成候选子集；2.如何评价候选子集的好坏</strong>，这便是早期特征选择的常用方法。书本上介绍了贪心算法，分为三种策略：</p><blockquote><p><strong>前向搜索</strong>：初始将每个特征当做一个候选特征子集，然后从当前所有的候选子集中选择出最佳的特征子集；接着在上一轮选出的特征子集中添加一个新的特征，同样地选出最佳特征子集；最后直至选不出比上一轮更好的特征子集。<br><strong>后向搜索</strong>：初始将所有特征作为一个候选特征子集；接着尝试去掉上一轮特征子集中的一个特征并选出当前最优的特征子集；最后直到选不出比上一轮更好的特征子集。<br><strong>双向搜索</strong>：将前向搜索与后向搜索结合起来，即在每一轮中既有添加操作也有剔除操作。</p></blockquote><p>对于特征子集的评价，书中给出了一些想法及基于信息熵的方法。假设数据集的属性皆为离散属性，这样给定一个特征子集，便可以通过这个特征子集的取值将数据集合划分为V个子集。例如：A1={男,女}，A2={本科,硕士}就可以将原数据集划分为2*2=4个子集，其中每个子集的取值完全相同。这时我们就可以像决策树选择划分属性那样，通过计算信息增益来评价该属性子集的好坏。</p><p><img src="https://i.loli.net/2018/10/18/5bc853eca1a43.png" alt="1.png"></p><p>此时，信息增益越大表示该属性子集包含有助于分类的特征越多，使用上述这种<strong>子集搜索与子集评价相结合的机制，便可以得到特征选择方法</strong>。值得一提的是若将前向搜索策略与信息增益结合在一起，与前面我们讲到的ID3决策树十分地相似。事实上，决策树也可以用于特征选择，树节点划分属性组成的集合便是选择出的特征子集。</p><h2 id="12-2-过滤式选择（Relief）"><a href="#12-2-过滤式选择（Relief）" class="headerlink" title="12.2 过滤式选择（Relief）"></a><strong>12.2 过滤式选择（Relief）</strong></h2><p>过滤式方法是一种将特征选择与学习器训练相分离的特征选择技术，即首先将相关特征挑选出来，再使用选择出的数据子集来训练学习器。Relief是其中著名的代表性算法，它使用一个“<strong>相关统计量</strong>”来度量特征的重要性，该统计量是一个向量，其中每个分量代表着相应特征的重要性，因此我们最终可以根据这个统计量各个分量的大小来选择出合适的特征子集。</p><p>易知Relief算法的核心在于如何计算出该相关统计量。对于数据集中的每个样例xi，Relief首先找出与xi同类别的最近邻与不同类别的最近邻，分别称为<strong>猜中近邻（near-hit）</strong>与<strong>猜错近邻（near-miss）</strong>，接着便可以分别计算出相关统计量中的每个分量。对于j分量：</p><p><img src="https://i.loli.net/2018/10/18/5bc853ec70c88.png" alt="2.png"></p><p>直观上理解：对于猜中近邻，两者j属性的距离越小越好，对于猜错近邻，j属性距离越大越好。更一般地，若xi为离散属性，diff取海明距离，即相同取0，不同取1；若xi为连续属性，则diff为曼哈顿距离，即取差的绝对值。分别计算每个分量，最终取平均便得到了整个相关统计量。</p><p>标准的Relief算法只用于二分类问题，后续产生的拓展变体Relief-F则解决了多分类问题。对于j分量，新的计算公式如下：</p><p><img src="https://i.loli.net/2018/10/18/5bc853ec93042.png" alt="3.png"></p><p>其中pl表示第l类样本在数据集中所占的比例，易知两者的不同之处在于：<strong>标准Relief 只有一个猜错近邻，而Relief-F有多个猜错近邻</strong>。</p><h2 id="12-3-包裹式选择（LVW）"><a href="#12-3-包裹式选择（LVW）" class="headerlink" title="12.3 包裹式选择（LVW）"></a><strong>12.3 包裹式选择（LVW）</strong></h2><p>与过滤式选择不同的是，包裹式选择将后续的学习器也考虑进来作为特征选择的评价准则。因此包裹式选择可以看作是为某种学习器<strong>量身定做</strong>的特征选择方法，由于在每一轮迭代中，包裹式选择都需要训练学习器，因此在获得较好性能的同时也产生了较大的开销。下面主要介绍一种经典的包裹式特征选择方法 —LVW（Las Vegas Wrapper），它在拉斯维加斯框架下使用随机策略来进行特征子集的搜索。拉斯维加斯？怎么听起来那么耳熟，不是那个声名显赫的赌场吗？歪果仁真会玩。怀着好奇科普一下，结果又顺带了一个赌场：</p><blockquote><p><strong>蒙特卡罗算法</strong>：采样越多，越近似最优解，一定会给出解，但给出的解不一定是正确解；<br><strong>拉斯维加斯算法</strong>：采样越多，越有机会找到最优解，不一定会给出解，且给出的解一定是正确解。</p></blockquote><p>举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找较好的，但不保证是最好的。</p><p>而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（正确解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。</p><p>LVW算法的具体流程如下所示，其中比较特别的是停止条件参数T的设置，即在每一轮寻找最优特征子集的过程中，若随机T次仍没找到，算法就会停止，从而保证了算法运行时间的可行性。</p><p><img src="https://i.loli.net/2018/10/18/5bc853ed5e08e.png" alt="4.png"></p><h2 id="12-4-嵌入式选择与正则化"><a href="#12-4-嵌入式选择与正则化" class="headerlink" title="12.4 嵌入式选择与正则化"></a><strong>12.4 嵌入式选择与正则化</strong></h2><p>前面提到了的两种特征选择方法：<strong>过滤式中特征选择与后续学习器完全分离，包裹式则是使用学习器作为特征选择的评价准则；嵌入式是一种将特征选择与学习器训练完全融合的特征选择方法，即将特征选择融入学习器的优化过程中</strong>。在之前《经验风险与结构风险》中已经提到：经验风险指的是模型与训练数据的契合度，结构风险则是模型的复杂程度，机器学习的核心任务就是：<strong>在模型简单的基础上保证模型的契合度</strong>。例如：岭回归就是加上了L2范数的最小二乘法，有效地解决了奇异矩阵、过拟合等诸多问题，下面的嵌入式特征选择则是在损失函数后加上了L1范数。</p><p><img src="https://i.loli.net/2018/10/18/5bc853ec8b203.png" alt="5.png"></p><p>L1范数美名又约<strong>Lasso Regularization</strong>，指的是向量中每个元素的绝对值之和，这样在优化目标函数的过程中，就会使得w尽可能地小，在一定程度上起到了防止过拟合的作用，同时与L2范数（Ridge Regularization ）不同的是，L1范数会使得部分w变为0， 从而达到了特征选择的效果。</p><p>总的来说：<strong>L1范数会趋向产生少量的特征，其他特征的权值都是0；L2会选择更多的特征，这些特征的权值都会接近于0</strong>。这样L1范数在特征选择上就十分有用，而L2范数则具备较强的控制过拟合能力。可以从下面两个方面来理解：</p><p>（1）<strong>下降速度</strong>：L1范数按照绝对值函数来下降，L2范数按照二次函数来下降。因此在0附近，L1范数的下降速度大于L2范数，故L1范数能很快地下降到0，而L2范数在0附近的下降速度非常慢，因此较大可能收敛在0的附近。</p><p><img src="https://i.loli.net/2018/10/18/5bc853ed0aaf5.png" alt="6.png"></p><p>（2）<strong>空间限制</strong>：L1范数与L2范数都试图在最小化损失函数的同时，让权值W也尽可能地小。我们可以将原优化问题看做为下面的问题，即让后面的规则则都小于某个阈值。这样从图中可以看出：L1范数相比L2范数更容易得到稀疏解。</p><p><img src="https://i.loli.net/2018/10/18/5bc853ecc223e.png" alt="7.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc853ed51aa1.png" alt="8.png"></p><h2 id="12-5-稀疏表示与字典学习"><a href="#12-5-稀疏表示与字典学习" class="headerlink" title="12.5 稀疏表示与字典学习"></a><strong>12.5 稀疏表示与字典学习</strong></h2><p>当样本数据是一个稀疏矩阵时，对学习任务来说会有不少的好处，例如很多问题变得线性可分，储存更为高效等。这便是稀疏表示与字典学习的基本出发点。稀疏矩阵即矩阵的每一行/列中都包含了大量的零元素，且这些零元素没有出现在同一行/列，对于一个给定的稠密矩阵，若我们能<strong>通过某种方法找到其合适的稀疏表示</strong>，则可以使得学习任务更加简单高效，我们称之为<strong>稀疏编码（sparse coding）</strong>或<strong>字典学习（dictionary learning）</strong>。</p><p>给定一个数据集，字典学习/稀疏编码指的便是通过一个字典将原数据转化为稀疏表示，因此最终的目标就是求得字典矩阵B及稀疏表示α，书中使用变量交替优化的策略能较好地求得解，深感陷进去短时间无法自拔，故先不进行深入…</p><p><img src="https://i.loli.net/2018/10/18/5bc853ed0ca43.png" alt="9.png"></p><h2 id="12-6-压缩感知"><a href="#12-6-压缩感知" class="headerlink" title="12.6 压缩感知"></a><strong>12.6 压缩感知</strong></h2><p>压缩感知在前些年也是风风火火，与特征选择、稀疏表示不同的是：它关注的是通过欠采样信息来恢复全部信息。在实际问题中，为了方便传输和存储，我们一般将数字信息进行压缩，这样就有可能损失部分信息，如何根据已有的信息来重构出全部信号，这便是压缩感知的来历，压缩感知的前提是已知的信息具有稀疏表示。下面是关于压缩感知的一些背景：</p><p><img src="https://i.loli.net/2018/10/18/5bc853ed431c6.png" alt="10.png"></p><p>在此，特征选择与稀疏学习就介绍完毕。在很多实际情形中，选了好的特征比选了好的模型更为重要，这也是为什么厉害的大牛能够很快地得出一些结论的原因，谓：吾昨晚夜观天象，星象云是否吃货乃无用也~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了经典的降维方法与度量学习，首先从“维数灾难”导致的样本稀疏以及距离难计算两大难题出发，引出了降维的概念，即通过某种数学变换将原始高维空间转变到一个低维的子空间，接着分别介绍了kNN、MDS、PCA、KPCA以及两种经典的流形学习方法，k近邻算法的核心在于k值的选取以及距离的度量，MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持，主成分分析试图找到一个低维超平面来表出原空间样本点，核化主成分分析先将样本点映射到高维空间，再在高维空间中使用线性降维的方法，从而解决了原空间样本非线性分布的情形，基于流形学习的降维则是一种“邻域保持”的思想，最后度量学习试图去学习出一个距离度量来等效降维的效果。本篇将讨论另一种常用方法—特征选择与稀疏学习。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(12)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-07-27T04:20:38.000Z</published>
    <updated>2020-07-27T11:03:08.270Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了几种常用的聚类算法，首先从距离度量与性能评估出发，列举了常见的距离计算公式与聚类评价指标，接着分别讨论了K-Means、LVQ、高斯混合聚类、密度聚类以及层次聚类算法。K-Means与LVQ都试图以类簇中心作为原型指导聚类，其中K-Means通过EM算法不断迭代直至收敛，LVQ使用真实类标辅助聚类；高斯混合聚类采用高斯分布来描述类簇原型；密度聚类则是将一个核心对象所有密度可达的样本形成类簇，直到所有核心对象都遍历完；最后层次聚类是一种自底向上的树形聚类方法，不断合并最相近的两个小类簇。本篇将讨论机器学习常用的方法—降维与度量学习。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="11、降维与度量学习"><a href="#11、降维与度量学习" class="headerlink" title="11、降维与度量学习"></a><strong>11、降维与度量学习</strong></h1><p>样本的特征数称为<strong>维数</strong>（dimensionality），当维数非常大时，也就是现在所说的“<strong>维数灾难</strong>”，具体表现在：在高维情形下，<strong>数据样本将变得十分稀疏</strong>，因为此时要满足训练样本为“<strong>密采样</strong>”的总体样本数目是一个触不可及的天文数字，谓可远观而不可亵玩焉…<strong>训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力</strong>；同时当维数很高时，<strong>计算距离也变得十分复杂</strong>，甚至连计算内积都不再容易，这也是为什么支持向量机（SVM）使用核函数<strong>“低维计算，高维表现”</strong>的原因。</p><p>缓解维数灾难的一个重要途径就是<strong>降维，即通过某种数学变换将原始高维空间转变到一个低维的子空间</strong>。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这时也许会有疑问，这样降维之后不是会丢失原始数据的一部分信息吗？这是因为在很多实际的问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个<strong>低维嵌入</strong>，例如：数据属性中存在噪声属性、相似属性或冗余属性等，<strong>对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果</strong>。</p><h2 id="11-1-K近邻学习"><a href="#11-1-K近邻学习" class="headerlink" title="11.1 K近邻学习"></a><strong>11.1 K近邻学习</strong></h2><p>k近邻算法简称<strong>kNN（k-Nearest Neighbor）</strong>，是一种经典的监督学习方法，同时也实力担当入选数据挖掘十大算法。其工作机制十分简单粗暴：给定某个测试样本，kNN基于某种<strong>距离度量</strong>在训练集中找出与其距离最近的k个带有真实标记的训练样本，然后给基于这k个邻居的真实标记来进行预测，类似于前面集成学习中所讲到的基学习器结合策略：分类任务采用投票法，回归任务则采用平均法。接下来本篇主要就kNN分类进行讨论。</p><p><img src="https://i.loli.net/2018/10/18/5bc851a43873a.png" alt="1.png"></p><p>从上图【来自Wiki】中我们可以看到，图中有两种类型的样本，一类是蓝色正方形，另一类是红色三角形。而那个绿色圆形是我们待分类的样本。基于kNN算法的思路，我们很容易得到以下结论：</p><blockquote><p>如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。<br>如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。</p></blockquote><p>可以发现：<strong>kNN虽然是一种监督学习方法，但是它却没有显式的训练过程</strong>，而是当有新样本需要预测时，才来计算出最近的k个邻居，因此<strong>kNN是一种典型的懒惰学习方法</strong>，再来回想一下朴素贝叶斯的流程，训练的过程就是参数估计，因此朴素贝叶斯也可以懒惰式学习，此类技术在<strong>训练阶段开销为零</strong>，待收到测试样本后再进行计算。相应地我们称那些一有训练数据立马开工的算法为“<strong>急切学习</strong>”，可见前面我们学习的大部分算法都归属于急切学习。</p><p>很容易看出：<strong>kNN算法的核心在于k值的选取以及距离的度量</strong>。k值选取太小，模型很容易受到噪声数据的干扰，例如：极端地取k=1，若待分类样本正好与一个噪声数据距离最近，就导致了分类错误；若k值太大， 则在更大的邻域内进行投票，此时模型的预测能力大大减弱，例如：极端取k=训练样本数，就相当于模型根本没有学习，所有测试样本的预测结果都是一样的。<strong>一般地我们都通过交叉验证法来选取一个适当的k值</strong>。</p><p><img src="https://i.loli.net/2018/10/18/5bc851a47db9a.png" alt="2.png"></p><p>对于距离度量，<strong>不同的度量方法得到的k个近邻不尽相同，从而对最终的投票结果产生了影响</strong>，因此选择一个合适的距离度量方法也十分重要。在上一篇聚类算法中，在度量样本相似性时介绍了常用的几种距离计算方法，包括<strong>闵可夫斯基距离，曼哈顿距离，VDM</strong>等。在实际应用中，<strong>kNN的距离度量函数一般根据样本的特性来选择合适的距离度量，同时应对数据进行去量纲/归一化处理来消除大量纲属性的强权政治影响</strong>。</p><h2 id="11-2-MDS算法"><a href="#11-2-MDS算法" class="headerlink" title="11.2 MDS算法"></a><strong>11.2 MDS算法</strong></h2><p>不管是使用核函数升维还是对数据降维，我们都希望<strong>原始空间样本点之间的距离在新空间中基本保持不变</strong>，这样才不会使得原始空间样本之间的关系及总体分布发生较大的改变。<strong>“多维缩放”（MDS）</strong>正是基于这样的思想，<strong>MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持</strong>。</p><p>假定m个样本在原始空间中任意两两样本之间的距离矩阵为D∈R(m<em>m)，我们的目标便是获得样本在低维空间中的表示Z∈R(d’</em>m , d’&lt; d)，且任意两个样本在低维空间中的欧式距离等于原始空间中的距离，即||zi-zj||=Dist(ij)。因此接下来我们要做的就是根据已有的距离矩阵D来求解出降维后的坐标矩阵Z。</p><p><img src="https://i.loli.net/2018/10/18/5bc851a4b664e.png" alt="3.png"></p><p>令降维后的样本坐标矩阵Z被中心化，<strong>中心化是指将每个样本向量减去整个样本集的均值向量，故所有样本向量求和得到一个零向量</strong>。这样易知：矩阵B的每一列以及每一列求和均为0，因为提取公因子后都有一项为所有样本向量的和向量。</p><p><img src="https://i.loli.net/2018/10/18/5bc851a4a4ee2.png" alt="4.png"></p><p>根据上面矩阵B的特征，我们很容易得到等式（2）、（3）以及（4）：</p><p><img src="https://i.loli.net/2018/10/18/5bc851a4a777b.png" alt="5.png"></p><p>这时根据(1)—(4)式我们便可以计算出bij，即<strong>bij=(1)-(2)<em>(1/m)-(3)</em>(1/m)+(4)*(1/(m^2))</strong>，再逐一地计算每个b(ij)，就得到了降维后低维空间中的内积矩阵B(B=Z’*Z)，只需对B进行特征值分解便可以得到Z。MDS的算法流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc851a5340dd.png" alt="6.png"></p><h2 id="11-3-主成分分析（PCA）"><a href="#11-3-主成分分析（PCA）" class="headerlink" title="11.3 主成分分析（PCA）"></a><strong>11.3 主成分分析（PCA）</strong></h2><p>不同于MDS采用距离保持的方法，<strong>主成分分析（PCA）直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中</strong>。简单来理解这一过程便是：<strong>PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。</strong></p><p>假设使用d’个新基向量来表示原来样本，实质上是将样本投影到一个由d’个基向量确定的一个<strong>超平面</strong>上（<strong>即舍弃了一些维度</strong>），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：<strong>若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来</strong>。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：</p><blockquote><p><strong>最近重构性</strong>：样本点到超平面的距离足够近，即尽可能在超平面附近；<br><strong>最大可分性</strong>：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。</p></blockquote><p>这里十分神奇的是：<strong>最近重构性与最大可分性虽然从不同的出发点来定义优化问题中的目标函数，但最终这两种特性得到了完全相同的优化问题</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc851a5213c1.png" alt="7.png"></p><p>接着使用拉格朗日乘子法求解上面的优化问题，得到：</p><p><img src="https://i.loli.net/2018/10/18/5bc851a4a102a.png" alt="8.png"></p><p>因此只需对协方差矩阵进行特征值分解即可求解出W，PCA算法的整个流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc851a540eb3.png" alt="9.png"></p><p>另一篇博客给出更通俗更详细的理解：<a href="http://blog.csdn.net/u011826404/article/details/57472730" target="_blank" rel="noopener">主成分分析解析（基于最大方差理论）</a></p><h2 id="11-4-核化线性降维"><a href="#11-4-核化线性降维" class="headerlink" title="11.4 核化线性降维"></a><strong>11.4 核化线性降维</strong></h2><p>说起机器学习你中有我/我中有你/水乳相融…在这里能够得到很好的体现。正如SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，<strong>即先将样本映射到高维空间，再在高维空间中使用线性降维的方法</strong>。下面主要介绍<strong>核化主成分分析（KPCA）</strong>的思想。</p><p>若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：<strong>即空间中的任一向量，都可以由该空间中的所有样本线性表示</strong>。证明过程也十分简单：</p><p><img src="https://i.loli.net/2018/10/18/5bc851a51bd2a.png" alt="10.png"></p><p>这样我们便可以将高维特征空间中的投影向量wi使用所有高维样本点线性表出，接着代入PCA的求解问题，得到：</p><p><img src="https://i.loli.net/2018/10/18/5bc851b74b083.png" alt="11.png"></p><p>化简到最后一步，发现结果十分的美妙，只需对核矩阵K进行特征分解，便可以得出投影向量wi对应的系数向量α，因此选取特征值前d’大对应的特征向量便是d’个系数向量。这时对于需要降维的样本点，只需按照以下步骤便可以求出其降维后的坐标。可以看出：KPCA在计算降维后的坐标表示时，需要与所有样本点计算核函数值并求和，因此该算法的计算开销十分大。</p><p><img src="https://i.loli.net/2018/10/18/5bc851b735754.png" alt="12.png"></p><h2 id="11-5-流形学习"><a href="#11-5-流形学习" class="headerlink" title="11.5 流形学习"></a><strong>11.5 流形学习</strong></h2><p><strong>流形学习（manifold learning）是一种借助拓扑流形概念的降维方法</strong>，<strong>流形是指在局部与欧式空间同胚的空间</strong>，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种<strong>“邻域保持”</strong>的思想。其中<strong>等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系</strong>，下面将分别对这两种著名的流行学习方法进行介绍。</p><h3 id="11-5-1-等度量映射（Isomap）"><a href="#11-5-1-等度量映射（Isomap）" class="headerlink" title="11.5.1 等度量映射（Isomap）"></a><strong>11.5.1 等度量映射（Isomap）</strong></h3><p>等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。<strong>因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离</strong>，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的<strong>Dijkstra算法</strong>或<strong>Floyd算法</strong>计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。</p><p><img src="https://i.loli.net/2018/10/18/5bc851b731a1e.png" alt="13.png"></p><p>从MDS算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵B，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量w，因此对于需要降维的新样本无从下手，书中给出的权宜之计是利用已知高/低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。Isomap算法流程如下图：</p><p><img src="https://i.loli.net/2018/10/18/5bc851b6c7e37.png" alt="14.png"></p><p>对于近邻图的构建，常用的有两种方法：<strong>一种是指定近邻点个数</strong>，像kNN一样选取k个最近的邻居；<strong>另一种是指定邻域半径</strong>，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：</p><blockquote><p>若<strong>邻域范围指定过大，则会造成“短路问题”</strong>，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。<br>若<strong>邻域范围指定过小，则会造成“断路问题”</strong>，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。</p></blockquote><h3 id="11-5-2-局部线性嵌入-LLE"><a href="#11-5-2-局部线性嵌入-LLE" class="headerlink" title="11.5.2 局部线性嵌入(LLE)"></a><strong>11.5.2 局部线性嵌入(LLE)</strong></h3><p>不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本xi的坐标可以通过它的邻域样本线性表出：</p><p><img src="https://i.loli.net/2018/10/18/5bc851b64236f.png" alt="15.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc851b6a7b9a.png" alt="16.png"></p><p>LLE算法分为两步走，<strong>首先第一步根据近邻关系计算出所有样本的邻域重构系数w</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc851b662815.png" alt="17.png"></p><p><strong>接着根据邻域重构系数不变，去求解低维坐标</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc851b648b98.png" alt="18.png"></p><p>这样利用矩阵M，优化问题可以重写为：</p><p><img src="https://i.loli.net/2018/10/18/5bc851b6948d7.png" alt="19.png"></p><p>M特征值分解后最小的d’个特征值对应的特征向量组成Z，LLE算法的具体流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc851b757d8c.png" alt="20.png"></p><h2 id="11-6-度量学习"><a href="#11-6-度量学习" class="headerlink" title="11.6 度量学习"></a><strong>11.6 度量学习</strong></h2><p>本篇一开始就提到维数灾难，即在高维空间进行机器学习任务遇到样本稀疏、距离难计算等诸多的问题，因此前面讨论的降维方法都试图将原空间投影到一个合适的低维空间中，接着在低维空间进行学习任务从而产生较好的性能。事实上，不管高维空间还是低维空间都潜在对应着一个距离度量，那可不可以直接学习出一个距离度量来等效降维呢？例如：<strong>咋们就按照降维后的方式来进行距离的计算，这便是度量学习的初衷</strong>。</p><p><strong>首先要学习出距离度量必须先定义一个合适的距离度量形式</strong>。对两个样本xi与xj，它们之间的平方欧式距离为：</p><p><img src="https://i.loli.net/2018/10/18/5bc851d3ca3d5.png" alt="21.png"></p><p>若各个属性重要程度不一样即都有一个权重，则得到加权的平方欧式距离：</p><p><img src="https://i.loli.net/2018/10/18/5bc851d3d82c5.png" alt="22.png"></p><p>此时各个属性之间都是相互独立无关的，但现实中往往会存在属性之间有关联的情形，例如：身高和体重，一般人越高，体重也会重一些，他们之间存在较大的相关性。这样计算距离就不能分属性单独计算，于是就引入经典的<strong>马氏距离(Mahalanobis distance)</strong>:</p><p><img src="https://i.loli.net/2018/10/18/5bc851d3dc303.png" alt="23.png"></p><p><strong>标准的马氏距离中M是协方差矩阵的逆，马氏距离是一种考虑属性之间相关性且尺度无关（即无须去量纲）的距离度量</strong>。</p><p><img src="https://i.loli.net/2018/10/18/5bc851d3e17c0.png" alt="24.png"></p><p><strong>矩阵M也称为“度量矩阵”，为保证距离度量的非负性与对称性，M必须为(半)正定对称矩阵</strong>，这样就为度量学习定义好了距离度量的形式，换句话说：<strong>度量学习便是对度量矩阵进行学习</strong>。现在来回想一下前面我们接触的机器学习不难发现：<strong>机器学习算法几乎都是在优化目标函数，从而求解目标函数中的参数</strong>。同样对于度量学习，也需要设置一个优化目标，书中简要介绍了错误率和相似性两种优化目标，此处限于篇幅不进行展开。</p><p>在此，降维和度量学习就介绍完毕。<strong>降维是将原高维空间嵌入到一个合适的低维子空间中，接着在低维空间中进行学习任务；度量学习则是试图去学习出一个距离度量来等效降维的效果</strong>，两者都是为了解决维数灾难带来的诸多问题。也许大家最后心存疑惑，那kNN呢，为什么一开头就说了kNN算法，但是好像和后面没有半毛钱关系？正是因为在降维算法中，低维子空间的维数d’通常都由人为指定，因此我们需要使用一些低开销的学习器来选取合适的d’，<strong>kNN这家伙懒到家了根本无心学习，在训练阶段开销为零，测试阶段也只是遍历计算了距离，因此拿kNN来进行交叉验证就十分有优势了~同时降维后样本密度增大同时距离计算变易，更为kNN来展示它独特的十八般手艺提供了用武之地。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了几种常用的聚类算法，首先从距离度量与性能评估出发，列举了常见的距离计算公式与聚类评价指标，接着分别讨论了K-Means、LVQ、高斯混合聚类、密度聚类以及层次聚类算法。K-Means与LVQ都试图以类簇中心作为原型指导聚类，其中K-Means通过EM算法不断迭代直至收敛，LVQ使用真实类标辅助聚类；高斯混合聚类采用高斯分布来描述类簇原型；密度聚类则是将一个核心对象所有密度可达的样本形成类簇，直到所有核心对象都遍历完；最后层次聚类是一种自底向上的树形聚类方法，不断合并最相近的两个小类簇。本篇将讨论机器学习常用的方法—降维与度量学习。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Python进阶</title>
    <link href="http://yoursite.com/2020/07/27/Python%E8%BF%9B%E9%98%B6/"/>
    <id>http://yoursite.com/2020/07/27/Python%E8%BF%9B%E9%98%B6/</id>
    <published>2020-07-27T04:13:45.000Z</published>
    <updated>2020-07-27T09:50:55.953Z</updated>
    
    <content type="html"><![CDATA[<p>参考<a href="https://eastlakeside.gitbook.io/interpy-zh/" target="_blank" rel="noopener">此处</a></p><h1 id="Python进阶"><a href="#Python进阶" class="headerlink" title="Python进阶 "></a>Python进阶 </h1><p>《Python进阶》是《Intermediate Python》的中文译本, 谨以此献给进击的 Python 和 Python 程序员们!<br><a id="more"></a></p><h3 id="快速阅读传送门"><a href="#快速阅读传送门" class="headerlink" title="快速阅读传送门"></a>快速阅读传送门</h3><ul><li>Github快速阅读任一章节：<a href="https://github.com/eastlakeside/interpy-zh/blob/master/SUMMARY.md" target="_blank" rel="noopener">进入目录</a></li><li>Gitbook完整顺序地阅读：<a href="https://eastlakeside.gitbooks.io/interpy-zh/content/" target="_blank" rel="noopener">进入Gitbook</a></li><li>本地或kindle上阅读：<a href="https://github.com/eastlakeside/interpy-zh/releases" target="_blank" rel="noopener">下载pdf/epub/mobi</a></li><li>国内推荐镜像（实时同步）：<a href="http://wiki.jikexueyuan.com/project/interpy-zh/" target="_blank" rel="noopener">极客学院收录</a></li><li>其他镜像（不定期同步）：<a href="http://docs.pythontab.com/interpy/" target="_blank" rel="noopener">Pythontab收录</a></li><li>纯代码阅读和演示：<a href="https://github.com/eastlakeside/interpy-zh/tree/master/code/" target="_blank" rel="noopener">进入code目录</a></li></ul><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Python，作为一个”老练”、”小清新”的开发语言，已受到广大才男俊女的喜爱。我们也从最基础的Python粉，经过时间的摧残慢慢的变成了Python老鬼。</p><p>IntermediatePython这本书具有如下几个优点：</p><ol><li>简单</li><li>易读</li><li>易译</li></ol><p>这些都不是重点，重点是：<strong>它是一本开脑洞的书</strong>。无论你是Python初学者，还是Python高手，它显现给你的永远是Python里最美好的事物。</p><blockquote><p>世上语言千万种<br>美好事物藏其中</p></blockquote><p>译者在翻译过程中，慢慢发现，本书作者的行文方式有着科普作家的风范，—那就是能将晦涩难懂的技术用比较清晰简洁的方式进行呈现，深入浅出的风格在每个章节的讨论中都得到了体现：</p><ul><li>每个章节都非常精简，5分钟就能看完，用最简洁的例子精辟地展现了原理</li><li>每个章节都会通过疑问，来引导读者主动思考答案</li><li>每个章节都引导读者做延伸阅读，让有兴趣的读者能进一步举一反三</li><li>每个章节都是独立的，你可以挑选任意的章节开始阅读，而不受影响</li></ul><p>总之，这本书非常方便随时选取一个章节进行阅读，而且每次阅读一个章节，你都可能会有一些新的发现。</p><h2 id="原书作者"><a href="#原书作者" class="headerlink" title="原书作者"></a>原书作者</h2><p>感谢英文原著作者 @yasoob《<a href="https://github.com/yasoob/intermediatePython" target="_blank" rel="noopener">Intermediate Python</a>》，有了他才有了这里的一切</p><h1 id="序"><a href="#序" class="headerlink" title="序"></a>序</h1><p>这是一本<a href="https://github.com/yasoob/intermediatePython" target="_blank" rel="noopener">Intermediate Python</a> 的中文译本, 谨以此献给进击的 Python 和 Python 程序员们!</p><p>这是一次团队建设、一次尝鲜、一次对自我的提升。相信每个有为青年，心里想藏着一个小宇宙：<strong>我想要做一件有意思的事</strong>。<script type="math/tex">什么是有意思的事？</script> <strong>别闹</strong></p><p>Python，作为一个”老练”、”小清新”的开发语言，已受到广大才男俊女的喜爱。我们也从最基础的Python粉，经过时间的摧残慢慢的变成了Python老鬼。因此一开始 @大牙 提出要翻译点什么的时候，我还是挺兴奋的，团队一起协作，不单可以磨练自己，也能加强团队之间的协作。为此在经过短暂的讨论后，翻译的内容就定为：《Intermediate Python》。</p><p>IntermediatePython这本书具有如下几个优点：</p><ol><li>简单</li><li>易读</li><li>易译</li></ol><p>这些都不是重点，重点是：<strong>它是一本开脑洞的书</strong>。无论你是Python初学者，还是Python高手，它显现给你的永远是Python里最美好的事物。</p><h1 id="关于原作者"><a href="#关于原作者" class="headerlink" title="关于原作者"></a>关于原作者</h1><p>我是 Muhammad Yasoob Ullah Khalid. </p><p>我已经广泛使用 Python 编程3年多了. 同时参与了很多开源项目. 并定期在<a href="http://pythontips.com/" target="_blank" rel="noopener">我的博客</a>里写一些关于Python有趣的话题. </p><p>2014年我在柏林举办的欧洲最大的Python会议<strong>EuroPython</strong>上做过精彩的演讲. </p><blockquote><p>译者注：分享的主题为：《Session: Web Scraping in Python 101》<br>地址：<a href="https://ep2014.europython.eu/en/schedule/sessions/20/" target="_blank" rel="noopener">https://ep2014.europython.eu/en/schedule/sessions/20/</a></p></blockquote><p>如果你能给我有意思的工作机会, 请联系我哦.</p><blockquote><p>译者注：嗯，硬广，你来中国么，HOHO</p></blockquote><h1 id="作者前言"><a href="#作者前言" class="headerlink" title="作者前言"></a>作者前言</h1><p>Hello 大家好! 我非常自豪地宣布我自己创作的书完成啦.<br>经过很多辛苦工作和决心, 终于将不可能变成了可能, “Intermediate Python”终于杀青.<br>ps: 它还将持续更新 :)</p><p>Python 是一门奇妙的语言, 还有一个强大而友爱的程序员社区.<br>然而, 在你理解消化掉 Python 的基础后, 关于下一步学习什么的资料比较缺乏. 而我正是要通过本书来解决这一问题.<br>我会给你一些可以进一步探索的有趣的话题的信息.</p><p>本书讨论的这些话题将会打开你的脑洞, 将你引导至 Python 语言的一些美好的地方. 我最开始学习 Python 时, 渴望见到Python最优雅的地方, 而本书正是这些渴望的结果.</p><p>无论你是个初学者, 中级或者甚至高级程序员, 你都会在这本书里有所收获.</p><p>请注意本书不是一个指导手册, 也不会教你 Python. 因为书中的话题并没有进行基础解释, 而只提供了展开讨论前所需的最少信息.</p><p>好啦，你肯定也和我一样兴奋, 那让我们开始吧!</p><h1 id="开源公告"><a href="#开源公告" class="headerlink" title="开源公告"></a>开源公告</h1><p>注意: 这本书是开源的, 也是一个持续进展中的工作. 如果你发现typo, 或者想添加更多内容进来, 或者可以改进的任意地方(我知道你会发现很多),  那么请慷慨地提交一个 pull request, 我会无比高兴地合并进来. :)</p><p>另外, 我决定将这本书免费发布!   我相信它会帮助到那些需要帮助的人. 祝你们好运!</p><p>这里是免费阅读链接:</p><ul><li><a href="http://book.pythontips.com/" target="_blank" rel="noopener">Html</a> </li><li><a href="http://readthedocs.org/projects/intermediatepythongithubio/downloads/pdf/latest/" target="_blank" rel="noopener">PDF</a></li><li><a href="https://github.com/IntermediatePython/intermediatePython" target="_blank" rel="noopener">GitHub</a></li></ul><h1 id="args-和-kwargs"><a href="#args-和-kwargs" class="headerlink" title="*args 和 **kwargs"></a><code>*args</code> 和 <code>**kwargs</code></h1><p>我观察到，大部分新的Python程序员都需要花上大量时间理解清楚 <code>*args</code> 和<code>**kwargs</code>这两个魔法变量。那么它们到底是什么? </p><p>首先让我告诉你, 其实并不是必须写成<code>*args</code> 和<code>**kwargs</code>。 只有变量前面的 <code>*</code>(星号)才是必须的. 你也可以写成<code>*var</code> 和<code>**vars</code>. 而写成<code>*args</code> 和<code>**kwargs</code>只是一个通俗的命名约定。 那就让我们先看一下<code>*args</code>吧。</p><h1 id="args-的用法"><a href="#args-的用法" class="headerlink" title="*args 的用法"></a>*args 的用法</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">和 &#96;&#96;&#96;**kwargs&#96;&#96;&#96; 主要用于函数定义。 你可以将不定数量的参数传递给一个函数。 </span><br><span class="line"></span><br><span class="line">这里的不定的意思是：预先并不知道, 函数使用者会传递多少个参数给你, 所以在这个场景下使用这两个关键字。 &#96;&#96;&#96;*args&#96;&#96;&#96; 是用来发送一个非键值对的可变数量的参数列表给一个函数. </span><br><span class="line"></span><br><span class="line">这里有个例子帮你理解这个概念:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">def test_var_args(f_arg, *argv):</span><br><span class="line">    print(&quot;first normal arg:&quot;, f_arg)</span><br><span class="line">    for arg in argv:</span><br><span class="line">        print(&quot;another arg through *argv:&quot;, arg)</span><br><span class="line"></span><br><span class="line">test_var_args(&#39;yasoob&#39;, &#39;python&#39;, &#39;eggs&#39;, &#39;test&#39;)</span><br></pre></td></tr></table></figure><p>这会产生如下输出:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">first normal arg: yasoob</span><br><span class="line">another arg through *argv: python</span><br><span class="line">another arg through *argv: eggs</span><br><span class="line">another arg through *argv: test</span><br></pre></td></tr></table></figure><p>我希望这解决了你所有的困惑. 那接下来让我们谈谈 <code>**kwargs</code></p><h1 id="kwargs-的用法"><a href="#kwargs-的用法" class="headerlink" title="**kwargs 的用法"></a>**kwargs 的用法</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">允许你将不定长度的**键值对**, 作为参数传递给一个函数。 如果你想要在一个函数里处理**带名字的参数**, 你应该使用&#96;&#96;&#96;**kwargs&#96;&#96;&#96;。 </span><br><span class="line"></span><br><span class="line">这里有个让你上手的例子:</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">def greet_me(**kwargs):</span><br><span class="line">    for key, value in kwargs.items():</span><br><span class="line">        print(&quot;&#123;0&#125; &#x3D;&#x3D; &#123;1&#125;&quot;.format(key, value))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; greet_me(name&#x3D;&quot;yasoob&quot;)</span><br><span class="line">name &#x3D;&#x3D; yasoob</span><br></pre></td></tr></table></figure><p>现在你可以看出我们怎样在一个函数里, 处理了一个<strong>键值对</strong>参数了。</p><p>这就是<code>**kwargs</code>的基础, 而且你可以看出它有多么管用。 接下来让我们谈谈，你怎样使用<code>*args</code> 和 <code>**kwargs</code>来调用一个参数为列表或者字典的函数。</p><h1 id="使用-args-和-kwargs-来调用函数"><a href="#使用-args-和-kwargs-来调用函数" class="headerlink" title="使用 *args 和 **kwargs 来调用函数"></a>使用 <code>*args</code> 和 <code>**kwargs</code> 来调用函数</h1><p>那现在我们将看到怎样使用<code>*args</code>和<code>**kwargs</code> 来调用一个函数。<br> 假设，你有这样一个小函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_args_kwargs</span><span class="params">(arg1, arg2, arg3)</span>:</span></span><br><span class="line">    print(<span class="string">"arg1:"</span>, arg1)</span><br><span class="line">    print(<span class="string">"arg2:"</span>, arg2)</span><br><span class="line">    print(<span class="string">"arg3:"</span>, arg3)</span><br></pre></td></tr></table></figure></p><p>你可以使用<code>*args</code>或<code>**kwargs</code>来给这个小函数传递参数。<br>下面是怎样做：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先使用 *args</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>args = (<span class="string">"two"</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_args_kwargs(*args)</span><br><span class="line">arg1: two</span><br><span class="line">arg2: <span class="number">3</span></span><br><span class="line">arg3: <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在使用 **kwargs:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kwargs = &#123;<span class="string">"arg3"</span>: <span class="number">3</span>, <span class="string">"arg2"</span>: <span class="string">"two"</span>, <span class="string">"arg1"</span>: <span class="number">5</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_args_kwargs(**kwargs)</span><br><span class="line">arg1: <span class="number">5</span></span><br><span class="line">arg2: two</span><br><span class="line">arg3: <span class="number">3</span></span><br></pre></td></tr></table></figure></p><h3 id="标准参数与-args、-kwargs在使用时的顺序"><a href="#标准参数与-args、-kwargs在使用时的顺序" class="headerlink" title="标准参数与*args、**kwargs在使用时的顺序"></a>标准参数与<code>*args、**kwargs</code>在使用时的顺序</h3><p>那么如果你想在函数里同时使用所有这三种参数， 顺序是这样的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">some_func(fargs, *args, **kwargs)</span><br></pre></td></tr></table></figure></p><h1 id="什么时候使用它们？"><a href="#什么时候使用它们？" class="headerlink" title="什么时候使用它们？"></a>什么时候使用它们？</h1><p>这还真的要看你的需求而定。</p><p>最常见的用例是在写函数装饰器的时候（会在另一章里讨论）。</p><p>此外它也可以用来做猴子补丁(monkey patching)。猴子补丁的意思是在程序运行时(runtime)修改某些代码。 打个比方，你有一个类，里面有个叫<code>get_info</code>的函数会调用一个API并返回相应的数据。如果我们想测试它，可以把API调用替换成一些测试数据。例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> someclass</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_info</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Test data"</span></span><br><span class="line"></span><br><span class="line">someclass.get_info = get_info</span><br></pre></td></tr></table></figure></p><p>我敢肯定你也可以想象到一些其他的用例。</p><h1 id="调试（Debugging）"><a href="#调试（Debugging）" class="headerlink" title="调试（Debugging）"></a>调试（Debugging）</h1><p>利用好调试，能大大提高你捕捉代码Bug的。大部分新人忽略了Python debugger(<code>pdb</code>)的重要性。 在这个章节我只会告诉你一些重要的命令，你可以从官方文档中学习到更多。</p><blockquote><p>译者注，参考：<a href="https://docs.python.org/2/library/pdb.html" target="_blank" rel="noopener">https://docs.python.org/2/library/pdb.html</a><br>Or <a href="https://docs.python.org/3/library/pdb.html" target="_blank" rel="noopener">https://docs.python.org/3/library/pdb.html</a></p></blockquote><h3 id="从命令行运行"><a href="#从命令行运行" class="headerlink" title="从命令行运行"></a>从命令行运行</h3><p>你可以在命令行使用Python debugger运行一个脚本， 举个例子：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python -m pdb my_script.py</span><br></pre></td></tr></table></figure></p><p>这会触发debugger在脚本第一行指令处停止执行。这在脚本很短时会很有帮助。你可以通过(Pdb)模式接着查看变量信息，并且逐行调试。</p><h3 id="从脚本内部运行"><a href="#从脚本内部运行" class="headerlink" title="从脚本内部运行"></a>从脚本内部运行</h3><p>同时，你也可以在脚本内部设置断点，这样就可以在某些特定点查看变量信息和各种执行时信息了。这里将使用<code>pdb.set_trace()</code>方法来实现。举个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_bread</span><span class="params">()</span>:</span></span><br><span class="line">    pdb.set_trace()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"I don't have time"</span></span><br><span class="line"></span><br><span class="line">print(make_bread())</span><br></pre></td></tr></table></figure></p><p>试下保存上面的脚本后运行之。你会在运行时马上进入debugger模式。现在是时候了解下debugger模式下的一些命令了。</p><h5 id="命令列表："><a href="#命令列表：" class="headerlink" title="命令列表："></a>命令列表：</h5><ul><li><code>c</code>: 继续执行</li><li><code>w</code>: 显示当前正在执行的代码行的上下文信息</li><li><code>a</code>: 打印当前函数的参数列表</li><li><code>s</code>: 执行当前代码行，并停在第一个能停的地方（相当于单步进入）</li><li><code>n</code>: 继续执行到当前函数的下一行，或者当前行直接返回（单步跳过）</li></ul><p>单步跳过（<code>n</code>ext）和单步进入（<code>s</code>tep）的区别在于， 单步进入会进入当前行调用的函数内部并停在里面， 而单步跳过会（几乎）全速执行完当前行调用的函数，并停在当前函数的下一行。</p><p>pdb真的是一个很方便的功能，上面仅列举少量用法，更多的命令强烈推荐你去看官方文档。</p><h1 id="生成器（Generators）"><a href="#生成器（Generators）" class="headerlink" title="生成器（Generators）"></a>生成器（Generators）</h1><p>首先我们要理解迭代器(iterators)。根据维基百科，迭代器是一个让程序员可以遍历一个容器（特别是列表）的对象。然而，一个迭代器在遍历并读取一个容器的数据元素时，并不会执行一个迭代。你可能有点晕了，那我们来个慢动作。换句话说这里有三个部分：</p><ul><li>可迭代对象(Iterable)</li><li>迭代器(Iterator)</li><li>迭代(Iteration)</li></ul><p>上面这些部分互相联系。我们会先各个击破来讨论他们，然后再讨论生成器(generators).</p><h1 id="可迭代对象-Iterable"><a href="#可迭代对象-Iterable" class="headerlink" title="可迭代对象(Iterable)"></a>可迭代对象(Iterable)</h1><p>Python中任意的对象，只要它定义了可以返回一个迭代器的<code>__iter__</code>方法，或者定义了可以支持下标索引的<code>__getitem__</code>方法(这些双下划线方法会在其他章节中全面解释)，那么它就是一个可迭代对象。简单说，可迭代对象就是能提供迭代器的任意对象。那迭代器又是什么呢？</p><h1 id="迭代器-Iterator"><a href="#迭代器-Iterator" class="headerlink" title="迭代器(Iterator)"></a>迭代器(Iterator)</h1><p>任意对象，只要定义了<code>next</code>(Python2) 或者<code>__next__</code>方法，它就是一个迭代器。就这么简单。现在我们来理解迭代(iteration)</p><h1 id="迭代-Iteration"><a href="#迭代-Iteration" class="headerlink" title="迭代(Iteration)"></a>迭代(Iteration)</h1><p>用简单的话讲，它就是从某个地方（比如一个列表）取出一个元素的过程。当我们使用一个循环来遍历某个东西时，这个过程本身就叫迭代。现在既然我们有了这些术语的基本理解，那我们开始理解生成器吧。</p><h1 id="生成器-Generators"><a href="#生成器-Generators" class="headerlink" title="生成器(Generators)"></a>生成器(Generators)</h1><p>生成器也是一种迭代器，但是你只能对其迭代一次。这是因为它们并没有把所有的值存在内存中，而是在运行时生成值。你通过遍历来使用它们，要么用一个“for”循环，要么将它们传递给任意可以进行迭代的函数和结构。大多数时候生成器是以函数来实现的。然而，它们并不返回一个值，而是<code>yield</code>(暂且译作“生出”)一个值。这里有个生成器函数的简单例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_function</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> generator_function():</span><br><span class="line">    print(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># 7</span></span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line"><span class="comment"># 9</span></span><br></pre></td></tr></table></figure></p><p>这个案例并不是非常实用。生成器最佳应用场景是：你不想同一时间将所有计算出来的大量结果集分配到内存当中，特别是结果集里还包含循环。</p><blockquote><p>译者注：这样做会消耗大量资源 </p></blockquote><p>许多Python 2里的标准库函数都会返回列表，而Python 3都修改成了返回生成器，因为生成器占用更少的资源。  </p><p>下面是一个计算斐波那契数列的生成器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generator version</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibon</span><span class="params">(n)</span>:</span></span><br><span class="line">    a = b = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line">        a, b = b, a + b</span><br></pre></td></tr></table></figure><p>函数使用方法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for x in fibon(1000000):</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><p>用这种方式，我们可以不用担心它会使用大量资源。然而，之前如果我们这样来实现的话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibon</span><span class="params">(n)</span>:</span></span><br><span class="line">    a = b = <span class="number">1</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        result.append(a)</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>这也许会在计算很大的输入参数时，用尽所有的资源。我们已经讨论过生成器使用一次迭代，但我们并没有测试过。在测试前你需要再知道一个Python内置函数：<code>next()</code>。它允许我们获取一个序列的下一个元素。那我们来验证下我们的理解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_function</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">gen = generator_function()</span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: 0</span></span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: 1</span></span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: 2</span></span><br><span class="line">print(next(gen))</span><br><span class="line"><span class="comment"># Output: Traceback (most recent call last):</span></span><br><span class="line"><span class="comment">#            File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span></span><br><span class="line"><span class="comment">#         StopIteration</span></span><br></pre></td></tr></table></figure><p>我们可以看到，在<code>yield</code>掉所有的值后，<code>next()</code>触发了一个<code>StopIteration</code>的异常。基本上这个异常告诉我们，所有的值都已经被<code>yield</code>完了。你也许会奇怪，为什么我们在使用<code>for</code>循环时没有这个异常呢？啊哈，答案很简单。<code>for</code>循环会自动捕捉到这个异常并停止调用<code>next()</code>。你知不知道Python中一些内置数据类型也支持迭代哦？我们这就去看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_string = <span class="string">"Yasoob"</span></span><br><span class="line">next(my_string)</span><br><span class="line"><span class="comment"># Output: Traceback (most recent call last):</span></span><br><span class="line"><span class="comment">#      File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span></span><br><span class="line"><span class="comment">#    TypeError: str object is not an iterator</span></span><br></pre></td></tr></table></figure><p>好吧，这不是我们预期的。这个异常说那个<code>str</code>对象不是一个迭代器。对，就是这样！它是一个可迭代对象，而不是一个迭代器。这意味着它支持迭代，但我们不能直接对其进行迭代操作。那我们怎样才能对它实施迭代呢？是时候学习下另一个内置函数，<code>iter</code>。它将根据一个可迭代对象返回一个迭代器对象。这里是我们如何使用它：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_string = <span class="string">"Yasoob"</span></span><br><span class="line">my_iter = iter(my_string)</span><br><span class="line">next(my_iter)</span><br><span class="line"><span class="comment"># Output: 'Y'</span></span><br></pre></td></tr></table></figure><br>现在好多啦。我肯定你已经爱上了学习生成器。一定要记住，想要完全掌握这个概念，你只有使用它。确保你按照这个模式，并在生成器对你有意义的任何时候都使用它。你绝对不会失望的！</p><h1 id="Map，Filter-和-Reduce"><a href="#Map，Filter-和-Reduce" class="headerlink" title="Map，Filter 和 Reduce"></a>Map，Filter 和 Reduce</h1><p>Map，Filter 和 Reduce 三个函数能为函数式编程提供便利。我们会通过实例一个一个讨论并理解它们。</p><h1 id="Map"><a href="#Map" class="headerlink" title="Map"></a><code>Map</code></h1><p><code>Map</code>会将一个函数映射到一个输入列表的所有元素上。这是它的规范：</p><p><strong>规范</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map(function_to_apply, list_of_inputs)</span><br></pre></td></tr></table></figure></p><p>大多数时候，我们要把列表中所有元素一个个地传递给一个函数，并收集输出。比方说：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">items = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">squared = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> items:</span><br><span class="line">    squared.append(i**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><code>Map</code>可以让我们用一种简单而漂亮得多的方式来实现。就是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">items = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">squared = list(map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>, items))</span><br></pre></td></tr></table></figure><p>大多数时候，我们使用匿名函数(lambdas)来配合<code>map</code>, 所以我在上面也是这么做的。<br> 不仅用于一列表的输入， 我们甚至可以用于一列表的函数！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (x*x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (x+x)</span><br><span class="line"></span><br><span class="line">funcs = [multiply, add]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    value = map(<span class="keyword">lambda</span> x: x(i), funcs)</span><br><span class="line">    print(list(value))</span><br><span class="line">    <span class="comment"># 译者注：上面print时，加了list转换，是为了python2/3的兼容性</span></span><br><span class="line">    <span class="comment">#        在python2中map直接返回列表，但在python3中返回迭代器</span></span><br><span class="line">    <span class="comment">#        因此为了兼容python3, 需要list转换一下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># [0, 0]</span></span><br><span class="line"><span class="comment"># [1, 2]</span></span><br><span class="line"><span class="comment"># [4, 4]</span></span><br><span class="line"><span class="comment"># [9, 6]</span></span><br><span class="line"><span class="comment"># [16, 8]</span></span><br></pre></td></tr></table></figure><h1 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a><code>Filter</code></h1><p>顾名思义，<code>filter</code>过滤列表中的元素，并且返回一个由所有符合要求的元素所构成的列表，<code>符合要求</code>即函数映射到该元素时返回值为True. 这里是一个简短的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">number_list = range(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">less_than_zero = filter(<span class="keyword">lambda</span> x: x &lt; <span class="number">0</span>, number_list)</span><br><span class="line">print(list(less_than_zero))  </span><br><span class="line"><span class="comment"># 译者注：上面print时，加了list转换，是为了python2/3的兼容性</span></span><br><span class="line"><span class="comment">#        在python2中filter直接返回列表，但在python3中返回迭代器</span></span><br><span class="line"><span class="comment">#        因此为了兼容python3, 需要list转换一下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: [-5, -4, -3, -2, -1]</span></span><br></pre></td></tr></table></figure><p>这个<code>filter</code>类似于一个<code>for</code>循环，但它是一个内置函数，并且更快。</p><p>注意：如果<code>map</code>和<code>filter</code>对你来说看起来并不优雅的话，那么你可以看看另外一章：列表/字典/元组推导式。</p><blockquote><p>译者注：大部分情况下推导式的可读性更好</p></blockquote><h1 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a><code>Reduce</code></h1><p>当需要对一个列表进行一些计算并返回结果时，<code>Reduce</code> 是个非常有用的函数。举个例子，当你需要计算一个整数列表的乘积时。</p><p>通常在 python 中你可能会使用基本的 for 循环来完成这个任务。</p><p>现在我们来试试 reduce：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">product &#x3D; reduce( (lambda x, y: x * y), [1, 2, 3, 4] )</span><br><span class="line"></span><br><span class="line"># Output: 24</span><br></pre></td></tr></table></figure><h1 id="set-集合-数据结构"><a href="#set-集合-数据结构" class="headerlink" title="set(集合)数据结构"></a><code>set</code>(集合)数据结构</h1><p><code>set</code>(集合)是一个非常有用的数据结构。它与列表(<code>list</code>)的行为类似，区别在于<code>set</code>不能包含重复的值。<br>这在很多情况下非常有用。例如你可能想检查列表中是否包含重复的元素，你有两个选择，第一个需要使用<code>for</code>循环，就像这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">some_list = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'d'</span>, <span class="string">'m'</span>, <span class="string">'n'</span>, <span class="string">'n'</span>]</span><br><span class="line"></span><br><span class="line">duplicates = []</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> some_list:</span><br><span class="line">    <span class="keyword">if</span> some_list.count(value) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> value <span class="keyword">not</span> <span class="keyword">in</span> duplicates:</span><br><span class="line">            duplicates.append(value)</span><br><span class="line"></span><br><span class="line">print(duplicates)</span><br><span class="line"><span class="comment">### 输出: ['b', 'n']</span></span><br></pre></td></tr></table></figure><p>但还有一种更简单更优雅的解决方案，那就是使用<code>集合(sets)</code>，你直接这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">some_list = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'d'</span>, <span class="string">'m'</span>, <span class="string">'n'</span>, <span class="string">'n'</span>]</span><br><span class="line">duplicates = set([x <span class="keyword">for</span> x <span class="keyword">in</span> some_list <span class="keyword">if</span> some_list.count(x) &gt; <span class="number">1</span>])</span><br><span class="line">print(duplicates)</span><br><span class="line"><span class="comment">### 输出: set(['b', 'n'])</span></span><br></pre></td></tr></table></figure><p>集合还有一些其它方法，下面我们介绍其中一部分。</p><h2 id="交集"><a href="#交集" class="headerlink" title="交集"></a>交集</h2><p>你可以对比两个集合的交集（两个集合中都有的数据），如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">valid = set([<span class="string">'yellow'</span>, <span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'black'</span>])</span><br><span class="line">input_set = set([<span class="string">'red'</span>, <span class="string">'brown'</span>])</span><br><span class="line">print(input_set.intersection(valid))</span><br><span class="line"><span class="comment">### 输出: set(['red'])</span></span><br></pre></td></tr></table></figure><h2 id="差集"><a href="#差集" class="headerlink" title="差集"></a>差集</h2><p>你可以用差集(difference)找出无效的数据，相当于用一个集合减去另一个集合的数据，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">valid = set([<span class="string">'yellow'</span>, <span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'black'</span>])</span><br><span class="line">input_set = set([<span class="string">'red'</span>, <span class="string">'brown'</span>])</span><br><span class="line">print(input_set.difference(valid))</span><br><span class="line"><span class="comment">### 输出: set(['brown'])</span></span><br></pre></td></tr></table></figure><p>你也可以用<code>{}</code>符号来创建集合，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a_set = &#123;<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>&#125;</span><br><span class="line">print(type(a_set))</span><br><span class="line"><span class="comment">### 输出: &lt;type 'set'&gt;</span></span><br></pre></td></tr></table></figure><p>集合还有一些其它方法，我会建议访问官方文档并做个快速阅读。</p><h1 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h1><p>三元运算符通常在Python里被称为条件表达式，这些表达式基于真(true)/假(false)的条件判断，在Python 2.4以上才有了三元操作。</p><p>下面是一个伪代码和例子：</p><p><strong>伪代码:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#如果条件为真，返回真 否则返回假</span></span><br><span class="line">condition_is_true <span class="keyword">if</span> condition <span class="keyword">else</span> condition_is_false</span><br></pre></td></tr></table></figure><p><strong>例子:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">is_fat = <span class="literal">True</span></span><br><span class="line">state = <span class="string">"fat"</span> <span class="keyword">if</span> is_fat <span class="keyword">else</span> <span class="string">"not fat"</span></span><br></pre></td></tr></table></figure><p>它允许用简单的一行快速判断，而不是使用复杂的多行<code>if</code>语句。<br>这在大多数时候非常有用，而且可以使代码简单可维护。</p><p>另一个晦涩一点的用法比较少见，它使用了元组，请继续看：</p><p><strong>伪代码:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#(返回假，返回真)[真或假]</span></span><br><span class="line">(if_test_is_false, if_test_is_true)[test]</span><br></pre></td></tr></table></figure><p><strong>例子:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fat = <span class="literal">True</span></span><br><span class="line">fitness = (<span class="string">"skinny"</span>, <span class="string">"fat"</span>)[fat]</span><br><span class="line">print(<span class="string">"Ali is"</span>, fitness)</span><br><span class="line"><span class="comment">#输出: Ali is fat</span></span><br></pre></td></tr></table></figure><p>这之所以能正常工作，是因为在Python中，True等于1，而False等于0，这就相当于在元组中使用0和1来选取数据。</p><p>上面的例子没有被广泛使用，而且Python玩家一般不喜欢那样，因为没有Python味儿(Pythonic)。这样的用法很容易把真正的数据与True/False弄混。</p><p>另外一个不使用元组条件表达式的缘故是因为在元组中会把两个条件都执行，而 <code>if-else</code> 的条件表达式不会这样。</p><p>例如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">condition = <span class="literal">True</span></span><br><span class="line">print(<span class="number">2</span> <span class="keyword">if</span> condition <span class="keyword">else</span> <span class="number">1</span> / <span class="number">0</span>)</span><br><span class="line"><span class="comment">#输出: 2</span></span><br><span class="line"></span><br><span class="line">print((<span class="number">1</span> / <span class="number">0</span>, <span class="number">2</span>)[condition])</span><br><span class="line"><span class="comment">#输出ZeroDivisionError异常</span></span><br></pre></td></tr></table></figure><p>这是因为在元组中是先建数据，然后用True(1)/False(0)来索引到数据。<br>而<code>if-else</code>条件表达式遵循普通的<code>if-else</code>逻辑树，<br>因此，如果逻辑中的条件异常，或者是重计算型（计算较久）的情况下，最好尽量避免使用元组条件表达式。</p><h1 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h1><p>装饰器(Decorators)是Python的一个重要部分。简单地说：他们是修改其他函数的功能的函数。他们有助于让我们的代码更简短，也更Pythonic（Python范儿）。大多数初学者不知道在哪儿使用它们，所以我将要分享下，哪些区域里装饰器可以让你的代码更简洁。</p><p>首先，让我们讨论下如何写你自己的装饰器。</p><p>这可能是最难掌握的概念之一。我们会每次只讨论一个步骤，这样你能完全理解它。</p><h1 id="一切皆对象"><a href="#一切皆对象" class="headerlink" title="一切皆对象"></a>一切皆对象</h1><p>首先我们来理解下Python中的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"hi "</span> + name</span><br><span class="line"></span><br><span class="line">print(hi())</span><br><span class="line"><span class="comment"># output: 'hi yasoob'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们甚至可以将一个函数赋值给一个变量，比如</span></span><br><span class="line">greet = hi</span><br><span class="line"><span class="comment"># 我们这里没有在使用小括号，因为我们并不是在调用hi函数</span></span><br><span class="line"><span class="comment"># 而是在将它放在greet变量里头。我们尝试运行下这个</span></span><br><span class="line"></span><br><span class="line">print(greet())</span><br><span class="line"><span class="comment"># output: 'hi yasoob'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果我们删掉旧的hi函数，看看会发生什么！</span></span><br><span class="line"><span class="keyword">del</span> hi</span><br><span class="line">print(hi())</span><br><span class="line"><span class="comment">#outputs: NameError</span></span><br><span class="line"></span><br><span class="line">print(greet())</span><br><span class="line"><span class="comment">#outputs: 'hi yasoob'</span></span><br></pre></td></tr></table></figure><h1 id="在函数中定义函数"><a href="#在函数中定义函数" class="headerlink" title="在函数中定义函数"></a>在函数中定义函数</h1><p>刚才那些就是函数的基本知识了。我们来让你的知识更进一步。在Python中我们可以在一个函数中定义另一个函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line">    print(<span class="string">"now you are inside the hi() function"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the greet() function"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">welcome</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the welcome() function"</span></span><br><span class="line"></span><br><span class="line">    print(greet())</span><br><span class="line">    print(welcome())</span><br><span class="line">    print(<span class="string">"now you are back in the hi() function"</span>)</span><br><span class="line"></span><br><span class="line">hi()</span><br><span class="line"><span class="comment">#output:now you are inside the hi() function</span></span><br><span class="line"><span class="comment">#       now you are in the greet() function</span></span><br><span class="line"><span class="comment">#       now you are in the welcome() function</span></span><br><span class="line"><span class="comment">#       now you are back in the hi() function</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面展示了无论何时你调用hi(), greet()和welcome()将会同时被调用。</span></span><br><span class="line"><span class="comment"># 然后greet()和welcome()函数在hi()函数之外是不能访问的，比如：</span></span><br><span class="line"></span><br><span class="line">greet()</span><br><span class="line"><span class="comment">#outputs: NameError: name 'greet' is not defined</span></span><br></pre></td></tr></table></figure><br>那现在我们知道了可以在函数中定义另外的函数。也就是说：我们可以创建嵌套的函数。现在你需要再多学一点，就是函数也能返回函数。</p><h1 id="从函数中返回函数"><a href="#从函数中返回函数" class="headerlink" title="从函数中返回函数"></a>从函数中返回函数</h1><p>其实并不需要在一个函数里去执行另一个函数，我们也可以将其作为输出返回出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the greet() function"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">welcome</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"now you are in the welcome() function"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">"yasoob"</span>:</span><br><span class="line">        <span class="keyword">return</span> greet</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> welcome</span><br><span class="line"></span><br><span class="line">a = hi()</span><br><span class="line">print(a)</span><br><span class="line"><span class="comment">#outputs: &lt;function greet at 0x7f2143c01500&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#上面清晰地展示了`a`现在指向到hi()函数中的greet()函数</span></span><br><span class="line"><span class="comment">#现在试试这个</span></span><br><span class="line"></span><br><span class="line">print(a())</span><br><span class="line"><span class="comment">#outputs: now you are in the greet() function</span></span><br></pre></td></tr></table></figure><p>再次看看这个代码。在<code>if/else</code>语句中我们返回<code>greet</code>和<code>welcome</code>，而不是<code>greet()</code>和<code>welcome()</code>。为什么那样？这是因为当你把一对小括号放在后面，这个函数就会执行；然而如果你不放括号在它后面，那它可以被到处传递，并且可以赋值给别的变量而不去执行它。</p><p>你明白了吗？让我再稍微多解释点细节。</p><p>当我们写下<code>a = hi()</code>，<code>hi()</code>会被执行，而由于<code>name</code>参数默认是<em>yasoob</em>，所以函数<code>greet</code>被返回了。如果我们把语句改为<code>a = hi(name = &quot;ali&quot;)</code>，那么<code>welcome</code>函数将被返回。我们还可以打印出<code>hi()()</code>，这会输出<em>now you are in the greet() function</em>。</p><h1 id="将函数作为参数传给另一个函数"><a href="#将函数作为参数传给另一个函数" class="headerlink" title="将函数作为参数传给另一个函数"></a>将函数作为参数传给另一个函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"hi yasoob!"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doSomethingBeforeHi</span><span class="params">(func)</span>:</span></span><br><span class="line">    print(<span class="string">"I am doing some boring work before executing hi()"</span>)</span><br><span class="line">    print(func())</span><br><span class="line"></span><br><span class="line">doSomethingBeforeHi(hi)</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing hi()</span></span><br><span class="line"><span class="comment">#        hi yasoob!</span></span><br></pre></td></tr></table></figure><p>现在你已经具备所有必需知识，来进一步学习装饰器真正是什么了。装饰器让你在一个函数的前后去执行代码。</p><h1 id="你的第一个装饰器"><a href="#你的第一个装饰器" class="headerlink" title="你的第一个装饰器"></a>你的第一个装饰器</h1><p>在上一个例子里，其实我们已经创建了一个装饰器！现在我们修改下上一个装饰器，并编写一个稍微更有用点的程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span><span class="params">(a_func)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span><span class="params">()</span>:</span></span><br><span class="line">        print(<span class="string">"I am doing some boring work before executing a_func()"</span>)</span><br><span class="line"></span><br><span class="line">        a_func()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"I am doing some boring work after executing a_func()"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: "I am the function which needs some decoration to remove my foul smell"</span></span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br><span class="line"><span class="comment">#now a_function_requiring_decoration is wrapped by wrapTheFunction()</span></span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#        I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#        I am doing some boring work after executing a_func()</span></span><br></pre></td></tr></table></figure><p>你看明白了吗？我们刚刚应用了之前学习到的原理。这正是python中装饰器做的事情！它们封装一个函数，并且用这样或者那样的方式来修改它的行为。现在你也许疑惑，我们在代码里并没有使用@符号？那只是一个简短的方式来生成一个被装饰的函数。这里是我们如何使用@来运行之前的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Hey you! Decorate me!"""</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to "</span></span><br><span class="line">          <span class="string">"remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#         I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#         I am doing some boring work after executing a_func()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#the @a_new_decorator is just a short way of saying:</span></span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br></pre></td></tr></table></figure><p>希望你现在对Python装饰器的工作原理有一个基本的理解。如果我们运行如下代码会存在一个问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a_function_requiring_decoration.__name__)</span><br><span class="line"><span class="comment"># Output: wrapTheFunction</span></span><br></pre></td></tr></table></figure><p>这并不是我们想要的！Ouput输出应该是“a_function_requiring_decoration”。这里的函数被warpTheFunction替代了。它重写了我们函数的名字和注释文档(docstring)。幸运的是Python提供给我们一个简单的函数来解决这个问题，那就是functools.wraps。我们修改上一个例子来使用functools.wraps：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span><span class="params">(a_func)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(a_func)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span><span class="params">()</span>:</span></span><br><span class="line">        print(<span class="string">"I am doing some boring work before executing a_func()"</span>)</span><br><span class="line">        a_func()</span><br><span class="line">        print(<span class="string">"I am doing some boring work after executing a_func()"</span>)</span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"></span><br><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Hey yo! Decorate me!"""</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to "</span></span><br><span class="line">          <span class="string">"remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">print(a_function_requiring_decoration.__name__)</span><br><span class="line"><span class="comment"># Output: a_function_requiring_decoration</span></span><br></pre></td></tr></table></figure><p>现在好多了。我们接下来学习装饰器的一些常用场景。</p><p>蓝本规范:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorator_name</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorated</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> can_run:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"Function will not run"</span></span><br><span class="line">        <span class="keyword">return</span> f(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> decorated</span><br><span class="line"></span><br><span class="line"><span class="meta">@decorator_name</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span>(<span class="string">"Function is running"</span>)</span><br><span class="line"></span><br><span class="line">can_run = <span class="literal">True</span></span><br><span class="line">print(func())</span><br><span class="line"><span class="comment"># Output: Function is running</span></span><br><span class="line"></span><br><span class="line">can_run = <span class="literal">False</span></span><br><span class="line">print(func())</span><br><span class="line"><span class="comment"># Output: Function will not run</span></span><br></pre></td></tr></table></figure><br>注意：@wraps接受一个函数来进行装饰，并加入了复制函数名称、注释文档、参数列表等等的功能。这可以让我们在装饰器里面访问在装饰之前的函数的属性。</p><h1 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h1><p>现在我们来看一下装饰器在哪些地方特别耀眼，以及使用它可以让一些事情管理起来变得更简单。</p><h1 id="授权-Authorization"><a href="#授权-Authorization" class="headerlink" title="授权(Authorization)"></a>授权(Authorization)</h1><p>装饰器能有助于检查某个人是否被授权去使用一个web应用的端点(endpoint)。它们被大量使用于Flask和Django web框架中。这里是一个例子来使用基于装饰器的授权：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">requires_auth</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorated</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        auth = request.authorization</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> auth <span class="keyword">or</span> <span class="keyword">not</span> check_auth(auth.username, auth.password):</span><br><span class="line">            authenticate()</span><br><span class="line">        <span class="keyword">return</span> f(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> decorated</span><br></pre></td></tr></table></figure><h1 id="日志-Logging"><a href="#日志-Logging" class="headerlink" title="日志(Logging)"></a>日志(Logging)</h1><p>日志是装饰器运用的另一个亮点。这是个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logit</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(func)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_logging</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        print(func.__name__ + <span class="string">" was called"</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> with_logging</span><br><span class="line"></span><br><span class="line"><span class="meta">@logit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addition_func</span><span class="params">(x)</span>:</span></span><br><span class="line">   <span class="string">"""Do some math."""</span></span><br><span class="line">   <span class="keyword">return</span> x + x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result = addition_func(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># Output: addition_func was called</span></span><br></pre></td></tr></table></figure></p><p>我敢肯定你已经在思考装饰器的一个其他聪明用法了。</p><h1 id="带参数的装饰器"><a href="#带参数的装饰器" class="headerlink" title="带参数的装饰器"></a>带参数的装饰器</h1><p>来想想这个问题，难道<code>@wraps</code>不也是个装饰器吗？但是，它接收一个参数，就像任何普通的函数能做的那样。那么，为什么我们不也那样做呢？</p><p>这是因为，当你使用<code>@my_decorator</code>语法时，你是在应用一个以单个函数作为参数的一个包裹函数。记住，Python里每个东西都是一个对象，而且这包括函数！记住了这些，我们可以编写一下能返回一个包裹函数的函数。</p><h1 id="在函数中嵌入装饰器"><a href="#在函数中嵌入装饰器" class="headerlink" title="在函数中嵌入装饰器"></a>在函数中嵌入装饰器</h1><p>我们回到日志的例子，并创建一个包裹函数，能让我们指定一个用于输出的日志文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logit</span><span class="params">(logfile=<span class="string">'out.log'</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">logging_decorator</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">        @wraps(func)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            log_string = func.__name__ + <span class="string">" was called"</span></span><br><span class="line">            print(log_string)</span><br><span class="line">            <span class="comment"># 打开logfile，并写入内容</span></span><br><span class="line">            <span class="keyword">with</span> open(logfile, <span class="string">'a'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                <span class="comment"># 现在将日志打到指定的logfile</span></span><br><span class="line">                opened_file.write(log_string + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapped_function</span><br><span class="line">    <span class="keyword">return</span> logging_decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@logit()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">myfunc1()</span><br><span class="line"><span class="comment"># Output: myfunc1 was called</span></span><br><span class="line"><span class="comment"># 现在一个叫做 out.log 的文件出现了，里面的内容就是上面的字符串</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@logit(logfile='func2.log')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">myfunc2()</span><br><span class="line"><span class="comment"># Output: myfunc2 was called</span></span><br><span class="line"><span class="comment"># 现在一个叫做 func2.log 的文件出现了，里面的内容就是上面的字符串</span></span><br></pre></td></tr></table></figure><h1 id="装饰器类"><a href="#装饰器类" class="headerlink" title="装饰器类"></a>装饰器类</h1><p>现在我们有了能用于正式环境的<code>logit</code>装饰器，但当我们的应用的某些部分还比较脆弱时，异常也许是需要更紧急关注的事情。比方说有时你只想打日志到一个文件。而有时你想把引起你注意的问题发送到一个email，同时也保留日志，留个记录。这是一个使用继承的场景，但目前为止我们只看到过用来构建装饰器的函数。</p><p>幸运的是，类也可以用来构建装饰器。那我们现在以一个类而不是一个函数的方式，来重新构建<code>logit</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">logit</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, logfile=<span class="string">'out.log'</span>)</span>:</span></span><br><span class="line">        self.logfile = logfile</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, func)</span>:</span></span><br><span class="line"><span class="meta">        @wraps(func)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            log_string = func.__name__ + <span class="string">" was called"</span></span><br><span class="line">            print(log_string)</span><br><span class="line">            <span class="comment"># 打开logfile并写入</span></span><br><span class="line">            <span class="keyword">with</span> open(self.logfile, <span class="string">'a'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                <span class="comment"># 现在将日志打到指定的文件</span></span><br><span class="line">                opened_file.write(log_string + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="comment"># 现在，发送一个通知</span></span><br><span class="line">            self.notify()</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapped_function</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">notify</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># logit只打日志，不做别的</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>这个实现有一个附加优势，在于比嵌套函数的方式更加整洁，而且包裹一个函数还是使用跟以前一样的语法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@logit()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>现在，我们给<code>logit</code>创建子类，来添加email的功能(虽然email这个话题不会在这里展开)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">email_logit</span><span class="params">(logit)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    一个logit的实现版本，可以在函数调用时发送email给管理员</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, email=<span class="string">'admin@myproject.com'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">        self.email = email</span><br><span class="line">        super(email_logit, self).__init__(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">notify</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 发送一封email到self.email</span></span><br><span class="line">        <span class="comment"># 这里就不做实现了</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>从现在起，<code>@email_logit</code>将会和<code>@logit</code>产生同样的效果，但是在打日志的基础上，还会多发送一封邮件给管理员。</p><h1 id="Global和Return"><a href="#Global和Return" class="headerlink" title="Global和Return"></a>Global和Return</h1><p>你也许遇到过, python中一些函数在最尾部有一个<code>return</code>关键字。你知道它是干嘛吗？它和其他语言的<code>return</code>类似。我们来检查下这个小函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1, value2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value1 + value2</span><br><span class="line"></span><br><span class="line">result = add(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># Output: 8</span></span><br></pre></td></tr></table></figure><p>上面这个函数将两个值作为输入，然后输出它们相加之和。我们也可以这样做：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1,value2)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> result</span><br><span class="line">    result = value1 + value2</span><br><span class="line"></span><br><span class="line">add(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># Output: 8</span></span><br></pre></td></tr></table></figure></p><p>那首先我们来谈谈第一段也就是包含<code>return</code>关键字的代码。那个函数把值赋给了调用它的变量（也就是例子中的result变量）。<br>大多数境况下，你并不需要使用<code>global</code>关键字。然而我们也来检查下另外一段也就是包含<code>global</code>关键字的代码。<br>那个函数生成了一个<code>global</code>（全局）变量result。</p><p><code>global</code>在这的意思是什么？<code>global</code>变量意味着我们可以在函数以外的区域都能访问这个变量。让我们通过一个例子来证明它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先，是没有使用global变量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1, value2)</span>:</span></span><br><span class="line">    result = value1 + value2</span><br><span class="line"></span><br><span class="line">add(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Oh 糟了，我们遇到异常了。为什么会这样？</span></span><br><span class="line"><span class="comment"># python解释器报错说没有一个叫result的变量。</span></span><br><span class="line"><span class="comment"># 这是因为result变量只能在创建它的函数内部才允许访问，除非它是全局的(global)。</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">""</span>, line <span class="number">1</span>, <span class="keyword">in</span></span><br><span class="line">    result</span><br><span class="line">NameError: name <span class="string">'result'</span> <span class="keyword">is</span> <span class="keyword">not</span> defined</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在我们运行相同的代码，不过是在将result变量设为global之后</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(value1, value2)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> result</span><br><span class="line">    result = value1 + value2</span><br><span class="line"></span><br><span class="line">add(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure><p>如我们所愿，在第二次运行时没有异常了。在实际的编程时，你应该试着避开<code>global</code>关键字，它只会让生活变得艰难，因为它引入了多余的变量到全局作用域了。</p><h1 id="多个return值"><a href="#多个return值" class="headerlink" title="多个return值"></a>多个return值</h1><p>那如果你想从一个函数里返回两个变量而不是一个呢？<br>新手们有若干种方法。最著名的方法，是使用<code>global</code>关键字。让我们看下这个没用的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">profile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> name</span><br><span class="line">    <span class="keyword">global</span> age</span><br><span class="line">    name = <span class="string">"Danny"</span></span><br><span class="line">    age = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">profile()</span><br><span class="line">print(name)</span><br><span class="line"><span class="comment"># Output: Danny</span></span><br><span class="line"></span><br><span class="line">print(age)</span><br><span class="line"><span class="comment"># Output: 30</span></span><br></pre></td></tr></table></figure><p><strong>注意:</strong> 不要试着使用上述方法。重要的事情说三遍，不要试着使用上述方法！</p><p>有些人试着在函数结束时，返回一个包含多个值的<code>tuple</code>(元组)，<code>list</code>(列表)或者<code>dict</code>(字典),来解决这个问题。这是一种可行的方式，而且使用起来像一个黑魔法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">profile</span><span class="params">()</span>:</span></span><br><span class="line">    name = <span class="string">"Danny"</span></span><br><span class="line">    age = <span class="number">30</span></span><br><span class="line">    <span class="keyword">return</span> (name, age)</span><br><span class="line"></span><br><span class="line">profile_data = profile()</span><br><span class="line">print(profile_data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Output: Danny</span></span><br><span class="line"></span><br><span class="line">print(profile_data[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Output: 30</span></span><br></pre></td></tr></table></figure><br>或者按照更常见的惯例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">profile</span><span class="params">()</span>:</span></span><br><span class="line">    name = <span class="string">"Danny"</span></span><br><span class="line">    age = <span class="number">30</span></span><br><span class="line">    <span class="keyword">return</span> name, age</span><br></pre></td></tr></table></figure><br>这是一种比列表和字典更好的方式。不要使用<code>global</code>关键字，除非你知道你正在做什么。<code>global</code>也许在某些场景下是一个更好的选择（但其中大多数情况都不是）。</p><h1 id="对象变动-Mutation"><a href="#对象变动-Mutation" class="headerlink" title="对象变动(Mutation)"></a>对象变动(Mutation)</h1><p>Python中可变(<strong>mutable</strong>)与不可变(<strong>immutable</strong>)的数据类型让新手很是头痛。简单的说，可变(mutable)意味着”可以被改动”，而不可变(immutable)的意思是“常量(constant)”。想把脑筋转动起来吗？考虑下这个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">foo = [<span class="string">'hi'</span>]</span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi']</span></span><br><span class="line"></span><br><span class="line">bar = foo</span><br><span class="line">bar += [<span class="string">'bye'</span>]</span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi', 'bye']</span></span><br></pre></td></tr></table></figure><p>刚刚发生了什么？我们预期的不是那样！我们期望看到是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">foo = [<span class="string">'hi'</span>]</span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi']</span></span><br><span class="line"></span><br><span class="line">bar = foo</span><br><span class="line">bar += [<span class="string">'bye'</span>]</span><br><span class="line"></span><br><span class="line">print(foo)</span><br><span class="line"><span class="comment"># Output: ['hi']</span></span><br><span class="line"></span><br><span class="line">print(bar)</span><br><span class="line"><span class="comment"># Output: ['hi', 'bye']</span></span><br></pre></td></tr></table></figure><p>这不是一个bug。这是对象可变性(<strong>mutability</strong>)在作怪。每当你将一个变量赋值为另一个可变类型的变量时，对这个数据的任意改动会同时反映到这两个变量上去。新变量只不过是老变量的一个别名而已。这个情况只是针对可变数据类型。下面的函数和可变数据类型让你一下就明白了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(num, target=[])</span>:</span></span><br><span class="line">    target.append(num)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line"></span><br><span class="line">add_to(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Output: [1]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Output: [1, 2]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">3</span>)</span><br><span class="line"><span class="comment"># Output: [1, 2, 3]</span></span><br></pre></td></tr></table></figure><p>你可能预期它表现的不是这样子。你可能希望，当你调用<code>add_to</code>时，有一个新的列表被创建，就像这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(num, target=[])</span>:</span></span><br><span class="line">    target.append(num)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line"></span><br><span class="line">add_to(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Output: [1]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Output: [2]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">3</span>)</span><br><span class="line"><span class="comment"># Output: [3]</span></span><br></pre></td></tr></table></figure><p>啊哈！这次又没有达到预期，是列表的可变性在作怪。在Python中当函数被定义时，默认参数只会运算一次，而不是每次被调用时都会重新运算。你应该永远不要定义可变类型的默认参数，除非你知道你正在做什么。你应该像这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(element, target=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> target <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        target = []</span><br><span class="line">    target.append(element)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line">``` </span><br><span class="line">现在每当你在调用这个函数不传入```target```参数的时候，一个新的列表会被创建。举个例子：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">add_to(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Output: [42]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Output: [42]</span></span><br><span class="line"></span><br><span class="line">add_to(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Output: [42]</span></span><br></pre></td></tr></table></figure><h1 id="slots-魔法"><a href="#slots-魔法" class="headerlink" title="__slots__魔法"></a><code>__slots__</code>魔法</h1><p>在Python中，每个类都有实例属性。默认情况下Python用一个字典来保存一个对象的实例属性。这非常有用，因为它允许我们在运行时去设置任意的新属性。</p><p>然而，对于有着已知属性的小类来说，它可能是个瓶颈。这个字典浪费了很多内存。Python不能在对象创建时直接分配一个固定量的内存来保存所有的属性。因此如果你创建许多对象（我指的是成千上万个），它会消耗掉很多内存。<br>不过还是有一个方法来规避这个问题。这个方法需要使用<code>__slots__</code>来告诉Python不要使用字典，而且只给一个固定集合的属性分配空间。</p><p>这里是一个使用与不使用<code>__slots__</code>的例子：</p><ul><li><p>不使用 <code>__slots__</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.identifier = identifier</span><br><span class="line">        self.set_up()</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure></li><li><p>使用 <code>__slots__</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    __slots__ = [<span class="string">'name'</span>, <span class="string">'identifier'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.identifier = identifier</span><br><span class="line">        self.set_up()</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure></li></ul><p>第二段代码会为你的内存减轻负担。通过这个技巧，有些人已经看到内存占用率几乎40%~50%的减少。</p><p>稍微备注一下，你也许需要试一下PyPy。它已经默认地做了所有这些优化。</p><p>以下你可以看到一个例子，它用IPython来展示在有与没有<code>__slots__</code>情况下的精确内存占用，感谢 <a href="https://github.com/ianozsvald/ipython_memory_usage" target="_blank" rel="noopener">https://github.com/ianozsvald/ipython_memory_usage</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Python <span class="number">3.4</span><span class="number">.3</span> (default, Jun  <span class="number">6</span> <span class="number">2015</span>, <span class="number">13</span>:<span class="number">32</span>:<span class="number">34</span>)</span><br><span class="line">Type <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line"></span><br><span class="line">IPython <span class="number">4.0</span><span class="number">.0</span> -- An enhanced Interactive Python.</span><br><span class="line">?         -&gt; Introduction and overview of IPython's features.</span><br><span class="line">%quickref -&gt; Quick reference.</span><br><span class="line">help      -&gt; Python's own help system.</span><br><span class="line">object?   -&gt; Details about 'object', use 'object??' for extra details.</span><br><span class="line"></span><br><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> ipython_memory_usage.ipython_memory_usage <span class="keyword">as</span> imu</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: imu.start_watching_memory()</span><br><span class="line">In [<span class="number">2</span>] used <span class="number">0.0000</span> MiB RAM <span class="keyword">in</span> <span class="number">5.31</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">15.57</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: %cat slots.py</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">        __slots__ = [<span class="string">'name'</span>, <span class="string">'identifier'</span>]</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">                self.name = name</span><br><span class="line">                self.identifier = identifier</span><br><span class="line"></span><br><span class="line">num = <span class="number">1024</span>*<span class="number">256</span></span><br><span class="line">x = [MyClass(<span class="number">1</span>,<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">In [<span class="number">3</span>] used <span class="number">0.2305</span> MiB RAM <span class="keyword">in</span> <span class="number">0.12</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">15.80</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="keyword">from</span> slots <span class="keyword">import</span> *</span><br><span class="line">In [<span class="number">4</span>] used <span class="number">9.3008</span> MiB RAM <span class="keyword">in</span> <span class="number">0.72</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">25.10</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: %cat noslots.py</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, identifier)</span>:</span></span><br><span class="line">                self.name = name</span><br><span class="line">                self.identifier = identifier</span><br><span class="line"></span><br><span class="line">num = <span class="number">1024</span>*<span class="number">256</span></span><br><span class="line">x = [MyClass(<span class="number">1</span>,<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">In [<span class="number">5</span>] used <span class="number">0.1758</span> MiB RAM <span class="keyword">in</span> <span class="number">0.12</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">25.28</span> MiB</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: <span class="keyword">from</span> noslots <span class="keyword">import</span> *</span><br><span class="line">In [<span class="number">6</span>] used <span class="number">22.6680</span> MiB RAM <span class="keyword">in</span> <span class="number">0.80</span>s, peaked <span class="number">0.00</span> MiB above current, total RAM usage <span class="number">47.95</span> MiB</span><br></pre></td></tr></table></figure><h1 id="虚拟环境-virtualenv"><a href="#虚拟环境-virtualenv" class="headerlink" title="虚拟环境(virtualenv)"></a>虚拟环境(virtualenv)</h1><h2 id="你听说过virtualenv吗？"><a href="#你听说过virtualenv吗？" class="headerlink" title="你听说过virtualenv吗？"></a>你听说过<code>virtualenv</code>吗？</h2><p>如果你是一位初学者，你可能没有听说过<code>virtualenv</code>；但如果你是位经验丰富的程序员，那么它可能是你的工具集的重要组成部分。</p><h2 id="那么，什么是virtualenv"><a href="#那么，什么是virtualenv" class="headerlink" title="那么，什么是virtualenv?"></a>那么，什么是<code>virtualenv</code>?</h2><p><code>Virtualenv</code> 是一个工具，它能够帮我们创建一个独立(隔离)的Python环境。想象你有一个应用程序，依赖于版本为2的第三方模块，但另一个程序依赖的版本是3，请问你如何使用和开发这些应用程序？</p><p>如果你把一切都安装到了<code>/usr/lib/python2.7/site-packages</code>（或者其它平台的标准位置），那很容易出现某个模块被升级而你却不知道的情况。</p><p>在另一种情况下，想象你有一个已经开发完成的程序，但是你不想更新它所依赖的第三方模块版本；但你已经开始另一个程序，需要这些第三方模块的版本。</p><h2 id="用什么方式解决？"><a href="#用什么方式解决？" class="headerlink" title="用什么方式解决？"></a>用什么方式解决？</h2><p>使用<code>virtualenv</code>！针对每个程序创建独立（隔离）的Python环境，而不是在全局安装所依赖的模块。</p><p>要安装它，只需要在命令行中输入以下命令：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install virtualenv</span><br></pre></td></tr></table></figure><p>最重要的命令是：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ virtualenv myproject</span><br><span class="line">$ <span class="built_in">source</span> myproject/bin/activate</span><br></pre></td></tr></table></figure><p>执行第一个命令在<code>myproject</code>文件夹创建一个隔离的virtualenv环境，第二个命令激活这个隔离的环境(<code>virtualenv</code>)。</p><p>在创建virtualenv时，你必须做出决定：这个virtualenv是使用系统全局的模块呢？还是只使用这个virtualenv内的模块。<br>默认情况下，virtualenv不会使用系统全局模块。</p><p>如果你想让你的virtualenv使用系统全局模块，请使用<code>--system-site-packages</code>参数创建你的virtualenv，例如：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtualenv --system-site-packages mycoolproject</span><br></pre></td></tr></table></figure><p>使用以下命令可以退出这个virtualenv:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ deactivate</span><br></pre></td></tr></table></figure><p>运行之后将恢复使用你系统全局的Python模块。</p><h1 id="福利"><a href="#福利" class="headerlink" title="福利"></a>福利</h1><p>你可以使用<code>smartcd</code>来帮助你管理你的环境，当你切换目录时，它可以帮助你激活（activate）和退出（deactivate）你的virtualenv。我已经用了很多次，很喜欢它。你可以在github(<a href="https://github.com/cxreg/smartcd" target="_blank" rel="noopener">https://github.com/cxreg/smartcd</a>) 上找到更多关于它的资料。</p><p>这只是一个virtualenv的简短介绍，你可以在 <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/" target="_blank" rel="noopener">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a> 找到更多信息。</p><h1 id="容器-Collections"><a href="#容器-Collections" class="headerlink" title="容器(Collections)"></a>容器(<code>Collections</code>)</h1><p>Python附带一个模块，它包含许多容器数据类型，名字叫作<code>collections</code>。我们将讨论它的作用和用法。</p><p>我们将讨论的是：</p><ul><li>defaultdict</li><li>counter</li><li>deque</li><li>namedtuple</li><li>enum.Enum (包含在Python 3.4以上)</li></ul><h1 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a>defaultdict</h1><p>我个人使用<code>defaultdict</code>较多，与<code>dict</code>类型不同，你不需要检查<strong>key</strong>是否存在，所以我们能这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">colours = (</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Yellow'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Blue'</span>),</span><br><span class="line">    (<span class="string">'Arham'</span>, <span class="string">'Green'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Black'</span>),</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Red'</span>),</span><br><span class="line">    (<span class="string">'Ahmed'</span>, <span class="string">'Silver'</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">favourite_colours = defaultdict(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, colour <span class="keyword">in</span> colours:</span><br><span class="line">    favourite_colours[name].append(colour)</span><br><span class="line"></span><br><span class="line">print(favourite_colours)</span><br></pre></td></tr></table></figure><h2 id="运行输出"><a href="#运行输出" class="headerlink" title="运行输出"></a>运行输出</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># defaultdict(&lt;type 'list'&gt;,</span></span><br><span class="line"><span class="comment">#    &#123;'Arham': ['Green'],</span></span><br><span class="line"><span class="comment">#     'Yasoob': ['Yellow', 'Red'],</span></span><br><span class="line"><span class="comment">#     'Ahmed': ['Silver'],</span></span><br><span class="line"><span class="comment">#     'Ali': ['Blue', 'Black']</span></span><br><span class="line"><span class="comment"># &#125;)</span></span><br></pre></td></tr></table></figure><p>另一种重要的是例子就是：当你在一个字典中对一个键进行嵌套赋值时，如果这个键不存在，会触发<code>keyError</code>异常。 <code>defaultdict</code>允许我们用一个聪明的方式绕过这个问题。<br> 首先我分享一个使用<code>dict</code>触发<code>KeyError</code>的例子，然后提供一个使用<code>defaultdict</code>的解决方案。</p><p><strong>问题</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">some_dict = &#123;&#125;</span><br><span class="line">some_dict[<span class="string">'colours'</span>][<span class="string">'favourite'</span>] = <span class="string">"yellow"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 异常输出：KeyError: 'colours'</span></span><br></pre></td></tr></table></figure><p><strong>解决方案</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line">tree = <span class="keyword">lambda</span>: collections.defaultdict(tree)</span><br><span class="line">some_dict = tree()</span><br><span class="line">some_dict[<span class="string">'colours'</span>][<span class="string">'favourite'</span>] = <span class="string">"yellow"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 运行正常</span></span><br></pre></td></tr></table></figure><p>你可以用<code>json.dumps</code>打印出<code>some_dict</code>，例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">print(json.dumps(some_dict))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: &#123;"colours": &#123;"favourite": "yellow"&#125;&#125;</span></span><br></pre></td></tr></table></figure></p><h1 id="counter"><a href="#counter" class="headerlink" title="counter"></a>counter</h1><p>Counter是一个计数器，它可以帮助我们针对某项数据进行计数。比如它可以用来计算每个人喜欢多少种颜色：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">colours = (</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Yellow'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Blue'</span>),</span><br><span class="line">    (<span class="string">'Arham'</span>, <span class="string">'Green'</span>),</span><br><span class="line">    (<span class="string">'Ali'</span>, <span class="string">'Black'</span>),</span><br><span class="line">    (<span class="string">'Yasoob'</span>, <span class="string">'Red'</span>),</span><br><span class="line">    (<span class="string">'Ahmed'</span>, <span class="string">'Silver'</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">favs = Counter(name <span class="keyword">for</span> name, colour <span class="keyword">in</span> colours)</span><br><span class="line">print(favs)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出:</span></span><br><span class="line"><span class="comment">## Counter(&#123;</span></span><br><span class="line"><span class="comment">##     'Yasoob': 2,</span></span><br><span class="line"><span class="comment">##     'Ali': 2,</span></span><br><span class="line"><span class="comment">##     'Arham': 1,</span></span><br><span class="line"><span class="comment">##     'Ahmed': 1</span></span><br><span class="line"><span class="comment">##  &#125;)</span></span><br></pre></td></tr></table></figure><p>我们也可以在利用它统计一个文件，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'filename'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    line_count = Counter(f)</span><br><span class="line">print(line_count)</span><br></pre></td></tr></table></figure><h1 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h1><p>deque提供了一个双端队列，你可以从头/尾两端添加或删除元素。要想使用它，首先我们要从<code>collections</code>中导入<code>deque</code>模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br></pre></td></tr></table></figure><p>现在，你可以创建一个<code>deque</code>对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = deque()</span><br></pre></td></tr></table></figure><p>它的用法就像python的<code>list</code>，并且提供了类似的方法，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">d = deque()</span><br><span class="line">d.append(<span class="string">'1'</span>)</span><br><span class="line">d.append(<span class="string">'2'</span>)</span><br><span class="line">d.append(<span class="string">'3'</span>)</span><br><span class="line"></span><br><span class="line">print(len(d))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 3</span></span><br><span class="line"></span><br><span class="line">print(d[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: '1'</span></span><br><span class="line"></span><br><span class="line">print(d[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: '3'</span></span><br></pre></td></tr></table></figure><p>你可以从两端取出(pop)数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">d = deque(range(<span class="number">5</span>))</span><br><span class="line">print(len(d))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 5</span></span><br><span class="line"></span><br><span class="line">d.popleft()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 0</span></span><br><span class="line"></span><br><span class="line">d.pop()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 4</span></span><br><span class="line"></span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: deque([1, 2, 3])</span></span><br></pre></td></tr></table></figure><p>我们也可以限制这个列表的大小，当超出你设定的限制时，数据会从对队列另一端被挤出去(pop)。<br>最好的解释是给出一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = deque(maxlen=<span class="number">30</span>)</span><br></pre></td></tr></table></figure><p>现在当你插入30条数据时，最左边一端的数据将从队列中删除。</p><p>你还可以从任一端扩展这个队列中的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d = deque([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">d.extendleft([<span class="number">0</span>])</span><br><span class="line">d.extend([<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: deque([0, 1, 2, 3, 4, 5, 6, 7, 8])</span></span><br></pre></td></tr></table></figure><h1 id="namedtuple"><a href="#namedtuple" class="headerlink" title="namedtuple"></a>namedtuple</h1><p>您可能已经熟悉元组。<br>一个元组是一个不可变的列表，你可以存储一个数据的序列，它和命名元组(<code>namedtuples</code>)非常像，但有几个关键的不同。<br>主要相似点是都不像列表，你不能修改元组中的数据。为了获取元组中的数据，你需要使用整数作为索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">man = (<span class="string">'Ali'</span>, <span class="number">30</span>)</span><br><span class="line">print(man[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: Ali</span></span><br></pre></td></tr></table></figure><p>嗯，那<code>namedtuples</code>是什么呢？它把元组变成一个针对简单任务的容器。你不必使用整数索引来访问一个<code>namedtuples</code>的数据。你可以像字典(<code>dict</code>)一样访问<code>namedtuples</code>，但<code>namedtuples</code>是不可变的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line"></span><br><span class="line">print(perry)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: Animal(name='perry', age=31, type='cat')</span></span><br><span class="line"></span><br><span class="line">print(perry.name)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: 'perry'</span></span><br></pre></td></tr></table></figure><p>现在你可以看到，我们可以用名字来访问<code>namedtuple</code>中的数据。我们再继续分析它。一个命名元组(<code>namedtuple</code>)有两个必需的参数。它们是元组名称和字段名称。</p><p>在上面的例子中，我们的元组名称是<code>Animal</code>，字段名称是’name’，’age’和’type’。<br><code>namedtuple</code>让你的元组变得<strong>自文档</strong>了。你只要看一眼就很容易理解代码是做什么的。<br>你也不必使用整数索引来访问一个命名元组，这让你的代码更易于维护。<br>而且，<strong><code>namedtuple</code>的每个实例没有对象字典</strong>，所以它们很轻量，与普通的元组比，并不需要更多的内存。这使得它们比字典更快。</p><p>然而，要记住它是一个元组，属性值在<code>namedtuple</code>中是不可变的，所以下面的代码不能工作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line">perry.age = <span class="number">42</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出:</span></span><br><span class="line"><span class="comment">## Traceback (most recent call last):</span></span><br><span class="line"><span class="comment">##     File "", line 1, in</span></span><br><span class="line"><span class="comment">## AttributeError: can't set attribute</span></span><br></pre></td></tr></table></figure><p>你应该使用命名元组来让代码<strong>自文档</strong>，<strong>它们向后兼容于普通的元组</strong>，这意味着你可以既使用整数索引，也可以使用名称来访问<code>namedtuple</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line">print(perry[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: perry</span></span><br></pre></td></tr></table></figure><p>最后，你可以将一个命名元组转换为字典，方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"Perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line">print(perry._asdict())</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出: OrderedDict([('name', 'Perry'), ('age', 31), ...</span></span><br></pre></td></tr></table></figure><h1 id="enum-Enum-Python-3-4"><a href="#enum-Enum-Python-3-4" class="headerlink" title="enum.Enum (Python 3.4+)"></a>enum.Enum (Python 3.4+)</h1><p>另一个有用的容器是枚举对象，它属于<code>enum</code>模块，存在于Python 3.4以上版本中（同时作为一个独立的PyPI包<code>enum34</code>供老版本使用）。Enums(枚举类型)基本上是一种组织各种东西的方式。</p><p>让我们回顾一下上一个’Animal’命名元组的例子。<br>它有一个type字段，问题是，type是一个字符串。<br>那么问题来了，万一程序员输入了<code>Cat</code>，因为他按到了Shift键，或者输入了’CAT’，甚至’kitten’？</p><p>枚举可以帮助我们避免这个问题，通过不使用字符串。考虑以下这个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Species</span><span class="params">(Enum)</span>:</span></span><br><span class="line">    cat = <span class="number">1</span></span><br><span class="line">    dog = <span class="number">2</span></span><br><span class="line">    horse = <span class="number">3</span></span><br><span class="line">    aardvark = <span class="number">4</span></span><br><span class="line">    butterfly = <span class="number">5</span></span><br><span class="line">    owl = <span class="number">6</span></span><br><span class="line">    platypus = <span class="number">7</span></span><br><span class="line">    dragon = <span class="number">8</span></span><br><span class="line">    unicorn = <span class="number">9</span></span><br><span class="line">    <span class="comment"># 依次类推</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 但我们并不想关心同一物种的年龄，所以我们可以使用一个别名</span></span><br><span class="line">    kitten = <span class="number">1</span>  <span class="comment"># (译者注：幼小的猫咪)</span></span><br><span class="line">    puppy = <span class="number">2</span>   <span class="comment"># (译者注：幼小的狗狗)</span></span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"Perry"</span>, age=<span class="number">31</span>, type=Species.cat)</span><br><span class="line">drogon = Animal(name=<span class="string">"Drogon"</span>, age=<span class="number">4</span>, type=Species.dragon)</span><br><span class="line">tom = Animal(name=<span class="string">"Tom"</span>, age=<span class="number">75</span>, type=Species.cat)</span><br><span class="line">charlie = Animal(name=<span class="string">"Charlie"</span>, age=<span class="number">2</span>, type=Species.kitten)</span><br></pre></td></tr></table></figure><h2 id="现在，我们进行一些测试："><a href="#现在，我们进行一些测试：" class="headerlink" title="现在，我们进行一些测试："></a>现在，我们进行一些测试：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>charlie.type == tom.type</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>charlie.type</span><br><span class="line">&lt;Species.cat: <span class="number">1</span>&gt;</span><br></pre></td></tr></table></figure><p>这样就没那么容易错误，我们必须更明确，而且我们应该只使用定义后的枚举类型。</p><p>有三种方法访问枚举数据，例如以下方法都可以获取到’cat’的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Species(<span class="number">1</span>)</span><br><span class="line">Species[<span class="string">'cat'</span>]</span><br><span class="line">Species.cat</span><br></pre></td></tr></table></figure><p>这只是一个快速浏览<code>collections</code>模块的介绍，建议你阅读本文最后的官方文档。</p><h1 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h1><p>枚举(<code>enumerate</code>)是Python内置函数。它的用处很难在简单的一行中说明，但是大多数的新人，甚至一些高级程序员都没有意识到它。</p><p>它允许我们遍历数据并自动计数，</p><p>下面是一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> counter, value <span class="keyword">in</span> enumerate(some_list):</span><br><span class="line">    print(counter, value)</span><br></pre></td></tr></table></figure><p>不只如此，<code>enumerate</code>也接受一些可选参数，这使它更有用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grapes'</span>, <span class="string">'pear'</span>]</span><br><span class="line"><span class="keyword">for</span> c, value <span class="keyword">in</span> enumerate(my_list, <span class="number">1</span>):</span><br><span class="line">    print(c, value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'apple'</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">'banana'</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">'grapes'</span>)</span><br><span class="line">(<span class="number">4</span>, <span class="string">'pear'</span>)</span><br></pre></td></tr></table></figure><p>上面这个可选参数允许我们定制从哪个数字开始枚举。<br>你还可以用来创建包含索引的元组列表，<br>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grapes'</span>, <span class="string">'pear'</span>]</span><br><span class="line">counter_list = list(enumerate(my_list, <span class="number">1</span>))</span><br><span class="line">print(counter_list)</span><br><span class="line"><span class="comment"># 输出: [(1, 'apple'), (2, 'banana'), (3, 'grapes'), (4, 'pear')]</span></span><br></pre></td></tr></table></figure><h1 id="对象自省"><a href="#对象自省" class="headerlink" title="对象自省"></a>对象自省</h1><p>自省(introspection)，在计算机编程领域里，是指在运行时来判断一个对象的类型的能力。它是Python的强项之一。Python中所有一切都是一个对象，而且我们可以仔细勘察那些对象。Python还包含了许多内置函数和模块来帮助我们。</p><h1 id="dir"><a href="#dir" class="headerlink" title="dir"></a><code>dir</code></h1><p>在这个小节里我们会学习到<code>dir</code>以及它在自省方面如何给我们提供便利。</p><p>它是用于自省的最重要的函数之一。它返回一个列表，列出了一个对象所拥有的属性和方法。这里是一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">dir(my_list)</span><br><span class="line"><span class="comment"># Output: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',</span></span><br><span class="line"><span class="comment"># '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',</span></span><br><span class="line"><span class="comment"># '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',</span></span><br><span class="line"><span class="comment"># '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',</span></span><br><span class="line"><span class="comment"># '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',</span></span><br><span class="line"><span class="comment"># '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',</span></span><br><span class="line"><span class="comment"># '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',</span></span><br><span class="line"><span class="comment"># 'remove', 'reverse', 'sort']</span></span><br></pre></td></tr></table></figure><p>上面的自省给了我们一个列表对象的所有方法的名字。当你没法回忆起一个方法的名字，这会非常有帮助。如果我们运行<code>dir()</code>而不传入参数，那么它会返回当前作用域的所有名字。</p><h1 id="type和id"><a href="#type和id" class="headerlink" title="type和id"></a><code>type</code>和<code>id</code></h1><p><code>type</code>函数返回一个对象的类型。举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(type(<span class="string">''</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'str'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type([]))</span><br><span class="line"><span class="comment"># Output: &lt;type 'list'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(&#123;&#125;))</span><br><span class="line"><span class="comment"># Output: &lt;type 'dict'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(dict))</span><br><span class="line"><span class="comment"># Output: &lt;type 'type'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'int'&gt;</span></span><br></pre></td></tr></table></figure><p><code>id()</code>函数返回任意不同种类对象的唯一ID，举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">"Yasoob"</span></span><br><span class="line">print(id(name))</span><br><span class="line"><span class="comment"># Output: 139972439030304</span></span><br></pre></td></tr></table></figure><h1 id="inspect模块"><a href="#inspect模块" class="headerlink" title="inspect模块"></a><code>inspect</code>模块</h1><p><code>inspect</code>模块也提供了许多有用的函数，来获取活跃对象的信息。比方说，你可以查看一个对象的成员，只需运行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line">print(inspect.getmembers(str))</span><br><span class="line"><span class="comment"># Output: [('__add__', &lt;slot wrapper '__add__' of ... ...</span></span><br></pre></td></tr></table></figure><p>还有好多个其他方法也能有助于自省。如果你愿意，你可以去探索它们。</p><h1 id="各种推导式-comprehensions"><a href="#各种推导式-comprehensions" class="headerlink" title="各种推导式(comprehensions)"></a>各种推导式(comprehensions)</h1><p>推导式（又称解析式）是Python的一种独有特性，如果我被迫离开了它，我会非常想念。推导式是可以从一个数据序列构建另一个新的数据序列的结构体。 共有三种推导，在Python2和3中都有支持：</p><ul><li>列表(<code>list</code>)推导式</li><li>字典(<code>dict</code>)推导式</li><li>集合(<code>set</code>)推导式</li></ul><p>我们将一一进行讨论。一旦你知道了使用列表推导式的诀窍，你就能轻易使用任意一种推导式了。</p><h1 id="列表推导式（list-comprehensions）"><a href="#列表推导式（list-comprehensions）" class="headerlink" title="列表推导式（list comprehensions）"></a>列表推导式（<code>list</code> comprehensions）</h1><p>列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表。<br>它的结构是在一个中括号里包含一个表达式，然后是一个<code>for</code>语句，然后是0个或多个<code>for</code>或者<code>if</code>语句。那个表达式可以是任意的，意思是你可以在列表中放入任意类型的对象。返回结果将是一个新的列表，在这个以<code>if</code>和<code>for</code>语句为上下文的表达式运行完成之后产生。</p><h3 id="规范"><a href="#规范" class="headerlink" title="规范"></a>规范</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">variable = [out_exp <span class="keyword">for</span> out_exp <span class="keyword">in</span> input_list <span class="keyword">if</span> out_exp == <span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>这里是另外一个简明例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">multiples = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>) <span class="keyword">if</span> i % <span class="number">3</span> <span class="keyword">is</span> <span class="number">0</span>]</span><br><span class="line">print(multiples)</span><br><span class="line"><span class="comment"># Output: [0, 3, 6, 9, 12, 15, 18, 21, 24, 27]</span></span><br></pre></td></tr></table></figure><p>这将对快速生成列表非常有用。<br>有些人甚至更喜欢使用它而不是<code>filter</code>函数。<br>列表推导式在有些情况下超赞，特别是当你需要使用<code>for</code>循环来生成一个新列表。举个例子，你通常会这样做：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">squared = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    squared.append(x**<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><p>你可以使用列表推导式来简化它，就像这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">squared = [x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure><h1 id="字典推导式（dict-comprehensions）"><a href="#字典推导式（dict-comprehensions）" class="headerlink" title="字典推导式（dict comprehensions）"></a>字典推导式（<code>dict</code> comprehensions）</h1><p>字典推导和列表推导的使用方法是类似的。这里有个我最近发现的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mcase = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">34</span>, <span class="string">'A'</span>: <span class="number">7</span>, <span class="string">'Z'</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">mcase_frequency = &#123;</span><br><span class="line">    k.lower(): mcase.get(k.lower(), <span class="number">0</span>) + mcase.get(k.upper(), <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> mcase.keys()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># mcase_frequency == &#123;'a': 17, 'z': 3, 'b': 34&#125;</span></span><br></pre></td></tr></table></figure></p><p>在上面的例子中我们把同一个字母但不同大小写的值合并起来了。  </p><p>就我个人来说没有大量使用字典推导式。</p><p>你还可以快速对换一个字典的键和值：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> some_dict.items()&#125;</span><br></pre></td></tr></table></figure></p><h1 id="集合推导式（set-comprehensions）"><a href="#集合推导式（set-comprehensions）" class="headerlink" title="集合推导式（set comprehensions）"></a>集合推导式（<code>set</code> comprehensions）</h1><p>它们跟列表推导式也是类似的。 唯一的区别在于它们使用大括号<code>{}</code>。 举个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">squared = &#123;x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]&#125;</span><br><span class="line">print(squared)</span><br><span class="line"><span class="comment"># Output: &#123;1, 4&#125;</span></span><br></pre></td></tr></table></figure></p><h1 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h1><p>异常处理是一种艺术，一旦你掌握，会授予你无穷的力量。我将要向你展示我们能处理异常的一些方式。</p><p>最基本的术语里我们知道了<code>try/except</code>从句。可能触发异常产生的代码会放到<code>try</code>语句块里，而处理异常的代码会在<code>except</code>语句块里实现。这是一个简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'An IOError occurred. &#123;&#125;'</span>.format(e.args[<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure><p>上面的例子里，我们仅仅在处理一个<code>IOError</code>的异常。大部分初学者还不知道的是，我们可以处理多个异常。</p><h1 id="处理多个异常"><a href="#处理多个异常" class="headerlink" title="处理多个异常"></a>处理多个异常</h1><p>我们可以使用三种方法来处理多个异常。</p><p>第一种方法需要把所有可能发生的异常放到一个元组里。像这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> (IOError, EOFError) <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"An error occurred. &#123;&#125;"</span>.format(e.args[<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure><p>另外一种方式是对每个单独的异常在单独的<code>except</code>语句块中处理。我们想要多少个<code>except</code>语句块都可以。这里是个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> EOFError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"An EOF error occurred."</span>)</span><br><span class="line">    <span class="keyword">raise</span> e</span><br><span class="line"><span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"An error occurred."</span>)</span><br><span class="line">    <span class="keyword">raise</span> e</span><br></pre></td></tr></table></figure><p>上面这个方式中，如果异常没有被第一个<code>except</code>语句块处理，那么它也许被下一个语句块处理，或者根本不会被处理。</p><p>现在，最后一种方式会捕获<strong>所有</strong>异常：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="comment"># 打印一些异常日志，如果你想要的话</span></span><br><span class="line">    <span class="keyword">raise</span></span><br></pre></td></tr></table></figure><p>当你不知道你的程序会抛出什么样的异常时，上面的方式可能非常有帮助。</p><h1 id="finally从句"><a href="#finally从句" class="headerlink" title="finally从句"></a><code>finally</code>从句</h1><p>我们把我们的主程序代码包裹进了<code>try</code>从句。然后我们把一些代码包裹进一个<code>except</code>从句，它会在<code>try</code>从句中的代码触发异常时执行。</p><p>在下面的例子中，我们还会使用第三个从句，那就是<code>finally</code>从句。包裹到<code>finally</code>从句中的代码不管异常是否触发都将会被执行。这可以被用来在脚本执行之后做清理工作。这里是个简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file = open(<span class="string">'test.txt'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'An IOError occurred. &#123;&#125;'</span>.format(e.args[<span class="number">-1</span>]))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    print(<span class="string">"This would be printed whether or not an exception occurred!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: An IOError occurred. No such file or directory</span></span><br><span class="line"><span class="comment"># This would be printed whether or not an exception occurred!</span></span><br></pre></td></tr></table></figure><h1 id="try-else从句"><a href="#try-else从句" class="headerlink" title="try/else从句"></a><code>try/else</code>从句</h1><p>我们常常想在没有触发异常的时候执行一些代码。这可以很轻松地通过一个<code>else</code>从句来达到。</p><p>有人也许问了：如果你只是想让一些代码在没有触发异常的情况下执行，为啥你不直接把代码放在<code>try</code>里面呢？<br>回答是，那样的话这段代码中的任意异常都还是会被<code>try</code>捕获，而你并不一定想要那样。</p><p>大多数人并不使用<code>else</code>从句，而且坦率地讲我自己也没有大范围使用。这里是个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    print(<span class="string">'I am sure no exception is going to occur!'</span>)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">'exception'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 这里的代码只会在try语句里没有触发异常时运行,</span></span><br><span class="line">    <span class="comment"># 但是这里的异常将 *不会* 被捕获</span></span><br><span class="line">    print(<span class="string">'This would only run if no exception occurs. And an error here '</span></span><br><span class="line">          <span class="string">'would NOT be caught.'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    print(<span class="string">'This would be printed in every case.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: I am sure no exception is going to occur!</span></span><br><span class="line"><span class="comment"># This would only run if no exception occurs.</span></span><br><span class="line"><span class="comment"># This would be printed in every case.</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># 17. &#96;&#96;&#96;lambda&#96;&#96;&#96;表达式</span><br><span class="line">&#96;lambda&#96;表达式是一行函数。  </span><br><span class="line">它们在其他语言中也被称为匿名函数。如果你不想在程序中对一个函数使用两次，你也许会想用lambda表达式，它们和普通的函数完全一样。</span><br><span class="line"></span><br><span class="line">__原型__</span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">    lambda 参数:操作(参数)</span><br></pre></td></tr></table></figure><p><strong>例子</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">add = <span class="keyword">lambda</span> x, y: x + y</span><br><span class="line"></span><br><span class="line">print(add(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># Output: 8</span></span><br></pre></td></tr></table></figure></p><p>这还有一些lambda表达式的应用案例，可以在一些特殊情况下使用：</p><p><strong>列表排序</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">9</span>, <span class="number">10</span>), (<span class="number">13</span>, <span class="number">-3</span>)]</span><br><span class="line">a.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># Output: [(13, -3), (4, 1), (1, 2), (9, 10)]</span></span><br></pre></td></tr></table></figure></p><p><strong>列表并行排序</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = zip(list1, list2)</span><br><span class="line">data = sorted(data)</span><br><span class="line">list1, list2 = map(<span class="keyword">lambda</span> t: list(t), zip(*data))</span><br></pre></td></tr></table></figure></p><h1 id="18-一行式"><a href="#18-一行式" class="headerlink" title="18. 一行式"></a>18. 一行式</h1><p>本章节,我将向大家展示一些一行式的Python命令，这些程序将对你非常有帮助。</p><p><strong>简易Web Server</strong></p><p>你是否想过通过网络快速共享文件？好消息，Python为你提供了这样的功能。进入到你要共享文件的目录下并在命令行中运行下面的代码：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python 2</span></span><br><span class="line">python -m SimpleHTTPServer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Python 3</span></span><br><span class="line">python -m http.server</span><br></pre></td></tr></table></figure><p><strong>漂亮的打印</strong></p><p>你可以在Python REPL漂亮的打印出列表和字典。这里是相关的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line">my_dict = &#123;<span class="string">'name'</span>: <span class="string">'Yasoob'</span>, <span class="string">'age'</span>: <span class="string">'undefined'</span>, <span class="string">'personality'</span>: <span class="string">'awesome'</span>&#125;</span><br><span class="line">pprint(my_dict)</span><br></pre></td></tr></table></figure><p>这种方法在字典上更为有效。此外，如果你想快速漂亮的从文件打印出json数据，那么你可以这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat file.json | python -m json.tool</span><br></pre></td></tr></table></figure></p><p><strong>脚本性能分析</strong><br>这可能在定位你的脚本中的性能瓶颈时，会非常奏效：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m cProfile my_script.py</span><br></pre></td></tr></table></figure><p>备注：<code>cProfile</code>是一个比<code>profile</code>更快的实现，因为它是用c写的</p><p><strong>CSV转换为json</strong></p><p>在命令行执行这条指令<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">"import csv,json;print json.dumps(list(csv.reader(open('csv_file.csv'))))"</span></span><br></pre></td></tr></table></figure><br>确保更换<code>csv_file.csv</code>为你想要转换的csv文件</p><p><strong>列表辗平</strong></p><p>您可以通过使用<code>itertools</code>包中的<code>itertools.chain.from_iterable</code>轻松快速的辗平一个列表。下面是一个简单的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a_list = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">print(list(itertools.chain.from_iterable(a_list)))</span><br><span class="line"><span class="comment"># Output: [1, 2, 3, 4, 5, 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">print(list(itertools.chain(*a_list)))</span><br><span class="line"><span class="comment"># Output: [1, 2, 3, 4, 5, 6]</span></span><br></pre></td></tr></table></figure></p><p><strong>一行式的构造器</strong></p><p>避免类初始化时大量重复的赋值语句<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, b, c, d, e, f)</span>:</span></span><br><span class="line">        self.__dict__.update(&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> locals().items() <span class="keyword">if</span> k != <span class="string">'self'</span>&#125;)</span><br></pre></td></tr></table></figure><br>更多的一行方法请参考<a href="https://wiki.python.org/moin/Powerful%20Python%20One-Liners" target="_blank" rel="noopener">Python官方文档</a>。</p><h1 id="For-Else"><a href="#For-Else" class="headerlink" title="For - Else"></a>For - Else</h1><p>循环是任何语言的一个必备要素。同样地，<code>for</code>循环就是Python的一个重要组成部分。然而还有一些东西是初学者并不知道的。我们将一个个讨论一下。</p><p>我们先从已经知道的开始。我们知道可以像这样使用<code>for</code>循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'mango'</span>]</span><br><span class="line"><span class="keyword">for</span> fruit <span class="keyword">in</span> fruits:</span><br><span class="line">    print(fruit.capitalize())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: Apple</span></span><br><span class="line"><span class="comment">#         Banana</span></span><br><span class="line"><span class="comment">#         Mango</span></span><br></pre></td></tr></table></figure><p>这是一个<code>for</code>循环非常基础的结构。现在我们继续看看，Python的<code>for</code>循环的一些鲜为人所知的特性。</p><h1 id="else从句"><a href="#else从句" class="headerlink" title="else从句"></a><code>else</code>从句</h1><figure class="highlight plain"><figcaption><span>一旦你掌握了何时何地使用它，它真的会非常有用。我自己对它真是相见恨晚。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">有个常见的构造是跑一个循环，并查找一个元素。如果这个元素被找到了，我们使用&#96;&#96;&#96;break&#96;&#96;&#96;来中断这个循环。有两个场景会让循环停下来。</span><br><span class="line">- 第一个是当一个元素被找到，&#96;&#96;&#96;break&#96;&#96;&#96;被触发。</span><br><span class="line">- 第二个场景是循环结束。  </span><br><span class="line"></span><br><span class="line">现在我们也许想知道其中哪一个，才是导致循环完成的原因。一个方法是先设置一个标记，然后在循环结束时打上标记。另一个是使用&#96;&#96;&#96;else&#96;&#96;&#96;从句。</span><br><span class="line"></span><br><span class="line">这就是&#96;&#96;&#96;for&#x2F;else&#96;&#96;&#96;循环的基本结构：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">for item in container:</span><br><span class="line">    if search_something(item):</span><br><span class="line">        # Found it!</span><br><span class="line">        process(item)</span><br><span class="line">        break</span><br><span class="line">else:</span><br><span class="line">    # Didn&#39;t find anything..</span><br><span class="line">    not_found_in_container()</span><br></pre></td></tr></table></figure><p>考虑下这个简单的案例，它是我从官方文档里拿来的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line">            print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n / x)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p><p>它会找出2到10之间的数字的因子。现在是趣味环节了。我们可以加上一个附加的else语句块，来抓住质数，并且告诉我们：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line">            print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n / x)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># loop fell through without finding a factor</span></span><br><span class="line">        print(n, <span class="string">'is a prime number'</span>)</span><br></pre></td></tr></table></figure><h1 id="使用C扩展"><a href="#使用C扩展" class="headerlink" title="使用C扩展"></a>使用C扩展</h1><p>CPython还为开发者实现了一个有趣的特性，使用Python可以轻松调用C代码</p><p>开发者有三种方法可以在自己的Python代码中来调用C编写的函数-<code>ctypes</code>，<code>SWIG</code>，<code>Python/C API</code>。每种方式也都有各自的利弊。 </p><p>首先，我们要明确为什么要在Python中调用C？</p><p>常见原因如下：</p><ul><li>你要提升代码的运行速度，而且你知道C要比Python快50倍以上</li><li>C语言中有很多传统类库，而且有些正是你想要的，但你又不想用Python去重写它们</li><li>想对从内存到文件接口这样的底层资源进行访问</li><li>不需要理由，就是想这样做</li></ul><h1 id="CTypes"><a href="#CTypes" class="headerlink" title="CTypes"></a>CTypes</h1><p>Python中的<a href="https://docs.python.org/2/library/ctypes.html" target="_blank" rel="noopener">ctypes模块</a>可能是Python调用C方法中最简单的一种。ctypes模块提供了和C语言兼容的数据类型和函数来加载dll文件，因此在调用时不需对源文件做任何的修改。也正是如此奠定了这种方法的简单性。</p><p>示例如下</p><p>实现两数求和的C代码，保存为<code>add.c</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//sample C file to add 2 numbers - int and floats</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span>, <span class="keyword">float</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span> num1, <span class="keyword">float</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>接下来将C文件编译为<code>.so</code>文件(windows下为DLL)。下面操作会生成adder.so文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">For Linux</span></span><br><span class="line"><span class="meta">$</span><span class="bash">  gcc -shared -Wl,-soname,adder -o adder.so -fPIC add.c</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">For Mac</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gcc -shared -Wl,-install_name,adder.so -o adder.so -fPIC add.c</span></span><br></pre></td></tr></table></figure></p><p>现在在你的Python代码中来调用它<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">#load the shared object file</span></span><br><span class="line">adder = CDLL(<span class="string">'./adder.so'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find sum of integers</span></span><br><span class="line">res_int = adder.add_int(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of 4 and 5 = "</span> + str(res_int)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find sum of floats</span></span><br><span class="line">a = c_float(<span class="number">5.5</span>)</span><br><span class="line">b = c_float(<span class="number">4.1</span>)</span><br><span class="line"></span><br><span class="line">add_float = adder.add_float</span><br><span class="line">add_float.restype = c_float</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of 5.5 and 4.1 = "</span>, str(add_float(a, b))</span><br></pre></td></tr></table></figure></p><p>输出如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sum of 4 and 5 &#x3D; 9</span><br><span class="line">Sum of 5.5 and 4.1 &#x3D;  9.60000038147</span><br></pre></td></tr></table></figure></p><p>在这个例子中，C文件是自解释的，它包含两个函数，分别实现了整形求和和浮点型求和。</p><p>在Python文件中，一开始先导入ctypes模块，然后使用CDLL函数来加载我们创建的库文件。这样我们就可以通过变量<code>adder</code>来使用C类库中的函数了。当<code>adder.add_int()</code>被调用时，内部将发起一个对C函数<code>add_int</code>的调用。ctypes接口允许我们在调用C函数时使用原生Python中默认的字符串型和整型。</p><p>而对于其他类似布尔型和浮点型这样的类型，必须要使用正确的ctype类型才可以。如向<code>adder.add_float()</code>函数传参时, 我们要先将Python中的十进制值转化为c_float类型，然后才能传送给C函数。这种方法虽然简单，清晰，但是却很受限。例如，并不能在C中对对象进行操作。</p><h1 id="SWIG"><a href="#SWIG" class="headerlink" title="SWIG"></a>SWIG</h1><p>SWIG是Simplified Wrapper and Interface Generator的缩写。是Python中调用C代码的另一种方法。在这个方法中，开发人员必须编写一个额外的接口文件来作为SWIG(终端工具)的入口。</p><p>Python开发者一般不会采用这种方法，因为大多数情况它会带来不必要的复杂。而当你有一个C/C++代码库需要被多种语言调用时，这将是个非常不错的选择。</p><p>示例如下(来自<a href="http://www.swig.org/tutorial.html" target="_blank" rel="noopener">SWIG官网</a>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#96;&#96;&#96;C</span><br><span class="line">#include &lt;time.h&gt;</span><br><span class="line">double My_variable &#x3D; 3.0;</span><br><span class="line"></span><br><span class="line">int fact(int n) &#123;</span><br><span class="line">    if (n &lt;&#x3D; 1) return 1;</span><br><span class="line">    else return n*fact(n-1);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int my_mod(int x, int y) &#123;</span><br><span class="line">    return (x%y);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">char *get_time()</span><br><span class="line">&#123;</span><br><span class="line">    time_t ltime;</span><br><span class="line">    time(&amp;ltime);</span><br><span class="line">    return ctime(&amp;ltime);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译它<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">unix % swig -python example.i</span><br><span class="line">unix % gcc -c example.c example_wrap.c \</span><br><span class="line">    -I/usr/local/include/python2.1</span><br><span class="line">unix % ld -shared example.o example_wrap.o -o _example.so</span><br></pre></td></tr></table></figure></p><p>最后，Python的输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> example</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.fact(<span class="number">5</span>)</span><br><span class="line"><span class="number">120</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.my_mod(<span class="number">7</span>,<span class="number">3</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.get_time()</span><br><span class="line"><span class="string">'Sun Feb 11 23:01:07 1996'</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>我们可以看到，使用SWIG确实达到了同样的效果，虽然下了更多的工夫，但如果你的目标是多语言还是很值得的。</p><h1 id="Python-C-API"><a href="#Python-C-API" class="headerlink" title="Python/C API"></a>Python/C API</h1><p><a href="https://docs.python.org/2/c-api/" target="_blank" rel="noopener">Python/C API</a>可能是被最广泛使用的方法。它不仅简单，而且可以在C代码中操作你的Python对象。</p><p>这种方法需要以特定的方式来编写C代码以供Python去调用它。所有的Python对象都被表示为一种叫做PyObject的结构体，并且<code>Python.h</code>头文件中提供了各种操作它的函数。例如，如果PyObject表示为PyListType(列表类型)时，那么我们便可以使用<code>PyList_Size()</code>函数来获取该结构的长度，类似Python中的<code>len(list)</code>函数。大部分对Python原生对象的基础函数和操作在<code>Python.h</code>头文件中都能找到。</p><p>示例</p><p>编写一个C扩展，添加所有元素到一个Python列表(所有元素都是数字)</p><p>来看一下我们要实现的效果，这里演示了用Python调用C扩展的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Though it looks like an ordinary python import, the addList module is implemented in C</span></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l))</span><br></pre></td></tr></table></figure></p><p>上面的代码和普通的Python文件并没有什么分别，导入并使用了另一个叫做<code>addList</code>的Python模块。唯一差别就是这个模块并不是用Python编写的，而是C。</p><p>接下来我们看看如何用C编写<code>addList</code>模块，这可能看起来有点让人难以接受，但是一旦你了解了这之中的各种组成，你就可以一往无前了。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Python.h has all the required function definitions to manipulate the Python objects</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Python.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//This is the function that is called from your python code</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> PyObject* <span class="title">addList_add</span><span class="params">(PyObject* self, PyObject* args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    PyObject * listObj;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//The input arguments come as a tuple, we parse the args to get the various variables</span></span><br><span class="line">    <span class="comment">//In this case it's only one list variable, which will now be referenced by listObj</span></span><br><span class="line">    <span class="keyword">if</span> (! PyArg_ParseTuple( args, <span class="string">"O"</span>, &amp;listObj ))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//length of the list</span></span><br><span class="line">    <span class="keyword">long</span> length = PyList_Size(listObj);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//iterate over all the elements</span></span><br><span class="line">    <span class="keyword">int</span> i, sum =<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">        <span class="comment">//get an element out of the list - the element is also a python objects</span></span><br><span class="line">        PyObject* temp = PyList_GetItem(listObj, i);</span><br><span class="line">        <span class="comment">//we know that object represents an integer - so convert it into C long</span></span><br><span class="line">        <span class="keyword">long</span> elem = PyInt_AsLong(temp);</span><br><span class="line">        sum += elem;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//value returned back to python code - another python object</span></span><br><span class="line">    <span class="comment">//build value here converts the C long to a python integer</span></span><br><span class="line">    <span class="keyword">return</span> Py_BuildValue(<span class="string">"i"</span>, sum);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//This is the docstring that corresponds to our 'add' function.</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> addList_docs[] =</span><br><span class="line"><span class="string">"add(  ): add all elements of the list\n"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* This table contains the relavent info mapping -</span></span><br><span class="line"><span class="comment">   &lt;function-name in python module&gt;, &lt;actual-function&gt;,</span></span><br><span class="line"><span class="comment">   &lt;type-of-args the function expects&gt;, &lt;docstring associated with the function&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> PyMethodDef addList_funcs[] = &#123;</span><br><span class="line">    &#123;<span class="string">"add"</span>, (PyCFunction)addList_add, METH_VARARGS, addList_docs&#125;,</span><br><span class="line">    &#123;<span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>&#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   addList is the module name, and this is the initialization block of the module.</span></span><br><span class="line"><span class="comment">   &lt;desired module name&gt;, &lt;the-info-table&gt;, &lt;module's-docstring&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">initaddList</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    Py_InitModule3(<span class="string">"addList"</span>, addList_funcs,</span><br><span class="line">            <span class="string">"Add all ze lists"</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>逐步解释</p><ul><li><code>Python.h</code>头文件中包含了所有需要的类型(Python对象类型的表示)和函数定义(对Python对象的操作)</li><li>接下来我们编写将要在Python调用的函数, 函数传统的命名方式由{模块名}_{函数名}组成，所以我们将其命名为<code>addList_add</code>   </li><li>然后填写想在模块内实现函数的相关信息表，每行一个函数，以空行作为结束</li><li>最后的模块初始化块签名为<code>PyMODINIT_FUNC init{模块名}</code>。</li></ul><p>函数<code>addList_add</code>接受的参数类型为PyObject类型结构(同时也表示为元组类型，因为Python中万物皆为对象，所以我们先用PyObject来定义)。传入的参数则通过<code>PyArg_ParseTuple()</code>来解析。第一个参数是被解析的参数变量。第二个参数是一个字符串，告诉我们如何去解析元组中每一个元素。字符串的第n个字母正是代表着元组中第n个参数的类型。例如，”i”代表整形，”s”代表字符串类型, “O”则代表一个Python对象。接下来的参数都是你想要通过<code>PyArg_ParseTuple()</code>函数解析并保存的元素。这样参数的数量和模块中函数期待得到的参数数量就可以保持一致，并保证了位置的完整性。例如，我们想传入一个字符串，一个整数和一个Python列表，可以这样去写<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">char</span> *s;</span><br><span class="line">PyObject* <span class="built_in">list</span>;</span><br><span class="line">PyArg_ParseTuple(args, <span class="string">"siO"</span>, &amp;n, &amp;s, &amp;<span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p><p>在这种情况下，我们只需要提取一个列表对象，并将它存储在<code>listObj</code>变量中。然后用列表对象中的<code>PyList_Size()</code>函数来获取它的长度。就像Python中调用<code>len(list)</code>。</p><p>现在我们通过循环列表，使用<code>PyList_GetItem(list, index)</code>函数来获取每个元素。这将返回一个<code>PyObject*</code>对象。既然Python对象也能表示<code>PyIntType</code>，我们只要使用<code>PyInt_AsLong(PyObj *)</code>函数便可获得我们所需要的值。我们对每个元素都这样处理，最后再得到它们的总和。</p><p>总和将被转化为一个Python对象并通过<code>Py_BuildValue()</code>返回给Python代码，这里的i表示我们要返回一个Python整形对象。</p><p>现在我们已经编写完C模块了。将下列代码保存为<code>setup.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#build the modules</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">'addList'</span>, version=<span class="string">'1.0'</span>,  \</span><br><span class="line">      ext_modules=[Extension(<span class="string">'addList'</span>, [<span class="string">'adder.c'</span>])])</span><br></pre></td></tr></table></figure></p><p>并且运行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p><p>现在应该已经将我们的C文件编译安装到我们的Python模块中了。</p><p>在一番辛苦后，让我们来验证下我们的模块是否有效<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#module that talks to the C code</span></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l))</span><br></pre></td></tr></table></figure></p><p>输出结果如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sum of List - [1, 2, 3, 4, 5] &#x3D; 15</span><br></pre></td></tr></table></figure></p><p>如你所见，我们已经使用Python.h API成功开发出了我们第一个Python C扩展。这种方法看似复杂，但你一旦习惯，它将变的非常有效。</p><p>Python调用C代码的另一种方式便是使用<a href="http://cython.org/" target="_blank" rel="noopener">Cython</a>让Python编译的更快。但是Cython和传统的Python比起来可以将它理解为另一种语言，所以我们就不在这里过多描述了。</p><h1 id="open函数"><a href="#open函数" class="headerlink" title="open函数"></a><code>open</code>函数</h1><p><a href="http://docs.python.org/dev/library/functions.html#open" target="_blank" rel="noopener">open</a> 函数可以打开一个文件。超级简单吧？大多数时候，我们看到它这样被使用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = open(<span class="string">'photo.jpg'</span>, <span class="string">'r+'</span>)</span><br><span class="line">jpgdata = f.read()</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure></p><p>我现在写这篇文章的原因，是大部分时间我看到<code>open</code>被这样使用。有<strong>三个</strong>错误存在于上面的代码中。你能把它们全指出来吗？如不能，请读下去。在这篇文章的结尾，你会知道上面的代码错在哪里，而且，更重要的是，你能在自己的代码里避免这些错误。现在我们从基础开始：</p><p><code>open</code>的返回值是一个文件句柄，从操作系统托付给你的Python程序。一旦你处理完文件，你会想要归还这个文件句柄，只有这样你的程序不会超出一次能打开的文件句柄的数量上限。</p><p>显式地调用<code>close</code>关闭了这个文件句柄，但前提是只有在read成功的情况下。如果有任意异常正好在<code>f = open(...)</code>之后产生，<code>f.close()</code>将不会被调用（取决于Python解释器的做法，文件句柄可能还是会被归还，但那是另外的话题了）。为了确保不管异常是否触发，文件都能关闭，我们将其包裹成一个<code>with</code>语句:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'photo.jpg'</span>, <span class="string">'r+'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    jpgdata = f.read()</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><figcaption><span>打开模式)决定了这个文件如何被打开。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 如果你想读取文件，传入&#96;&#96;&#96;r</span><br></pre></td></tr></table></figure><ul><li>如果你想读取并写入文件，传入<code>r+</code></li><li>如果你想覆盖写入文件，传入<code>w</code></li><li>如果你想在文件末尾附加内容，传入<code>a</code></li></ul><p>虽然有若干个其他的有效的<code>mode</code>字符串，但有可能你将永远不会使用它们。<code>mode</code>很重要，不仅因为它改变了行为，而且它可能导致权限错误。举个例子，我们要是在一个写保护的目录里打开一个jpg文件， <code>open(.., &#39;r+&#39;)</code>就失败了。<code>mode</code>可能包含一个扩展字符；让我们还可以以二进制方式打开文件(你将得到字节串)或者文本模式(字符串)</p><p>一般来说，如果文件格式是由人写的，那么它更可能是文本模式。jpg图像文件一般不是人写的（而且其实不是人直接可读的），因此你应该以二进制模式来打开它们，方法是在<code>mode</code>字符串后加一个<code>b</code>(你可以看看开头的例子里，正确的方式应该是<code>rb</code>)。<br>如果你以文本模式打开一些东西（比如，加一个<code>t</code>,或者就用<code>r/r+/w/a</code>），你还必须知道要使用哪种编码。对于计算机来说，所有的文件都是字节，而不是字符。</p><p>可惜，在Pyhon 2.x版本里，<code>open</code>不支持显示地指定编码。然而，<a href="http://docs.python.org/2/library/io.html#io.open" target="_blank" rel="noopener">io.open</a>函数在Python 2.x中和3.x(其中它是<code>open</code>的别名)中都有提供，它能做正确的事。你可以传入<code>encoding</code>这个关键字参数来传入编码。<br>如果你不传入任意编码，一个系统 - 以及Python -指定的默认选项将被选中。你也许被诱惑去依赖这个默认选项，但这个默认选项经常是错误的，或者默认编码实际上不能表达文件里的所有字符（这将经常发生在Python 2.x和/或Windows）。<br>所以去挑选一个编码吧。<code>utf-8</code>是一个非常好的编码。当你写入一个文件，你可以选一个你喜欢的编码（或者最终读你文件的程序所喜欢的编码）。</p><p>那你怎么找出正在读的文件是用哪种编码写的呢？好吧，不幸的是，并没有一个十分简单的方式来检测编码。在不同的编码中，同样的字节可以表示不同，但同样有效的字符。因此，你必须依赖一个元数据（比如，在HTTP头信息里）来找出编码。越来越多的是，文件格式将编码定义成<code>UTF-8</code>。</p><p>有了这些基础知识，我们来写一个程序，读取一个文件，检测它是否是JPG（提示：这些文件头部以字节<code>FF D8</code>开始），把对输入文件的描述写入一个文本文件。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'photo.jpg'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> inf:</span><br><span class="line">    jpgdata = inf.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> jpgdata.startswith(<span class="string">b'\xff\xd8'</span>):</span><br><span class="line">    text = <span class="string">u'This is a JPEG file (%d bytes long)\n'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    text = <span class="string">u'This is a random file (%d bytes long)\n'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> io.open(<span class="string">'summary.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> outf:</span><br><span class="line">    outf.write(text % len(jpgdata))</span><br></pre></td></tr></table></figure><br>我敢肯定，现在你会正确地使用<code>open</code>啦！</p><h1 id="22-目标Python2-3"><a href="#22-目标Python2-3" class="headerlink" title="22. 目标Python2+3"></a>22. 目标Python2+3</h1><p>很多时候你可能希望你开发的程序能够同时兼容Python2+和Python3+。</p><p>试想你有一个非常出名的Python模块被很多开发者使用着，但并不是所有人都只使用Python2或者Python3。这时候你有两个办法。第一个办法是开发两个模块，针对Python2一个，针对Python3一个。还有一个办法就是调整你现在的代码使其同时兼容Python2和Python3。</p><p>本节中，我将介绍一些技巧，让你的脚本同时兼容Python2和Python3。</p><p><strong>Future模块导入</strong></p><p>第一种也是最重要的方法，就是导入<code>__future__</code>模块。它可以帮你在Python2中导入Python3的功能。这有一组例子：</p><p>上下文管理器是Python2.6+引入的新特性，如果你想在Python2.5中使用它可以这样做：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> with_statement</span><br></pre></td></tr></table></figure></p><p>在Python3中<code>print</code>已经变为一个函数。如果你想在Python2中使用它可以通过<code>__future__</code>导入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line">print(<span class="keyword">print</span>)</span><br><span class="line"><span class="comment"># Output: &lt;built-in function print&gt;</span></span><br></pre></td></tr></table></figure><p><strong>模块重命名</strong></p><p>首先，告诉我你是如何在你的脚本中导入模块的。大多时候我们会这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> foo </span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="keyword">from</span> foo <span class="keyword">import</span> bar</span><br></pre></td></tr></table></figure><p>你知道么，其实你也可以这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> foo <span class="keyword">as</span> foo</span><br></pre></td></tr></table></figure><p>这样做可以起到和上面代码同样的功能，但最重要的是它能让你的脚本同时兼容Python2和Python3。现在我们来看下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> urllib.request <span class="keyword">as</span> urllib_request  <span class="comment"># for Python 3</span></span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">import</span> urllib2 <span class="keyword">as</span> urllib_request  <span class="comment"># for Python 2</span></span><br></pre></td></tr></table></figure><p>让我来稍微解释一下上面的代码。<br>我们将模块导入代码包装在<code>try/except</code>语句中。我们是这样做是因为在Python 2中并没有<code>urllib.request</code>模块。这将引起一个<code>ImportError</code>异常。而在Python2中<code>urllib.request</code>的功能则是由<code>urllib2</code>提供的。所以,当我们试图在Python2中导入<code>urllib.request</code>模块的时候，一旦我们捕获到<code>ImportError</code>我们将通过导入<code>urllib2</code>模块来代替它。</p><p>最后，你要了解<code>as</code>关键字的作用。它将导入的模块映射到<code>urllib.request</code>，所以我们通过<code>urllib_request</code>这个别名就可以使用<code>urllib2</code>中的所有类和方法了。</p><p><strong>过期的Python2内置功能</strong></p><p>另一个需要了解的事情就是Python2中有12个内置功能在Python3中已经被移除了。要确保在Python2代码中不要出现这些功能来保证对Python3的兼容。这有一个强制让你放弃12内置功能的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> future.builtins.disabled <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><p>现在，只要你尝试在Python3中使用这些被遗弃的模块时，就会抛出一个<code>NameError</code>异常如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> future.builtins.disabled <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">apply()</span><br><span class="line"><span class="comment"># Output: NameError: obsolete Python 2 builtin apply is disabled</span></span><br></pre></td></tr></table></figure><p><strong>标准库向下兼容的外部支持</strong></p><p>有一些包在非官方的支持下为Python2提供了Python3的功能。例如，我们有：</p><ul><li>enum <code>pip install enum34</code></li><li>singledispatch <code>pip install singledispatch</code></li><li>pathlib <code>pip install pathlib</code></li></ul><p>想更多了解，在Python文档中有一个<a href="https://docs.python.org/3/howto/pyporting.html" target="_blank" rel="noopener">全面的指南</a>可以帮助你让你的代码同时兼容Python2和Python3。</p><h1 id="23-协程"><a href="#23-协程" class="headerlink" title="23. 协程"></a>23. 协程</h1><p>Python中的协程和生成器很相似但又稍有不同。主要区别在于：</p><ul><li>生成器是数据的生产者</li><li>协程则是数据的消费者</li></ul><p>首先我们先来回顾下生成器的创建过程。我们可以这样去创建一个生成器:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">()</span>:</span></span><br><span class="line">    a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line">        a, b = b, a+b</span><br></pre></td></tr></table></figure><p>然后我们经常在<code>for</code>循环中这样使用它:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> fib():</span><br><span class="line">    <span class="keyword">print</span> i</span><br></pre></td></tr></table></figure><p>这样做不仅快而且不会给内存带来压力，因为我们所需要的值都是动态生成的而不是将他们存储在一个列表中。更概括的说如果现在我们在上面的例子中使用<code>yield</code>便可获得了一个协程。协程会消费掉发送给它的值。Python实现的<code>grep</code>就是个很好的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grep</span><span class="params">(pattern)</span>:</span></span><br><span class="line">    print(<span class="string">"Searching for"</span>, pattern)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = (<span class="keyword">yield</span>)</span><br><span class="line">        <span class="keyword">if</span> pattern <span class="keyword">in</span> line:</span><br><span class="line">            print(line)</span><br></pre></td></tr></table></figure><p>等等！<code>yield</code>返回了什么？啊哈，我们已经把它变成了一个协程。它将不再包含任何初始值，相反要从外部传值给它。我们可以通过<code>send()</code>方法向它传值。这有个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">search = grep(<span class="string">'coroutine'</span>)</span><br><span class="line">next(search)</span><br><span class="line"><span class="comment">#output: Searching for coroutine</span></span><br><span class="line">search.send(<span class="string">"I love you"</span>)</span><br><span class="line">search.send(<span class="string">"Don't you love me?"</span>)</span><br><span class="line">search.send(<span class="string">"I love coroutine instead!"</span>)</span><br><span class="line"><span class="comment">#output: I love coroutine instead!</span></span><br></pre></td></tr></table></figure><p>发送的值会被<code>yield</code>接收。我们为什么要运行<code>next()</code>方法呢？这样做正是为了启动一个协程。就像协程中包含的生成器并不是立刻执行，而是通过<code>next()</code>方法来响应<code>send()</code>方法。因此，你必须通过<code>next()</code>方法来执行<code>yield</code>表达式。</p><p>我们可以通过调用<code>close()</code>方法来关闭一个协程。像这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">search = grep(<span class="string">'coroutine'</span>)</span><br><span class="line">search.close()</span><br></pre></td></tr></table></figure><p>更多协程相关知识的学习大家可以参考David Beazley的这份<a href="http://www.dabeaz.com/coroutines/Coroutines.pdf" target="_blank" rel="noopener">精彩演讲</a>。</p><h1 id="函数缓存-Function-caching"><a href="#函数缓存-Function-caching" class="headerlink" title="函数缓存 (Function caching)"></a>函数缓存 (Function caching)</h1><p>函数缓存允许我们将一个函数对于给定参数的返回值缓存起来。<br>当一个I/O密集的函数被频繁使用相同的参数调用的时候，函数缓存可以节约时间。<br>在Python 3.2版本以前我们只有写一个自定义的实现。在Python 3.2以后版本，有个<code>lru_cache</code>的装饰器，允许我们将一个函数的返回值快速地缓存或取消缓存。</p><p>我们来看看，Python 3.2前后的版本分别如何使用它。</p><h1 id="Python-3-2及以后版本"><a href="#Python-3-2及以后版本" class="headerlink" title="Python 3.2及以后版本"></a>Python 3.2及以后版本</h1><p>我们来实现一个斐波那契计算器，并使用<code>lru_cache</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache(maxsize=32)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fib(n<span class="number">-1</span>) + fib(n<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print([fib(n) <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line"><span class="comment"># Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</span></span><br></pre></td></tr></table></figure><p>那个<code>maxsize</code>参数是告诉<code>lru_cache</code>，最多缓存最近多少个返回值。</p><p>我们也可以轻松地对返回值清空缓存，通过这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fib.cache_clear()</span><br></pre></td></tr></table></figure><h1 id="Python-2系列版本"><a href="#Python-2系列版本" class="headerlink" title="Python 2系列版本"></a>Python 2系列版本</h1><p>你可以创建任意种类的缓存机制，有若干种方式来达到相同的效果，这完全取决于你的需要。<br>这里是一个一般的缓存：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">memoize</span><span class="params">(function)</span>:</span></span><br><span class="line">    memo = &#123;&#125;</span><br><span class="line"><span class="meta">    @wraps(function)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> args <span class="keyword">in</span> memo:</span><br><span class="line">            <span class="keyword">return</span> memo[args]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rv = function(*args)</span><br><span class="line">            memo[args] = rv</span><br><span class="line">            <span class="keyword">return</span> rv</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@memoize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>: <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fibonacci(n - <span class="number">1</span>) + fibonacci(n - <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fibonacci(<span class="number">25</span>)</span><br></pre></td></tr></table></figure><br>这里有一篇<a href="https://www.caktusgroup.com/blog/2015/06/08/testing-client-side-applications-django-post-mortem/" target="_blank" rel="noopener">Caktus Group的不错的文章</a>，在其中他们发现一个Django框架的由lru_cache导致的bug。读起来很有意思。一定要打开去看一下。</p><h1 id="上下文管理器-Context-managers"><a href="#上下文管理器-Context-managers" class="headerlink" title="上下文管理器(Context managers)"></a>上下文管理器(Context managers)</h1><p>上下文管理器允许你在有需要的时候，精确地分配和释放资源。  </p><p>使用上下文管理器最广泛的案例就是<code>with</code>语句了。<br>想象下你有两个需要结对执行的相关操作，然后还要在它们中间放置一段代码。<br>上下文管理器就是专门让你做这种事情的。举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'some_file'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.write(<span class="string">'Hola!'</span>)</span><br></pre></td></tr></table></figure><p>上面这段代码打开了一个文件，往里面写入了一些数据，然后关闭该文件。如果在往文件写数据时发生异常，它也会尝试去关闭文件。上面那段代码与这一段是等价的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file = open(<span class="string">'some_file'</span>, <span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file.write(<span class="string">'Hola!'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    file.close()</span><br></pre></td></tr></table></figure><p>当与第一个例子对比时，我们可以看到，通过使用<code>with</code>，许多样板代码(boilerplate code)被消掉了。 这就是<code>with</code>语句的主要优势，它确保我们的文件会被关闭，而不用关注嵌套代码如何退出。</p><p>上下文管理器的一个常见用例，是资源的加锁和解锁，以及关闭已打开的文件（就像我已经展示给你看的）。</p><p>让我们看看如何来实现我们自己的上下文管理器。这会让我们更完全地理解在这些场景背后都发生着什么。</p><h1 id="基于类的实现"><a href="#基于类的实现" class="headerlink" title="基于类的实现"></a>基于类的实现</h1><p>一个上下文管理器的类，最起码要定义<code>__enter__</code>和<code>__exit__</code>方法。<br>让我们来构造我们自己的开启文件的上下文管理器，并学习下基础知识。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">File</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_name, method)</span>:</span></span><br><span class="line">        self.file_obj = open(file_name, method)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.file_obj</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, type, value, traceback)</span>:</span></span><br><span class="line">        self.file_obj.close()</span><br></pre></td></tr></table></figure><p>通过定义<code>__enter__</code>和<code>__exit__</code>方法，我们可以在<code>with</code>语句里使用它。我们来试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> File(<span class="string">'demo.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.write(<span class="string">'Hola!'</span>)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">我们的```__exit__```函数接受三个参数。这些参数对于每个上下文管理器类中的```__exit__```方法都是必须的。我们来谈谈在底层都发生了什么。</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> ```<span class="keyword">with</span>```语句先暂存了```File```类的```__exit__```方法</span><br><span class="line"><span class="number">2.</span> 然后它调用```File```类的```__enter__```方法</span><br><span class="line"><span class="number">3.</span> ```__enter__```方法打开文件并返回给```<span class="keyword">with</span>```语句</span><br><span class="line"><span class="number">4.</span> 打开的文件句柄被传递给```opened_file```参数</span><br><span class="line"><span class="number">5.</span> 我们使用```.write()```来写文件</span><br><span class="line"><span class="number">6.</span> ```<span class="keyword">with</span>```语句调用之前暂存的```__exit__```方法</span><br><span class="line"><span class="number">7.</span> ```__exit__```方法关闭了文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理异常</span></span><br><span class="line"></span><br><span class="line">我们还没有谈到```__exit__```方法的这三个参数：```type```, ```value```和```traceback```。  </span><br><span class="line">在第<span class="number">4</span>步和第<span class="number">6</span>步之间，如果发生异常，Python会将异常的```type```,```value```和```traceback```传递给```__exit__```方法。  </span><br><span class="line">它让```__exit__```方法来决定如何关闭文件以及是否需要其他步骤。在我们的案例中，我们并没有注意它们。</span><br><span class="line"></span><br><span class="line">那如果我们的文件对象抛出一个异常呢？万一我们尝试访问文件对象的一个不支持的方法。举个例子：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">with</span> File(<span class="string">'demo.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.undefined_function(<span class="string">'Hola!'</span>)</span><br></pre></td></tr></table></figure><p>我们来列一下，当异常发生时，<code>with</code>语句会采取哪些步骤。</p><ol><li>它把异常的<code>type</code>,<code>value</code>和<code>traceback</code>传递给<code>__exit__</code>方法</li><li>它让<code>__exit__</code>方法来处理异常</li><li>如果<code>__exit__</code>返回的是True，那么这个异常就被优雅地处理了。</li><li>如果<code>__exit__</code>返回的是True以外的任何东西，那么这个异常将被<code>with</code>语句抛出。</li></ol><p>在我们的案例中，<code>__exit__</code>方法返回的是<code>None</code>(如果没有<code>return</code>语句那么方法会返回<code>None</code>)。因此，<code>with</code>语句抛出了那个异常。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">2</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AttributeError: <span class="string">'file'</span> object has no attribute <span class="string">'undefined_function'</span></span><br></pre></td></tr></table></figure><p>我们尝试下在<code>__exit__</code>方法中处理异常：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">File</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_name, method)</span>:</span></span><br><span class="line">        self.file_obj = open(file_name, method)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.file_obj</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, type, value, traceback)</span>:</span></span><br><span class="line">        print(<span class="string">"Exception has been handled"</span>)</span><br><span class="line">        self.file_obj.close()</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> File(<span class="string">'demo.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.undefined_function()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: Exception has been handled</span></span><br></pre></td></tr></table></figure></p><p>我们的<code>__exit__</code>方法返回了<code>True</code>,因此没有异常会被<code>with</code>语句抛出。</p><p>这还不是实现上下文管理器的唯一方式。还有一种方式，我们会在下一节中一起看看。</p><h1 id="基于生成器的实现"><a href="#基于生成器的实现" class="headerlink" title="基于生成器的实现"></a>基于生成器的实现</h1><p>我们还可以用装饰器(decorators)和生成器(generators)来实现上下文管理器。<br>Python有个<code>contextlib</code>模块专门用于这个目的。我们可以使用一个生成器函数来实现一个上下文管理器，而不是使用一个类。<br>让我们看看一个基本的，没用的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</span><br><span class="line"></span><br><span class="line"><span class="meta">@contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_file</span><span class="params">(name)</span>:</span></span><br><span class="line">    f = open(name, <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">yield</span> f</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure><p>OK啦！这个实现方式看起来更加直观和简单。然而，这个方法需要关于生成器、<code>yield</code>和装饰器的一些知识。在这个例子中我们还没有捕捉可能产生的任何异常。它的工作方式和之前的方法大致相同。</p><p>让我们小小地剖析下这个方法。</p><ol><li>Python解释器遇到了<code>yield</code>关键字。因为这个缘故它创建了一个生成器而不是一个普通的函数。</li><li>因为这个装饰器，<code>contextmanager</code>会被调用并传入函数名（<code>open_file</code>）作为参数。</li><li><code>contextmanager</code>函数返回一个以<code>GeneratorContextManager</code>对象封装过的生成器。</li><li>这个<code>GeneratorContextManager</code>被赋值给<code>open_file</code>函数，我们实际上是在调用<code>GeneratorContextManager</code>对象。</li></ol><p>那现在我们既然知道了所有这些，我们可以用这个新生成的上下文管理器了，像这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open_file(<span class="string">'some_file'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'hola!'</span>)</span><br></pre></td></tr></table></figure></p><h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><h2 id="本书推送贴的留言和讨论"><a href="#本书推送贴的留言和讨论" class="headerlink" title="本书推送贴的留言和讨论"></a>本书推送贴的留言和讨论</h2><ul><li>v2ex: <a href="http://www.v2ex.com/t/267557" target="_blank" rel="noopener">http://www.v2ex.com/t/267557</a></li><li>微博长文: <a href="http://weibo.com/1054764633/DoN6Z5Haq?type=repost" target="_blank" rel="noopener">http://weibo.com/1054764633/DoN6Z5Haq?type=repost</a></li></ul><h2 id="v2ex网友florije推荐"><a href="#v2ex网友florije推荐" class="headerlink" title="v2ex网友florije推荐"></a>v2ex网友florije推荐</h2><ul><li>另外一本同名IntermediatePython的更新的书  <a href="https://leanpub.com/intermediatepython" target="_blank" rel="noopener">https://leanpub.com/intermediatepython</a></li></ul><h2 id="v2ex网友xiaket推荐"><a href="#v2ex网友xiaket推荐" class="headerlink" title="v2ex网友xiaket推荐"></a>v2ex网友xiaket推荐</h2><ul><li>对于Python提高类的书，推荐Fluent Python 或 Pro Python</li></ul><h2 id="v2ex网友shishen10-推荐"><a href="#v2ex网友shishen10-推荐" class="headerlink" title="v2ex网友shishen10 推荐"></a>v2ex网友shishen10 推荐</h2><ul><li>老齐的教程 <a href="https://github.com/qiwsir/StarterLearningPython" target="_blank" rel="noopener">https://github.com/qiwsir/StarterLearningPython</a></li><li>老齐还整理了很多精华 <a href="https://github.com/qiwsir/ITArticles" target="_blank" rel="noopener">https://github.com/qiwsir/ITArticles</a></li></ul><h2 id="v2ex网友xiaowangge推荐"><a href="#v2ex网友xiaowangge推荐" class="headerlink" title="v2ex网友xiaowangge推荐"></a>v2ex网友xiaowangge推荐</h2><p><a href="https://github.com/Yixiaohan" target="_blank" rel="noopener">Yixiaohan</a>整理了一个不错的推荐：<a href="https://github.com/Yixiaohan/codeparkshare" target="_blank" rel="noopener">Python初学者（零基础学习Python、Python入门）书籍、视频、资料、社区推荐</a>大家可以前去Fork。</p><h2 id="v2ex推荐学习书目"><a href="#v2ex推荐学习书目" class="headerlink" title="v2ex推荐学习书目"></a>v2ex推荐学习书目</h2><ul><li><a href="https://flyouting.gitbooks.io/learn-python-the-hard-way-cn/content/" target="_blank" rel="noopener">Learn Python the Hard Way</a></li><li><a href="https://www.gitbook.com/book/yulongjun/learning-python-in-chinese/details" target="_blank" rel="noopener">Python 学习手册-第五版中文版</a> </li><li><a href="http://python3-cookbook.readthedocs.org/zh_CN/latest/" target="_blank" rel="noopener">Python Cookbook</a></li><li><a href="https://book.douban.com/subject/4866934/" target="_blank" rel="noopener">Python 基础教程</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考&lt;a href=&quot;https://eastlakeside.gitbook.io/interpy-zh/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;此处&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Python进阶&quot;&gt;&lt;a href=&quot;#Python进阶&quot; class=&quot;headerlink&quot; title=&quot;Python进阶 &quot;&gt;&lt;/a&gt;Python进阶 &lt;/h1&gt;&lt;p&gt;《Python进阶》是《Intermediate Python》的中文译本, 谨以此献给进击的 Python 和 Python 程序员们!&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="python书籍" scheme="http://yoursite.com/tags/python%E4%B9%A6%E7%B1%8D/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(11)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/</id>
    <published>2020-07-27T03:20:38.000Z</published>
    <updated>2020-07-27T11:03:00.443Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了一种机器学习的通用框架—集成学习方法，首先从准确性和差异性两个重要概念引出集成学习“<strong>好而不同</strong>”的四字真言，接着介绍了现阶段主流的三种集成学习方法：AdaBoost、Bagging及Random Forest，AdaBoost采用最小化指数损失函数迭代式更新样本分布权重和计算基学习器权重，Bagging通过自助采样引入样本扰动增加了基学习器之间的差异性，随机森林则进一步引入了属性扰动，最后简单概述了集成模型中的三类结合策略：平均法、投票法及学习法，其中Stacking是学习法的典型代表。本篇将讨论无监督学习中应用最为广泛的学习算法—聚类。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="10、聚类算法"><a href="#10、聚类算法" class="headerlink" title="10、聚类算法"></a><strong>10、聚类算法</strong></h1><p>聚类是一种经典的<strong>无监督学习</strong>方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p><p>聚类直观上来说是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。那首先的问题是如何来<strong>度量相似性</strong>（similarity measure）呢？这便是<strong>距离度量</strong>，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是<strong>性能度量</strong>，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p><h2 id="10-1-距离度量"><a href="#10-1-距离度量" class="headerlink" title="10.1 距离度量"></a><strong>10.1 距离度量</strong></h2><p>谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed4c0390.png" alt="1.png"></p><p>最常用的距离度量方法是<strong>“闵可夫斯基距离”（Minkowski distance)</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed49e31f.png" alt="2.png"></p><p>当p=1时，闵可夫斯基距离即<strong>曼哈顿距离（Manhattan distance）</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed49c31f.png" alt="3.png"></p><p>当p=2时，闵可夫斯基距离即<strong>欧氏距离（Euclidean distance）</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed497613.png" alt="4.png"></p><p>我们知道属性分为两种：<strong>连续属性</strong>和<strong>离散属性</strong>（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p><blockquote><p>若属性值之间<strong>存在序关系</strong>，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。<br>若属性值之间<strong>不存在序关系</strong>，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。</p></blockquote><p>在进行距离度量时，易知<strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，因为它们都可以反映一种程度，我们称其为“<strong>有序属性</strong>”；而对于不存在序关系的离散属性，我们称其为：“<strong>无序属性</strong>”，显然无序属性再使用闵可夫斯基距离就行不通了。</p><p><strong>对于无序属性，我们一般采用VDM进行距离的计算</strong>，例如：对于离散属性的两个取值a和b，定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed4e9560.png" alt="5.png"></p><p>于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed507bc7.png" alt="6.png"></p><p>若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：<strong>非度量距离（non-metric distance）</strong>。</p><h2 id="10-2-性能度量"><a href="#10-2-性能度量" class="headerlink" title="10.2 性能度量"></a><strong>10.2 性能度量</strong></h2><p>由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：<strong>外部指标</strong>和<strong>内部指标</strong>。</p><h3 id="10-2-1-外部指标"><a href="#10-2-1-外部指标" class="headerlink" title="10.2.1 外部指标"></a><strong>10.2.1 外部指标</strong></h3><p>即将聚类结果与某个参考模型的结果进行比较，<strong>以参考模型的输出作为标准，来评价聚类好坏</strong>。假设聚类给出的结果为λ，参考模型给出的结果是λ*，则我们将样本进行两两配对，定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed59160e.png" alt="7.png"></p><p>显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed587438.png" alt="8.png"></p><h3 id="10-2-2-内部指标"><a href="#10-2-2-内部指标" class="headerlink" title="10.2.2 内部指标"></a><strong>10.2.2 内部指标</strong></h3><p>内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：<strong>簇内高内聚紧紧抱团，簇间低耦合老死不相往来</strong>。定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed581852.png" alt="9.png"></p><p>基于上面的四个距离，可以导出下面这些常用的内部评价指标：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed582854.png" alt="10.png"></p><h2 id="10-3-原型聚类"><a href="#10-3-原型聚类" class="headerlink" title="10.3 原型聚类"></a><strong>10.3 原型聚类</strong></h2><p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</p><h3 id="10-3-1-K-Means"><a href="#10-3-1-K-Means" class="headerlink" title="10.3.1 K-Means"></a><strong>10.3.1 K-Means</strong></h3><p>K-Means的思想十分简单，<strong>首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛</strong>。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过<strong>EM算法</strong>的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb82b5d3.png" alt="11.png"></p><p>K-Means的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9c0817.png" alt="12.png"></p><h3 id="10-3-2-学习向量量化（LVQ）"><a href="#10-3-2-学习向量量化（LVQ）" class="headerlink" title="10.3.2 学习向量量化（LVQ）"></a><strong>10.3.2 学习向量量化（LVQ）</strong></h3><p>LVQ也是基于原型的聚类算法，与K-Means不同的是，<strong>LVQ使用样本真实类标记辅助聚类</strong>，首先LVQ根据样本的类标记，从各类中分别随机选出一个样本作为该类簇的原型，从而组成了一个<strong>原型特征向量组</strong>，接着从样本集中随机挑选一个样本，计算其与原型向量组中每个向量的距离，并选取距离最小的原型向量所在的类簇作为它的划分结果，再与真实类标比较。</p><blockquote><p><strong>若划分结果正确，则对应原型向量向这个样本靠近一些</strong><br><strong>若划分结果不正确，则对应原型向量向这个样本远离一些</strong></p></blockquote><p>LVQ算法的流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9d59f2.png" alt="13.png"></p><h3 id="10-3-3-高斯混合聚类"><a href="#10-3-3-高斯混合聚类" class="headerlink" title="10.3.3 高斯混合聚类"></a><strong>10.3.3 高斯混合聚类</strong></h3><p>现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设<strong>每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成</strong>。</p><p>对于多维高斯分布，其概率密度函数如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb870d98.png" alt="14.png"></p><p>其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb876794.png" alt="15.png"></p><p>α称为混合系数，这样空间中样本的采集过程则可以抽象为：<strong>（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样</strong>，这时候贝叶斯公式又能大展身手了：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9191d9.png" alt="16.png"></p><p>此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：<strong>这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法美好地计算出来</strong>，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb871d4a.png" alt="17.png"></p><p>可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。<strong>这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新</strong>。</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb8a6f32.png" alt="18.png"></p><p>高斯混合聚类的算法流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9c4fa4.png" alt="19.png"></p><h2 id="10-4-密度聚类"><a href="#10-4-密度聚类" class="headerlink" title="10.4 密度聚类"></a><strong>10.4 密度聚类</strong></h2><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是<strong>DBSCAN</strong>算法，首先定义以下概念：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9bd69c.png" alt="20.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc8509f8d619.png" alt="21.png"></p><p>简单来理解DBSCAN便是：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc8509feb587.png" alt="22.png"></p><h2 id="10-5-层次聚类"><a href="#10-5-层次聚类" class="headerlink" title="10.5 层次聚类"></a><strong>10.5 层次聚类</strong></h2><p>层次聚类是一种基于树形结构的聚类方法，常用的是<strong>自底向上</strong>的结合策略（<strong>AGNES算法</strong>）。假设有N个待聚类的样本，其基本步骤是：</p><blockquote><p>1.初始化—&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；<br>2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；<br>3.重新计算新生成的这个<strong>类与各个旧类之间的相似度</strong>；<br>4.重复2和3直到所有样本点都归为一类，结束。</p></blockquote><p>可以看出其中最关键的一步就是<strong>计算两个类簇的相似度</strong>，这里有多种度量方法：</p><pre><code>* 单链接（single-linkage）:取类间最小距离。</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509ebb022.png" alt="23.png"></p><pre><code>* 全链接（complete-linkage）:取类间最大距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509eb2b30.png" alt="24.png"></p><pre><code>* 均链接（average-linkage）:取类间两两的平均距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509f089a7.png" alt="25.png"></p><p>很容易看出：<strong>单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局</strong>。层次聚类法的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc8509f9d4a0.png" alt="26.png"></p><blockquote><p>在此聚类算法就介绍完毕，分类/聚类都是机器学习中最常见的任务，我实验室的大Boss也是靠着聚类起家，从此走上人生事业钱途…之巅峰，在书最后的阅读材料还看见Boss的名字，所以这章也是必读不可了…</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了一种机器学习的通用框架—集成学习方法，首先从准确性和差异性两个重要概念引出集成学习“&lt;strong&gt;好而不同&lt;/strong&gt;”的四字真言，接着介绍了现阶段主流的三种集成学习方法：AdaBoost、Bagging及Random Forest，AdaBoost采用最小化指数损失函数迭代式更新样本分布权重和计算基学习器权重，Bagging通过自助采样引入样本扰动增加了基学习器之间的差异性，随机森林则进一步引入了属性扰动，最后简单概述了集成模型中的三类结合策略：平均法、投票法及学习法，其中Stacking是学习法的典型代表。本篇将讨论无监督学习中应用最为广泛的学习算法—聚类。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(10)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-07-27T02:20:38.000Z</published>
    <updated>2020-07-27T11:02:50.148Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了鼎鼎大名的EM算法，从算法思想到数学公式推导（边际似然引入隐变量，Jensen不等式简化求导），EM算法实际上可以理解为一种坐标下降法，首先固定一个变量，接着求另外变量的最优解，通过其优美的“两步走”策略能较好地估计隐变量的值。本篇将继续讨论下一类经典算法—集成学习。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="9、集成学习"><a href="#9、集成学习" class="headerlink" title="9、集成学习"></a><strong>9、集成学习</strong></h1><p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生~…~，即其泛化性能要能优于其中任何一个学习器。</p><h2 id="9-1-个体与集成"><a href="#9-1-个体与集成" class="headerlink" title="9.1 个体与集成"></a><strong>9.1 个体与集成</strong></h2><p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0c15683.png" alt="1.png"></p><p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p><blockquote><p><strong>同质集成</strong>：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。<br><strong>异质集成</strong>：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p></blockquote><p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d23e13.png" alt="2.png"></p><p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0cce0bb.png" alt="3.png"></p><p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A=1 | B=1）&gt; P（A=1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，<strong>个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量</strong>，准确性高意味着牺牲多样性，所以产生“<strong>好而不同</strong>”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p><h2 id="9-2-Boosting"><a href="#9-2-Boosting" class="headerlink" title="9.2 Boosting"></a><strong>9.2 Boosting</strong></h2><p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p><p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p><p>定义基学习器的集成为加权结合，则有：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0ca2ca5.png" alt="4.png"></p><p>AdaBoost算法的指数损失函数定义为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d10461.png" alt="5.png"></p><p>具体说来，整个Adaboost 迭代算法分为3步：</p><ul><li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li><li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li><li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li></ul><p>整个AdaBoost的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d7c057.png" alt="6.png"></p><p>可以看出：<strong>AdaBoost的核心步骤就是计算基学习器权重和样本权重分布</strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为<strong>大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到</strong>，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开。</p><p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p><blockquote><p><strong>重赋权法</strong> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><strong>重采样法</strong> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p></blockquote><p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。在此，当选为数据挖掘十大算法之一的AdaBoost介绍到这里，能够当选正是说明这个算法十分婀娜多姿，背后的数学证明和推导充分证明了这一点，限于篇幅不再继续展开。</p><h2 id="9-3-Bagging与Random-Forest"><a href="#9-3-Bagging与Random-Forest" class="headerlink" title="9.3 Bagging与Random Forest"></a><strong>9.3 Bagging与Random Forest</strong></h2><p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p><h3 id="9-3-1-Bagging"><a href="#9-3-1-Bagging" class="headerlink" title="9.3.1 Bagging"></a><strong>9.3.1 Bagging</strong></h3><p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0ce62fc.png" alt="7.png"></p><p>Bagging算法的流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d0e761.png" alt="8.png"></p><p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p><h3 id="9-3-2-随机森林"><a href="#9-3-2-随机森林" class="headerlink" title="9.3.2 随机森林"></a><strong>9.3.2 随机森林</strong></h3><p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K=log2（d）。</p><p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png" alt="9.png"></p><h2 id="9-4-结合策略"><a href="#9-4-结合策略" class="headerlink" title="9.4 结合策略"></a><strong>9.4 结合策略</strong></h2><p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p><h3 id="9-4-1-平均法（回归问题）"><a href="#9-4-1-平均法（回归问题）" class="headerlink" title="9.4.1 平均法（回归问题）"></a><strong>9.4.1 平均法（回归问题）</strong></h3><p><img src="https://i.loli.net/2018/10/18/5bc84d0d07983.png" alt="10.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de1b74ff.png" alt="11.png"></p><p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，<strong>一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法</strong>。</p><h3 id="9-4-2-投票法（分类问题）"><a href="#9-4-2-投票法（分类问题）" class="headerlink" title="9.4.2 投票法（分类问题）"></a><strong>9.4.2 投票法（分类问题）</strong></h3><p><img src="https://i.loli.net/2018/10/18/5bc84de2629c4.png" alt="12.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de25a74b.png" alt="13.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de1bacc4.png" alt="14.png"></p><p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p><p><img src="https://i.loli.net/2018/10/18/5bc84de2768c1.png" alt="15.png"></p><p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p><h3 id="9-4-3-学习法"><a href="#9-4-3-学习法" class="headerlink" title="9.4.3 学习法"></a><strong>9.4.3 学习法</strong></h3><p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m<em>T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：<em>*投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果</em></em>。</p><p><img src="https://i.loli.net/2018/10/18/5bc84de25cbaf.png" alt="16.png"></p><h2 id="9-5-多样性（diversity）"><a href="#9-5-多样性（diversity）" class="headerlink" title="9.5 多样性（diversity）"></a><strong>9.5 多样性（diversity）</strong></h2><p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p><blockquote><p><strong>数据样本扰动</strong>，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br><strong>输入属性扰动</strong>，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br><strong>输出表示扰动</strong>，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br><strong>算法参数扰动</strong>，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p></blockquote><p>在此，集成学习就介绍完毕，看到这里，大家也会发现集成学习实质上是一种通用框架，可以使用任何一种基学习器，从而改进单个学习器的泛化性能。据说数据挖掘竞赛KDDCup历年的冠军几乎都使用了集成学习，看来的确是个好东西~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了鼎鼎大名的EM算法，从算法思想到数学公式推导（边际似然引入隐变量，Jensen不等式简化求导），EM算法实际上可以理解为一种坐标下降法，首先固定一个变量，接着求另外变量的最优解，通过其优美的“两步走”策略能较好地估计隐变量的值。本篇将继续讨论下一类经典算法—集成学习。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(9)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/</id>
    <published>2020-07-27T01:20:38.000Z</published>
    <updated>2020-07-27T11:02:42.228Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了贝叶斯分类器，从贝叶斯公式到贝叶斯决策论，再到通过极大似然法估计类条件概率，贝叶斯分类器的训练就是参数估计的过程。朴素贝叶斯则是“属性条件独立性假设”下的特例，它避免了假设属性联合分布过于经验性和训练集不足引起参数估计较大偏差两个大问题，最后介绍的拉普拉斯修正将概率值进行平滑处理。本篇将介绍另一个当选为数据挖掘十大算法之一的<strong>EM算法</strong>。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="8、EM算法"><a href="#8、EM算法" class="headerlink" title="8、EM算法"></a><strong>8、EM算法</strong></h1><p>EM（Expectation-Maximization）算法是一种常用的估计参数隐变量的利器，也称为“期望最大算法”，是数据挖掘的十大经典算法之一。EM算法主要应用于训练集样本不完整即存在隐变量时的情形（例如某个属性值未知），通过其独特的“两步走”策略能较好地估计出隐变量的值。</p><h2 id="8-1-EM算法思想"><a href="#8-1-EM算法思想" class="headerlink" title="8.1 EM算法思想"></a><strong>8.1 EM算法思想</strong></h2><p>EM是一种迭代式的方法，它的基本思想就是：若样本服从的分布参数θ已知，则可以根据已观测到的训练样本推断出隐变量Z的期望值（E步），若Z的值已知则运用最大似然法估计出新的θ值（M步）。重复这个过程直到Z和θ值不再发生变化。</p><p>简单来讲：假设我们想估计A和B这两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。</p><p><img src="https://i.loli.net/2018/10/18/5bc843bf53eb2.png" alt="1.png"></p><p>现在再来回想聚类的代表算法K-Means：【首先随机选择类中心=&gt;将样本点划分到类簇中=&gt;重新计算类中心=&gt;不断迭代直至收敛】，不难发现这个过程和EM迭代的方法极其相似，事实上，若将样本的类别看做为“隐变量”（latent variable）Z，类中心看作样本的分布参数θ，K-Means就是通过EM算法来进行迭代的，与我们这里不同的是，K-Means的目标是最小化样本点到其对应类中心的距离和，上述为极大化似然函数。</p><h2 id="8-2-EM算法数学推导"><a href="#8-2-EM算法数学推导" class="headerlink" title="8.2 EM算法数学推导"></a><strong>8.2 EM算法数学推导</strong></h2><p>在上篇极大似然法中，当样本属性值都已知时，我们很容易通过极大化对数似然，接着对每个参数求偏导计算出参数的值。但当存在隐变量时，就无法直接求解，此时我们通常最大化已观察数据的对数“边际似然”（marginal likelihood）。</p><p><img src="https://i.loli.net/2018/10/18/5bc843bfd84d2.png" alt="2.png"></p><p>这时候，通过边缘似然将隐变量Z引入进来，对于参数估计，现在与最大似然不同的只是似然函数式中多了一个未知的变量Z，也就是说我们的目标是找到适合的θ和Z让L(θ)最大，这样我们也可以分别对未知的θ和Z求偏导，再令其等于0。</p><p>然而观察上式可以发现，和的对数（ln(x1+x2+x3)）求导十分复杂，那能否通过变换上式得到一种求导简单的新表达式呢？这时候 Jensen不等式就派上用场了，先回顾一下高等数学凸函数的内容：</p><p><strong>Jensen’s inequality</strong>：过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方。理解起来也十分简单，对于凸函数f(x)’’&gt;0，即曲线的变化率是越来越大单调递增的，所以函数越到后面增长越厉害，这样在一个区间下，函数的均值就会大一些了。</p><p><img src="https://i.loli.net/2018/10/18/5bc843c064c72.png" alt="3.png"></p><p>因为ln(*)函数为凹函数，故可以将上式“和的对数”变为“对数的和”，这样就很容易求导了。</p><p><img src="https://i.loli.net/2018/10/18/5bc843c3490ad.png" alt="4.png"></p><p>接着求解Qi和θ：首先固定θ（初始值），通过求解Qi使得J（θ，Q）在θ处与L（θ）相等，即求出L（θ）的下界；然后再固定Qi，调整θ，最大化下界J（θ，Q）。不断重复两个步骤直到稳定。通过jensen不等式的性质，Qi的计算公式实际上就是后验概率：</p><p><img src="https://i.loli.net/2018/10/18/5bc843c21276c.png" alt="5.png"></p><p>通过数学公式的推导，简单来理解这一过程：固定θ计算Q的过程就是在建立L（θ）的下界，即通过jenson不等式得到的下界（E步）；固定Q计算θ则是使得下界极大化（M步），从而不断推高边缘似然L（θ）。从而循序渐进地计算出L（θ）取得极大值时隐变量Z的估计值。</p><p>EM算法也可以看作一种“坐标下降法”，首先固定一个值，对另外一个值求极值，不断重复直到收敛。这时候也许大家就有疑问，问什么不直接这两个家伙求偏导用梯度下降呢？这时候就是坐标下降的优势，有些特殊的函数，例如曲线函数z=y^2+x^2+x^2y+xy+…，无法直接求导，这时如果先固定其中的一个变量，再对另一个变量求极值，则变得可行。</p><p><img src="https://i.loli.net/2018/10/18/5bc843c34e7ff.png" alt="6.png"></p><h2 id="8-3-EM算法流程"><a href="#8-3-EM算法流程" class="headerlink" title="8.3 EM算法流程"></a><strong>8.3 EM算法流程</strong></h2><p>看完数学推导，算法的流程也就十分简单了，这里有两个版本，版本一来自西瓜书，周天使的介绍十分简洁；版本二来自于大牛的博客。结合着数学推导，自认为版本二更具有逻辑性，两者唯一的区别就在于版本二多出了红框的部分，这里我也没得到答案，欢迎骚扰讨论~</p><p><strong>版本一：</strong></p><p><img src="https://i.loli.net/2018/10/18/5bc843c0e19db.png" alt="7.png"></p><p><strong>版本二：</strong></p><p><img src="https://i.loli.net/2018/10/18/5bc843c34775b.png" alt="8.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了贝叶斯分类器，从贝叶斯公式到贝叶斯决策论，再到通过极大似然法估计类条件概率，贝叶斯分类器的训练就是参数估计的过程。朴素贝叶斯则是“属性条件独立性假设”下的特例，它避免了假设属性联合分布过于经验性和训练集不足引起参数估计较大偏差两个大问题，最后介绍的拉普拉斯修正将概率值进行平滑处理。本篇将介绍另一个当选为数据挖掘十大算法之一的&lt;strong&gt;EM算法&lt;/strong&gt;。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(8)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</id>
    <published>2020-07-27T00:20:38.000Z</published>
    <updated>2020-07-27T11:05:50.627Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍和讨论了支持向量机。从最初的分类函数，通过最大化分类间隔，max(1/||w||)，min(1/2||w||^2)，凸二次规划，朗格朗日函数，对偶问题，一直到最后的SMO算法求解，都为寻找一个最优解。接着引入核函数将低维空间映射到高维特征空间，解决了非线性可分的情形。最后介绍了软间隔支持向量机，解决了outlier挤歪超平面的问题。本篇将讨论一个经典的统计学习算法—<strong>贝叶斯分类器</strong>。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="7、贝叶斯分类器"><a href="#7、贝叶斯分类器" class="headerlink" title="7、贝叶斯分类器"></a><strong>7、贝叶斯分类器</strong></h1><p>贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委—贝叶斯公式。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd7a2575.png" alt="1.png"></p><h2 id="7-1-贝叶斯决策论"><a href="#7-1-贝叶斯决策论" class="headerlink" title="7.1 贝叶斯决策论"></a><strong>7.1 贝叶斯决策论</strong></h2><p>若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“<strong>条件风险</strong>”（conditional risk）。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd15db94.png" alt="2.png"></p><p>我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了<strong>贝叶斯判定准则</strong>（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd308600.png" alt="3.png"></p><p>若损失函数λ取0-1损失，则有：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd37c502.png" alt="4.png"></p><p>即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：</p><pre><code>* 判别式模型：直接对 P（c | x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。</code></pre><p>贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换，巴拉拉能量…. P（c | x）变身：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd501ad3.png" alt="5.png"></p><p>对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，在看这章之前也是模糊了我好久，这里普及一下它们的基本概念。</p><pre><code>* 先验概率： 根据以往经验和分析得到的概率。* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</code></pre><p>实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。下面直接上Wiki上的一个例子，简单粗暴快速完事…</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd799610.png" alt="6.png"></p><p>回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。</p><h2 id="7-2-极大似然法"><a href="#7-2-极大似然法" class="headerlink" title="7.2 极大似然法"></a><strong>7.2 极大似然法</strong></h2><p>极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd70fb73.png" alt="7.png"></p><p>所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p><pre><code>* 1.写出似然函数；* 2.对似然函数取对数，并整理；* 3.求导数，令偏导数为0，得到似然方程组；* 4.解似然方程组，得到所有参数即为所求。</code></pre><p>例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd705729.png" alt="8.png"></p><p>上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识，搞统计越来越值钱也是这个道理，大牛们掐指一算比我们搬砖几天更有效果。</p><h2 id="7-3-朴素贝叶斯分类器"><a href="#7-3-朴素贝叶斯分类器" class="headerlink" title="7.3 朴素贝叶斯分类器"></a><strong>7.3 朴素贝叶斯分类器</strong></h2><p>不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd55e102.png" alt="9.png"></p><p>这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd6678cd.png" alt="10.png"></p><p>相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fe54aaed.png" alt="11.png"></p><p>当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。</p><p>针对朴素贝叶斯，人们觉得它too sample，sometimes too naive！因此又提出了半朴素的贝叶斯分类器，具体有SPODE、TAN、贝叶斯网络等来刻画属性之间的依赖关系，此处不进行深入，等哪天和贝叶斯邂逅了再回来讨论。在此鼎鼎大名的贝叶斯介绍完毕，下一篇将介绍这一章剩下的内容—EM算法，朴素贝叶斯和EM算法同为数据挖掘的十大经典算法，想着还是单独介绍吧~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍和讨论了支持向量机。从最初的分类函数，通过最大化分类间隔，max(1/||w||)，min(1/2||w||^2)，凸二次规划，朗格朗日函数，对偶问题，一直到最后的SMO算法求解，都为寻找一个最优解。接着引入核函数将低维空间映射到高维特征空间，解决了非线性可分的情形。最后介绍了软间隔支持向量机，解决了outlier挤歪超平面的问题。本篇将讨论一个经典的统计学习算法—&lt;strong&gt;贝叶斯分类器&lt;/strong&gt;。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(7)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</id>
    <published>2020-07-26T23:20:38.000Z</published>
    <updated>2020-07-27T11:05:45.960Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍了神经网络。首先从生物学神经元出发，引出了它的数学抽象模型—MP神经元以及由两层神经元组成的感知机模型，并基于梯度下降的方法描述了感知机模型的权值调整规则。由于简单的感知机不能处理线性不可分的情形，因此接着引入了含隐层的前馈型神经网络，BP神经网络则是其中最为成功的一种学习方法，它使用误差逆传播的方法来逐层调节连接权。最后简单介绍了局部/全局最小以及目前十分火热的深度学习的概念。本篇围绕的核心则是曾经一度取代过神经网络的另一种监督学习算法—<strong>支持向量机</strong>（Support Vector Machine），简称<strong>SVM</strong>。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="6、支持向量机"><a href="#6、支持向量机" class="headerlink" title="6、支持向量机"></a><strong>6、支持向量机</strong></h1><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p><h2 id="6-1-函数间隔与几何间隔"><a href="#6-1-函数间隔与几何间隔" class="headerlink" title="6.1 函数间隔与几何间隔"></a><strong>6.1 函数间隔与几何间隔</strong></h2><p>对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png" alt="1.png"></p><p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a06d5a.png" alt="2.png"></p><h3 id="6-1-1-函数间隔"><a href="#6-1-1-函数间隔" class="headerlink" title="6.1.1 函数间隔"></a><strong>6.1.1 函数间隔</strong></h3><p>在超平面w’x+b=0确定的情况下，|w’x<em>+b|能够代表点x</em>距离超平面的远近，易知：当w’x<em>+b&gt;0时，表示x</em>在超平面的一侧（正类，类标为1），而当w’x<em>+b&lt;0时，则表示x</em>在超平面的另外一侧（负类，类别为-1），因此（w’x<em>+b）y</em> 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了<em>*函数间隔</em></em>的定义（functional margin）:</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690a14b.png" alt="3.png"></p><p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690ac26.png" alt="4.png"></p><p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念—几何间隔（geometrical margin）。</p><h3 id="6-1-2-几何间隔"><a href="#6-1-2-几何间隔" class="headerlink" title="6.1.2 几何间隔"></a><strong>6.1.2 几何间隔</strong></h3><p><strong>几何间隔</strong>代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x<em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x</em>=x-r(w/||w||)，又x<em>在超平面上，即w’x</em>+b=0，代入即可得：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697d499.png" alt="5.png"></p><p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f696fd10.png" alt="6.png"></p><p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p><h2 id="6-2-最大间隔与支持向量"><a href="#6-2-最大间隔与支持向量" class="headerlink" title="6.2 最大间隔与支持向量"></a><strong>6.2 最大间隔与支持向量</strong></h2><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f69af163.png" alt="7.png"></p><p>一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697bb1d.png" alt="8.png"></p><p>对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为<strong>支持向量</strong>（support vector），易知：对于所有的支持向量，它们恰好满足y<em>(w’x</em>+b)=1，而所有不是支持向量的点，有y<em>(w’x</em>+b)&gt;1。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a838c4.png" alt="9.png"></p><h2 id="6-3-从原始优化问题到对偶问题"><a href="#6-3-从原始优化问题到对偶问题" class="headerlink" title="6.3 从原始优化问题到对偶问题"></a><strong>6.3 从原始优化问题到对偶问题</strong></h2><p>对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6978cbb.png" alt="10.png"></p><p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的<strong>对偶问题</strong>，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p><pre><code>* 一是因为使用对偶问题更容易求解；* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。</code></pre><p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9332be7.png" alt="11.png"></p><p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93321c5.png" alt="12.png"></p><p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9330967.png" alt="13.png"></p><p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p><p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9333e66.png" alt="14.png"></p><p>将上述结果代入L得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f935ae21.png" alt="15.png"></p><p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9338a9d.png" alt="16.png"></p><p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93419ca.png" alt="17.png"></p><p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9353166.png" alt="18.png"></p><p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f933c947.png" alt="19.png">        </p><h2 id="6-4-核函数"><a href="#6-4-核函数" class="headerlink" title="6.4 核函数"></a><strong>6.4 核函数</strong></h2><p>由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用<strong>映射</strong>的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f934303e.png" alt="20.png"></p><p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p><p>（1）原对偶问题变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc68b3b.png" alt="21.png"></p><p>（2）原分类函数变为：<br>​    <img src="https://i.loli.net/2018/10/17/5bc730cc1b673.png" alt="22.png"></p><p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了<strong>核函数</strong>（Kernel）的概念。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc49adc.png" alt="23.png"></p><p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效<strong>（低维计算，高维表现）</strong>，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p><p>（1）对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc173b2.png" alt="24.png"></p><p>（2）分类函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc05959.png" alt="25.png"></p><p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc468c.png" alt="26.png"></p><p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc541a.png" alt="27.png"></p><h2 id="6-5-软间隔支持向量机"><a href="#6-5-软间隔支持向量机" class="headerlink" title="6.5 软间隔支持向量机"></a><strong>6.5 软间隔支持向量机</strong></h2><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有<strong>噪声</strong>的情形，噪声数据（<strong>outlier</strong>）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccce68e.png" alt="28.png"></p><p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了<strong>“软间隔”支持向量机</strong>的概念</p><pre><code>* 允许某些数据点不满足约束y(w&#39;x+b)≥1；* 同时又使得不满足约束的样本尽可能少。</code></pre><p>这样优化目标变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc6c9fe.png" alt="29.png"></p><p>如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc5e5a9.png" alt="30.png"></p><p>支持向量机中的损失函数为<strong>hinge损失</strong>，引入<strong>“松弛变量”</strong>，目标函数与约束条件可以写为：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317aa3411.png" alt="31.png"></p><p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a4c96e.png" alt="32.png"></p><p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a6dff2.png" alt="33.png"></p><p>将w代入L化简，便得到其对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317ab6646.png" alt="34.png"></p><p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p><p>——在此SVM就介绍完毕。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍了神经网络。首先从生物学神经元出发，引出了它的数学抽象模型—MP神经元以及由两层神经元组成的感知机模型，并基于梯度下降的方法描述了感知机模型的权值调整规则。由于简单的感知机不能处理线性不可分的情形，因此接着引入了含隐层的前馈型神经网络，BP神经网络则是其中最为成功的一种学习方法，它使用误差逆传播的方法来逐层调节连接权。最后简单介绍了局部/全局最小以及目前十分火热的深度学习的概念。本篇围绕的核心则是曾经一度取代过神经网络的另一种监督学习算法—&lt;strong&gt;支持向量机&lt;/strong&gt;（Support Vector Machine），简称&lt;strong&gt;SVM&lt;/strong&gt;。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(6)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2020-07-26T22:20:38.000Z</published>
    <updated>2020-07-27T11:02:23.402Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要讨论了决策树算法。首先从决策树的基本概念出发，引出决策树基于树形结构进行决策，进一步介绍了构造决策树的递归流程以及其递归终止条件，在递归的过程中，划分属性的选择起到了关键作用，因此紧接着讨论了三种评估属性划分效果的经典算法，介绍了剪枝策略来解决原生决策树容易产生的过拟合问题，最后简述了属性连续值/缺失值的处理方法。本篇将讨论现阶段十分热门的另一个经典监督学习算法—神经网络（neural network）。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="5、神经网络"><a href="#5、神经网络" class="headerlink" title="5、神经网络"></a><strong>5、神经网络</strong></h1><p>在机器学习中，神经网络一般指的是“神经网络学习”，是机器学习与神经网络两个学科的交叉部分。所谓神经网络，目前用得最广泛的一个定义是“神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应”。</p><h2 id="5-1-神经元模型"><a href="#5-1-神经元模型" class="headerlink" title="5.1 神经元模型"></a><strong>5.1 神经元模型</strong></h2><p>神经网络中最基本的单元是神经元模型（neuron）。在生物神经网络的原始机制中，每个神经元通常都有多个树突（dendrite），一个轴突（axon）和一个细胞体（cell body），树突短而多分支，轴突长而只有一个；在功能上，树突用于传入其它神经元传递的神经冲动，而轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。神经元的生物学结构如下图所示，不得不说高中的生化知识大学忘得可是真干净…</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb6cc11.png" alt="1.png"></p><p>一直沿用至今的“M-P神经元模型”正是对这一结构进行了抽象，也称“阈值逻辑单元“，其中树突对应于输入部分，每个神经元收到n个其他神经元传递过来的输入信号，这些信号通过带权重的连接传递给细胞体，这些权重又称为连接权（connection weight）。细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb7be44.png" alt="2.png"></p><p>与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb40dc5.png" alt="3.png"></p><p>将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种包含多个参数的模型，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。</p><h2 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="5.2 感知机与多层网络"></a><strong>5.2 感知机与多层网络</strong></h2><p>感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。</p><p>给定训练集，则感知机的n+1个参数（n个权重+1个阈值）都可以通过学习得到。阈值Θ可以看作一个输入值固定为-1的哑结点的权重ωn+1，即假设有一个固定输入xn+1=-1的输入层神经元，其对应的权重为ωn+1，这样就把权重和阈值统一为权重的学习了。简单感知机的结构如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb3fdf0.png" alt="4.png"></p><p>感知机权重的学习规则如下：对于训练样本（x，y），当该样本进入感知机学习后，会产生一个输出值，若该输出值与样本的真实标记不一致，则感知机会对权重进行调整，若激活函数为阶跃函数，则调整的方法为（基于梯度下降法）：</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb3ba63.png" alt="5.png"></p><p>其中 η∈（0，1）称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。</p><p>由于感知机模型只有一层功能神经元，因此其功能十分有限，只能处理线性可分的问题，对于这类问题，感知机的学习过程一定会收敛（converge），因此总是可以求出适当的权值。但是对于像书上提到的异或问题，只通过一层功能神经元往往不能解决，因此要解决非线性可分问题，需要考虑使用多层功能神经元，即神经网络。多层神经网络的拓扑结构如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb58ec6.png" alt="6.png"></p><p>在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点：</p><pre><code>* 每层神经元与下一层神经元之间完全互连* 神经元之间不存在同层连接* 神经元之间不存在跨层连接</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72cbb47ff8.png" alt="7.png"></p><p>根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播（下节中的BP神经网络正是基于前馈神经网络而增加了反馈调节机制）。神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个神经元的阈值，换句话说：神经网络所学习到的东西都蕴含在网络的连接权与阈值中。</p><h2 id="5-3-BP神经网络算法"><a href="#5-3-BP神经网络算法" class="headerlink" title="5.3 BP神经网络算法"></a><strong>5.3 BP神经网络算法</strong></h2><p>由上面可以得知：神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP神经网络算法即误差逆传播算法（error BackPropagation）正是为学习多层前馈神经网络而设计，BP神经网络算法是迄今为止最成功的的神经网络学习算法。</p><p>一般而言，只需包含一个足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数[Hornik et al.,1989]，故下面以训练单隐层的前馈神经网络为例，介绍BP神经网络的算法思想。</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb92ff5.png" alt="8.png"></p><p>上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节。可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程：</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb86229.png" alt="9.png"></p><p>学习率η∈（0，1）控制着沿反梯度方向下降的步长，若步长太大则下降太快容易产生震荡，若步长太小则收敛速度太慢，一般地常把η设置为0.1，有时更新权重时会将输出层与隐含层设置为不同的学习率。BP算法的基本流程如下所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72cbb59e99.png" alt="10.png"></p><p>BP算法的更新规则是基于每个样本的预测值与真实类标的均方误差来进行权值调节，即BP算法每次更新只针对于单个样例。需要注意的是：BP算法的最终目标是要最小化整个训练集D上的累积误差，即：</p><p><img src="https://i.loli.net/2018/10/17/5bc72ce222a96.png" alt="11.png"></p><p>如果基于累积误差最小化的更新规则，则得到了累积误差逆传播算法（accumulated error backpropagation），即每次读取全部的数据集一遍，进行一轮学习，从而基于当前的累积误差进行权值调整，因此参数更新的频率相比标准BP算法低了很多，但在很多任务中，尤其是在数据量很大的时候，往往标准BP算法会获得较好的结果。另外对于如何设置隐层神经元个数的问题，至今仍然没有好的解决方案，常使用“试错法”进行调整。</p><p>前面提到，BP神经网络强大的学习能力常常容易造成过拟合问题，有以下两种策略来缓解BP网络的过拟合问题：</p><ul><li>早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。</li><li>引入正则化（regularization）：基本思想是在累积误差函数中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc72ce227ff1.png" alt="12.png"></p><h2 id="5-4-全局最小与局部最小"><a href="#5-4-全局最小与局部最小" class="headerlink" title="5.4 全局最小与局部最小"></a><strong>5.4 全局最小与局部最小</strong></h2><p>模型学习的过程实质上就是一个寻找最优参数的过程，例如BP算法试图通过最速下降来寻找使得累积经验误差最小的权值与阈值，在谈到最优时，一般会提到局部极小（local minimum）和全局最小（global minimum）。</p><pre><code>* 局部极小解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的误差函数值。* 全局最小解：参数空间中的某个点，所有其他点的误差函数值均不小于该点的误差函数值。</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72ce2803dc.png" alt="13.png"></p><p>要成为局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。</p><pre><code>* 以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。* 使用“模拟退火”技术，这里不做具体介绍。* 使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。</code></pre><h2 id="5-5-深度学习"><a href="#5-5-深度学习" class="headerlink" title="5.5 深度学习"></a><strong>5.5 深度学习</strong></h2><p>理论上，参数越多，模型复杂度就越高，容量（capability）就越大，从而能完成更复杂的学习任务。深度学习（deep learning）正是一种极其复杂而强大的模型。</p><p>怎么增大模型复杂度呢？两个办法，一是增加隐层的数目，二是增加隐层神经元的数目。前者更有效一些，因为它不仅增加了功能神经元的数量，还增加了激活函数嵌套的层数。但是对于多隐层神经网络，经典算法如标准BP算法往往会在误差逆传播时发散（diverge），无法收敛达到稳定状态。</p><p>那要怎么有效地训练多隐层神经网络呢？一般来说有以下两种方法：</p><ul><li><p>无监督逐层训练（unsupervised layer-wise training）：每次训练一层隐节点，把上一层隐节点的输出当作输入来训练，本层隐结点训练好后，输出再作为下一层的输入来训练，这称为预训练（pre-training）。全部预训练完成后，再对整个网络进行微调（fine-tuning）训练。一个典型例子就是深度信念网络（deep belief network，简称DBN）。这种做法其实可以视为把大量的参数进行分组，先找出每组较好的设置，再基于这些局部最优的结果来训练全局最优。</p></li><li><p>权共享（weight sharing）：令同一层神经元使用完全相同的连接权，典型的例子是卷积神经网络（Convolutional Neural Network，简称CNN）。这样做可以大大减少需要训练的参数数目。</p></li></ul><p><img src="https://i.loli.net/2018/10/17/5bc72ce28d756.png" alt="14.png"></p><p>深度学习可以理解为一种特征学习（feature learning）或者表示学习（representation learning），无论是DBN还是CNN，都是通过多个隐层来把与输出目标联系不大的初始输入转化为与输出目标更加密切的表示，使原来只通过单层映射难以完成的任务变为可能。即通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示，从而使得最后可以用简单的模型来完成复杂的学习任务。</p><p>传统任务中，样本的特征需要人类专家来设计，这称为特征工程（feature engineering）。特征好坏对泛化性能有至关重要的影响。而深度学习为全自动数据分析带来了可能，可以自动产生更好的特征。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要讨论了决策树算法。首先从决策树的基本概念出发，引出决策树基于树形结构进行决策，进一步介绍了构造决策树的递归流程以及其递归终止条件，在递归的过程中，划分属性的选择起到了关键作用，因此紧接着讨论了三种评估属性划分效果的经典算法，介绍了剪枝策略来解决原生决策树容易产生的过拟合问题，最后简述了属性连续值/缺失值的处理方法。本篇将讨论现阶段十分热门的另一个经典监督学习算法—神经网络（neural network）。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(5)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2020-07-26T21:20:38.000Z</published>
    <updated>2020-07-27T11:02:16.974Z</updated>
    
    <content type="html"><![CDATA[<p>上篇主要介绍和讨论了线性模型。首先从最简单的最小二乘法开始，讨论输入属性有一个和多个的情形，接着通过广义线性模型延伸开来，将预测连续值的回归问题转化为分类问题，从而引入了对数几率回归，最后线性判别分析LDA将样本点进行投影，多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。本篇将讨论另一种被广泛使用的分类算法—决策树（Decision Tree）。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="4、决策树"><a href="#4、决策树" class="headerlink" title="4、决策树"></a><strong>4、决策树</strong></h1><h2 id="4-1-决策树基本概念"><a href="#4-1-决策树基本概念" class="headerlink" title="4.1 决策树基本概念"></a><strong>4.1 决策树基本概念</strong></h2><p>顾名思义，决策树是基于树结构来进行决策的，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p><hr><pre><code>  女儿：多大年纪了？  母亲：26。  女儿：长的帅不帅？  母亲：挺帅的。  女儿：收入高不？  母亲：不算很高，中等情况。  女儿：是公务员不？  母亲：是，在税务局上班呢。  女儿：那好，我去见见。</code></pre><hr><p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec84a77.png" alt="1.png"></p><p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p><pre><code>* 每个非叶节点表示一个特征属性测试。* 每个分支代表这个特征属性在某个值域上的输出。* 每个叶子节点存放一个类别。* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</code></pre><h2 id="4-2-决策树的构造"><a href="#4-2-决策树的构造" class="headerlink" title="4.2 决策树的构造"></a><strong>4.2 决策树的构造</strong></h2><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ecc27fe.png" alt="2.png"></p><p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。</p><h3 id="4-2-1-ID3算法"><a href="#4-2-1-ID3算法" class="headerlink" title="4.2.1 ID3算法"></a><strong>4.2.1 ID3算法</strong></h3><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec515a5.png" alt="3.png"></p><p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec3e067.png" alt="4.png"></p><p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p><h3 id="4-2-2-C4-5算法"><a href="#4-2-2-C4-5算法" class="headerlink" title="4.2.2 C4.5算法"></a><strong>4.2.2 C4.5算法</strong></h3><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec69647.png" alt="5.png"></p><h3 id="4-2-3-CART算法"><a href="#4-2-3-CART算法" class="headerlink" title="4.2.3 CART算法"></a><strong>4.2.3 CART算法</strong></h3><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，基尼指数定义如下：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec5a2ff.png" alt="6.png"></p><p>进而，使用属性α划分后的基尼指数为：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec62eaf.png" alt="7.png"></p><h2 id="4-3-剪枝处理"><a href="#4-3-剪枝处理" class="headerlink" title="4.3 剪枝处理"></a><strong>4.3 剪枝处理</strong></h2><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p><pre><code>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。</code></pre><p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec80d34.png" alt="8.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc728ec9e330.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc728ec9d497.png" alt="10.png"></p><p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p><h2 id="4-4-连续值与缺失值处理"><a href="#4-4-连续值与缺失值处理" class="headerlink" title="4.4 连续值与缺失值处理"></a><strong>4.4 连续值与缺失值处理</strong></h2><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p><pre><code>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。* 选择最大信息增益的划分点作为最优划分点。</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72a0968fad.png" alt="11.png"></p><p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a098f3be.png" alt="12.png"></p><p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a096ccc3.png" alt="13.png"></p><p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a093ed3c.png" alt="14.png"></p><p>​    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇主要介绍和讨论了线性模型。首先从最简单的最小二乘法开始，讨论输入属性有一个和多个的情形，接着通过广义线性模型延伸开来，将预测连续值的回归问题转化为分类问题，从而引入了对数几率回归，最后线性判别分析LDA将样本点进行投影，多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。本篇将讨论另一种被广泛使用的分类算法—决策树（Decision Tree）。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(4)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-07-26T20:20:38.000Z</published>
    <updated>2020-07-27T11:02:10.190Z</updated>
    
    <content type="html"><![CDATA[<p>笔记的前一部分主要是对机器学习预备知识的概括，包括机器学习的定义/术语、学习器性能的评估/度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容—线性模型。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h1 id="3、线性模型"><a href="#3、线性模型" class="headerlink" title="3、线性模型"></a><strong>3、线性模型</strong></h1><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p><h2 id="3-1-线性回归"><a href="#3-1-线性回归" class="headerlink" title="3.1 线性回归"></a><strong>3.1 线性回归</strong></h2><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000—&gt;13亿…2016—&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）—&gt;15k。</p><p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p><ul><li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p></li><li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p></li></ul><p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p><p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p><p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p><p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p><p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p><p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p><h2 id="3-2-线性几率回归"><a href="#3-2-线性几率回归" class="headerlink" title="3.2 线性几率回归"></a><strong>3.2 线性几率回归</strong></h2><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p><p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p><h2 id="3-3-线性判别分析"><a href="#3-3-线性判别分析" class="headerlink" title="3.3 线性判别分析"></a><strong>3.3 线性判别分析</strong></h2><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p><p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p><ul><li>类内散度矩阵（within-class scatter matrix）</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p><ul><li>类间散度矩阵(between-class scaltter matrix)</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p><p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p><p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p><p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。<br>​             </p><h2 id="3-4-多分类学习"><a href="#3-4-多分类学习" class="headerlink" title="3.4 多分类学习"></a><strong>3.4 多分类学习</strong></h2><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p><ul><li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p></li><li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p></li><li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p></li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p><h2 id="3-5-类别不平衡问题"><a href="#3-5-类别不平衡问题" class="headerlink" title="3.5 类别不平衡问题"></a><strong>3.5 类别不平衡问题</strong></h2><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p><ol><li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li><li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li><li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li></ol><p>​<br>​      </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;笔记的前一部分主要是对机器学习预备知识的概括，包括机器学习的定义/术语、学习器性能的评估/度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容—线性模型。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(3)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/</id>
    <published>2020-07-26T19:20:38.000Z</published>
    <updated>2020-07-27T11:02:02.555Z</updated>
    
    <content type="html"><![CDATA[<p>在上两篇中，我们介绍了多种常见的评估方法和性能度量标准，这样我们就可以根据数据集以及模型任务的特征，选择出最合适的评估和性能度量方法来计算出学习器的“测试误差“。但由于“测试误差”受到很多因素的影响，例如：算法随机性(例如常见的K-Means)或测试集本身的选择，使得同一模型每次得到的结果不尽相同，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能，那如何对单个或多个学习器在不同或相同测试集上的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。本篇延续上一篇的内容，主要讨论了比较检验、方差与偏差。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><h2 id="2-6-比较检验"><a href="#2-6-比较检验" class="headerlink" title="2.6 比较检验"></a><strong>2.6 比较检验</strong></h2><p>在比较学习器泛化性能的过程中，统计假设检验（hypothesis test）为学习器性能比较提供了重要依据，即若A在某测试集上的性能优于B，那A学习器比B好的把握有多大。 为方便论述，本篇中都是以“错误率”作为性能度量的标准。</p><h3 id="2-6-1-假设检验"><a href="#2-6-1-假设检验" class="headerlink" title="2.6.1 假设检验"></a><strong>2.6.1 假设检验</strong></h3><p>“假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想，例如：假设总体服从泊松分布，或假设正态总体的期望u=u0。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，这就是一种假设检验。</p><p><img src="https://i.loli.net/2018/10/17/5bc7211aed8e3.png" alt="1.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc7211a5817d.png" alt="2.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc7211a336b5.png" alt="3.png"></p><h3 id="2-6-2-交叉验证t检验"><a href="#2-6-2-交叉验证t检验" class="headerlink" title="2.6.2 交叉验证t检验"></a><strong>2.6.2 交叉验证t检验</strong></h3><p><img src="https://i.loli.net/2018/10/17/5bc7211a68ef9.png" alt="4.png"></p><h3 id="2-6-3-McNemar检验"><a href="#2-6-3-McNemar检验" class="headerlink" title="2.6.3 McNemar检验"></a><strong>2.6.3 McNemar检验</strong></h3><p>MaNemar主要用于二分类问题，与成对t检验一样也是用于比较两个学习器的性能大小。主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即e01=e10，且|e01-e10|服从N（1，e01+e10）分布。</p><p><img src="https://i.loli.net/2018/10/17/5bc7211a2c7f9.png" alt="5.png"></p><p>因此，如下所示的变量服从自由度为1的卡方分布，即服从标准正态分布N（0,1）的随机变量的平方和，下式只有一个变量，故自由度为1，检验的方法同上：做出假设—&gt;求出满足显著度的临界点—&gt;给出拒绝域—&gt;验证假设。</p><p><img src="https://i.loli.net/2018/10/17/5bc7211a34e96.png" alt="6.png"></p><h3 id="2-6-4-Friedman检验与Nemenyi后续检验"><a href="#2-6-4-Friedman检验与Nemenyi后续检验" class="headerlink" title="2.6.4 Friedman检验与Nemenyi后续检验"></a><strong>2.6.4 Friedman检验与Nemenyi后续检验</strong></h3><p>上述的三种检验都只能在一组数据集上，F检验则可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3…，相同则平分序值，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc7211a2db45.png" alt="7.png"></p><p>若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值ri服从正态分布N（（k+1）/2，（k+1）(k-1)/12），则有：</p><p><img src="https://i.loli.net/2018/10/17/5bc7211a45349.png" alt="8.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc7211a2684c.png" alt="9.png"></p><p>服从自由度为k-1和(k-1)(N-1)的F分布。下面是F检验常用的临界值：</p><p><img src="https://i.loli.net/2018/10/17/5bc7211a7e3f0.png" alt="10.png"></p><p>若“H0：所有算法的性能相同”这个假设被拒绝，则需要进行后续检验，来得到具体的算法之间的差异。常用的就是Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域，下表是常用的qa值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度1-α拒绝“两个算法性能相同”的假设。</p><p><img src="https://i.loli.net/2018/10/17/5bc722232932b.png" alt="11.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc7222348519.png" alt="12.png"></p><h2 id="2-7-偏差与方差"><a href="#2-7-偏差与方差" class="headerlink" title="2.7 偏差与方差"></a><strong>2.7 偏差与方差</strong></h2><p>偏差-方差分解是解释学习器泛化性能的重要工具。在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值得期望之间的差均方。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。通过对泛化误差的进行分解，可以得到：</p><ul><li><strong>期望泛化误差=方差+偏差</strong>    </li><li><strong>偏差刻画学习器的拟合能力</strong></li><li><strong>方差体现学习器的稳定性</strong></li></ul><p>易知：方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。</p><p><img src="https://i.loli.net/2018/10/17/5bc722234b09f.png" alt="13.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上两篇中，我们介绍了多种常见的评估方法和性能度量标准，这样我们就可以根据数据集以及模型任务的特征，选择出最合适的评估和性能度量方法来计算出学习器的“测试误差“。但由于“测试误差”受到很多因素的影响，例如：算法随机性(例如常见的K-Means)或测试集本身的选择，使得同一模型每次得到的结果不尽相同，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能，那如何对单个或多个学习器在不同或相同测试集上的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。本篇延续上一篇的内容，主要讨论了比较检验、方差与偏差。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(2)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/</id>
    <published>2020-07-26T18:20:38.000Z</published>
    <updated>2020-07-27T11:01:56.526Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要是对第二章剩余知识的理解，包括：性能度量、比较检验和偏差与方差。在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。写到后面发现冗长之后读起来十分没有快感，故本篇主要知识点为性能度量。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><p><strong>2.5 性能度量</strong></p><p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p><p><strong>2.5.1 最常见的性能度量</strong></p><p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf76276.png" alt="1.png"></p><p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4c704.png" alt="2.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf6fb84.png" alt="3.png"></p><p><strong>2.5.2 查准率/查全率/F1</strong></p><p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf885a4.png" alt="4.png"></p><p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf871a6.png" alt="5.png"></p><p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p><p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71dafc4411.png" alt="6.png"></p><p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。</p><p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf40ff6.png" alt="7.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf75407.png" alt="8.png"></p><p>特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf20885.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4b90a.png" alt="10.png"></p><p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70230e.png" alt="11.png"></p><p><strong>2.5.3 ROC与AUC</strong></p><p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6bee91.png" alt="12.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed75cefe.png" alt="13.png"></p><p>简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p><p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed740a24.png" alt="14.png"></p><p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p><p><strong>2.5.4 代价敏感错误率与代价曲线</strong></p><p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病—&gt;有疾病只是增多了检查，但有疾病—&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p><p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p><p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p><p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p><p>在此模型的性能度量方法就介绍完了，以前一直以为均方误差和精准度就可以了，现在才发现天空如此广阔~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇主要是对第二章剩余知识的理解，包括：性能度量、比较检验和偏差与方差。在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。写到后面发现冗长之后读起来十分没有快感，故本篇主要知识点为性能度量。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>周志华《Machine Learning》学习笔记(1)</title>
    <link href="http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/"/>
    <id>http://yoursite.com/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/</id>
    <published>2020-07-26T17:20:38.000Z</published>
    <updated>2020-07-27T11:01:48.029Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习是目前信息技术中最激动人心的方向之一，其应用已经深入到生活的各个层面且与普通人的日常生活密切相关。本文为清华大学最新出版的《机器学习》教材的Learning Notes，书作者是南京大学周志华教授，多个大陆首位彰显其学术奢华。本篇主要介绍了该教材前两个章节的知识点以及自己一点浅陋的理解。<br><a id="more"></a></p><p>参考<a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">此项目</a></p><div style="text-align:center;font-size:25px">目录</div><ul><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01--%E7%BB%AA%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(1)—绪论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(2)—性能度量</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;%E6%96%B9%E5%B7%AE&amp;%E5%81%8F%E5%B7%AE/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(3)—假设检验&amp;方差&amp;偏差</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(4)—线性模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05--%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(5)—决策树</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06--%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(6)—神经网络</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(7)—支持向量机</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(8)—贝叶斯分类器</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09--EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(9)—EM算法</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(10)—集成学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011--%E8%81%9A%E7%B1%BB/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(11)—聚类</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012--%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(12)—降维与度量学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(13)—特征选择与稀疏学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(14)—计算学习理论</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(15)—半监督学习</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016--%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(16)—概率图模型</a></li><li><a href="https://githubzhangshuai.github.io/2020/07/27/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017--%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">周志华《Machine Learning》学习笔记(17)—强化学习</a></li></ul><p><strong>1  绪论</strong></p><p>傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！哈哈，也希望自己这学期的machine learning课程取得一个好成绩！</p><p><strong>1.1 机器学习的定义</strong></p><p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p><p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p><ul><li>P：计算机程序在某任务类T上的性能。</li><li>T：计算机程序希望实现的任务类。</li><li>E：表示经验，即历史的数据集。</li></ul><p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p><p><strong>1.2 机器学习的一些基本术语</strong></p><p>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：     </p><ul><li>所有记录的集合为：数据集。</li><li>每一条记录为：一个实例（instance）或样本（sample）。</li><li>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</li><li>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</li><li><p>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p><p> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：    </p></li><li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p></li><li>所有测试样本的集合为：测试集（test set），[一般]。  </li><li><p>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p><p> 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：    </p></li><li><p>预测值为离散值的问题为：分类（classification）。</p></li><li><p>预测值为连续值的问题为：回归（regression）。</p><p> 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：    </p></li><li><p>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p></li><li>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</li></ul><p><strong>2  模型的评估与选择</strong></p><p><strong>2.1 误差与过拟合</strong></p><p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：    </p><ul><li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li><li>在测试集上的误差称为测试误差（test error）。</li><li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li></ul><p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p><ul><li>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</li><li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li></ul><p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p><p><img src="https://i.loli.net/2018/10/17/5bc7181172996.png" alt></p><p><strong>2.2 评估方法</strong></p><p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p><p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p><p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p><p><strong>2.3 训练集与测试集的划分方法</strong></p><p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p><p><strong>2.3.1 留出法</strong></p><p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p><p><strong>2.3.2 交叉验证法</strong></p><p>将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p><p><img src="https://i.loli.net/2018/10/17/5bc718115d224.png" alt></p><p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p><p><strong>2.3.3 自助法</strong></p><p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p><p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71811246dd.png" alt></p><p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p><p><strong>2.4 调参</strong></p><p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p><p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5<em>5</em>5= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p><p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。用上面考试的例子来比喻：就像高中时大家每次考试完，要将考卷的题目消化掉（大多数题目都还是之前没有见过的吧？），这样即使考差了也能开心的玩耍了~。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习是目前信息技术中最激动人心的方向之一，其应用已经深入到生活的各个层面且与普通人的日常生活密切相关。本文为清华大学最新出版的《机器学习》教材的Learning Notes，书作者是南京大学周志华教授，多个大陆首位彰显其学术奢华。本篇主要介绍了该教材前两个章节的知识点以及自己一点浅陋的理解。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/categories/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华《Machine Learning》学习笔记" scheme="http://yoursite.com/tags/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
